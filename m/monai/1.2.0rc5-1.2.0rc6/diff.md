# Comparing `tmp/monai-1.2.0rc5-202304241345-py3-none-any.whl.zip` & `tmp/monai-1.2.0rc6-202305081217-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,367 +1,367 @@
-Zip file size: 1253120 bytes, number of entries: 365
--rw-r--r--  2.0 unx     2276 b- defN 23-Apr-24 13:45 monai/__init__.py
--rw-r--r--  2.0 unx      500 b- defN 23-Apr-24 13:47 monai/_version.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-24 13:45 monai/py.typed
--rw-r--r--  2.0 unx      642 b- defN 23-Apr-24 13:45 monai/_extensions/__init__.py
--rw-r--r--  2.0 unx     3643 b- defN 23-Apr-24 13:45 monai/_extensions/loader.py
--rw-r--r--  2.0 unx     2931 b- defN 23-Apr-24 13:45 monai/_extensions/gmm/gmm.cpp
--rw-r--r--  2.0 unx     1760 b- defN 23-Apr-24 13:45 monai/_extensions/gmm/gmm.h
--rw-r--r--  2.0 unx     1118 b- defN 23-Apr-24 13:45 monai/_extensions/gmm/gmm_cpu.cpp
--rw-r--r--  2.0 unx    16213 b- defN 23-Apr-24 13:45 monai/_extensions/gmm/gmm_cuda.cu
--rw-r--r--  2.0 unx     3520 b- defN 23-Apr-24 13:45 monai/_extensions/gmm/gmm_cuda_linalg.cuh
--rw-r--r--  2.0 unx      908 b- defN 23-Apr-24 13:45 monai/apps/__init__.py
--rw-r--r--  2.0 unx    34568 b- defN 23-Apr-24 13:45 monai/apps/datasets.py
--rw-r--r--  2.0 unx    13505 b- defN 23-Apr-24 13:45 monai/apps/utils.py
--rw-r--r--  2.0 unx      993 b- defN 23-Apr-24 13:45 monai/apps/auto3dseg/__init__.py
--rw-r--r--  2.0 unx     1411 b- defN 23-Apr-24 13:45 monai/apps/auto3dseg/__main__.py
--rw-r--r--  2.0 unx    37585 b- defN 23-Apr-24 13:45 monai/apps/auto3dseg/auto_runner.py
--rw-r--r--  2.0 unx    26014 b- defN 23-Apr-24 13:45 monai/apps/auto3dseg/bundle_gen.py
--rw-r--r--  2.0 unx    17319 b- defN 23-Apr-24 13:45 monai/apps/auto3dseg/data_analyzer.py
--rw-r--r--  2.0 unx    24328 b- defN 23-Apr-24 13:45 monai/apps/auto3dseg/ensemble_builder.py
--rw-r--r--  2.0 unx    17523 b- defN 23-Apr-24 13:45 monai/apps/auto3dseg/hpo_gen.py
--rw-r--r--  2.0 unx     3733 b- defN 23-Apr-24 13:45 monai/apps/auto3dseg/transforms.py
--rw-r--r--  2.0 unx     2917 b- defN 23-Apr-24 13:45 monai/apps/auto3dseg/utils.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/deepedit/__init__.py
--rw-r--r--  2.0 unx     4501 b- defN 23-Apr-24 13:45 monai/apps/deepedit/interaction.py
--rw-r--r--  2.0 unx    37435 b- defN 23-Apr-24 13:45 monai/apps/deepedit/transforms.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/deepgrow/__init__.py
--rw-r--r--  2.0 unx    10054 b- defN 23-Apr-24 13:45 monai/apps/deepgrow/dataset.py
--rw-r--r--  2.0 unx     3739 b- defN 23-Apr-24 13:45 monai/apps/deepgrow/interaction.py
--rw-r--r--  2.0 unx    42011 b- defN 23-Apr-24 13:45 monai/apps/deepgrow/transforms.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/detection/__init__.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/detection/metrics/__init__.py
--rw-r--r--  2.0 unx    26617 b- defN 23-Apr-24 13:45 monai/apps/detection/metrics/coco.py
--rw-r--r--  2.0 unx    17161 b- defN 23-Apr-24 13:45 monai/apps/detection/metrics/matching.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/detection/networks/__init__.py
--rw-r--r--  2.0 unx    53221 b- defN 23-Apr-24 13:45 monai/apps/detection/networks/retinanet_detector.py
--rw-r--r--  2.0 unx    18850 b- defN 23-Apr-24 13:45 monai/apps/detection/networks/retinanet_network.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/detection/transforms/__init__.py
--rw-r--r--  2.0 unx    24519 b- defN 23-Apr-24 13:45 monai/apps/detection/transforms/array.py
--rw-r--r--  2.0 unx    17916 b- defN 23-Apr-24 13:45 monai/apps/detection/transforms/box_ops.py
--rw-r--r--  2.0 unx    68979 b- defN 23-Apr-24 13:45 monai/apps/detection/transforms/dictionary.py
--rw-r--r--  2.0 unx    13531 b- defN 23-Apr-24 13:45 monai/apps/detection/utils/ATSS_matcher.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/detection/utils/__init__.py
--rw-r--r--  2.0 unx    18681 b- defN 23-Apr-24 13:45 monai/apps/detection/utils/anchor_utils.py
--rw-r--r--  2.0 unx    11120 b- defN 23-Apr-24 13:45 monai/apps/detection/utils/box_coder.py
--rw-r--r--  2.0 unx     9031 b- defN 23-Apr-24 13:45 monai/apps/detection/utils/box_selector.py
--rw-r--r--  2.0 unx    10306 b- defN 23-Apr-24 13:45 monai/apps/detection/utils/detector_utils.py
--rw-r--r--  2.0 unx    13890 b- defN 23-Apr-24 13:45 monai/apps/detection/utils/hard_negative_sampler.py
--rw-r--r--  2.0 unx     5818 b- defN 23-Apr-24 13:45 monai/apps/detection/utils/predict_utils.py
--rw-r--r--  2.0 unx      726 b- defN 23-Apr-24 13:45 monai/apps/mmars/__init__.py
--rw-r--r--  2.0 unx    13115 b- defN 23-Apr-24 13:45 monai/apps/mmars/mmars.py
--rw-r--r--  2.0 unx     9996 b- defN 23-Apr-24 13:45 monai/apps/mmars/model_desc.py
--rw-r--r--  2.0 unx      745 b- defN 23-Apr-24 13:45 monai/apps/nnunet/__init__.py
--rw-r--r--  2.0 unx     2975 b- defN 23-Apr-24 13:45 monai/apps/nnunet/__main__.py
--rw-r--r--  2.0 unx    38587 b- defN 23-Apr-24 13:45 monai/apps/nnunet/nnunetv2_runner.py
--rw-r--r--  2.0 unx     6791 b- defN 23-Apr-24 13:45 monai/apps/nnunet/utils.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/nuclick/__init__.py
--rw-r--r--  2.0 unx    24948 b- defN 23-Apr-24 13:45 monai/apps/nuclick/transforms.py
--rw-r--r--  2.0 unx     1030 b- defN 23-Apr-24 13:45 monai/apps/pathology/__init__.py
--rw-r--r--  2.0 unx     2860 b- defN 23-Apr-24 13:45 monai/apps/pathology/utils.py
--rw-r--r--  2.0 unx      650 b- defN 23-Apr-24 13:45 monai/apps/pathology/engines/__init__.py
--rw-r--r--  2.0 unx     2397 b- defN 23-Apr-24 13:45 monai/apps/pathology/engines/utils.py
--rw-r--r--  2.0 unx      609 b- defN 23-Apr-24 13:45 monai/apps/pathology/handlers/__init__.py
--rw-r--r--  2.0 unx     2315 b- defN 23-Apr-24 13:45 monai/apps/pathology/handlers/utils.py
--rw-r--r--  2.0 unx      660 b- defN 23-Apr-24 13:45 monai/apps/pathology/inferers/__init__.py
--rw-r--r--  2.0 unx     9148 b- defN 23-Apr-24 13:45 monai/apps/pathology/inferers/inferer.py
--rw-r--r--  2.0 unx      650 b- defN 23-Apr-24 13:45 monai/apps/pathology/losses/__init__.py
--rw-r--r--  2.0 unx     7293 b- defN 23-Apr-24 13:45 monai/apps/pathology/losses/hovernet_loss.py
--rw-r--r--  2.0 unx      646 b- defN 23-Apr-24 13:45 monai/apps/pathology/metrics/__init__.py
--rw-r--r--  2.0 unx     7225 b- defN 23-Apr-24 13:45 monai/apps/pathology/metrics/lesion_froc.py
--rw-r--r--  2.0 unx     2243 b- defN 23-Apr-24 13:45 monai/apps/pathology/transforms/__init__.py
--rw-r--r--  2.0 unx     1995 b- defN 23-Apr-24 13:45 monai/apps/pathology/transforms/post/__init__.py
--rw-r--r--  2.0 unx    37322 b- defN 23-Apr-24 13:45 monai/apps/pathology/transforms/post/array.py
--rw-r--r--  2.0 unx    25928 b- defN 23-Apr-24 13:45 monai/apps/pathology/transforms/post/dictionary.py
--rw-r--r--  2.0 unx      836 b- defN 23-Apr-24 13:45 monai/apps/pathology/transforms/stain/__init__.py
--rw-r--r--  2.0 unx     8366 b- defN 23-Apr-24 13:45 monai/apps/pathology/transforms/stain/array.py
--rw-r--r--  2.0 unx     4761 b- defN 23-Apr-24 13:45 monai/apps/pathology/transforms/stain/dictionary.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/__init__.py
--rw-r--r--  2.0 unx     8393 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/complex_utils.py
--rw-r--r--  2.0 unx     3644 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/fastmri_reader.py
--rw-r--r--  2.0 unx     2000 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/mri_utils.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/networks/__init__.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/networks/blocks/__init__.py
--rw-r--r--  2.0 unx     4183 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/networks/blocks/varnetblock.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/networks/nets/__init__.py
--rw-r--r--  2.0 unx     6215 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/networks/nets/coil_sensitivity_model.py
--rw-r--r--  2.0 unx     4775 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/networks/nets/complex_unet.py
--rw-r--r--  2.0 unx    11377 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/networks/nets/utils.py
--rw-r--r--  2.0 unx     3831 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/networks/nets/varnet.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/transforms/__init__.py
--rw-r--r--  2.0 unx    12240 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/transforms/array.py
--rw-r--r--  2.0 unx    15844 b- defN 23-Apr-24 13:45 monai/apps/reconstruction/transforms/dictionary.py
--rw-r--r--  2.0 unx      765 b- defN 23-Apr-24 13:45 monai/apps/tcia/__init__.py
--rw-r--r--  2.0 unx     1582 b- defN 23-Apr-24 13:45 monai/apps/tcia/label_desc.py
--rw-r--r--  2.0 unx     6152 b- defN 23-Apr-24 13:45 monai/apps/tcia/utils.py
--rw-r--r--  2.0 unx     1164 b- defN 23-Apr-24 13:45 monai/auto3dseg/__init__.py
--rw-r--r--  2.0 unx     4209 b- defN 23-Apr-24 13:45 monai/auto3dseg/algo_gen.py
--rw-r--r--  2.0 unx    41223 b- defN 23-Apr-24 13:45 monai/auto3dseg/analyzer.py
--rw-r--r--  2.0 unx     5110 b- defN 23-Apr-24 13:45 monai/auto3dseg/operations.py
--rw-r--r--  2.0 unx     8725 b- defN 23-Apr-24 13:45 monai/auto3dseg/seg_summarizer.py
--rw-r--r--  2.0 unx    12193 b- defN 23-Apr-24 13:45 monai/auto3dseg/utils.py
--rw-r--r--  2.0 unx     1341 b- defN 23-Apr-24 13:45 monai/bundle/__init__.py
--rw-r--r--  2.0 unx      926 b- defN 23-Apr-24 13:45 monai/bundle/__main__.py
--rw-r--r--  2.0 unx    16035 b- defN 23-Apr-24 13:45 monai/bundle/config_item.py
--rw-r--r--  2.0 unx    22410 b- defN 23-Apr-24 13:45 monai/bundle/config_parser.py
--rw-r--r--  2.0 unx     8811 b- defN 23-Apr-24 13:45 monai/bundle/properties.py
--rw-r--r--  2.0 unx    14353 b- defN 23-Apr-24 13:45 monai/bundle/reference_resolver.py
--rw-r--r--  2.0 unx    63940 b- defN 23-Apr-24 13:45 monai/bundle/scripts.py
--rw-r--r--  2.0 unx     8911 b- defN 23-Apr-24 13:45 monai/bundle/utils.py
--rw-r--r--  2.0 unx    17082 b- defN 23-Apr-24 13:45 monai/bundle/workflows.py
--rw-r--r--  2.0 unx     1048 b- defN 23-Apr-24 13:45 monai/config/__init__.py
--rw-r--r--  2.0 unx     9913 b- defN 23-Apr-24 13:45 monai/config/deviceconfig.py
--rw-r--r--  2.0 unx     3485 b- defN 23-Apr-24 13:45 monai/config/type_definitions.py
--rw-r--r--  2.0 unx     5087 b- defN 23-Apr-24 13:45 monai/data/__init__.py
--rw-r--r--  2.0 unx    50102 b- defN 23-Apr-24 13:45 monai/data/box_utils.py
--rw-r--r--  2.0 unx     4952 b- defN 23-Apr-24 13:45 monai/data/csv_saver.py
--rw-r--r--  2.0 unx     3835 b- defN 23-Apr-24 13:45 monai/data/dataloader.py
--rw-r--r--  2.0 unx    68927 b- defN 23-Apr-24 13:45 monai/data/dataset.py
--rw-r--r--  2.0 unx    10216 b- defN 23-Apr-24 13:45 monai/data/dataset_summary.py
--rw-r--r--  2.0 unx    10318 b- defN 23-Apr-24 13:45 monai/data/decathlon_datalist.py
--rw-r--r--  2.0 unx     4448 b- defN 23-Apr-24 13:45 monai/data/fft_utils.py
--rw-r--r--  2.0 unx     6344 b- defN 23-Apr-24 13:45 monai/data/folder_layout.py
--rw-r--r--  2.0 unx    12484 b- defN 23-Apr-24 13:45 monai/data/grid_dataset.py
--rw-r--r--  2.0 unx     7008 b- defN 23-Apr-24 13:45 monai/data/image_dataset.py
--rw-r--r--  2.0 unx    60619 b- defN 23-Apr-24 13:45 monai/data/image_reader.py
--rw-r--r--  2.0 unx    39872 b- defN 23-Apr-24 13:45 monai/data/image_writer.py
--rw-r--r--  2.0 unx    13309 b- defN 23-Apr-24 13:45 monai/data/iterable_dataset.py
--rw-r--r--  2.0 unx    14097 b- defN 23-Apr-24 13:45 monai/data/itk_torch_bridge.py
--rw-r--r--  2.0 unx     8800 b- defN 23-Apr-24 13:45 monai/data/meta_obj.py
--rw-r--r--  2.0 unx    27321 b- defN 23-Apr-24 13:45 monai/data/meta_tensor.py
--rw-r--r--  2.0 unx     5268 b- defN 23-Apr-24 13:45 monai/data/samplers.py
--rw-r--r--  2.0 unx     7375 b- defN 23-Apr-24 13:45 monai/data/synthetic.py
--rw-r--r--  2.0 unx     9780 b- defN 23-Apr-24 13:45 monai/data/test_time_augmentation.py
--rw-r--r--  2.0 unx     8840 b- defN 23-Apr-24 13:45 monai/data/thread_buffer.py
--rw-r--r--  2.0 unx     5500 b- defN 23-Apr-24 13:45 monai/data/torchscript_utils.py
--rw-r--r--  2.0 unx    64795 b- defN 23-Apr-24 13:45 monai/data/utils.py
--rw-r--r--  2.0 unx     9059 b- defN 23-Apr-24 13:45 monai/data/video_dataset.py
--rw-r--r--  2.0 unx    18619 b- defN 23-Apr-24 13:45 monai/data/wsi_datasets.py
--rw-r--r--  2.0 unx    49442 b- defN 23-Apr-24 13:45 monai/data/wsi_reader.py
--rw-r--r--  2.0 unx     1133 b- defN 23-Apr-24 13:45 monai/engines/__init__.py
--rw-r--r--  2.0 unx    24568 b- defN 23-Apr-24 13:45 monai/engines/evaluator.py
--rw-r--r--  2.0 unx     7278 b- defN 23-Apr-24 13:45 monai/engines/multi_gpu_supervised_trainer.py
--rw-r--r--  2.0 unx    21347 b- defN 23-Apr-24 13:45 monai/engines/trainer.py
--rw-r--r--  2.0 unx    11631 b- defN 23-Apr-24 13:45 monai/engines/utils.py
--rw-r--r--  2.0 unx    15250 b- defN 23-Apr-24 13:45 monai/engines/workflow.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/fl/__init__.py
--rw-r--r--  2.0 unx      725 b- defN 23-Apr-24 13:45 monai/fl/client/__init__.py
--rw-r--r--  2.0 unx     5097 b- defN 23-Apr-24 13:45 monai/fl/client/client_algo.py
--rw-r--r--  2.0 unx    33232 b- defN 23-Apr-24 13:45 monai/fl/client/monai_algo.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/fl/utils/__init__.py
--rw-r--r--  2.0 unx     1713 b- defN 23-Apr-24 13:45 monai/fl/utils/constants.py
--rw-r--r--  2.0 unx     3527 b- defN 23-Apr-24 13:45 monai/fl/utils/exchange_object.py
--rw-r--r--  2.0 unx     1633 b- defN 23-Apr-24 13:45 monai/fl/utils/filters.py
--rw-r--r--  2.0 unx     2351 b- defN 23-Apr-24 13:45 monai/handlers/__init__.py
--rw-r--r--  2.0 unx     6798 b- defN 23-Apr-24 13:45 monai/handlers/checkpoint_loader.py
--rw-r--r--  2.0 unx    16071 b- defN 23-Apr-24 13:45 monai/handlers/checkpoint_saver.py
--rw-r--r--  2.0 unx     7606 b- defN 23-Apr-24 13:45 monai/handlers/classification_saver.py
--rw-r--r--  2.0 unx     7506 b- defN 23-Apr-24 13:45 monai/handlers/clearml_handlers.py
--rw-r--r--  2.0 unx     3989 b- defN 23-Apr-24 13:45 monai/handlers/confusion_matrix.py
--rw-r--r--  2.0 unx     4425 b- defN 23-Apr-24 13:45 monai/handlers/decollate_batch.py
--rw-r--r--  2.0 unx     4381 b- defN 23-Apr-24 13:45 monai/handlers/earlystop_handler.py
--rw-r--r--  2.0 unx     3338 b- defN 23-Apr-24 13:45 monai/handlers/garbage_collector.py
--rw-r--r--  2.0 unx     3580 b- defN 23-Apr-24 13:45 monai/handlers/hausdorff_distance.py
--rw-r--r--  2.0 unx     5578 b- defN 23-Apr-24 13:45 monai/handlers/ignite_metric.py
--rw-r--r--  2.0 unx     3931 b- defN 23-Apr-24 13:45 monai/handlers/logfile_handler.py
--rw-r--r--  2.0 unx     3575 b- defN 23-Apr-24 13:45 monai/handlers/lr_schedule_handler.py
--rw-r--r--  2.0 unx     3220 b- defN 23-Apr-24 13:45 monai/handlers/mean_dice.py
--rw-r--r--  2.0 unx     2831 b- defN 23-Apr-24 13:45 monai/handlers/mean_iou.py
--rw-r--r--  2.0 unx     5477 b- defN 23-Apr-24 13:45 monai/handlers/metric_logger.py
--rw-r--r--  2.0 unx     6168 b- defN 23-Apr-24 13:45 monai/handlers/metrics_reloaded_handler.py
--rw-r--r--  2.0 unx     8560 b- defN 23-Apr-24 13:45 monai/handlers/metrics_saver.py
--rw-r--r--  2.0 unx    16326 b- defN 23-Apr-24 13:45 monai/handlers/mlflow_handler.py
--rw-r--r--  2.0 unx     6819 b- defN 23-Apr-24 13:45 monai/handlers/nvtx_handlers.py
--rw-r--r--  2.0 unx     3637 b- defN 23-Apr-24 13:45 monai/handlers/panoptic_quality.py
--rw-r--r--  2.0 unx     7119 b- defN 23-Apr-24 13:45 monai/handlers/parameter_scheduler.py
--rw-r--r--  2.0 unx     3285 b- defN 23-Apr-24 13:45 monai/handlers/postprocessing.py
--rw-r--r--  2.0 unx     5336 b- defN 23-Apr-24 13:45 monai/handlers/probability_maps.py
--rw-r--r--  2.0 unx     8422 b- defN 23-Apr-24 13:45 monai/handlers/regression_metrics.py
--rw-r--r--  2.0 unx     2730 b- defN 23-Apr-24 13:45 monai/handlers/roc_auc.py
--rw-r--r--  2.0 unx     3051 b- defN 23-Apr-24 13:45 monai/handlers/smartcache_handler.py
--rw-r--r--  2.0 unx    14251 b- defN 23-Apr-24 13:45 monai/handlers/stats_handler.py
--rw-r--r--  2.0 unx     3313 b- defN 23-Apr-24 13:45 monai/handlers/surface_distance.py
--rw-r--r--  2.0 unx    23325 b- defN 23-Apr-24 13:45 monai/handlers/tensorboard_handlers.py
--rw-r--r--  2.0 unx     9855 b- defN 23-Apr-24 13:45 monai/handlers/utils.py
--rw-r--r--  2.0 unx     3269 b- defN 23-Apr-24 13:45 monai/handlers/validation_handler.py
--rw-r--r--  2.0 unx      917 b- defN 23-Apr-24 13:45 monai/inferers/__init__.py
--rw-r--r--  2.0 unx    32038 b- defN 23-Apr-24 13:45 monai/inferers/inferer.py
--rw-r--r--  2.0 unx     6393 b- defN 23-Apr-24 13:45 monai/inferers/merger.py
--rw-r--r--  2.0 unx     9397 b- defN 23-Apr-24 13:45 monai/inferers/splitter.py
--rw-r--r--  2.0 unx    20017 b- defN 23-Apr-24 13:45 monai/inferers/utils.py
--rw-r--r--  2.0 unx     1409 b- defN 23-Apr-24 13:45 monai/losses/__init__.py
--rw-r--r--  2.0 unx     3430 b- defN 23-Apr-24 13:45 monai/losses/contrastive.py
--rw-r--r--  2.0 unx     4979 b- defN 23-Apr-24 13:45 monai/losses/deform.py
--rw-r--r--  2.0 unx    46326 b- defN 23-Apr-24 13:45 monai/losses/dice.py
--rw-r--r--  2.0 unx     3733 b- defN 23-Apr-24 13:45 monai/losses/ds_loss.py
--rw-r--r--  2.0 unx     9490 b- defN 23-Apr-24 13:45 monai/losses/focal_loss.py
--rw-r--r--  2.0 unx     2795 b- defN 23-Apr-24 13:45 monai/losses/giou_loss.py
--rw-r--r--  2.0 unx    15492 b- defN 23-Apr-24 13:45 monai/losses/image_dissimilarity.py
--rw-r--r--  2.0 unx     3636 b- defN 23-Apr-24 13:45 monai/losses/multi_scale.py
--rw-r--r--  2.0 unx     2942 b- defN 23-Apr-24 13:45 monai/losses/spatial_mask.py
--rw-r--r--  2.0 unx     4825 b- defN 23-Apr-24 13:45 monai/losses/ssim_loss.py
--rw-r--r--  2.0 unx     6645 b- defN 23-Apr-24 13:45 monai/losses/tversky.py
--rw-r--r--  2.0 unx    10224 b- defN 23-Apr-24 13:45 monai/losses/unified_focal_loss.py
--rw-r--r--  2.0 unx     1977 b- defN 23-Apr-24 13:45 monai/metrics/__init__.py
--rw-r--r--  2.0 unx     8211 b- defN 23-Apr-24 13:45 monai/metrics/active_learning_metrics.py
--rw-r--r--  2.0 unx    15101 b- defN 23-Apr-24 13:45 monai/metrics/confusion_matrix.py
--rw-r--r--  2.0 unx     5578 b- defN 23-Apr-24 13:45 monai/metrics/cumulative_average.py
--rw-r--r--  2.0 unx     4026 b- defN 23-Apr-24 13:45 monai/metrics/f_beta_score.py
--rw-r--r--  2.0 unx     6157 b- defN 23-Apr-24 13:45 monai/metrics/froc.py
--rw-r--r--  2.0 unx     8262 b- defN 23-Apr-24 13:45 monai/metrics/generalized_dice.py
--rw-r--r--  2.0 unx    11470 b- defN 23-Apr-24 13:45 monai/metrics/hausdorff_distance.py
--rw-r--r--  2.0 unx     4907 b- defN 23-Apr-24 13:45 monai/metrics/loss_metric.py
--rw-r--r--  2.0 unx    12472 b- defN 23-Apr-24 13:45 monai/metrics/meandice.py
--rw-r--r--  2.0 unx     7209 b- defN 23-Apr-24 13:45 monai/metrics/meaniou.py
--rw-r--r--  2.0 unx    15140 b- defN 23-Apr-24 13:45 monai/metrics/metric.py
--rw-r--r--  2.0 unx    13679 b- defN 23-Apr-24 13:45 monai/metrics/panoptic_quality.py
--rw-r--r--  2.0 unx    19719 b- defN 23-Apr-24 13:45 monai/metrics/regression.py
--rw-r--r--  2.0 unx     8038 b- defN 23-Apr-24 13:45 monai/metrics/rocauc.py
--rw-r--r--  2.0 unx    14415 b- defN 23-Apr-24 13:45 monai/metrics/surface_dice.py
--rw-r--r--  2.0 unx    10186 b- defN 23-Apr-24 13:45 monai/metrics/surface_distance.py
--rw-r--r--  2.0 unx    15094 b- defN 23-Apr-24 13:45 monai/metrics/utils.py
--rw-r--r--  2.0 unx    11772 b- defN 23-Apr-24 13:45 monai/metrics/wrapper.py
--rw-r--r--  2.0 unx     1020 b- defN 23-Apr-24 13:45 monai/networks/__init__.py
--rw-r--r--  2.0 unx    46938 b- defN 23-Apr-24 13:45 monai/networks/utils.py
--rw-r--r--  2.0 unx     2134 b- defN 23-Apr-24 13:45 monai/networks/blocks/__init__.py
--rw-r--r--  2.0 unx     4275 b- defN 23-Apr-24 13:45 monai/networks/blocks/acti_norm.py
--rw-r--r--  2.0 unx     5839 b- defN 23-Apr-24 13:45 monai/networks/blocks/activation.py
--rw-r--r--  2.0 unx     4380 b- defN 23-Apr-24 13:45 monai/networks/blocks/aspp.py
--rw-r--r--  2.0 unx     7488 b- defN 23-Apr-24 13:45 monai/networks/blocks/backbone_fpn_utils.py
--rw-r--r--  2.0 unx    11686 b- defN 23-Apr-24 13:45 monai/networks/blocks/convolutions.py
--rw-r--r--  2.0 unx     5009 b- defN 23-Apr-24 13:45 monai/networks/blocks/crf.py
--rw-r--r--  2.0 unx     4740 b- defN 23-Apr-24 13:45 monai/networks/blocks/denseblock.py
--rw-r--r--  2.0 unx     9255 b- defN 23-Apr-24 13:45 monai/networks/blocks/dints_block.py
--rw-r--r--  2.0 unx     2413 b- defN 23-Apr-24 13:45 monai/networks/blocks/downsample.py
--rw-r--r--  2.0 unx    11062 b- defN 23-Apr-24 13:45 monai/networks/blocks/dynunet_block.py
--rw-r--r--  2.0 unx     3669 b- defN 23-Apr-24 13:45 monai/networks/blocks/encoder.py
--rw-r--r--  2.0 unx     9024 b- defN 23-Apr-24 13:45 monai/networks/blocks/fcn.py
--rw-r--r--  2.0 unx    10586 b- defN 23-Apr-24 13:45 monai/networks/blocks/feature_pyramid_network.py
--rw-r--r--  2.0 unx     8263 b- defN 23-Apr-24 13:45 monai/networks/blocks/fft_utils_t.py
--rw-r--r--  2.0 unx    11454 b- defN 23-Apr-24 13:45 monai/networks/blocks/localnet_block.py
--rw-r--r--  2.0 unx     2813 b- defN 23-Apr-24 13:45 monai/networks/blocks/mlp.py
--rw-r--r--  2.0 unx     7987 b- defN 23-Apr-24 13:45 monai/networks/blocks/patchembedding.py
--rw-r--r--  2.0 unx     8825 b- defN 23-Apr-24 13:45 monai/networks/blocks/regunet_block.py
--rw-r--r--  2.0 unx     3245 b- defN 23-Apr-24 13:45 monai/networks/blocks/segresnet_block.py
--rw-r--r--  2.0 unx     3099 b- defN 23-Apr-24 13:45 monai/networks/blocks/selfattention.py
--rw-r--r--  2.0 unx    12752 b- defN 23-Apr-24 13:45 monai/networks/blocks/squeeze_and_excitation.py
--rw-r--r--  2.0 unx     3811 b- defN 23-Apr-24 13:45 monai/networks/blocks/text_embedding.py
--rw-r--r--  2.0 unx     2322 b- defN 23-Apr-24 13:45 monai/networks/blocks/transformerblock.py
--rw-r--r--  2.0 unx     9049 b- defN 23-Apr-24 13:45 monai/networks/blocks/unetr_block.py
--rw-r--r--  2.0 unx    13312 b- defN 23-Apr-24 13:45 monai/networks/blocks/upsample.py
--rw-r--r--  2.0 unx     6656 b- defN 23-Apr-24 13:45 monai/networks/blocks/warp.py
--rw-r--r--  2.0 unx     1562 b- defN 23-Apr-24 13:45 monai/networks/layers/__init__.py
--rw-r--r--  2.0 unx     8288 b- defN 23-Apr-24 13:45 monai/networks/layers/convutils.py
--rw-r--r--  2.0 unx     1802 b- defN 23-Apr-24 13:45 monai/networks/layers/drop_path.py
--rw-r--r--  2.0 unx    12638 b- defN 23-Apr-24 13:45 monai/networks/layers/factories.py
--rw-r--r--  2.0 unx    17992 b- defN 23-Apr-24 13:45 monai/networks/layers/filtering.py
--rw-r--r--  2.0 unx     3324 b- defN 23-Apr-24 13:45 monai/networks/layers/gmm.py
--rw-r--r--  2.0 unx    28470 b- defN 23-Apr-24 13:45 monai/networks/layers/simplelayers.py
--rw-r--r--  2.0 unx    25576 b- defN 23-Apr-24 13:45 monai/networks/layers/spatial_transforms.py
--rw-r--r--  2.0 unx     4296 b- defN 23-Apr-24 13:45 monai/networks/layers/utils.py
--rw-r--r--  2.0 unx     2253 b- defN 23-Apr-24 13:45 monai/networks/layers/weight_init.py
--rw-r--r--  2.0 unx     3141 b- defN 23-Apr-24 13:45 monai/networks/nets/__init__.py
--rw-r--r--  2.0 unx    21533 b- defN 23-Apr-24 13:45 monai/networks/nets/ahnet.py
--rw-r--r--  2.0 unx     9202 b- defN 23-Apr-24 13:45 monai/networks/nets/attentionunet.py
--rw-r--r--  2.0 unx    12089 b- defN 23-Apr-24 13:45 monai/networks/nets/autoencoder.py
--rw-r--r--  2.0 unx    10950 b- defN 23-Apr-24 13:45 monai/networks/nets/basic_unet.py
--rw-r--r--  2.0 unx     7960 b- defN 23-Apr-24 13:45 monai/networks/nets/basic_unetplusplus.py
--rw-r--r--  2.0 unx     6293 b- defN 23-Apr-24 13:45 monai/networks/nets/classifier.py
--rw-r--r--  2.0 unx    15820 b- defN 23-Apr-24 13:45 monai/networks/nets/densenet.py
--rw-r--r--  2.0 unx    44771 b- defN 23-Apr-24 13:45 monai/networks/nets/dints.py
--rw-r--r--  2.0 unx    18210 b- defN 23-Apr-24 13:45 monai/networks/nets/dynunet.py
--rw-r--r--  2.0 unx    40643 b- defN 23-Apr-24 13:45 monai/networks/nets/efficientnet.py
--rw-r--r--  2.0 unx    14147 b- defN 23-Apr-24 13:45 monai/networks/nets/flexible_unet.py
--rw-r--r--  2.0 unx     7212 b- defN 23-Apr-24 13:45 monai/networks/nets/fullyconnectednet.py
--rw-r--r--  2.0 unx     6581 b- defN 23-Apr-24 13:45 monai/networks/nets/generator.py
--rw-r--r--  2.0 unx     8882 b- defN 23-Apr-24 13:45 monai/networks/nets/highresnet.py
--rw-r--r--  2.0 unx    28678 b- defN 23-Apr-24 13:45 monai/networks/nets/hovernet.py
--rw-r--r--  2.0 unx     9812 b- defN 23-Apr-24 13:45 monai/networks/nets/milmodel.py
--rw-r--r--  2.0 unx     6102 b- defN 23-Apr-24 13:45 monai/networks/nets/netadapter.py
--rw-r--r--  2.0 unx     6488 b- defN 23-Apr-24 13:45 monai/networks/nets/regressor.py
--rw-r--r--  2.0 unx    17189 b- defN 23-Apr-24 13:45 monai/networks/nets/regunet.py
--rw-r--r--  2.0 unx    16785 b- defN 23-Apr-24 13:45 monai/networks/nets/resnet.py
--rw-r--r--  2.0 unx    13994 b- defN 23-Apr-24 13:45 monai/networks/nets/segresnet.py
--rw-r--r--  2.0 unx    15667 b- defN 23-Apr-24 13:45 monai/networks/nets/segresnet_ds.py
--rw-r--r--  2.0 unx    19289 b- defN 23-Apr-24 13:45 monai/networks/nets/senet.py
--rw-r--r--  2.0 unx    42000 b- defN 23-Apr-24 13:45 monai/networks/nets/swin_unetr.py
--rw-r--r--  2.0 unx     6309 b- defN 23-Apr-24 13:45 monai/networks/nets/torchvision_fc.py
--rw-r--r--  2.0 unx    16626 b- defN 23-Apr-24 13:45 monai/networks/nets/transchex.py
--rw-r--r--  2.0 unx    13722 b- defN 23-Apr-24 13:45 monai/networks/nets/unet.py
--rw-r--r--  2.0 unx     7943 b- defN 23-Apr-24 13:45 monai/networks/nets/unetr.py
--rw-r--r--  2.0 unx     6285 b- defN 23-Apr-24 13:45 monai/networks/nets/varautoencoder.py
--rw-r--r--  2.0 unx     5655 b- defN 23-Apr-24 13:45 monai/networks/nets/vit.py
--rw-r--r--  2.0 unx     4817 b- defN 23-Apr-24 13:45 monai/networks/nets/vitautoenc.py
--rw-r--r--  2.0 unx    10011 b- defN 23-Apr-24 13:45 monai/networks/nets/vnet.py
--rw-r--r--  2.0 unx      796 b- defN 23-Apr-24 13:45 monai/optimizers/__init__.py
--rw-r--r--  2.0 unx    21952 b- defN 23-Apr-24 13:45 monai/optimizers/lr_finder.py
--rw-r--r--  2.0 unx     3652 b- defN 23-Apr-24 13:45 monai/optimizers/lr_scheduler.py
--rw-r--r--  2.0 unx     5661 b- defN 23-Apr-24 13:45 monai/optimizers/novograd.py
--rw-r--r--  2.0 unx     4131 b- defN 23-Apr-24 13:45 monai/optimizers/utils.py
--rw-r--r--  2.0 unx    15248 b- defN 23-Apr-24 13:45 monai/transforms/__init__.py
--rw-r--r--  2.0 unx     8946 b- defN 23-Apr-24 13:45 monai/transforms/adaptors.py
--rw-r--r--  2.0 unx    40812 b- defN 23-Apr-24 13:45 monai/transforms/compose.py
--rw-r--r--  2.0 unx    18000 b- defN 23-Apr-24 13:45 monai/transforms/inverse.py
--rw-r--r--  2.0 unx     7054 b- defN 23-Apr-24 13:45 monai/transforms/inverse_batch_transform.py
--rw-r--r--  2.0 unx     3386 b- defN 23-Apr-24 13:45 monai/transforms/nvtx.py
--rw-r--r--  2.0 unx     2885 b- defN 23-Apr-24 13:45 monai/transforms/traits.py
--rw-r--r--  2.0 unx    18234 b- defN 23-Apr-24 13:45 monai/transforms/transform.py
--rw-r--r--  2.0 unx    72516 b- defN 23-Apr-24 13:45 monai/transforms/utils.py
--rw-r--r--  2.0 unx    31081 b- defN 23-Apr-24 13:45 monai/transforms/utils_create_transform_ims.py
--rw-r--r--  2.0 unx    18397 b- defN 23-Apr-24 13:45 monai/transforms/utils_pytorch_numpy_unification.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/transforms/croppad/__init__.py
--rw-r--r--  2.0 unx    68231 b- defN 23-Apr-24 13:45 monai/transforms/croppad/array.py
--rw-r--r--  2.0 unx     6194 b- defN 23-Apr-24 13:45 monai/transforms/croppad/batch.py
--rw-r--r--  2.0 unx    54071 b- defN 23-Apr-24 13:45 monai/transforms/croppad/dictionary.py
--rw-r--r--  2.0 unx    12673 b- defN 23-Apr-24 13:45 monai/transforms/croppad/functional.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/transforms/intensity/__init__.py
--rw-r--r--  2.0 unx    98613 b- defN 23-Apr-24 13:45 monai/transforms/intensity/array.py
--rw-r--r--  2.0 unx    76501 b- defN 23-Apr-24 13:45 monai/transforms/intensity/dictionary.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/transforms/io/__init__.py
--rw-r--r--  2.0 unx    25430 b- defN 23-Apr-24 13:45 monai/transforms/io/array.py
--rw-r--r--  2.0 unx    17821 b- defN 23-Apr-24 13:45 monai/transforms/io/dictionary.py
--rw-r--r--  2.0 unx      699 b- defN 23-Apr-24 13:45 monai/transforms/lazy/__init__.py
--rw-r--r--  2.0 unx     5552 b- defN 23-Apr-24 13:45 monai/transforms/lazy/functional.py
--rw-r--r--  2.0 unx     9851 b- defN 23-Apr-24 13:45 monai/transforms/lazy/utils.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/transforms/meta_utility/__init__.py
--rw-r--r--  2.0 unx     4896 b- defN 23-Apr-24 13:45 monai/transforms/meta_utility/dictionary.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/transforms/post/__init__.py
--rw-r--r--  2.0 unx    40858 b- defN 23-Apr-24 13:45 monai/transforms/post/array.py
--rw-r--r--  2.0 unx    39905 b- defN 23-Apr-24 13:45 monai/transforms/post/dictionary.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/transforms/signal/__init__.py
--rw-r--r--  2.0 unx    16378 b- defN 23-Apr-24 13:45 monai/transforms/signal/array.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/transforms/smooth_field/__init__.py
--rw-r--r--  2.0 unx    17833 b- defN 23-Apr-24 13:45 monai/transforms/smooth_field/array.py
--rw-r--r--  2.0 unx    11194 b- defN 23-Apr-24 13:45 monai/transforms/smooth_field/dictionary.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/transforms/spatial/__init__.py
--rw-r--r--  2.0 unx   168618 b- defN 23-Apr-24 13:45 monai/transforms/spatial/array.py
--rw-r--r--  2.0 unx   107151 b- defN 23-Apr-24 13:45 monai/transforms/spatial/dictionary.py
--rw-r--r--  2.0 unx    31494 b- defN 23-Apr-24 13:45 monai/transforms/spatial/functional.py
--rw-r--r--  2.0 unx      573 b- defN 23-Apr-24 13:45 monai/transforms/utility/__init__.py
--rw-r--r--  2.0 unx    70839 b- defN 23-Apr-24 13:45 monai/transforms/utility/array.py
--rw-r--r--  2.0 unx    76330 b- defN 23-Apr-24 13:45 monai/transforms/utility/dictionary.py
--rw-r--r--  2.0 unx     3380 b- defN 23-Apr-24 13:45 monai/utils/__init__.py
--rw-r--r--  2.0 unx     4096 b- defN 23-Apr-24 13:45 monai/utils/aliases.py
--rw-r--r--  2.0 unx     3129 b- defN 23-Apr-24 13:45 monai/utils/decorators.py
--rw-r--r--  2.0 unx    14759 b- defN 23-Apr-24 13:45 monai/utils/deprecate_utils.py
--rw-r--r--  2.0 unx     8526 b- defN 23-Apr-24 13:45 monai/utils/dist.py
--rw-r--r--  2.0 unx    16807 b- defN 23-Apr-24 13:45 monai/utils/enums.py
--rw-r--r--  2.0 unx    15637 b- defN 23-Apr-24 13:45 monai/utils/jupyter_utils.py
--rw-r--r--  2.0 unx    28185 b- defN 23-Apr-24 13:45 monai/utils/misc.py
--rw-r--r--  2.0 unx    23631 b- defN 23-Apr-24 13:45 monai/utils/module.py
--rw-r--r--  2.0 unx     6876 b- defN 23-Apr-24 13:45 monai/utils/nvtx.py
--rw-r--r--  2.0 unx    15936 b- defN 23-Apr-24 13:45 monai/utils/profiling.py
--rw-r--r--  2.0 unx     5955 b- defN 23-Apr-24 13:45 monai/utils/state_cacher.py
--rw-r--r--  2.0 unx    21147 b- defN 23-Apr-24 13:45 monai/utils/type_conversion.py
--rw-r--r--  2.0 unx     1038 b- defN 23-Apr-24 13:45 monai/visualize/__init__.py
--rw-r--r--  2.0 unx    16156 b- defN 23-Apr-24 13:45 monai/visualize/class_activation_maps.py
--rw-r--r--  2.0 unx     6277 b- defN 23-Apr-24 13:45 monai/visualize/gradient_based.py
--rw-r--r--  2.0 unx     9200 b- defN 23-Apr-24 13:45 monai/visualize/img2tensorboard.py
--rw-r--r--  2.0 unx    18816 b- defN 23-Apr-24 13:45 monai/visualize/occlusion_sensitivity.py
--rw-r--r--  2.0 unx     9966 b- defN 23-Apr-24 13:45 monai/visualize/utils.py
--rw-r--r--  2.0 unx     1377 b- defN 23-Apr-24 13:45 monai/visualize/visualizer.py
--rw-r--r--  2.0 unx    11357 b- defN 23-Apr-24 13:47 monai-1.2.0rc5.dist-info/LICENSE
--rw-r--r--  2.0 unx    10172 b- defN 23-Apr-24 13:47 monai-1.2.0rc5.dist-info/METADATA
--rw-r--r--  2.0 unx      112 b- defN 23-Apr-24 13:47 monai-1.2.0rc5.dist-info/WHEEL
--rw-r--r--  2.0 unx        6 b- defN 23-Apr-24 13:47 monai-1.2.0rc5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    32699 b- defN 23-Apr-24 13:47 monai-1.2.0rc5.dist-info/RECORD
-365 files, 4746247 bytes uncompressed, 1201648 bytes compressed:  74.7%
+Zip file size: 1257509 bytes, number of entries: 365
+-rw-r--r--  2.0 unx     2276 b- defN 23-May-08 12:17 monai/__init__.py
+-rw-r--r--  2.0 unx      500 b- defN 23-May-08 12:19 monai/_version.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-08 12:17 monai/py.typed
+-rw-r--r--  2.0 unx      642 b- defN 23-May-08 12:17 monai/_extensions/__init__.py
+-rw-r--r--  2.0 unx     3643 b- defN 23-May-08 12:17 monai/_extensions/loader.py
+-rw-r--r--  2.0 unx     2931 b- defN 23-May-08 12:17 monai/_extensions/gmm/gmm.cpp
+-rw-r--r--  2.0 unx     1760 b- defN 23-May-08 12:17 monai/_extensions/gmm/gmm.h
+-rw-r--r--  2.0 unx     1118 b- defN 23-May-08 12:17 monai/_extensions/gmm/gmm_cpu.cpp
+-rw-r--r--  2.0 unx    16213 b- defN 23-May-08 12:17 monai/_extensions/gmm/gmm_cuda.cu
+-rw-r--r--  2.0 unx     3520 b- defN 23-May-08 12:17 monai/_extensions/gmm/gmm_cuda_linalg.cuh
+-rw-r--r--  2.0 unx      908 b- defN 23-May-08 12:17 monai/apps/__init__.py
+-rw-r--r--  2.0 unx    34568 b- defN 23-May-08 12:17 monai/apps/datasets.py
+-rw-r--r--  2.0 unx    13505 b- defN 23-May-08 12:17 monai/apps/utils.py
+-rw-r--r--  2.0 unx     1016 b- defN 23-May-08 12:17 monai/apps/auto3dseg/__init__.py
+-rw-r--r--  2.0 unx     1411 b- defN 23-May-08 12:17 monai/apps/auto3dseg/__main__.py
+-rw-r--r--  2.0 unx    37011 b- defN 23-May-08 12:17 monai/apps/auto3dseg/auto_runner.py
+-rw-r--r--  2.0 unx    26063 b- defN 23-May-08 12:17 monai/apps/auto3dseg/bundle_gen.py
+-rw-r--r--  2.0 unx    17262 b- defN 23-May-08 12:17 monai/apps/auto3dseg/data_analyzer.py
+-rw-r--r--  2.0 unx    27516 b- defN 23-May-08 12:17 monai/apps/auto3dseg/ensemble_builder.py
+-rw-r--r--  2.0 unx    16619 b- defN 23-May-08 12:17 monai/apps/auto3dseg/hpo_gen.py
+-rw-r--r--  2.0 unx     3991 b- defN 23-May-08 12:17 monai/apps/auto3dseg/transforms.py
+-rw-r--r--  2.0 unx     3138 b- defN 23-May-08 12:17 monai/apps/auto3dseg/utils.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/deepedit/__init__.py
+-rw-r--r--  2.0 unx     4501 b- defN 23-May-08 12:17 monai/apps/deepedit/interaction.py
+-rw-r--r--  2.0 unx    37435 b- defN 23-May-08 12:17 monai/apps/deepedit/transforms.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/deepgrow/__init__.py
+-rw-r--r--  2.0 unx    10054 b- defN 23-May-08 12:17 monai/apps/deepgrow/dataset.py
+-rw-r--r--  2.0 unx     3739 b- defN 23-May-08 12:17 monai/apps/deepgrow/interaction.py
+-rw-r--r--  2.0 unx    42011 b- defN 23-May-08 12:17 monai/apps/deepgrow/transforms.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/detection/__init__.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/detection/metrics/__init__.py
+-rw-r--r--  2.0 unx    26617 b- defN 23-May-08 12:17 monai/apps/detection/metrics/coco.py
+-rw-r--r--  2.0 unx    17161 b- defN 23-May-08 12:17 monai/apps/detection/metrics/matching.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/detection/networks/__init__.py
+-rw-r--r--  2.0 unx    53640 b- defN 23-May-08 12:17 monai/apps/detection/networks/retinanet_detector.py
+-rw-r--r--  2.0 unx    19136 b- defN 23-May-08 12:17 monai/apps/detection/networks/retinanet_network.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/detection/transforms/__init__.py
+-rw-r--r--  2.0 unx    24519 b- defN 23-May-08 12:17 monai/apps/detection/transforms/array.py
+-rw-r--r--  2.0 unx    17916 b- defN 23-May-08 12:17 monai/apps/detection/transforms/box_ops.py
+-rw-r--r--  2.0 unx    68979 b- defN 23-May-08 12:17 monai/apps/detection/transforms/dictionary.py
+-rw-r--r--  2.0 unx    13531 b- defN 23-May-08 12:17 monai/apps/detection/utils/ATSS_matcher.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/detection/utils/__init__.py
+-rw-r--r--  2.0 unx    18681 b- defN 23-May-08 12:17 monai/apps/detection/utils/anchor_utils.py
+-rw-r--r--  2.0 unx    11120 b- defN 23-May-08 12:17 monai/apps/detection/utils/box_coder.py
+-rw-r--r--  2.0 unx     9031 b- defN 23-May-08 12:17 monai/apps/detection/utils/box_selector.py
+-rw-r--r--  2.0 unx    10306 b- defN 23-May-08 12:17 monai/apps/detection/utils/detector_utils.py
+-rw-r--r--  2.0 unx    13890 b- defN 23-May-08 12:17 monai/apps/detection/utils/hard_negative_sampler.py
+-rw-r--r--  2.0 unx     5818 b- defN 23-May-08 12:17 monai/apps/detection/utils/predict_utils.py
+-rw-r--r--  2.0 unx      726 b- defN 23-May-08 12:17 monai/apps/mmars/__init__.py
+-rw-r--r--  2.0 unx    13115 b- defN 23-May-08 12:17 monai/apps/mmars/mmars.py
+-rw-r--r--  2.0 unx     9996 b- defN 23-May-08 12:17 monai/apps/mmars/model_desc.py
+-rw-r--r--  2.0 unx      745 b- defN 23-May-08 12:17 monai/apps/nnunet/__init__.py
+-rw-r--r--  2.0 unx      832 b- defN 23-May-08 12:17 monai/apps/nnunet/__main__.py
+-rw-r--r--  2.0 unx    47981 b- defN 23-May-08 12:17 monai/apps/nnunet/nnunetv2_runner.py
+-rw-r--r--  2.0 unx     6761 b- defN 23-May-08 12:17 monai/apps/nnunet/utils.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/nuclick/__init__.py
+-rw-r--r--  2.0 unx    24948 b- defN 23-May-08 12:17 monai/apps/nuclick/transforms.py
+-rw-r--r--  2.0 unx     1030 b- defN 23-May-08 12:17 monai/apps/pathology/__init__.py
+-rw-r--r--  2.0 unx     2860 b- defN 23-May-08 12:17 monai/apps/pathology/utils.py
+-rw-r--r--  2.0 unx      650 b- defN 23-May-08 12:17 monai/apps/pathology/engines/__init__.py
+-rw-r--r--  2.0 unx     2397 b- defN 23-May-08 12:17 monai/apps/pathology/engines/utils.py
+-rw-r--r--  2.0 unx      609 b- defN 23-May-08 12:17 monai/apps/pathology/handlers/__init__.py
+-rw-r--r--  2.0 unx     2315 b- defN 23-May-08 12:17 monai/apps/pathology/handlers/utils.py
+-rw-r--r--  2.0 unx      660 b- defN 23-May-08 12:17 monai/apps/pathology/inferers/__init__.py
+-rw-r--r--  2.0 unx     9148 b- defN 23-May-08 12:17 monai/apps/pathology/inferers/inferer.py
+-rw-r--r--  2.0 unx      650 b- defN 23-May-08 12:17 monai/apps/pathology/losses/__init__.py
+-rw-r--r--  2.0 unx     7293 b- defN 23-May-08 12:17 monai/apps/pathology/losses/hovernet_loss.py
+-rw-r--r--  2.0 unx      646 b- defN 23-May-08 12:17 monai/apps/pathology/metrics/__init__.py
+-rw-r--r--  2.0 unx     7225 b- defN 23-May-08 12:17 monai/apps/pathology/metrics/lesion_froc.py
+-rw-r--r--  2.0 unx     2243 b- defN 23-May-08 12:17 monai/apps/pathology/transforms/__init__.py
+-rw-r--r--  2.0 unx     1995 b- defN 23-May-08 12:17 monai/apps/pathology/transforms/post/__init__.py
+-rw-r--r--  2.0 unx    37322 b- defN 23-May-08 12:17 monai/apps/pathology/transforms/post/array.py
+-rw-r--r--  2.0 unx    25928 b- defN 23-May-08 12:17 monai/apps/pathology/transforms/post/dictionary.py
+-rw-r--r--  2.0 unx      836 b- defN 23-May-08 12:17 monai/apps/pathology/transforms/stain/__init__.py
+-rw-r--r--  2.0 unx     8366 b- defN 23-May-08 12:17 monai/apps/pathology/transforms/stain/array.py
+-rw-r--r--  2.0 unx     4761 b- defN 23-May-08 12:17 monai/apps/pathology/transforms/stain/dictionary.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/reconstruction/__init__.py
+-rw-r--r--  2.0 unx     8393 b- defN 23-May-08 12:17 monai/apps/reconstruction/complex_utils.py
+-rw-r--r--  2.0 unx     3644 b- defN 23-May-08 12:17 monai/apps/reconstruction/fastmri_reader.py
+-rw-r--r--  2.0 unx     2000 b- defN 23-May-08 12:17 monai/apps/reconstruction/mri_utils.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/reconstruction/networks/__init__.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/reconstruction/networks/blocks/__init__.py
+-rw-r--r--  2.0 unx     4183 b- defN 23-May-08 12:17 monai/apps/reconstruction/networks/blocks/varnetblock.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/reconstruction/networks/nets/__init__.py
+-rw-r--r--  2.0 unx     6215 b- defN 23-May-08 12:17 monai/apps/reconstruction/networks/nets/coil_sensitivity_model.py
+-rw-r--r--  2.0 unx     4775 b- defN 23-May-08 12:17 monai/apps/reconstruction/networks/nets/complex_unet.py
+-rw-r--r--  2.0 unx    11377 b- defN 23-May-08 12:17 monai/apps/reconstruction/networks/nets/utils.py
+-rw-r--r--  2.0 unx     3831 b- defN 23-May-08 12:17 monai/apps/reconstruction/networks/nets/varnet.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/apps/reconstruction/transforms/__init__.py
+-rw-r--r--  2.0 unx    12240 b- defN 23-May-08 12:17 monai/apps/reconstruction/transforms/array.py
+-rw-r--r--  2.0 unx    15844 b- defN 23-May-08 12:17 monai/apps/reconstruction/transforms/dictionary.py
+-rw-r--r--  2.0 unx      765 b- defN 23-May-08 12:17 monai/apps/tcia/__init__.py
+-rw-r--r--  2.0 unx     1582 b- defN 23-May-08 12:17 monai/apps/tcia/label_desc.py
+-rw-r--r--  2.0 unx     6152 b- defN 23-May-08 12:17 monai/apps/tcia/utils.py
+-rw-r--r--  2.0 unx     1164 b- defN 23-May-08 12:17 monai/auto3dseg/__init__.py
+-rw-r--r--  2.0 unx     4286 b- defN 23-May-08 12:17 monai/auto3dseg/algo_gen.py
+-rw-r--r--  2.0 unx    41223 b- defN 23-May-08 12:17 monai/auto3dseg/analyzer.py
+-rw-r--r--  2.0 unx     5110 b- defN 23-May-08 12:17 monai/auto3dseg/operations.py
+-rw-r--r--  2.0 unx     8725 b- defN 23-May-08 12:17 monai/auto3dseg/seg_summarizer.py
+-rw-r--r--  2.0 unx    13428 b- defN 23-May-08 12:17 monai/auto3dseg/utils.py
+-rw-r--r--  2.0 unx     1341 b- defN 23-May-08 12:17 monai/bundle/__init__.py
+-rw-r--r--  2.0 unx      926 b- defN 23-May-08 12:17 monai/bundle/__main__.py
+-rw-r--r--  2.0 unx    16035 b- defN 23-May-08 12:17 monai/bundle/config_item.py
+-rw-r--r--  2.0 unx    22360 b- defN 23-May-08 12:17 monai/bundle/config_parser.py
+-rw-r--r--  2.0 unx     9672 b- defN 23-May-08 12:17 monai/bundle/properties.py
+-rw-r--r--  2.0 unx    14353 b- defN 23-May-08 12:17 monai/bundle/reference_resolver.py
+-rw-r--r--  2.0 unx    64631 b- defN 23-May-08 12:17 monai/bundle/scripts.py
+-rw-r--r--  2.0 unx     8911 b- defN 23-May-08 12:17 monai/bundle/utils.py
+-rw-r--r--  2.0 unx    19001 b- defN 23-May-08 12:17 monai/bundle/workflows.py
+-rw-r--r--  2.0 unx     1048 b- defN 23-May-08 12:17 monai/config/__init__.py
+-rw-r--r--  2.0 unx     9913 b- defN 23-May-08 12:17 monai/config/deviceconfig.py
+-rw-r--r--  2.0 unx     3485 b- defN 23-May-08 12:17 monai/config/type_definitions.py
+-rw-r--r--  2.0 unx     5087 b- defN 23-May-08 12:17 monai/data/__init__.py
+-rw-r--r--  2.0 unx    50102 b- defN 23-May-08 12:17 monai/data/box_utils.py
+-rw-r--r--  2.0 unx     4952 b- defN 23-May-08 12:17 monai/data/csv_saver.py
+-rw-r--r--  2.0 unx     3835 b- defN 23-May-08 12:17 monai/data/dataloader.py
+-rw-r--r--  2.0 unx    68927 b- defN 23-May-08 12:17 monai/data/dataset.py
+-rw-r--r--  2.0 unx    10216 b- defN 23-May-08 12:17 monai/data/dataset_summary.py
+-rw-r--r--  2.0 unx    10318 b- defN 23-May-08 12:17 monai/data/decathlon_datalist.py
+-rw-r--r--  2.0 unx     4448 b- defN 23-May-08 12:17 monai/data/fft_utils.py
+-rw-r--r--  2.0 unx     6344 b- defN 23-May-08 12:17 monai/data/folder_layout.py
+-rw-r--r--  2.0 unx    12484 b- defN 23-May-08 12:17 monai/data/grid_dataset.py
+-rw-r--r--  2.0 unx     7008 b- defN 23-May-08 12:17 monai/data/image_dataset.py
+-rw-r--r--  2.0 unx    60619 b- defN 23-May-08 12:17 monai/data/image_reader.py
+-rw-r--r--  2.0 unx    39872 b- defN 23-May-08 12:17 monai/data/image_writer.py
+-rw-r--r--  2.0 unx    13309 b- defN 23-May-08 12:17 monai/data/iterable_dataset.py
+-rw-r--r--  2.0 unx    14097 b- defN 23-May-08 12:17 monai/data/itk_torch_bridge.py
+-rw-r--r--  2.0 unx     8800 b- defN 23-May-08 12:17 monai/data/meta_obj.py
+-rw-r--r--  2.0 unx    27321 b- defN 23-May-08 12:17 monai/data/meta_tensor.py
+-rw-r--r--  2.0 unx     5268 b- defN 23-May-08 12:17 monai/data/samplers.py
+-rw-r--r--  2.0 unx     7375 b- defN 23-May-08 12:17 monai/data/synthetic.py
+-rw-r--r--  2.0 unx     9780 b- defN 23-May-08 12:17 monai/data/test_time_augmentation.py
+-rw-r--r--  2.0 unx     8840 b- defN 23-May-08 12:17 monai/data/thread_buffer.py
+-rw-r--r--  2.0 unx     5500 b- defN 23-May-08 12:17 monai/data/torchscript_utils.py
+-rw-r--r--  2.0 unx    64795 b- defN 23-May-08 12:17 monai/data/utils.py
+-rw-r--r--  2.0 unx     9059 b- defN 23-May-08 12:17 monai/data/video_dataset.py
+-rw-r--r--  2.0 unx    18619 b- defN 23-May-08 12:17 monai/data/wsi_datasets.py
+-rw-r--r--  2.0 unx    49442 b- defN 23-May-08 12:17 monai/data/wsi_reader.py
+-rw-r--r--  2.0 unx     1133 b- defN 23-May-08 12:17 monai/engines/__init__.py
+-rw-r--r--  2.0 unx    24568 b- defN 23-May-08 12:17 monai/engines/evaluator.py
+-rw-r--r--  2.0 unx     7278 b- defN 23-May-08 12:17 monai/engines/multi_gpu_supervised_trainer.py
+-rw-r--r--  2.0 unx    21347 b- defN 23-May-08 12:17 monai/engines/trainer.py
+-rw-r--r--  2.0 unx    11631 b- defN 23-May-08 12:17 monai/engines/utils.py
+-rw-r--r--  2.0 unx    15250 b- defN 23-May-08 12:17 monai/engines/workflow.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/fl/__init__.py
+-rw-r--r--  2.0 unx      725 b- defN 23-May-08 12:17 monai/fl/client/__init__.py
+-rw-r--r--  2.0 unx     5097 b- defN 23-May-08 12:17 monai/fl/client/client_algo.py
+-rw-r--r--  2.0 unx    33232 b- defN 23-May-08 12:17 monai/fl/client/monai_algo.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/fl/utils/__init__.py
+-rw-r--r--  2.0 unx     1713 b- defN 23-May-08 12:17 monai/fl/utils/constants.py
+-rw-r--r--  2.0 unx     3527 b- defN 23-May-08 12:17 monai/fl/utils/exchange_object.py
+-rw-r--r--  2.0 unx     1633 b- defN 23-May-08 12:17 monai/fl/utils/filters.py
+-rw-r--r--  2.0 unx     2351 b- defN 23-May-08 12:17 monai/handlers/__init__.py
+-rw-r--r--  2.0 unx     6798 b- defN 23-May-08 12:17 monai/handlers/checkpoint_loader.py
+-rw-r--r--  2.0 unx    16071 b- defN 23-May-08 12:17 monai/handlers/checkpoint_saver.py
+-rw-r--r--  2.0 unx     7606 b- defN 23-May-08 12:17 monai/handlers/classification_saver.py
+-rw-r--r--  2.0 unx     7506 b- defN 23-May-08 12:17 monai/handlers/clearml_handlers.py
+-rw-r--r--  2.0 unx     3989 b- defN 23-May-08 12:17 monai/handlers/confusion_matrix.py
+-rw-r--r--  2.0 unx     4425 b- defN 23-May-08 12:17 monai/handlers/decollate_batch.py
+-rw-r--r--  2.0 unx     4381 b- defN 23-May-08 12:17 monai/handlers/earlystop_handler.py
+-rw-r--r--  2.0 unx     3338 b- defN 23-May-08 12:17 monai/handlers/garbage_collector.py
+-rw-r--r--  2.0 unx     3580 b- defN 23-May-08 12:17 monai/handlers/hausdorff_distance.py
+-rw-r--r--  2.0 unx     5578 b- defN 23-May-08 12:17 monai/handlers/ignite_metric.py
+-rw-r--r--  2.0 unx     3931 b- defN 23-May-08 12:17 monai/handlers/logfile_handler.py
+-rw-r--r--  2.0 unx     3575 b- defN 23-May-08 12:17 monai/handlers/lr_schedule_handler.py
+-rw-r--r--  2.0 unx     3220 b- defN 23-May-08 12:17 monai/handlers/mean_dice.py
+-rw-r--r--  2.0 unx     2831 b- defN 23-May-08 12:17 monai/handlers/mean_iou.py
+-rw-r--r--  2.0 unx     5477 b- defN 23-May-08 12:17 monai/handlers/metric_logger.py
+-rw-r--r--  2.0 unx     6168 b- defN 23-May-08 12:17 monai/handlers/metrics_reloaded_handler.py
+-rw-r--r--  2.0 unx     8560 b- defN 23-May-08 12:17 monai/handlers/metrics_saver.py
+-rw-r--r--  2.0 unx    16845 b- defN 23-May-08 12:17 monai/handlers/mlflow_handler.py
+-rw-r--r--  2.0 unx     6819 b- defN 23-May-08 12:17 monai/handlers/nvtx_handlers.py
+-rw-r--r--  2.0 unx     3637 b- defN 23-May-08 12:17 monai/handlers/panoptic_quality.py
+-rw-r--r--  2.0 unx     7119 b- defN 23-May-08 12:17 monai/handlers/parameter_scheduler.py
+-rw-r--r--  2.0 unx     3285 b- defN 23-May-08 12:17 monai/handlers/postprocessing.py
+-rw-r--r--  2.0 unx     5336 b- defN 23-May-08 12:17 monai/handlers/probability_maps.py
+-rw-r--r--  2.0 unx     8422 b- defN 23-May-08 12:17 monai/handlers/regression_metrics.py
+-rw-r--r--  2.0 unx     2730 b- defN 23-May-08 12:17 monai/handlers/roc_auc.py
+-rw-r--r--  2.0 unx     3051 b- defN 23-May-08 12:17 monai/handlers/smartcache_handler.py
+-rw-r--r--  2.0 unx    14251 b- defN 23-May-08 12:17 monai/handlers/stats_handler.py
+-rw-r--r--  2.0 unx     3313 b- defN 23-May-08 12:17 monai/handlers/surface_distance.py
+-rw-r--r--  2.0 unx    23325 b- defN 23-May-08 12:17 monai/handlers/tensorboard_handlers.py
+-rw-r--r--  2.0 unx     9855 b- defN 23-May-08 12:17 monai/handlers/utils.py
+-rw-r--r--  2.0 unx     3269 b- defN 23-May-08 12:17 monai/handlers/validation_handler.py
+-rw-r--r--  2.0 unx      917 b- defN 23-May-08 12:17 monai/inferers/__init__.py
+-rw-r--r--  2.0 unx    32256 b- defN 23-May-08 12:17 monai/inferers/inferer.py
+-rw-r--r--  2.0 unx     6393 b- defN 23-May-08 12:17 monai/inferers/merger.py
+-rw-r--r--  2.0 unx     9397 b- defN 23-May-08 12:17 monai/inferers/splitter.py
+-rw-r--r--  2.0 unx    20017 b- defN 23-May-08 12:17 monai/inferers/utils.py
+-rw-r--r--  2.0 unx     1409 b- defN 23-May-08 12:17 monai/losses/__init__.py
+-rw-r--r--  2.0 unx     3430 b- defN 23-May-08 12:17 monai/losses/contrastive.py
+-rw-r--r--  2.0 unx     4979 b- defN 23-May-08 12:17 monai/losses/deform.py
+-rw-r--r--  2.0 unx    46326 b- defN 23-May-08 12:17 monai/losses/dice.py
+-rw-r--r--  2.0 unx     3733 b- defN 23-May-08 12:17 monai/losses/ds_loss.py
+-rw-r--r--  2.0 unx     9490 b- defN 23-May-08 12:17 monai/losses/focal_loss.py
+-rw-r--r--  2.0 unx     2795 b- defN 23-May-08 12:17 monai/losses/giou_loss.py
+-rw-r--r--  2.0 unx    15492 b- defN 23-May-08 12:17 monai/losses/image_dissimilarity.py
+-rw-r--r--  2.0 unx     3636 b- defN 23-May-08 12:17 monai/losses/multi_scale.py
+-rw-r--r--  2.0 unx     2942 b- defN 23-May-08 12:17 monai/losses/spatial_mask.py
+-rw-r--r--  2.0 unx     4825 b- defN 23-May-08 12:17 monai/losses/ssim_loss.py
+-rw-r--r--  2.0 unx     6645 b- defN 23-May-08 12:17 monai/losses/tversky.py
+-rw-r--r--  2.0 unx    10224 b- defN 23-May-08 12:17 monai/losses/unified_focal_loss.py
+-rw-r--r--  2.0 unx     1977 b- defN 23-May-08 12:17 monai/metrics/__init__.py
+-rw-r--r--  2.0 unx     8211 b- defN 23-May-08 12:17 monai/metrics/active_learning_metrics.py
+-rw-r--r--  2.0 unx    15101 b- defN 23-May-08 12:17 monai/metrics/confusion_matrix.py
+-rw-r--r--  2.0 unx     5578 b- defN 23-May-08 12:17 monai/metrics/cumulative_average.py
+-rw-r--r--  2.0 unx     4026 b- defN 23-May-08 12:17 monai/metrics/f_beta_score.py
+-rw-r--r--  2.0 unx     6157 b- defN 23-May-08 12:17 monai/metrics/froc.py
+-rw-r--r--  2.0 unx     8262 b- defN 23-May-08 12:17 monai/metrics/generalized_dice.py
+-rw-r--r--  2.0 unx    11470 b- defN 23-May-08 12:17 monai/metrics/hausdorff_distance.py
+-rw-r--r--  2.0 unx     4907 b- defN 23-May-08 12:17 monai/metrics/loss_metric.py
+-rw-r--r--  2.0 unx    12472 b- defN 23-May-08 12:17 monai/metrics/meandice.py
+-rw-r--r--  2.0 unx     7209 b- defN 23-May-08 12:17 monai/metrics/meaniou.py
+-rw-r--r--  2.0 unx    15140 b- defN 23-May-08 12:17 monai/metrics/metric.py
+-rw-r--r--  2.0 unx    13679 b- defN 23-May-08 12:17 monai/metrics/panoptic_quality.py
+-rw-r--r--  2.0 unx    19719 b- defN 23-May-08 12:17 monai/metrics/regression.py
+-rw-r--r--  2.0 unx     8038 b- defN 23-May-08 12:17 monai/metrics/rocauc.py
+-rw-r--r--  2.0 unx    14415 b- defN 23-May-08 12:17 monai/metrics/surface_dice.py
+-rw-r--r--  2.0 unx    10186 b- defN 23-May-08 12:17 monai/metrics/surface_distance.py
+-rw-r--r--  2.0 unx    15094 b- defN 23-May-08 12:17 monai/metrics/utils.py
+-rw-r--r--  2.0 unx    11772 b- defN 23-May-08 12:17 monai/metrics/wrapper.py
+-rw-r--r--  2.0 unx     1020 b- defN 23-May-08 12:17 monai/networks/__init__.py
+-rw-r--r--  2.0 unx    46938 b- defN 23-May-08 12:17 monai/networks/utils.py
+-rw-r--r--  2.0 unx     2134 b- defN 23-May-08 12:17 monai/networks/blocks/__init__.py
+-rw-r--r--  2.0 unx     4275 b- defN 23-May-08 12:17 monai/networks/blocks/acti_norm.py
+-rw-r--r--  2.0 unx     5839 b- defN 23-May-08 12:17 monai/networks/blocks/activation.py
+-rw-r--r--  2.0 unx     4380 b- defN 23-May-08 12:17 monai/networks/blocks/aspp.py
+-rw-r--r--  2.0 unx     7488 b- defN 23-May-08 12:17 monai/networks/blocks/backbone_fpn_utils.py
+-rw-r--r--  2.0 unx    11686 b- defN 23-May-08 12:17 monai/networks/blocks/convolutions.py
+-rw-r--r--  2.0 unx     5009 b- defN 23-May-08 12:17 monai/networks/blocks/crf.py
+-rw-r--r--  2.0 unx     4740 b- defN 23-May-08 12:17 monai/networks/blocks/denseblock.py
+-rw-r--r--  2.0 unx     9255 b- defN 23-May-08 12:17 monai/networks/blocks/dints_block.py
+-rw-r--r--  2.0 unx     2413 b- defN 23-May-08 12:17 monai/networks/blocks/downsample.py
+-rw-r--r--  2.0 unx    11062 b- defN 23-May-08 12:17 monai/networks/blocks/dynunet_block.py
+-rw-r--r--  2.0 unx     3669 b- defN 23-May-08 12:17 monai/networks/blocks/encoder.py
+-rw-r--r--  2.0 unx     9024 b- defN 23-May-08 12:17 monai/networks/blocks/fcn.py
+-rw-r--r--  2.0 unx    10586 b- defN 23-May-08 12:17 monai/networks/blocks/feature_pyramid_network.py
+-rw-r--r--  2.0 unx     8263 b- defN 23-May-08 12:17 monai/networks/blocks/fft_utils_t.py
+-rw-r--r--  2.0 unx    11454 b- defN 23-May-08 12:17 monai/networks/blocks/localnet_block.py
+-rw-r--r--  2.0 unx     2813 b- defN 23-May-08 12:17 monai/networks/blocks/mlp.py
+-rw-r--r--  2.0 unx     7987 b- defN 23-May-08 12:17 monai/networks/blocks/patchembedding.py
+-rw-r--r--  2.0 unx     8825 b- defN 23-May-08 12:17 monai/networks/blocks/regunet_block.py
+-rw-r--r--  2.0 unx     3245 b- defN 23-May-08 12:17 monai/networks/blocks/segresnet_block.py
+-rw-r--r--  2.0 unx     3099 b- defN 23-May-08 12:17 monai/networks/blocks/selfattention.py
+-rw-r--r--  2.0 unx    12752 b- defN 23-May-08 12:17 monai/networks/blocks/squeeze_and_excitation.py
+-rw-r--r--  2.0 unx     3811 b- defN 23-May-08 12:17 monai/networks/blocks/text_embedding.py
+-rw-r--r--  2.0 unx     2322 b- defN 23-May-08 12:17 monai/networks/blocks/transformerblock.py
+-rw-r--r--  2.0 unx     9049 b- defN 23-May-08 12:17 monai/networks/blocks/unetr_block.py
+-rw-r--r--  2.0 unx    13312 b- defN 23-May-08 12:17 monai/networks/blocks/upsample.py
+-rw-r--r--  2.0 unx     6656 b- defN 23-May-08 12:17 monai/networks/blocks/warp.py
+-rw-r--r--  2.0 unx     1562 b- defN 23-May-08 12:17 monai/networks/layers/__init__.py
+-rw-r--r--  2.0 unx     8288 b- defN 23-May-08 12:17 monai/networks/layers/convutils.py
+-rw-r--r--  2.0 unx     1802 b- defN 23-May-08 12:17 monai/networks/layers/drop_path.py
+-rw-r--r--  2.0 unx    12638 b- defN 23-May-08 12:17 monai/networks/layers/factories.py
+-rw-r--r--  2.0 unx    17992 b- defN 23-May-08 12:17 monai/networks/layers/filtering.py
+-rw-r--r--  2.0 unx     3324 b- defN 23-May-08 12:17 monai/networks/layers/gmm.py
+-rw-r--r--  2.0 unx    28470 b- defN 23-May-08 12:17 monai/networks/layers/simplelayers.py
+-rw-r--r--  2.0 unx    25576 b- defN 23-May-08 12:17 monai/networks/layers/spatial_transforms.py
+-rw-r--r--  2.0 unx     4296 b- defN 23-May-08 12:17 monai/networks/layers/utils.py
+-rw-r--r--  2.0 unx     2253 b- defN 23-May-08 12:17 monai/networks/layers/weight_init.py
+-rw-r--r--  2.0 unx     3141 b- defN 23-May-08 12:17 monai/networks/nets/__init__.py
+-rw-r--r--  2.0 unx    21533 b- defN 23-May-08 12:17 monai/networks/nets/ahnet.py
+-rw-r--r--  2.0 unx     9202 b- defN 23-May-08 12:17 monai/networks/nets/attentionunet.py
+-rw-r--r--  2.0 unx    12089 b- defN 23-May-08 12:17 monai/networks/nets/autoencoder.py
+-rw-r--r--  2.0 unx    10950 b- defN 23-May-08 12:17 monai/networks/nets/basic_unet.py
+-rw-r--r--  2.0 unx     7960 b- defN 23-May-08 12:17 monai/networks/nets/basic_unetplusplus.py
+-rw-r--r--  2.0 unx     6293 b- defN 23-May-08 12:17 monai/networks/nets/classifier.py
+-rw-r--r--  2.0 unx    15820 b- defN 23-May-08 12:17 monai/networks/nets/densenet.py
+-rw-r--r--  2.0 unx    44771 b- defN 23-May-08 12:17 monai/networks/nets/dints.py
+-rw-r--r--  2.0 unx    18337 b- defN 23-May-08 12:17 monai/networks/nets/dynunet.py
+-rw-r--r--  2.0 unx    40643 b- defN 23-May-08 12:17 monai/networks/nets/efficientnet.py
+-rw-r--r--  2.0 unx    14147 b- defN 23-May-08 12:17 monai/networks/nets/flexible_unet.py
+-rw-r--r--  2.0 unx     7212 b- defN 23-May-08 12:17 monai/networks/nets/fullyconnectednet.py
+-rw-r--r--  2.0 unx     6581 b- defN 23-May-08 12:17 monai/networks/nets/generator.py
+-rw-r--r--  2.0 unx     8882 b- defN 23-May-08 12:17 monai/networks/nets/highresnet.py
+-rw-r--r--  2.0 unx    28678 b- defN 23-May-08 12:17 monai/networks/nets/hovernet.py
+-rw-r--r--  2.0 unx     9812 b- defN 23-May-08 12:17 monai/networks/nets/milmodel.py
+-rw-r--r--  2.0 unx     6102 b- defN 23-May-08 12:17 monai/networks/nets/netadapter.py
+-rw-r--r--  2.0 unx     6488 b- defN 23-May-08 12:17 monai/networks/nets/regressor.py
+-rw-r--r--  2.0 unx    18662 b- defN 23-May-08 12:17 monai/networks/nets/regunet.py
+-rw-r--r--  2.0 unx    16785 b- defN 23-May-08 12:17 monai/networks/nets/resnet.py
+-rw-r--r--  2.0 unx    13994 b- defN 23-May-08 12:17 monai/networks/nets/segresnet.py
+-rw-r--r--  2.0 unx    15667 b- defN 23-May-08 12:17 monai/networks/nets/segresnet_ds.py
+-rw-r--r--  2.0 unx    19289 b- defN 23-May-08 12:17 monai/networks/nets/senet.py
+-rw-r--r--  2.0 unx    42000 b- defN 23-May-08 12:17 monai/networks/nets/swin_unetr.py
+-rw-r--r--  2.0 unx     6309 b- defN 23-May-08 12:17 monai/networks/nets/torchvision_fc.py
+-rw-r--r--  2.0 unx    16626 b- defN 23-May-08 12:17 monai/networks/nets/transchex.py
+-rw-r--r--  2.0 unx    13722 b- defN 23-May-08 12:17 monai/networks/nets/unet.py
+-rw-r--r--  2.0 unx     7943 b- defN 23-May-08 12:17 monai/networks/nets/unetr.py
+-rw-r--r--  2.0 unx     6285 b- defN 23-May-08 12:17 monai/networks/nets/varautoencoder.py
+-rw-r--r--  2.0 unx     5655 b- defN 23-May-08 12:17 monai/networks/nets/vit.py
+-rw-r--r--  2.0 unx     4817 b- defN 23-May-08 12:17 monai/networks/nets/vitautoenc.py
+-rw-r--r--  2.0 unx    10011 b- defN 23-May-08 12:17 monai/networks/nets/vnet.py
+-rw-r--r--  2.0 unx      796 b- defN 23-May-08 12:17 monai/optimizers/__init__.py
+-rw-r--r--  2.0 unx    21952 b- defN 23-May-08 12:17 monai/optimizers/lr_finder.py
+-rw-r--r--  2.0 unx     3652 b- defN 23-May-08 12:17 monai/optimizers/lr_scheduler.py
+-rw-r--r--  2.0 unx     5661 b- defN 23-May-08 12:17 monai/optimizers/novograd.py
+-rw-r--r--  2.0 unx     4131 b- defN 23-May-08 12:17 monai/optimizers/utils.py
+-rw-r--r--  2.0 unx    15268 b- defN 23-May-08 12:17 monai/transforms/__init__.py
+-rw-r--r--  2.0 unx     8946 b- defN 23-May-08 12:17 monai/transforms/adaptors.py
+-rw-r--r--  2.0 unx    40812 b- defN 23-May-08 12:17 monai/transforms/compose.py
+-rw-r--r--  2.0 unx    18604 b- defN 23-May-08 12:17 monai/transforms/inverse.py
+-rw-r--r--  2.0 unx     7054 b- defN 23-May-08 12:17 monai/transforms/inverse_batch_transform.py
+-rw-r--r--  2.0 unx     3386 b- defN 23-May-08 12:17 monai/transforms/nvtx.py
+-rw-r--r--  2.0 unx     2885 b- defN 23-May-08 12:17 monai/transforms/traits.py
+-rw-r--r--  2.0 unx    18234 b- defN 23-May-08 12:17 monai/transforms/transform.py
+-rw-r--r--  2.0 unx    77805 b- defN 23-May-08 12:17 monai/transforms/utils.py
+-rw-r--r--  2.0 unx    31081 b- defN 23-May-08 12:17 monai/transforms/utils_create_transform_ims.py
+-rw-r--r--  2.0 unx    18397 b- defN 23-May-08 12:17 monai/transforms/utils_pytorch_numpy_unification.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/transforms/croppad/__init__.py
+-rw-r--r--  2.0 unx    68231 b- defN 23-May-08 12:17 monai/transforms/croppad/array.py
+-rw-r--r--  2.0 unx     6194 b- defN 23-May-08 12:17 monai/transforms/croppad/batch.py
+-rw-r--r--  2.0 unx    54071 b- defN 23-May-08 12:17 monai/transforms/croppad/dictionary.py
+-rw-r--r--  2.0 unx    12673 b- defN 23-May-08 12:17 monai/transforms/croppad/functional.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/transforms/intensity/__init__.py
+-rw-r--r--  2.0 unx    98613 b- defN 23-May-08 12:17 monai/transforms/intensity/array.py
+-rw-r--r--  2.0 unx    76501 b- defN 23-May-08 12:17 monai/transforms/intensity/dictionary.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/transforms/io/__init__.py
+-rw-r--r--  2.0 unx    25430 b- defN 23-May-08 12:17 monai/transforms/io/array.py
+-rw-r--r--  2.0 unx    17821 b- defN 23-May-08 12:17 monai/transforms/io/dictionary.py
+-rw-r--r--  2.0 unx      699 b- defN 23-May-08 12:17 monai/transforms/lazy/__init__.py
+-rw-r--r--  2.0 unx     5788 b- defN 23-May-08 12:17 monai/transforms/lazy/functional.py
+-rw-r--r--  2.0 unx     9851 b- defN 23-May-08 12:17 monai/transforms/lazy/utils.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/transforms/meta_utility/__init__.py
+-rw-r--r--  2.0 unx     4896 b- defN 23-May-08 12:17 monai/transforms/meta_utility/dictionary.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/transforms/post/__init__.py
+-rw-r--r--  2.0 unx    40858 b- defN 23-May-08 12:17 monai/transforms/post/array.py
+-rw-r--r--  2.0 unx    39905 b- defN 23-May-08 12:17 monai/transforms/post/dictionary.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/transforms/signal/__init__.py
+-rw-r--r--  2.0 unx    16378 b- defN 23-May-08 12:17 monai/transforms/signal/array.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/transforms/smooth_field/__init__.py
+-rw-r--r--  2.0 unx    17833 b- defN 23-May-08 12:17 monai/transforms/smooth_field/array.py
+-rw-r--r--  2.0 unx    11194 b- defN 23-May-08 12:17 monai/transforms/smooth_field/dictionary.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/transforms/spatial/__init__.py
+-rw-r--r--  2.0 unx   166745 b- defN 23-May-08 12:17 monai/transforms/spatial/array.py
+-rw-r--r--  2.0 unx   107151 b- defN 23-May-08 12:17 monai/transforms/spatial/dictionary.py
+-rw-r--r--  2.0 unx    31773 b- defN 23-May-08 12:17 monai/transforms/spatial/functional.py
+-rw-r--r--  2.0 unx      573 b- defN 23-May-08 12:17 monai/transforms/utility/__init__.py
+-rw-r--r--  2.0 unx    70839 b- defN 23-May-08 12:17 monai/transforms/utility/array.py
+-rw-r--r--  2.0 unx    76330 b- defN 23-May-08 12:17 monai/transforms/utility/dictionary.py
+-rw-r--r--  2.0 unx     3380 b- defN 23-May-08 12:17 monai/utils/__init__.py
+-rw-r--r--  2.0 unx     4096 b- defN 23-May-08 12:17 monai/utils/aliases.py
+-rw-r--r--  2.0 unx     3129 b- defN 23-May-08 12:17 monai/utils/decorators.py
+-rw-r--r--  2.0 unx    14759 b- defN 23-May-08 12:17 monai/utils/deprecate_utils.py
+-rw-r--r--  2.0 unx     8526 b- defN 23-May-08 12:17 monai/utils/dist.py
+-rw-r--r--  2.0 unx    16807 b- defN 23-May-08 12:17 monai/utils/enums.py
+-rw-r--r--  2.0 unx    15637 b- defN 23-May-08 12:17 monai/utils/jupyter_utils.py
+-rw-r--r--  2.0 unx    28185 b- defN 23-May-08 12:17 monai/utils/misc.py
+-rw-r--r--  2.0 unx    23631 b- defN 23-May-08 12:17 monai/utils/module.py
+-rw-r--r--  2.0 unx     6876 b- defN 23-May-08 12:17 monai/utils/nvtx.py
+-rw-r--r--  2.0 unx    15936 b- defN 23-May-08 12:17 monai/utils/profiling.py
+-rw-r--r--  2.0 unx     5955 b- defN 23-May-08 12:17 monai/utils/state_cacher.py
+-rw-r--r--  2.0 unx    21147 b- defN 23-May-08 12:17 monai/utils/type_conversion.py
+-rw-r--r--  2.0 unx     1038 b- defN 23-May-08 12:17 monai/visualize/__init__.py
+-rw-r--r--  2.0 unx    16156 b- defN 23-May-08 12:17 monai/visualize/class_activation_maps.py
+-rw-r--r--  2.0 unx     6277 b- defN 23-May-08 12:17 monai/visualize/gradient_based.py
+-rw-r--r--  2.0 unx     9200 b- defN 23-May-08 12:17 monai/visualize/img2tensorboard.py
+-rw-r--r--  2.0 unx    18816 b- defN 23-May-08 12:17 monai/visualize/occlusion_sensitivity.py
+-rw-r--r--  2.0 unx     9966 b- defN 23-May-08 12:17 monai/visualize/utils.py
+-rw-r--r--  2.0 unx     1377 b- defN 23-May-08 12:17 monai/visualize/visualizer.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-May-08 12:19 monai-1.2.0rc6.dist-info/LICENSE
+-rw-r--r--  2.0 unx    10172 b- defN 23-May-08 12:19 monai-1.2.0rc6.dist-info/METADATA
+-rw-r--r--  2.0 unx      112 b- defN 23-May-08 12:19 monai-1.2.0rc6.dist-info/WHEEL
+-rw-r--r--  2.0 unx        6 b- defN 23-May-08 12:19 monai-1.2.0rc6.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    32699 b- defN 23-May-08 12:19 monai-1.2.0rc6.dist-info/RECORD
+365 files, 4768002 bytes uncompressed, 1206037 bytes compressed:  74.7%
```

## zipnote {}

```diff
@@ -1074,23 +1074,23 @@
 
 Filename: monai/visualize/utils.py
 Comment: 
 
 Filename: monai/visualize/visualizer.py
 Comment: 
 
-Filename: monai-1.2.0rc5.dist-info/LICENSE
+Filename: monai-1.2.0rc6.dist-info/LICENSE
 Comment: 
 
-Filename: monai-1.2.0rc5.dist-info/METADATA
+Filename: monai-1.2.0rc6.dist-info/METADATA
 Comment: 
 
-Filename: monai-1.2.0rc5.dist-info/WHEEL
+Filename: monai-1.2.0rc6.dist-info/WHEEL
 Comment: 
 
-Filename: monai-1.2.0rc5.dist-info/top_level.txt
+Filename: monai-1.2.0rc6.dist-info/top_level.txt
 Comment: 
 
-Filename: monai-1.2.0rc5.dist-info/RECORD
+Filename: monai-1.2.0rc6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## monai/_version.py

```diff
@@ -4,18 +4,18 @@
 # unpacked source archive. Distribution tarballs contain a pre-generated copy
 # of this file.
 
 import json
 
 version_json = '''
 {
- "date": "2023-04-23T21:17:43+0100",
+ "date": "2023-05-08T13:06:25+0100",
  "dirty": false,
  "error": null,
- "full-revisionid": "9c9777751ab4f96e059a6597b9aa7ac6e7ca3b92",
- "version": "1.2.0rc5"
+ "full-revisionid": "bf5f406de40cdc5b0b19600bf57be7babff89b1b",
+ "version": "1.2.0rc6"
 }
 '''  # END VERSION_JSON
 
 
 def get_versions():
     return json.loads(version_json)
```

## monai/apps/auto3dseg/__init__.py

```diff
@@ -18,8 +18,8 @@
     AlgoEnsemble,
     AlgoEnsembleBestByFold,
     AlgoEnsembleBestN,
     AlgoEnsembleBuilder,
     EnsembleRunner,
 )
 from .hpo_gen import NNIGen, OptunaGen
-from .utils import export_bundle_algo_history, import_bundle_algo_history
+from .utils import export_bundle_algo_history, get_name_from_algo_id, import_bundle_algo_history
```

## monai/apps/auto3dseg/auto_runner.py

```diff
@@ -103,90 +103,81 @@
             python -m monai.apps.auto3dseg AutoRunner run --input=./input.yaml
 
         - User can specify work_dir and data source config input and run AutoRunner:
 
         .. code-block:: python
 
             work_dir = "./work_dir"
-            input = "path_to_yaml_data_cfg"
+            input = "path/to/input_yaml"
             runner = AutoRunner(work_dir=work_dir, input=input)
             runner.run()
 
         - User can specify a subset of algorithms to use and run AutoRunner:
 
         .. code-block:: python
 
             work_dir = "./work_dir"
-            input = "path_to_yaml_data_cfg"
+            input = "path/to/input_yaml"
             algos = ["segresnet", "dints"]
             runner = AutoRunner(work_dir=work_dir, input=input, algos=algos)
             runner.run()
 
         - User can specify a a local folder with algorithms templates and run AutoRunner:
 
         .. code-block:: python
 
             work_dir = "./work_dir"
-            input = "path_to_yaml_data_cfg"
+            input = "path/to/input_yaml"
             algos = "segresnet"
             templates_path_or_url = "./local_path_to/algorithm_templates"
             runner = AutoRunner(work_dir=work_dir, input=input, algos=algos, templates_path_or_url=templates_path_or_url)
             runner.run()
 
         - User can specify training parameters by:
 
         .. code-block:: python
 
-            input = "path_to_yaml_data_cfg"
+            input = "path/to/input_yaml"
             runner = AutoRunner(input=input)
             train_param = {
-                "CUDA_VISIBLE_DEVICES": [0],
-                "num_iterations": 8,
-                "num_iterations_per_validation": 4,
+                "num_epochs_per_validation": 1,
                 "num_images_per_batch": 2,
                 "num_epochs": 2,
             }
             runner.set_training_params(params=train_param)  # 2 epochs
             runner.run()
 
         - User can specify the fold number of cross validation
 
         .. code-block:: python
 
-            input = "path_to_yaml_data_cfg"
+            input = "path/to/input_yaml"
             runner = AutoRunner(input=input)
             runner.set_num_fold(n_fold = 2)
             runner.run()
 
         - User can specify the prediction parameters during algo ensemble inference:
 
         .. code-block:: python
 
-            input = "path_to_yaml_data_cfg"
+            input = "path/to/input_yaml"
             pred_params = {
                 'files_slices': slice(0,2),
                 'mode': "vote",
                 'sigmoid': True,
             }
             runner = AutoRunner(input=input)
             runner.set_prediction_params(params=pred_params)
             runner.run()
 
         - User can define a grid search space and use the HPO during training.
 
         .. code-block:: python
 
-            input = "path_to_yaml_data_cfg"
-            pred_param = {
-                "CUDA_VISIBLE_DEVICES": [0],
-                "num_iterations": 8,
-                "num_iterations_per_validation": 4,
-                "num_images_per_batch": 2,
-                "num_epochs": 2,
-            }
+            input = "path/to/input_yaml"
             runner = AutoRunner(input=input, hpo=True)
             runner.set_nni_search_space({"learning_rate": {"_type": "choice", "_value": [0.0001, 0.001, 0.01, 0.1]}})
             runner.run()
 
     Notes:
         Expected results in the work_dir as below::
 
@@ -467,15 +458,15 @@
 
         Args:
             params: a dict that defines the overriding key-value pairs during training. The overriding method
                 is defined by the algo class.
 
         Examples:
             For BundleAlgo objects, the training parameter to shorten the training time to a few epochs can be
-                {"num_iterations": 8, "num_iterations_per_validation": 4}
+                {"num_epochs": 2, "num_epochs_per_validation": 1}
 
         """
         self.train_params = deepcopy(params) if params is not None else {}
         if "CUDA_VISIBLE_DEVICES" in self.train_params:
             warnings.warn(
                 "CUDA_VISIBLE_DEVICES is deprecated from 'set_training_params'. Use 'set_device_info' intead.",
                 DeprecationWarning,
@@ -559,20 +550,20 @@
 
         Args:
             kwargs: image writing parameters for the ensemble inference. The kwargs format follows SaveImage
                 transform. For more information, check https://docs.monai.io/en/stable/transforms.html#saveimage.
 
         """
 
-        are_all_args_present, missing_args = check_kwargs_exist_in_class_init(SaveImage, kwargs)
+        are_all_args_present, extra_args = check_kwargs_exist_in_class_init(SaveImage, kwargs)
         if are_all_args_present:
             self.kwargs.update(kwargs)
         else:
             raise ValueError(
-                f"{missing_args} are not supported in monai.transforms.SaveImage,"
+                f"{extra_args} are not supported in monai.transforms.SaveImage,"
                 "Check https://docs.monai.io/en/stable/transforms.html#saveimage for more information."
             )
 
     def set_prediction_params(self, params: dict[str, Any] | None = None) -> None:
         """
         Set the prediction params for all algos.
 
@@ -592,18 +583,14 @@
         """
         Set the data analysis extra params.
 
         Args:
             params: a dict that defines the overriding key-value pairs during training. The overriding method
                 is defined by the algo class.
 
-        Examples:
-            For BundleAlgo objects, the training parameter to shorten the training time to a few epochs can be
-                {"num_iterations": 8, "num_iterations_per_validation": 4}
-
         """
         if params is None:
             self.analyze_params = {"do_ccp": False, "device": "cuda"}
         else:
             self.analyze_params = deepcopy(params)
 
     def set_hpo_params(self, params: dict[str, Any] | None = None) -> None:
```

## monai/apps/auto3dseg/bundle_gen.py

```diff
@@ -28,19 +28,20 @@
 import torch
 
 from monai.apps import download_and_extract
 from monai.apps.utils import get_logger
 from monai.auto3dseg.algo_gen import Algo, AlgoGen
 from monai.auto3dseg.utils import algo_to_pickle
 from monai.bundle.config_parser import ConfigParser
+from monai.config import PathLike
 from monai.utils import ensure_tuple
 from monai.utils.enums import AlgoKeys
 
 logger = get_logger(module_name=__name__)
-ALGO_HASH = os.environ.get("MONAI_ALGO_HASH", "23ea143")
+ALGO_HASH = os.environ.get("MONAI_ALGO_HASH", "14a695e")
 
 __all__ = ["BundleAlgo", "BundleGen"]
 
 
 class BundleAlgo(Algo):
     """
     An algorithm represented by a set of bundle configurations and scripts.
@@ -59,15 +60,15 @@
 
     This class creates MONAI bundles from a directory of 'bundle template'. Different from the regular MONAI bundle
     format, the bundle template may contain placeholders that must be filled using ``fill_template_config`` during
     ``export_to_disk``. Then created bundle keeps the same file structure as the template.
 
     """
 
-    def __init__(self, template_path: str):
+    def __init__(self, template_path: PathLike):
         """
         Create an Algo instance based on the predefined Algo template.
 
         Args:
             template_path: path to the root of the algo template.
 
         """
@@ -149,17 +150,17 @@
 
         """
         if kwargs.pop("copy_dirs", True):
             self.output_path = os.path.join(output_path, algo_name)
             os.makedirs(self.output_path, exist_ok=True)
             if os.path.isdir(self.output_path):
                 shutil.rmtree(self.output_path)
-            shutil.copytree(self.template_path, self.output_path)
+            shutil.copytree(str(self.template_path), self.output_path)
         else:
-            self.output_path = self.template_path
+            self.output_path = str(self.template_path)
         if kwargs.pop("fill_template", True):
             self.fill_records = self.fill_template_config(self.data_stats_files, self.output_path, **kwargs)
         logger.info(f"Generated:{self.output_path}")
 
     def _create_cmd(self, train_params: None | dict = None) -> tuple[str, str]:
         """
         Create the command to execute training.
```

## monai/apps/auto3dseg/data_analyzer.py

```diff
@@ -234,15 +234,14 @@
             hist_bins=self.hist_bins,
             hist_range=self.hist_range,
             histogram_only=self.histogram_only,
         )
         n_cases = len(result_bycase[DataStatsKeys.BY_CASE])
         result[DataStatsKeys.SUMMARY] = summarizer.summarize(cast(list, result_bycase[DataStatsKeys.BY_CASE]))
         result[DataStatsKeys.SUMMARY]["n_cases"] = n_cases
-        result[DataStatsKeys.BY_CASE] = [None] * n_cases
         result_bycase[DataStatsKeys.SUMMARY] = result[DataStatsKeys.SUMMARY]
         if not self._check_data_uniformity([ImageStatsKeys.SPACING], result):
             logger.info("Data spacing is not completely uniform. MONAI transforms may provide unexpected result")
         if self.output_path:
             ConfigParser.export_config_file(
                 result, self.output_path, fmt=self.fmt, default_flow_style=None, sort_keys=False
             )
```

## monai/apps/auto3dseg/ensemble_builder.py

```diff
@@ -20,21 +20,22 @@
 from warnings import warn
 
 import numpy as np
 import torch
 import torch.distributed as dist
 
 from monai.apps.auto3dseg.bundle_gen import BundleAlgo
-from monai.apps.auto3dseg.utils import import_bundle_algo_history
+from monai.apps.auto3dseg.utils import get_name_from_algo_id, import_bundle_algo_history
 from monai.apps.utils import get_logger
 from monai.auto3dseg import concat_val_to_np
 from monai.auto3dseg.utils import datafold_read
 from monai.bundle import ConfigParser
 from monai.data import partition_dataset
 from monai.transforms import MeanEnsemble, SaveImage, VoteEnsemble
+from monai.utils import RankFilter, deprecated_arg
 from monai.utils.enums import AlgoKeys
 from monai.utils.misc import check_kwargs_exist_in_class_init, prob2class
 from monai.utils.module import look_up_option, optional_import
 
 tqdm, has_tqdm = optional_import("tqdm", name="tqdm")
 
 logger = get_logger(module_name=__name__)
@@ -122,31 +123,60 @@
         elif self.mode == "vote":
             classes = [prob2class(p, dim=0, keepdim=True, sigmoid=sigmoid) for p in preds]
             if sigmoid:
                 return VoteEnsemble()(classes)  # do not specify num_classes for one-hot encoding
             else:
                 return VoteEnsemble(num_classes=preds[0].shape[0])(classes)
 
+    def _apply_algo_specific_param(self, algo_spec_param: dict, param: dict, algo_name: str) -> dict:
+        """
+        Apply the model-specific params to the prediction params based on the name of the Algo.
+
+        Args:
+            algo_spec_param: a dict that has structure of {"<name of algo>": "<pred_params for that algo>"}.
+            param: the prediction params to override.
+            algo_name: name of the Algo
+
+        Returns:
+            param after being updated with the model-specific param
+        """
+        _param_to_override = deepcopy(algo_spec_param)
+        _param = deepcopy(param)
+        for k, v in _param_to_override.items():
+            if k.lower() == algo_name.lower():
+                _param.update(v)
+        return _param
+
     def __call__(self, pred_param: dict | None = None) -> list:
         """
         Use the ensembled model to predict result.
 
         Args:
             pred_param: prediction parameter dictionary. The key has two groups: the first one will be consumed
                 in this function, and the second group will be passed to the `InferClass` to override the
                 parameters of the class functions.
                 The first group contains:
-                'files_slices': a value type of `slice`. The files_slices will slice the infer_files and only
-                    make prediction on the infer_files[file_slices].
-                'mode': ensemble mode. Currently "mean" and "vote" (majority voting) schemes are supported.
-                'sigmoid': use the sigmoid function (e.g. x>0.5) to convert the prediction probability map to
-                    the label class prediction, otherwise argmax(x) is used.
+
+                    - ``"infer_files"``: file paths to the images to read in a list.
+                    - ``"files_slices"``: a value type of `slice`. The files_slices will slice the ``"infer_files"`` and
+                      only make prediction on the infer_files[file_slices].
+                    - ``"mode"``: ensemble mode. Currently "mean" and "vote" (majority voting) schemes are supported.
+                    - ``"image_save_func"``: a dictionary used to instantiate the ``SaveImage`` transform. When specified,
+                      the ensemble prediction will save the prediciton files, instead of keeping the files in the memory.
+                      Example: `{"_target_": "SaveImage", "output_dir": "./"}`
+                    - ``"sigmoid"``: use the sigmoid function (e.g. x > 0.5) to convert the prediction probability map
+                      to the label class prediction, otherwise argmax(x) is used.
+                    - ``"algo_spec_params"``: a dictionary to add pred_params that are specific to a model.
+                      The dict has a format of {"<name of algo>": "<pred_params for that algo>"}.
+
+                The parameters in the second group is defined in the ``config`` of each Algo templates. Please check:
+                https://github.com/Project-MONAI/research-contributions/tree/main/auto3dseg/algorithm_templates
 
         Returns:
-            A list of tensors.
+            A list of tensors or file paths, depending on whether ``"image_save_func"`` is set.
         """
         param = {} if pred_param is None else deepcopy(pred_param)
         files = self.infer_files
 
         if "infer_files" in param:
             files = param.pop("infer_files")
 
@@ -159,24 +189,28 @@
             self.mode = look_up_option(mode, supported=["mean", "vote"])
 
         sigmoid = param.pop("sigmoid", False)
 
         if "image_save_func" in param:
             img_saver = ConfigParser(param["image_save_func"]).get_parsed_content()
 
+        algo_spec_params = param.pop("algo_spec_params", {})
+
         outputs = []
         for _, file in (
             enumerate(tqdm(files, desc="Ensembling (rank 0)..."))
             if has_tqdm and pred_param and pred_param.get("rank", 0) == 0
             else enumerate(files)
         ):
             preds = []
             for algo in self.algo_ensemble:
+                infer_algo_name = get_name_from_algo_id(algo[AlgoKeys.ID])
                 infer_instance = algo[AlgoKeys.ALGO]
-                pred = infer_instance.predict(predict_files=[file], predict_params=param)
+                _param = self._apply_algo_specific_param(algo_spec_params, param, infer_algo_name)
+                pred = infer_instance.predict(predict_files=[file], predict_params=_param)
                 preds.append(pred[0])
             if "image_save_func" in param:
                 try:
                     ensemble_preds = self.ensemble_pred(preds, sigmoid=sigmoid)
                 except BaseException:
                     ensemble_preds = self.ensemble_pred([_.to("cpu") for _ in preds], sigmoid=sigmoid)
                 res = img_saver(ensemble_preds)
@@ -277,33 +311,40 @@
 
 class AlgoEnsembleBuilder:
     """
     Build ensemble workflow from configs and arguments.
 
     Args:
         history: a collection of trained bundleAlgo algorithms.
-        data_src_cfg_filename: filename of the data source.
+        data_src_cfg_name: filename of the data source.
 
     Examples:
 
         .. code-block:: python
 
             builder = AlgoEnsembleBuilder(history, data_src_cfg)
             builder.set_ensemble_method(BundleAlgoEnsembleBestN(3))
             ensemble = builder.get_ensemble()
 
     """
 
-    def __init__(self, history: Sequence[dict[str, Any]], data_src_cfg_filename: str | None = None):
+    @deprecated_arg(
+        "data_src_cfg_filename",
+        since="1.2",
+        removed="1.3",
+        new_name="data_src_cfg_name",
+        msg_suffix="please use `data_src_cfg_name` instead.",
+    )
+    def __init__(self, history: Sequence[dict[str, Any]], data_src_cfg_name: str | None = None):
         self.infer_algos: list[dict[AlgoKeys, Any]] = []
         self.ensemble: AlgoEnsemble
         self.data_src_cfg = ConfigParser(globals=False)
 
-        if data_src_cfg_filename is not None and os.path.exists(str(data_src_cfg_filename)):
-            self.data_src_cfg.read_config(data_src_cfg_filename)
+        if data_src_cfg_name is not None and os.path.exists(str(data_src_cfg_name)):
+            self.data_src_cfg.read_config(data_src_cfg_name)
 
         for algo_dict in history:
             # load inference_config_paths
 
             name = algo_dict[AlgoKeys.ID]
             gen_algo = algo_dict[AlgoKeys.ALGO]
 
@@ -353,25 +394,29 @@
         """Get the ensemble"""
 
         return self.ensemble
 
 
 class EnsembleRunner:
     """
-    The Runner for ensembler
+    The Runner for ensembler. It ensembles predictions and saves them to the disk with a support of using multi-GPU.
 
     Args:
-        work_dir: working directory to save the intermediate and final results.
         data_src_cfg_name: filename of the data source.
-        num_fold: number of fold.
-        ensemble_method_name: method to ensemble predictions from different model.
+        work_dir: working directory to save the intermediate and final results. Default is `./work_dir`.
+        num_fold: number of fold. Default is 5.
+        ensemble_method_name: method to ensemble predictions from different model. Default is AlgoEnsembleBestByFold.
                               Suported methods: ["AlgoEnsembleBestN", "AlgoEnsembleBestByFold"].
-        mgpu: if using multi-gpu.
+        mgpu: if using multi-gpu. Default is True.
         kwargs: additional image writing, ensembling parameters and prediction parameters for the ensemble inference.
-    Examples:
+              - for image saving, please check the supported parameters in SaveImage transform.
+              - for prediction parameters, please check the supported parameters in the ``AlgoEnsemble`` callables.
+              - for ensemble parameters, please check the documentation of the selected AlgoEnsemble callable.
+
+    Example:
 
         .. code-block:: python
 
             ensemble_runner = EnsembleRunner(data_src_cfg_name,
                                              work_dir,
                                              ensemble_method_name,
                                              mgpu=device_setting['n_devices']>1,
@@ -379,15 +424,15 @@
                                              **pred_params)
             ensemble_runner.run(device_setting)
 
     """
 
     def __init__(
         self,
-        data_src_cfg_name: str = "./work_dir/input.yaml",
+        data_src_cfg_name: str,
         work_dir: str = "./work_dir",
         num_fold: int = 5,
         ensemble_method_name: str = "AlgoEnsembleBestByFold",
         mgpu: bool = True,
         **kwargs: Any,
     ) -> None:
         self.data_src_cfg_name = data_src_cfg_name
@@ -402,17 +447,14 @@
             "CUDA_VISIBLE_DEVICES": ",".join([str(x) for x in range(torch.cuda.device_count())]),
             "n_devices": torch.cuda.device_count(),
             "NUM_NODES": int(os.environ.get("NUM_NODES", 1)),
             "MN_START_METHOD": os.environ.get("MN_START_METHOD", "bcprun"),
             "CMD_PREFIX": os.environ.get("CMD_PREFIX"),  # type: ignore
         }
 
-        # self.kwargs needs to pop out args for set_image_save_transform
-        self.save_image: dict[str, Any] = self._pop_kwargs_to_get_image_save_transform(**self.kwargs)
-
     def set_ensemble_method(self, ensemble_method_name: str = "AlgoEnsembleBestByFold", **kwargs: Any) -> None:
         """
         Set the bundle ensemble method
 
         Args:
             ensemble_method_name: the name of the ensemble method. Only two methods are supported "AlgoEnsembleBestN"
                 and "AlgoEnsembleBestByFold".
@@ -449,46 +491,57 @@
             output_dir = os.path.join(self.work_dir, "ensemble_output")
             logger.info(f"The output_dir is not specified. {output_dir} will be used to save ensemble predictions.")
 
         if not os.path.isdir(output_dir):
             os.makedirs(output_dir, exist_ok=True)
             logger.info(f"Directory {output_dir} is created to save ensemble predictions")
 
+        input_yaml = ConfigParser.load_config_file(self.data_src_cfg_name)
+        data_root_dir = input_yaml.get("dataroot", "")
+
         save_image = {
             "_target_": "SaveImage",
             "output_dir": output_dir,
             "output_postfix": kwargs.pop("output_postfix", "ensemble"),
             "output_dtype": kwargs.pop("output_dtype", "$np.uint8"),
             "resample": kwargs.pop("resample", False),
             "print_log": False,
             "savepath_in_metadict": True,
+            "data_root_dir": kwargs.pop("data_root_dir", data_root_dir),
+            "separate_folder": kwargs.pop("separate_folder", False),
         }
 
         are_all_args_save_image, extra_args = check_kwargs_exist_in_class_init(SaveImage, kwargs)
         if are_all_args_save_image:
             save_image.update(kwargs)
         else:
             # kwargs has extra values for other purposes, for example, pred_params
             for args in list(kwargs):
                 if args not in extra_args:
                     save_image.update({args: kwargs.pop(args)})
 
         return save_image
 
-    def set_image_save_transform(self, **kwargs):
+    def set_image_save_transform(self, **kwargs: Any) -> None:
         """
         Set the ensemble output transform.
 
         Args:
             kwargs: image writing parameters for the ensemble inference. The kwargs format follows SaveImage
                 transform. For more information, check https://docs.monai.io/en/stable/transforms.html#saveimage .
 
         """
-        kwargs_copy = deepcopy(kwargs)
-        self.save_image = self._pop_kwargs_to_get_image_save_transform(**kwargs_copy)
+        are_all_args_present, extra_args = check_kwargs_exist_in_class_init(SaveImage, kwargs)
+        if are_all_args_present:
+            self.kwargs.update(kwargs)
+        else:
+            raise ValueError(
+                f"{extra_args} are not supported in monai.transforms.SaveImage,"
+                "Check https://docs.monai.io/en/stable/transforms.html#saveimage for more information."
+            )
 
     def set_num_fold(self, num_fold: int = 5) -> None:
         """
         Set the number of cross validation folds for all algos.
 
         Args:
             num_fold: a positive integer to define the number of folds.
@@ -500,26 +553,28 @@
 
     def ensemble(self):
         if self.mgpu:  # torch.cuda.device_count() is not used because env is not set by autorruner
             # init multiprocessing and update infer_files
             dist.init_process_group(backend="nccl", init_method="env://")
             self.world_size = dist.get_world_size()
             self.rank = dist.get_rank()
+            logger.addFilter(RankFilter())
         # set params after init_process_group to know the rank
         self.set_num_fold(num_fold=self.num_fold)
         self.set_ensemble_method(self.ensemble_method_name, **self.kwargs)
+        # self.kwargs needs to pop out args for set_image_save_transform
+        save_image = self._pop_kwargs_to_get_image_save_transform(**self.kwargs)
 
         history = import_bundle_algo_history(self.work_dir, only_trained=False)
         history_untrained = [h for h in history if not h[AlgoKeys.IS_TRAINED]]
         if history_untrained:
-            if self.rank == 0:
-                warn(
-                    f"Ensembling step will skip {[h[AlgoKeys.ID] for h in history_untrained]} untrained algos."
-                    "Generally it means these algos did not complete training."
-                )
+            logger.warning(
+                f"Ensembling step will skip {[h[AlgoKeys.ID] for h in history_untrained]} untrained algos."
+                "Generally it means these algos did not complete training."
+            )
             history = [h for h in history if h[AlgoKeys.IS_TRAINED]]
         if len(history) == 0:
             raise ValueError(
                 f"Could not find the trained results in {self.work_dir}. "
                 "Possibly the required training step was not completed."
             )
 
@@ -530,21 +585,20 @@
         infer_files = partition_dataset(
             data=infer_files, shuffle=False, num_partitions=self.world_size, even_divisible=True
         )[self.rank]
         # TO DO: Add some function in ensembler for infer_files update?
         self.ensembler.infer_files = infer_files
         # add rank to pred_params
         self.kwargs["rank"] = self.rank
-        self.kwargs["image_save_func"] = self.save_image
-        if self.rank == 0:
-            logger.info("Auto3Dseg picked the following networks to ensemble:")
-            for algo in self.ensembler.get_algo_ensemble():
-                logger.info(algo[AlgoKeys.ID])
-            output_dir = self.save_image["output_dir"]
-            logger.info(f"Auto3Dseg ensemble prediction outputs will be saved in {output_dir}.")
+        self.kwargs["image_save_func"] = save_image
+        logger.info("Auto3Dseg picked the following networks to ensemble:")
+        for algo in self.ensembler.get_algo_ensemble():
+            logger.info(algo[AlgoKeys.ID])
+        output_dir = save_image["output_dir"]
+        logger.info(f"Auto3Dseg ensemble prediction outputs will be saved in {output_dir}.")
         self.ensembler(pred_param=self.kwargs)
 
         if self.mgpu:
             dist.destroy_process_group()
 
     def run(self, device_setting: dict | None = None) -> None:
         """
```

## monai/apps/auto3dseg/hpo_gen.py

```diff
@@ -125,19 +125,15 @@
                     output_folder = os.path.dirname(algo.get_output_path())
 
                     params.update({"fill_with_datastats": False})  # just copy, not using datastats to fill
                     self.algo.export_to_disk(output_folder, name, **params)
             else:
                 self.algo = algo
 
-            if isinstance(self.algo, BundleAlgo):
-                self.obj_filename = algo_to_pickle(self.algo, template_path=self.algo.template_path)
-            else:
-                self.obj_filename = algo_to_pickle(self.algo)
-                # nni instruction unknown
+            self.obj_filename = algo_to_pickle(self.algo, template_path=self.algo.template_path)
 
     def get_obj_filename(self):
         """Return the filename of the dumped pickle algo object."""
         return self.obj_filename
 
     def print_bundle_algo_instruction(self):
         """
@@ -222,32 +218,26 @@
                 ``{algorithm_templates_dir}/{network}/scripts/algo.py``
         """
         if not os.path.isfile(obj_filename):
             raise ValueError(f"{obj_filename} is not found")
 
         self.algo, algo_meta_data = algo_from_pickle(obj_filename, template_path=template_path)
 
-        if isinstance(self.algo, BundleAlgo):  # algo's template path needs override
-            self.algo.template_path = algo_meta_data["template_path"]
-
         # step 1 sample hyperparams
         params = self.get_hyperparameters()
         # step 2 set the update params for the algo to run in the next trial
         self.update_params(params)
         # step 3 generate the folder to save checkpoints and train
         self.generate(output_folder)
         self.algo.train(self.params)
         # step 4 report validation acc to controller
         acc = self.algo.get_score()
         algo_meta_data = {str(AlgoKeys.SCORE): acc}
 
-        if isinstance(self.algo, BundleAlgo):
-            algo_to_pickle(self.algo, template_path=self.algo.template_path, **algo_meta_data)
-        else:
-            algo_to_pickle(self.algo, **algo_meta_data)
+        algo_to_pickle(self.algo, template_path=self.algo.template_path, **algo_meta_data)
         self.set_score(acc)
 
 
 class OptunaGen(HPOGen):
     """
     Generate algorithms for the Optuna to automate hyperparameter tuning. Please refer to NNI and Optuna
     (https://optuna.readthedocs.io/en/stable/) for more information. Optuna has different running scheme
@@ -300,19 +290,15 @@
                     output_folder = os.path.dirname(algo.get_output_path())
 
                     params.update({"fill_with_datastats": False})  # just copy, not using datastats to fill
                     self.algo.export_to_disk(output_folder, name, **params)
             else:
                 self.algo = algo
 
-            if isinstance(self.algo, BundleAlgo):
-                self.obj_filename = algo_to_pickle(self.algo, template_path=self.algo.template_path)
-            else:
-                self.obj_filename = algo_to_pickle(self.algo)
-                # nni instruction unknown
+            self.obj_filename = algo_to_pickle(self.algo, template_path=self.algo.template_path)
 
     def get_obj_filename(self):
         """Return the dumped pickle object of algo."""
         return self.obj_filename
 
     def get_hyperparameters(self):
         """
@@ -395,25 +381,19 @@
                 ``{algorithm_templates_dir}/{network}/scripts/algo.py``
         """
         if not os.path.isfile(obj_filename):
             raise ValueError(f"{obj_filename} is not found")
 
         self.algo, algo_meta_data = algo_from_pickle(obj_filename, template_path=template_path)
 
-        if isinstance(self.algo, BundleAlgo):  # algo's template path needs override
-            self.algo.template_path = algo_meta_data["template_path"]
-
         # step 1 sample hyperparams
         params = self.get_hyperparameters()
         # step 2 set the update params for the algo to run in the next trial
         self.update_params(params)
         # step 3 generate the folder to save checkpoints and train
         self.generate(output_folder)
         self.algo.train(self.params)
         # step 4 report validation acc to controller
         acc = self.algo.get_score()
         algo_meta_data = {str(AlgoKeys.SCORE): acc}
-        if isinstance(self.algo, BundleAlgo):
-            algo_to_pickle(self.algo, template_path=self.algo.template_path, **algo_meta_data)
-        else:
-            algo_to_pickle(self.algo, **algo_meta_data)
+        algo_to_pickle(self.algo, template_path=self.algo.template_path, **algo_meta_data)
         self.set_score(acc)
```

## monai/apps/auto3dseg/transforms.py

```diff
@@ -35,41 +35,51 @@
 
     def __init__(
         self,
         keys: KeysCollection = "label",
         allow_missing_keys: bool = False,
         source_key: str = "image",
         allowed_shape_difference: int = 5,
+        warn: bool = True,
     ) -> None:
         """
         Args:
             keys: keys of the corresponding items to be compared to the source_key item shape.
             allow_missing_keys: do not raise exception if key is missing.
             source_key: key of the item with the reference shape.
             allowed_shape_difference: raises error if shapes are different more than this value in any dimension,
                 otherwise corrects for the shape mismatch using nearest interpolation.
+            warn: if `True` prints a warning if the label image is resized
+
 
         """
         super().__init__(keys=keys, allow_missing_keys=allow_missing_keys)
         self.source_key = source_key
         self.allowed_shape_difference = allowed_shape_difference
+        self.warn = warn
 
     def __call__(self, data: Mapping[Hashable, torch.Tensor]) -> dict[Hashable, torch.Tensor]:
         d = dict(data)
         image_shape = d[self.source_key].shape[1:]
         for key in self.key_iterator(d):
             label_shape = d[key].shape[1:]
             if label_shape != image_shape:
+                filename = ""
+                if hasattr(d[key], "meta") and isinstance(d[key].meta, Mapping):  # type: ignore[attr-defined]
+                    filename = d[key].meta.get(ImageMetaKey.FILENAME_OR_OBJ)  # type: ignore[attr-defined]
+
                 if np.allclose(list(label_shape), list(image_shape), atol=self.allowed_shape_difference):
-                    msg = f"The {key} with shape {label_shape} was resized to match the source shape {image_shape}"
-                    if hasattr(d[key], "meta") and isinstance(d[key].meta, Mapping):  # type: ignore[attr-defined]
-                        filename = d[key].meta.get(ImageMetaKey.FILENAME_OR_OBJ)  # type: ignore[attr-defined]
-                        msg += f", the metadata was not updated: filename={filename}"
-                    warnings.warn(msg)
+                    if self.warn:
+                        warnings.warn(
+                            f"The {key} with shape {label_shape} was resized to match the source shape {image_shape}"
+                            f", the metadata was not updated {filename}."
+                        )
                     d[key] = torch.nn.functional.interpolate(
                         input=d[key].unsqueeze(0),
                         size=image_shape,
                         mode="nearest-exact" if pytorch_after(1, 11) else "nearest",
                     ).squeeze(0)
                 else:
-                    raise ValueError(f"The {key} shape {label_shape} is different from the source shape {image_shape}.")
+                    raise ValueError(
+                        f"The {key} shape {label_shape} is different from the source shape {image_shape} {filename}."
+                    )
         return d
```

## monai/apps/auto3dseg/utils.py

```diff
@@ -13,14 +13,16 @@
 
 import os
 
 from monai.apps.auto3dseg.bundle_gen import BundleAlgo
 from monai.auto3dseg import algo_from_pickle, algo_to_pickle
 from monai.utils.enums import AlgoKeys
 
+__all__ = ["import_bundle_algo_history", "export_bundle_algo_history", "get_name_from_algo_id"]
+
 
 def import_bundle_algo_history(
     output_folder: str = ".", template_path: str | None = None, only_trained: bool = True
 ) -> list:
     """
     import the history of the bundleAlgo objects as a list of algo dicts.
     each algo_dict has keys name (folder name), algo (bundleAlgo), is_trained (bool),
@@ -42,17 +44,14 @@
 
         obj_filename = os.path.join(write_path, "algo_object.pkl")
         if not os.path.isfile(obj_filename):  # saved mode pkl
             continue
 
         algo, algo_meta_data = algo_from_pickle(obj_filename, template_path=template_path)
 
-        if isinstance(algo, BundleAlgo):  # algo's template path needs override
-            algo.template_path = algo_meta_data["template_path"]
-
         best_metric = algo_meta_data.get(AlgoKeys.SCORE, None)
         if best_metric is None:
             try:
                 best_metric = algo.get_score()
             except BaseException:
                 pass
 
@@ -72,7 +71,20 @@
 
     Args:
         history: a List of Bundle. Typically, the history can be obtained from BundleGen get_history method
     """
     for algo_dict in history:
         algo = algo_dict[AlgoKeys.ALGO]
         algo_to_pickle(algo, template_path=algo.template_path)
+
+
+def get_name_from_algo_id(id: str) -> str:
+    """
+    Get the name of Algo from the identifier of the Algo.
+
+    Args:
+        id: identifier which follows a convention of "name_fold_other".
+
+    Returns:
+        name of the Algo.
+    """
+    return id.split("_")[0]
```

## monai/apps/detection/networks/retinanet_detector.py

```diff
@@ -613,15 +613,18 @@
             else:
                 ValueError("Images can only be 2D or 3D.")
 
             # reshaped_result_map will become (B, HWA, num_channel) or (B, HWDA, num_channel)
             reshaped_result_map = reshaped_result_map.reshape(batch_size, -1, num_channel)
 
             if torch.isnan(reshaped_result_map).any() or torch.isinf(reshaped_result_map).any():
-                raise ValueError("Concatenated result is NaN or Inf.")
+                if torch.is_grad_enabled():
+                    raise ValueError("Concatenated result is NaN or Inf.")
+                else:
+                    warnings.warn("Concatenated result is NaN or Inf.")
 
             all_reshaped_result_map.append(reshaped_result_map)
 
         return torch.cat(all_reshaped_result_map, dim=1)
 
     def postprocess_detections(
         self,
@@ -889,15 +892,18 @@
                 BELOW_LOW_THRESHOLD = -1, BETWEEN_THRESHOLDS = -2
 
         Return:
             paired predicted and GT samples from one image for classification losses computation
         """
 
         if torch.isnan(cls_logits_per_image).any() or torch.isinf(cls_logits_per_image).any():
-            raise ValueError("NaN or Inf in predicted classification logits.")
+            if torch.is_grad_enabled():
+                raise ValueError("NaN or Inf in predicted classification logits.")
+            else:
+                warnings.warn("NaN or Inf in predicted classification logits.")
 
         foreground_idxs_per_image = matched_idxs_per_image >= 0
 
         num_foreground = int(foreground_idxs_per_image.sum())
         num_gt_box = targets_per_image[self.target_box_key].shape[0]
 
         if self.debug:
@@ -969,15 +975,18 @@
             matched_idxs_per_image: matched index, sized (sum(HWA),) or  (sum(HWDA),)
 
         Return:
             paired predicted and GT samples from one image for box regression losses computation
         """
 
         if torch.isnan(box_regression_per_image).any() or torch.isinf(box_regression_per_image).any():
-            raise ValueError("NaN or Inf in predicted box regression.")
+            if torch.is_grad_enabled():
+                raise ValueError("NaN or Inf in predicted box regression.")
+            else:
+                warnings.warn("NaN or Inf in predicted box regression.")
 
         foreground_idxs_per_image = torch.where(matched_idxs_per_image >= 0)[0]
         num_gt_box = targets_per_image[self.target_box_key].shape[0]
 
         # if no GT box, return empty arrays
         if num_gt_box == 0:
             return box_regression_per_image[0:0, :], box_regression_per_image[0:0, :]
```

## monai/apps/detection/networks/retinanet_network.py

```diff
@@ -36,14 +36,15 @@
 Part of this script is adapted from
 https://github.com/pytorch/vision/blob/main/torchvision/models/detection/retinanet.py
 """
 
 from __future__ import annotations
 
 import math
+import warnings
 from collections.abc import Callable, Sequence
 from typing import Any, Dict
 
 import torch
 from torch import Tensor, nn
 
 from monai.networks.blocks.backbone_fpn_utils import BackboneWithFPN, _resnet_fpn_extractor
@@ -121,15 +122,18 @@
         for features in feature_maps:
             cls_logits = self.conv(features)
             cls_logits = self.cls_logits(cls_logits)
 
             cls_logits_maps.append(cls_logits)
 
             if torch.isnan(cls_logits).any() or torch.isinf(cls_logits).any():
-                raise ValueError("cls_logits is NaN or Inf.")
+                if torch.is_grad_enabled():
+                    raise ValueError("cls_logits is NaN or Inf.")
+                else:
+                    warnings.warn("cls_logits is NaN or Inf.")
 
         return cls_logits_maps
 
 
 class RetinaNetRegressionHead(nn.Module):
     """
     A regression head for use in RetinaNet.
@@ -190,15 +194,18 @@
         for features in feature_maps:
             box_regression = self.conv(features)
             box_regression = self.bbox_reg(box_regression)
 
             box_regression_maps.append(box_regression)
 
             if torch.isnan(box_regression).any() or torch.isinf(box_regression).any():
-                raise ValueError("box_regression is NaN or Inf.")
+                if torch.is_grad_enabled():
+                    raise ValueError("box_regression is NaN or Inf.")
+                else:
+                    warnings.warn("box_regression is NaN or Inf.")
 
         return box_regression_maps
 
 
 class RetinaNet(nn.Module):
     """
     The network used in RetinaNet.
```

## monai/apps/nnunet/__main__.py

```diff
@@ -4,89 +4,14 @@
 # You may obtain a copy of the License at
 #     http://www.apache.org/licenses/LICENSE-2.0
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""
-Examples:
-    - User can use the one-liner to start the nnU-Net workflow
-
-    .. code-block:: bash
-
-        python -m monai.apps.nnunet nnUNetV2Runner run --input "./input.yaml"
-
-    - convert dataset
-
-    .. code-block:: bash
-
-        python -m monai.apps.nnunet nnUNetRunner convert_dataset --input "./input_new.yaml"
-
-    - convert msd datasets
-
-    .. code-block:: bash
-
-        python -m monai.apps.nnunet nnUNetRunner convert_msd_dataset \\
-            --input "./input.yaml" --data_dir "Task05_Prostate"
-
-    - experiment planning and data pre-processing
-
-    .. code-block:: bash
-
-        python -m monai.apps.nnunet nnUNetRunner plan_and_process --input "./input.yaml"
-
-    - single-gpu training for all 20 models
-
-    .. code-block:: bash
-
-        python -m monai.apps.nnunet nnUNetV2Runner train --input "./input.yaml"
-
-    - single-gpu training for a single model
-
-    .. code-block:: bash
-
-        python -m monai.apps.nnunet nnUNetV2Runner train_single_model --input "./input.yaml" \\
-            --config "3d_fullres" \\
-            --fold 0 \\
-            --trainer_class_name "nnUNetTrainer_5epochs" \\
-            --export_validation_probabilities true
-
-    - multi-gpu training for all 20 models
-
-    .. code-block:: bash
-
-        export CUDA_VISIBLE_DEVICES=0,1  # optional
-        python -m monai.apps.nnunet nnUNetV2Runner train --input "./input.yaml" --num_gpus 2
-
-    - multi-gpu training for a single model
-
-    .. code-block:: bash
-
-        export CUDA_VISIBLE_DEVICES=0,1 # optional
-        python -m monai.apps.nnunet nnUNetV2Runner train_single_model --input "./input.yaml" \\
-            --config "3d_fullres" \\
-            --fold 0 \\
-            --trainer_class_name "nnUNetTrainer_5epochs" \\
-            --export_validation_probabilities true \\
-            --num_gpus 2
-
-    - find best configuration
-
-    .. code-block:: bash
-
-        python -m monai.apps.nnunet nnUNetRunner find_best_configuration --input "./input.yaml"
-
-    - predict, ensemble, and post-process
-
-    .. code-block:: bash
-
-        python -m monai.apps.nnunet nnUNetRunner predict_ensemble_postprocessing --input "./input.yaml"
-
-"""
 
 from __future__ import annotations
 
 from monai.apps.nnunet.nnunetv2_runner import nnUNetV2Runner
 
 if __name__ == "__main__":
     from monai.utils import optional_import
```

## monai/apps/nnunet/nnunetv2_runner.py

```diff
@@ -30,35 +30,138 @@
 
 logger = monai.apps.utils.get_logger(__name__)
 
 __all__ = ["nnUNetV2Runner"]
 
 
 class nnUNetV2Runner:  # noqa: N801
-    def __init__(self, input_config: Any, work_dir: str = "work_dir") -> None:
-        """
-        An interface for handling `nnU-Net` V2.
-        The users can run the nnU-Net V2 with default settings in one line of code.
-        They can also run parts of the process of nnU-Net V2 instead of the complete pipeline.
+    """
+    ``nnUNetV2Runner`` provides an interface in MONAI to use `nnU-Net` V2 library to analyze, train, and evaluate
+    neural networks for medical image segmentation tasks.
 
-        The output of the interface is a directory that contains:
+    ``nnUNetV2Runner`` can be used in two ways:
 
-            - converted dataset met requirement of nnU-Net V2
-            - data analysis results
-            - checkpoints from the trained U-Net models
-            - validation accuracy in each fold of cross-validation
-            - the predictions on the testing datasets from the final algorithm ensemble and potential post-processing
+    #. with one line of code to execute the complete pipeline.
+    #. with a series of commands to run each modules in the pipeline.
 
-        Args:
-            input_config: the configuration dictionary or the file path to the configuration in form of YAML.
-                The configuration should contain ``datalist``, ``dataroot``, ``modality``.
-            work_dir: working directory to save the intermediate and final results.
-        """
+    The output of the interface is a directory that contains:
+
+    #. converted dataset met the requirement of nnU-Net V2
+    #. data analysis results
+    #. checkpoints from the trained U-Net models
+    #. validation accuracy in each fold of cross-validation
+    #. the predictions on the testing datasets from the final algorithm ensemble and potential post-processing
+
+    Args:
+        input_config: the configuration dictionary or the file path to the configuration in the form of YAML.
+            The keys required in the configuration are:
+            - ``"datalist"``: File path to the datalist for the train/testing splits
+            - ``"dataroot"``: File path to the dataset
+            - ``"modality"``: Imaging modality, e.g. "CT", ["T2", "ADC"]
+            Currently, the configuration supports these optional keys:
+            - ``"nnunet_raw"``: File path that will be written to env variable for nnU-Net
+            - ``"nnunet_preprocessed"``: File path that will be written to env variable for nnU-Net
+            - ``"nnunet_results"``: File path that will be written to env variable for nnU-Net
+            - ``"nnUNet_trained_models"``
+            - ``"dataset_name_or_id"``: Name or Integer ID of the dataset
+            If an optional key is not specified, then the pipeline will use the default values.
+        trainer_class_name: the trainer class names offered by nnUNetV2 exhibit variations in training duration.
+            Default: "nnUNetTrainer". Other options: "nnUNetTrainer_Xepoch". X could be one of 1,5,10,20,50,100,
+            250,2000,4000,8000.
+        export_validation_probabilities: True to save softmax predictions from final validation as npz
+            files (in addition to predicted segmentations). Needed for finding the best ensemble.
+            Default: True.
+        work_dir: working directory to save the intermediate and final results.
+
+    Examples:
+        - Use the one-liner to start the nnU-Net workflow
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner run --input_config ./input.yaml
+
+        - Use `convert_dataset` to prepare the data to meet nnU-Net requirements, generate dataset JSON file,
+            and copy the dataset to a location specified by ``nnunet_raw`` in the input config file
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner convert_dataset --input_config="./input.yaml"
+
+        - `convert_msd_dataset` is an alternative option to prepare the data if the dataset is MSD.
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner convert_msd_dataset \\
+                --input_config "./input.yaml" --data_dir "/path/to/Task09_Spleen"
+
+        - experiment planning and data pre-processing
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner plan_and_process --input_config "./input.yaml"
+
+        - training all 20 models using all GPUs available.
+            "CUDA_VISIBLE_DEVICES" environment variable is not supported.
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner train --input_config "./input.yaml"
+
+        - training a single model on a single GPU for 5 epochs. Here ``config`` is used to specify the configuration.
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner train_single_model --input_config "./input.yaml" \\
+                --config "3d_fullres" \\
+                --fold 0 \\
+                --gpu_id 0 \\
+                --trainer_class_name "nnUNetTrainer_5epochs" \\
+                --export_validation_probabilities True
+
+        - training for all 20 models (4 configurations by 5 folds) on 2 GPUs
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner train --input_config "./input.yaml" --device_ids "(0,1)"
+
+        - 5-fold training for a single model on 2 GPUs. Here ``configs`` is used to specify the configurations.
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner train --input_config "./input.yaml" \\
+                --configs "3d_fullres" \\
+                --trainer_class_name "nnUNetTrainer_5epochs" \\
+                --export_validation_probabilities True \\
+                --device_ids "(0,1)"
+
+        - find the best configuration
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner find_best_configuration --input_config "./input.yaml"
+
+        - predict, ensemble, and post-process
+
+        .. code-block:: bash
+
+            python -m monai.apps.nnunet nnUNetV2Runner predict_ensemble_postprocessing --input_config "./input.yaml"
+
+    """
+
+    def __init__(
+        self,
+        input_config: Any,
+        trainer_class_name: str = "nnUNetTrainer",
+        work_dir: str = "work_dir",
+        export_validation_probabilities: bool = True,
+    ) -> None:
         self.input_info: dict = {}
         self.input_config_or_dict = input_config
+        self.trainer_class_name = trainer_class_name
+        self.export_validation_probabilities = export_validation_probabilities
         self.work_dir = work_dir
 
         if isinstance(self.input_config_or_dict, dict):
             self.input_info = self.input_config_or_dict
         elif isinstance(self.input_config_or_dict, str) and os.path.isfile(self.input_config_or_dict):
             self.input_info = ConfigParser.load_config_file(self.input_config_or_dict)
         else:
@@ -91,29 +194,34 @@
         self.dataset_name_or_id = str(self.input_info.pop("dataset_name_or_id", 1))
 
         try:
             from nnunetv2.utilities.dataset_name_id_conversion import maybe_convert_to_dataset_name
 
             self.dataset_name = maybe_convert_to_dataset_name(int(self.dataset_name_or_id))
         except BaseException:
-            logger.warning("Dataset ID does not exist! Check input '.yaml' if this is unexpected.")
+            logger.warning(
+                f"Dataset with name/ID: {self.dataset_name_or_id} cannot be found in the record. "
+                "Please ignore the message above if you are running the pipeline from a fresh start. "
+                "But if the dataset is expected to be found, please check your input_config."
+            )
 
         from nnunetv2.configuration import default_num_processes
 
         self.default_num_processes = default_num_processes
 
         self.num_folds = 5
         self.best_configuration: dict = {}
 
     def convert_dataset(self):
+        """Convert and make a copy the dataset to meet the requirements of nnU-Net workflow."""
         try:
             raw_data_foldername_prefix = str(int(self.dataset_name_or_id) + 1000)
             raw_data_foldername_prefix = "Dataset" + raw_data_foldername_prefix[-3:]
 
-            # check if dataset is created
+            # check if the dataset is created
             subdirs = glob.glob(f"{self.nnunet_raw}/*")
             dataset_ids = [_item.split(os.sep)[-1] for _item in subdirs]
             dataset_ids = [_item.split("_")[0] for _item in dataset_ids]
             if raw_data_foldername_prefix in dataset_ids:
                 logger.warning("Dataset with the same ID exists!")
                 return
 
@@ -132,15 +240,15 @@
 
             datalist_json = ConfigParser.load_config_file(self.input_info.pop("datalist"))
 
             if "training" in datalist_json:
                 os.makedirs(os.path.join(raw_data_foldername, "imagesTr"))
                 os.makedirs(os.path.join(raw_data_foldername, "labelsTr"))
             else:
-                logger.warning("Input '.json' data list is incorrect.")
+                logger.error("The datalist file has incorrect format: the `training` key is not found.")
                 return
 
             test_key = None
             if "test" in datalist_json or "testing" in datalist_json:
                 os.makedirs(os.path.join(raw_data_foldername, "imagesTs"))
                 test_key = "test" if "test" in datalist_json else "testing"
                 if isinstance(datalist_json[test_key][0], dict) and "label" in datalist_json[test_key][0]:
@@ -164,24 +272,26 @@
                 test_key=test_key,  # type: ignore
                 datalist_json=datalist_json,
                 data_dir=data_dir,
                 num_input_channels=num_input_channels,
                 output_datafolder=raw_data_foldername,
             )
         except BaseException:
-            logger.warning("Input '.yaml' is incorrect.")
+            logger.warning("Input config may be incorrect. Detail info: error/exception message is:\n {err}")
             return
 
     def convert_msd_dataset(self, data_dir: str, overwrite_id: str | None = None, n_proc: int = -1) -> None:
         """
+        Convert and make a copy the MSD dataset to meet requirements of nnU-Net workflow.
+
         Args:
             data_dir: downloaded and extracted MSD dataset folder. CANNOT be nnUNetv1 dataset!
                 Example: "/workspace/downloads/Task05_Prostate".
             overwrite_id: Overwrite the dataset id. If not set then use the id of the MSD task (inferred from
-                folder name). Only use this if you already have an equivalently numbered dataset!
+                the folder name). Only use this if you already have an equivalently numbered dataset!
             n_proc: Number of processes used.
         """
         from nnunetv2.dataset_conversion.convert_MSD_dataset import convert_msd_dataset
 
         num_processes = None if n_proc < 0 else self.default_num_processes
         convert_msd_dataset(data_dir, overwrite_id, num_processes)
 
@@ -190,66 +300,70 @@
         fpe: str = "DatasetFingerprintExtractor",
         npfp: int = -1,
         verify_dataset_integrity: bool = False,
         clean: bool = False,
         verbose: bool = False,
     ) -> None:
         """
+        Extracts the dataset fingerprint used for experiment planning.
+
         Args:
             fpe: [OPTIONAL] Name of the Dataset Fingerprint Extractor class that should be used. Default is
-                'DatasetFingerprintExtractor'
+                "DatasetFingerprintExtractor".
             npfp: [OPTIONAL] Number of processes used for fingerprint extraction.
             verify_dataset_integrity: [RECOMMENDED] set this flag to check the dataset integrity. This is
                 useful and should be done once for each dataset!
             clean: [OPTIONAL] Set this flag to overwrite existing fingerprints. If this flag is not set and a
                 fingerprint already exists, the fingerprint extractor will not run.
             verbose: set this to print a lot of stuff. Useful for debugging. Will disable progress bar!
-                Recommended for cluster environments
+                Recommended for cluster environments.
         """
         from nnunetv2.experiment_planning.plan_and_preprocess_api import extract_fingerprints
 
         npfp = self.default_num_processes if npfp < 0 else npfp
 
-        logger.warning("Fingerprint extraction...")
+        logger.info("Fingerprint extraction...")
         extract_fingerprints([int(self.dataset_name_or_id)], fpe, npfp, verify_dataset_integrity, clean, verbose)
 
     def plan_experiments(
         self,
         pl: str = "ExperimentPlanner",
         gpu_memory_target: float = 8,
         preprocessor_name: str = "DefaultPreprocessor",
         overwrite_target_spacing: Any = None,
         overwrite_plans_name: str = "nnUNetPlans",
     ) -> None:
         """
+        Generate a configuration file that specifies the details of the experiment.
+
         Args:
-            pl: [OPTIONAL] Name of the Experiment Planner class that should be used. Default is 'ExperimentPlanner'.
-                Note: There is no longer a distinction between 2d and 3d planner. It's an all in one solution now.
-            gpu_memory_target:[OPTIONAL] DANGER ZONE! Sets a custom GPU memory target. Default: 8 [GB].
-                Changing this will affect patch and batch size and will definitely affect your models performance!
+            pl: [OPTIONAL] Name of the Experiment Planner class that should be used. Default is "ExperimentPlanner".
+                Note: There is no longer a distinction between 2d and 3d planner. It's an all-in-one solution now.
+            gpu_memory_target: [OPTIONAL] DANGER ZONE! Sets a custom GPU memory target. Default: 8 [GB].
+                Changing this will affect patch and batch size and will definitely affect your models' performance!
                 Only use this if you really know what you are doing and NEVER use this without running the
                 default nnU-Net first (as a baseline).
-            preprocessor_name:[OPTIONAL] DANGER ZONE! Sets a custom preprocessor class. This class must be located in
-                nnunetv2.preprocessing. Default: 'DefaultPreprocessor'. Changing this may affect your models performance!
-                Only use this if you really know what you are doing and NEVER use this without running the
+            preprocessor_name: [OPTIONAL] DANGER ZONE! Sets a custom preprocessor class. This class must be located in
+                nnunetv2.preprocessing. Default: "DefaultPreprocessor". Changing this may affect your models'
+                performance! Only use this if you really know what you are doing and NEVER use this without running the
                 default nnU-Net first (as a baseline).
-            overwrite_target_spacing':[OPTIONAL] DANGER ZONE! Sets a custom target spacing for the 3d_fullres
+            overwrite_target_spacing: [OPTIONAL] DANGER ZONE! Sets a custom target spacing for the 3d_fullres
                 and 3d_cascade_fullres configurations. Default: None [no changes]. Changing this will affect
-                image size and potentially patch and batch size. This will definitely affect your models performance!
+                image size and potentially patch and batch size. This will definitely affect your models' performance!
                 Only use this if you really know what you are doing and NEVER use this without running the
                 default nnU-Net first (as a baseline). Changing the target spacing for the other configurations
                 is currently not implemented. New target spacing must be a list of three numbers!
-            overwrite_plans_name':[OPTIONAL] DANGER ZONE! If you used -gpu_memory_target, -preprocessor_name or
+            overwrite_plans_name: [OPTIONAL] DANGER ZONE! If you used -gpu_memory_target, -preprocessor_name or
                 -overwrite_target_spacing it is best practice to use -overwrite_plans_name to generate
                 a differently named plans file such that the nnunet default plans are not overwritten.
-                You will then need to specify your custom plan
+                You will then need to specify your custom plan.
         """
         from nnunetv2.experiment_planning.plan_and_preprocess_api import plan_experiments
 
-        logger.warning("Experiment planning...")
+        logger.info("Experiment planning...")
         plan_experiments(
             [int(self.dataset_name_or_id)],
             pl,
             gpu_memory_target,
             preprocessor_name,
             overwrite_target_spacing,
             overwrite_plans_name,
@@ -259,37 +373,39 @@
         self,
         c: tuple = (M.N_2D, M.N_3D_FULLRES, M.N_3D_LOWRES),
         n_proc: tuple = (8, 8, 8),
         overwrite_plans_name: str = "nnUNetPlans",
         verbose: bool = False,
     ) -> None:
         """
+        Apply a set of preprocessing operations to the input data before the training.
+
         Args:
             overwrite_plans_name: [OPTIONAL] You can use this to specify a custom plans file that you may have
                 generated.
             c: [OPTIONAL] Configurations for which the preprocessing should be run. Default: 2d 3f_fullres
                 3d_lowres. 3d_cascade_fullres does not need to be specified because it uses the data
-                from 3f_fullres. Configurations that do not exist for some dataset will be skipped)
+                from 3f_fullres. Configurations that do not exist for some datasets will be skipped).
             n_proc: [OPTIONAL] Use this to define how many processes are to be used. If this is just one number then
                 this number of processes is used for all configurations specified with -c. If it's a
                 list of numbers this list must have as many elements as there are configurations. We
-                then iterate over zip(configs, num_processes) to determine then umber of processes
-                used for each configuration. More processes is always faster (up to the number of
-                threads your PC can support, so 8 for a 4 core CPU with hyperthreading. If you don't
+                then iterate over zip(configs, num_processes) to determine the number of processes
+                used for each configuration. More processes are always faster (up to the number of
+                threads your PC can support, so 8 for a 4-core CPU with hyperthreading. If you don't
                 know what that is then don't touch it, or at least don't increase it!). DANGER: More
                 often than not the number of processes that can be used is limited by the amount of
                 RAM available. Image resampling takes up a lot of RAM. MONITOR RAM USAGE AND
-                DECREASE -n_proc IF YOUR RAM FILLS UP TOO MUCH!. Default: 8 4 8 (=8 processes for 2d, 4
-                for 3d_fullres and 8 for 3d_lowres if -c is at its default)
-            verbose:Set this to print a lot of stuff. Useful for debugging. Will disable progress bar!
-                Recommended for cluster environments
+                DECREASE -n_proc IF YOUR RAM FILLS UP TOO MUCH! Default: 8 4 8 (=8 processes for 2d, 4
+                for 3d_fullres and 8 for 3d_lowres if -c is at its default).
+            verbose: Set this to print a lot of stuff. Useful for debugging. Will disable the progress bar!
+                Recommended for cluster environments.
         """
         from nnunetv2.experiment_planning.plan_and_preprocess_api import preprocess
 
-        logger.warning("Preprocessing...")
+        logger.info("Preprocessing...")
         preprocess(
             [int(self.dataset_name_or_id)],
             overwrite_plans_name,
             configurations=c,
             num_processes=n_proc,
             verbose=verbose,
         )
@@ -307,137 +423,191 @@
         overwrite_target_spacing: Any = None,
         overwrite_plans_name: str = "nnUNetPlans",
         c: tuple = (M.N_2D, M.N_3D_FULLRES, M.N_3D_LOWRES),
         n_proc: tuple = (8, 8, 8),
         verbose: bool = False,
     ) -> None:
         """
+        Performs experiment planning and preprocessing before the training.
+
         Args:
             fpe: [OPTIONAL] Name of the Dataset Fingerprint Extractor class that should be used. Default is
-                'DatasetFingerprintExtractor'
-            npfp: [OPTIONAL] Number of processes used for fingerprint extraction. Default: 8
+                "DatasetFingerprintExtractor".
+            npfp: [OPTIONAL] Number of processes used for fingerprint extraction. Default: 8.
             verify_dataset_integrity: [RECOMMENDED] set this flag to check the dataset integrity.
                 This is useful and should be done once for each dataset!
             no_pp: [OPTIONAL] Set this to only run fingerprint extraction and experiment planning (no
                 preprocessing). Useful for debugging.
             clean:[OPTIONAL] Set this flag to overwrite existing fingerprints. If this flag is not set and a
                 fingerprint already exists, the fingerprint extractor will not run. REQUIRED IF YOU
                 CHANGE THE DATASET FINGERPRINT EXTRACTOR OR MAKE CHANGES TO THE DATASET!
-            pl: [OPTIONAL] Name of the Experiment Planner class that should be used. Default is
-                ExperimentPlanner'. Note: There is no longer a distinction between 2d and 3d planner.
-                It's an all in one solution now.
+            pl: [OPTIONAL] Name of the Experiment Planner class that should be used. Default is "ExperimentPlanner".
+                Note: There is no longer a distinction between 2d and 3d planner. It's an all-in-one solution now.
             gpu_memory_target: [OPTIONAL] DANGER ZONE! Sets a custom GPU memory target. Default: 8 [GB].
                 Changing this will affect patch and batch size and will
-                definitely affect your models performance! Only use this if you really know what you
+                definitely affect your models' performance! Only use this if you really know what you
                 are doing and NEVER use this without running the default nnU-Net first (as a baseline).
             preprocessor_name: [OPTIONAL] DANGER ZONE! Sets a custom preprocessor class. This class must be located in
-                nnunetv2.preprocessing. Default: 'DefaultPreprocessor'. Changing this may affect your
-                models performance! Only use this if you really know what you
+                nnunetv2.preprocessing. Default: "DefaultPreprocessor". Changing this may affect your
+                models' performance! Only use this if you really know what you
                 are doing and NEVER use this without running the default nnU-Net first (as a baseline).
             overwrite_target_spacing: [OPTIONAL] DANGER ZONE! Sets a custom target spacing for the 3d_fullres and
                 3d_cascade_fullres configurations. Default: None [no changes]. Changing this will affect image size and
                 potentially patch and batch size. This will definitely affect your models performance!
                 Only use this if you really know what you are doing and NEVER use this without running the
                 default nnU-Net first (as a baseline). Changing the target spacing for the other
                 configurations is currently not implemented. New target spacing must be a list of three numbers!
-            overwrite_plans_name: [OPTIONAL] uSE A CUSTOM PLANS IDENTIFIER. If you used -gpu_memory_target,
+            overwrite_plans_name: [OPTIONAL] USE A CUSTOM PLANS IDENTIFIER. If you used -gpu_memory_target,
                 -preprocessor_name or -overwrite_target_spacing it is best practice to use -overwrite_plans_name to
                 generate a differently named plans file such that the nnunet default plans are not
                 overwritten. You will then need to specify your custom plans file with -p whenever
-                running other nnunet commands (training, inference etc)
+                running other nnunet commands (training, inference, etc)
             c: [OPTIONAL] Configurations for which the preprocessing should be run. Default: 2d 3f_fullres
                 3d_lowres. 3d_cascade_fullres does not need to be specified because it uses the data
-                from 3f_fullres. Configurations that do not exist for some dataset will be skipped.
+                from 3f_fullres. Configurations that do not exist for some datasets will be skipped.
             n_proc: [OPTIONAL] Use this to define how many processes are to be used. If this is just one number then
                 this number of processes is used for all configurations specified with -c. If it's a
                 list of numbers this list must have as many elements as there are configurations. We
-                then iterate over zip(configs, num_processes) to determine then umber of processes
-                used for each configuration. More processes is always faster (up to the number of
-                threads your PC can support, so 8 for a 4 core CPU with hyperthreading. If you don't
+                then iterate over zip(configs, num_processes) to determine the number of processes
+                used for each configuration. More processes are always faster (up to the number of
+                threads your PC can support, so 8 for a 4-core CPU with hyperthreading. If you don't
                 know what that is then don't touch it, or at least don't increase it!). DANGER: More
                 often than not the number of processes that can be used is limited by the amount of
                 RAM available. Image resampling takes up a lot of RAM. MONITOR RAM USAGE AND
-                DECREASE -n_proc IF YOUR RAM FILLS UP TOO MUCH!. Default: 8 4 8 (=8 processes for 2d, 4
-                for 3d_fullres and 8 for 3d_lowres if -c is at its default)
+                DECREASE -n_proc IF YOUR RAM FILLS UP TOO MUCH! Default: 8 4 8 (=8 processes for 2d, 4
+                for 3d_fullres and 8 for 3d_lowres if -c is at its default).
             verbose: Set this to print a lot of stuff. Useful for debugging. Will disable progress bar!
-                (Recommended for cluster environments')
+                (Recommended for cluster environments).
         """
         self.extract_fingerprints(fpe, npfp, verify_dataset_integrity, clean, verbose)
         self.plan_experiments(pl, gpu_memory_target, preprocessor_name, overwrite_target_spacing, overwrite_plans_name)
 
         if not no_pp:
             self.preprocess(c, n_proc, overwrite_plans_name, verbose)
 
-    def train_single_model(self, config: Any, fold: int, gpu_id: int = 0, **kwargs: Any) -> None:
+    def train_single_model(self, config: Any, fold: int, gpu_id: tuple | list | int = 0, **kwargs: Any) -> None:
         """
+        Run the training on a single GPU with one specified configuration provided.
+        Note: this will override the environment variable `CUDA_VISIBLE_DEVICES`.
+
         Args:
-            config: configuration that should be trained.
+            config: configuration that should be trained. Examples: "2d", "3d_fullres", "3d_lowres".
             fold: fold of the 5-fold cross-validation. Should be an int between 0 and 4.
-            trainer_class_name: name of the custom trainer class. default: 'nnUNetTrainer'.
-            plans_identifier: custom plans identifier. default: 'nnUNetPlans'.
-            pretrained_weights: path to nnU-Net checkpoint file to be used as pretrained model. Will only be used
-                when actually training. Beta. Use with caution. default: False.
-            num_gpus: number of GPUs to use for training. default: 1.
-            use_compressed_data: true to use compressed data for training. Reading compressed data is much more CPU and
-                (potentially) RAM intensive and should only be used if you know what you are doing default: False.
-            export_validation_probabilities: true to save softmax predictions from final validation as npz files
-                (in addition to predicted segmentations). Needed for finding the best ensemble. default: False.
-            continue_training: continue training from latest checkpoint. default: False.
-            only_run_validation: true to run the validation only. Requires training to have finished. default: False.
-            disable_checkpointing: true to disable checkpointing. Ideal for testing things out and you don't want to
-                flood your hard drive with checkpoints. default: False.
-        """
-        os.environ["CUDA_VISIBLE_DEVICES"] = f"{gpu_id}"
+            gpu_id: an integer to select the device to use, or a tuple/list of GPU device indices used for multi-GPU
+                training (e.g., (0,1)). Default: 0.
+        from nnunetv2.run.run_training import run_training
+            kwargs: this optional parameter allows you to specify additional arguments in
+                ``nnunetv2.run.run_training.run_training``. Currently supported args are
+                    - trainer_class_name: name of the custom trainer class. Default: "nnUNetTrainer".
+                    - plans_identifier: custom plans identifier. Default: "nnUNetPlans".
+                    - pretrained_weights: path to nnU-Net checkpoint file to be used as pretrained model. Will only be
+                        used when actually training. Beta. Use with caution. Default: False.
+                    - use_compressed_data: True to use compressed data for training. Reading compressed data is much
+                        more CPU and (potentially) RAM intensive and should only be used if you know what you are
+                        doing. Default: False.
+                    - continue_training: continue training from latest checkpoint. Default: False.
+                    - only_run_validation: True to run the validation only. Requires training to have finished.
+                        Default: False.
+                    - disable_checkpointing: True to disable checkpointing. Ideal for testing things out and you
+                        don't want to flood your hard drive with checkpoints. Default: False.
+        """
+        if "num_gpus" in kwargs:
+            kwargs.pop("num_gpus")
+            logger.warning("please use gpu_id to set the GPUs to use")
+
+        if isinstance(gpu_id, tuple) or isinstance(gpu_id, list):
+            if len(gpu_id) > 1:
+                gpu_ids_str = ""
+                for _i in range(len(gpu_id)):
+                    gpu_ids_str += f"{gpu_id[_i]},"
+                os.environ["CUDA_VISIBLE_DEVICES"] = gpu_ids_str[:-1]
+            else:
+                os.environ["CUDA_VISIBLE_DEVICES"] = f"{gpu_id[0]}"
+        else:
+            os.environ["CUDA_VISIBLE_DEVICES"] = f"{gpu_id}"
 
         from nnunetv2.run.run_training import run_training
 
-        run_training(dataset_name_or_id=self.dataset_name_or_id, configuration=config, fold=fold, **kwargs)
+        if isinstance(gpu_id, int) or len(gpu_id) == 1:
+            run_training(
+                dataset_name_or_id=self.dataset_name_or_id,
+                configuration=config,
+                fold=fold,
+                trainer_class_name=self.trainer_class_name,
+                export_validation_probabilities=self.export_validation_probabilities,
+                **kwargs,
+            )
+        else:
+            run_training(
+                dataset_name_or_id=self.dataset_name_or_id,
+                configuration=config,
+                fold=fold,
+                num_gpus=len(gpu_id),
+                trainer_class_name=self.trainer_class_name,
+                export_validation_probabilities=self.export_validation_probabilities,
+                **kwargs,
+            )
 
     def train(
         self,
         configs: tuple | str = (M.N_3D_FULLRES, M.N_2D, M.N_3D_LOWRES, M.N_3D_CASCADE_FULLRES),
-        device_ids: tuple | None = None,
+        device_ids: tuple | list | None = None,
         **kwargs: Any,
     ) -> None:
         """
+        Run the training for all the models specified by the configurations.
+        Note: to set the number of GPUs to use, use ``devices_ids`` instead of the `CUDA_VISIBLE_DEVICES`
+        environment variable.
+
         Args:
             configs: configurations that should be trained.
-                default: ("2d", "3d_fullres", "3d_lowres", "3d_cascade_fullres").
-            device_ids: ids of GPUs to use for training. default: None (all available GPUs).
+                Default: ("2d", "3d_fullres", "3d_lowres", "3d_cascade_fullres").
+            device_ids: a tuple/list of GPU device IDs to use for the training. Default: None (all available GPUs).
+            kwargs: this optional parameter allows you to specify additional arguments defined in the
+                ``train_single_model`` method.
         """
-
         if device_ids is None:
             result = subprocess.run(["nvidia-smi", "--list-gpus"], stdout=subprocess.PIPE)
             output = result.stdout.decode("utf-8")
             num_gpus = len(output.strip().split("\n"))
             device_ids = tuple(range(num_gpus))
-        logger.warning(f"number of gpus is {len(device_ids)}, device ids are {device_ids}")
+        logger.info(f"number of GPUs is {len(device_ids)}, device ids are {device_ids}")
         if len(device_ids) > 1:
             self.train_parallel(configs=ensure_tuple(configs), device_ids=device_ids, **kwargs)
         else:
             for cfg in ensure_tuple(configs):
                 for _fold in range(self.num_folds):
-                    self.train_single_model(config=cfg, fold=_fold, **kwargs)
+                    self.train_single_model(config=cfg, fold=_fold, gpu_id=device_ids, **kwargs)
 
     def train_parallel_cmd(
         self,
         configs: tuple | str = (M.N_3D_FULLRES, M.N_2D, M.N_3D_LOWRES, M.N_3D_CASCADE_FULLRES),
-        device_ids: tuple | None = None,
+        device_ids: tuple | list | None = None,
         **kwargs: Any,
     ) -> list:
+        """
+        Create the line command for subprocess call for parallel training.
+
+        Args:
+            configs: configurations that should be trained.
+                Default: ("2d", "3d_fullres", "3d_lowres", "3d_cascade_fullres").
+            device_ids: a tuple/list of GPU device IDs to use for the training. Default: None (all available GPUs).
+            kwargs: this optional parameter allows you to specify additional arguments defined in the
+                ``train_single_model`` method.
+        """
         # unpack compressed files
         folder_names = []
         for root, _, files in os.walk(os.path.join(self.nnunet_preprocessed, self.dataset_name)):
             if any(file.endswith(".npz") for file in files):
                 folder_names.append(root)
 
         from nnunetv2.training.dataloading.utils import unpack_dataset
 
         for folder_name in folder_names:
-            logger.warning(f"[info] unpacking '{folder_name}'...")
+            logger.info(f"unpacking '{folder_name}'...")
             unpack_dataset(
                 folder=folder_name,
                 unpack_segmentation=True,
                 overwrite_existing=False,
                 num_processes=self.default_num_processes,
             )
 
@@ -453,106 +623,128 @@
 
             for _config in _configs[_stage]:
                 if _config in ensure_tuple(configs):
                     for _i in range(self.num_folds):
                         the_device = device_ids[_index % n_devices]  # type: ignore
                         cmd = (
                             "python -m monai.apps.nnunet nnUNetV2Runner train_single_model "
-                            + f"--input_config '{self.input_config_or_dict}' --config '{_config}' "
-                            + f"--fold {_i} --gpu_id {the_device}"
+                            + f"--input_config '{self.input_config_or_dict}' --work_dir '{self.work_dir}' "
+                            + f"--config '{_config}' --fold {_i} --gpu_id {the_device} "
+                            + f"--trainer_class_name {self.trainer_class_name} "
+                            + f"--export_validation_probabilities {self.export_validation_probabilities}"
                         )
                         for _key, _value in kwargs.items():
                             cmd += f" --{_key} {_value}"
                         all_cmds[-1][the_device].append(cmd)
                         _index += 1
         return all_cmds
 
     def train_parallel(
         self,
         configs: tuple | str = (M.N_3D_FULLRES, M.N_2D, M.N_3D_LOWRES, M.N_3D_CASCADE_FULLRES),
-        device_ids: tuple | None = None,
+        device_ids: tuple | list | None = None,
         **kwargs: Any,
     ) -> None:
         """
+        Create the line command for subprocess call for parallel training.
+        Note: to set the number of GPUs to use, use ``devices_ids`` instead of the `CUDA_VISIBLE_DEVICES`
+        environment variable.
+
         Args:
             configs: configurations that should be trained.
                 default: ("2d", "3d_fullres", "3d_lowres", "3d_cascade_fullres").
-            device_ids: device ids to use for training. default: None.
+            device_ids: a tuple of GPU device IDs to use for the training. Default: None (all available GPUs).
+            kwargs: this optional parameter allows you to specify additional arguments defined in the
+                ``train_single_model`` method.
         """
         all_cmds = self.train_parallel_cmd(configs=configs, device_ids=device_ids, **kwargs)
         for s, cmds in enumerate(all_cmds):
             for gpu_id, gpu_cmd in cmds.items():
                 if not gpu_cmd:
                     continue
-                logger.warning(
-                    f"\n[info] training - stage {s + 1}:\n"
-                    f"[info] for gpu {gpu_id}, commands: {gpu_cmd}\n"
-                    f"[info] log '.txt' inside '{os.path.join(self.nnunet_results, self.dataset_name)}'"
+                logger.info(
+                    f"training - stage {s + 1}:\n"
+                    f"for gpu {gpu_id}, commands: {gpu_cmd}\n"
+                    f"log '.txt' inside '{os.path.join(self.nnunet_results, self.dataset_name)}'"
                 )
         for stage in all_cmds:
             processes = []
             for device_id in stage:
                 if not stage[device_id]:
                     continue
                 cmd_str = "; ".join(stage[device_id])
+                logger.info(f"Current running command on GPU device {device_id}:\n{cmd_str}\n")
                 processes.append(subprocess.Popen(cmd_str, shell=True, stdout=subprocess.DEVNULL))
             # finish this stage first
             for p in processes:
                 p.wait()
 
     def validate_single_model(self, config: str, fold: int, **kwargs: Any) -> None:
         """
+        Perform validation on single model.
+
         Args:
             config: configuration that should be trained.
             fold: fold of the 5-fold cross-validation. Should be an int between 0 and 4.
+            kwargs: this optional parameter allows you to specify additional arguments defined in the
+                ``train_single_model`` method.
         """
         self.train_single_model(config=config, fold=fold, only_run_validation=True, **kwargs)
 
     def validate(
         self, configs: tuple = (M.N_3D_FULLRES, M.N_2D, M.N_3D_LOWRES, M.N_3D_CASCADE_FULLRES), **kwargs: Any
     ) -> None:
         """
+        Perform validation in all models defined by the configurations over 5 folds.
+
         Args:
             configs: configurations that should be trained.
                 default: ("2d", "3d_fullres", "3d_lowres", "3d_cascade_fullres").
+            kwargs: this optional parameter allows you to specify additional arguments defined in the
+                ``train_single_model`` method.
         """
         for cfg in ensure_tuple(configs):
             for _fold in range(self.num_folds):
                 self.validate_single_model(config=cfg, fold=_fold, **kwargs)
 
     def find_best_configuration(
         self,
         plans: tuple | str = "nnUNetPlans",
         configs: tuple | str = (M.N_2D, M.N_3D_FULLRES, M.N_3D_LOWRES, M.N_3D_CASCADE_FULLRES),
-        trainers: tuple | str = "nnUNetTrainer",
+        trainers: tuple | str | None = None,
         allow_ensembling: bool = True,
         num_processes: int = -1,
         overwrite: bool = True,
         folds: list[int] | tuple[int, ...] = (0, 1, 2, 3, 4),
         strict: bool = False,
     ) -> None:
         """
+        Find the best model configurations.
+
         Args:
-            plans: list of plan identifiers. Default: nnUNetPlans
-            configs: list of configurations. Default: ['2d', '3d_fullres', '3d_lowres', '3d_cascade_fullres']
-            trainers: list of trainers. Default: nnUNetTrainer
-            num_processes: number of processes to use for ensembling, postprocessing etc
-            folds: folds to use. Default: 0 1 2 3 4
-            allow_ensembling: Set this flag to enable ensembling
-            overwrite: If set we will overwrite already ensembled files etc. May speed up consecutive
-                runs of this command (why would oyu want to do that?) at the risk of not updating
-                outdated results.
+            plans: list of plan identifiers. Default: nnUNetPlans.
+            configs: list of configurations. Default: ["2d", "3d_fullres", "3d_lowres", "3d_cascade_fullres"].
+            trainers: list of trainers. Default: nnUNetTrainer.
+            allow_ensembling: set this flag to enable ensembling.
+            num_processes: number of processes to use for ensembling, postprocessing, etc.
+            overwrite: if set we will overwrite already ensembled files etc. May speed up consecutive
+                runs of this command (not recommended) at the risk of not updating outdated results.
+            folds: folds to use. Default: (0, 1, 2, 3, 4).
+            strict: a switch that triggers RunTimeError if the logging folder cannot be found. Default: False.
         """
         from nnunetv2.evaluation.find_best_configuration import (
             dumb_trainer_config_plans_to_trained_models_dict,
             find_best_configuration,
         )
 
         configs = ensure_tuple(configs)
         plans = ensure_tuple(plans)
+
+        if trainers is None:
+            trainers = self.trainer_class_name
         trainers = ensure_tuple(trainers)
 
         models = dumb_trainer_config_plans_to_trained_models_dict(trainers, configs, plans)
         num_processes = self.default_num_processes if num_processes < 0 else num_processes
         find_best_configuration(
             int(self.dataset_name_or_id),
             models,
@@ -612,15 +804,15 @@
                 call predicts everything).
             part_id: if multiple nnUNetv2_predict exist, which one is this? IDs start with 0 can end with
                 num_parts - 1. So when you submit 5 nnUNetv2_predict calls you need to set -num_parts
                 5 and use -part_id 0, 1, 2, 3 and 4.
             num_processes_preprocessing: out-of-RAM issues.
             num_processes_segmentation_export: Number of processes used for segmentation export.
                 More is not always better. Beware of out-of-RAM issues.
-            gpu_id: which gpu to use for prediction.
+            gpu_id: which GPU to use for prediction.
         """
         os.environ["CUDA_VISIBLE_DEVICES"] = f"{gpu_id}"
 
         from nnunetv2.inference.predict_from_raw_data import predict_from_raw_data
 
         n_processes_preprocessing = (
             self.default_num_processes if num_processes_preprocessing < 0 else num_processes_preprocessing
@@ -654,19 +846,23 @@
         folds: tuple = (0, 1, 2, 3, 4),
         run_ensemble: bool = True,
         run_predict: bool = True,
         run_postprocessing: bool = True,
         **kwargs: Any,
     ) -> None:
         """
+        Run prediction, ensemble, and/or postprocessing optionally.
+
         Args:
             folds: which folds to use
             run_ensemble: whether to run ensembling.
             run_predict: whether to predict using trained checkpoints
             run_postprocessing: whether to conduct post-processing
+            kwargs: this optional parameter allows you to specify additional arguments defined in the
+                ``predict`` method.
         """
         from nnunetv2.ensembling.ensemble import ensemble_folders
         from nnunetv2.postprocessing.remove_connected_components import apply_postprocessing_to_folder
         from nnunetv2.utilities.file_path_utilities import get_output_folder
 
         source_dir = join(self.nnunet_raw, self.dataset_name, "imagesTs")
         target_dir_base = join(self.nnunet_results, self.dataset_name)
@@ -726,20 +922,22 @@
         run_convert_dataset: bool = True,
         run_plan_and_process: bool = True,
         run_train: bool = True,
         run_find_best_configuration: bool = True,
         run_predict_ensemble_postprocessing: bool = True,
     ) -> None:
         """
+        Run the nnU-Net pipeline.
+
         Args:
-            run_convert_dataset: whether to convert datasets, defaults to False.
-            run_plan_and_process: whether to preprocess and analyze the dataset, defaults to False.
-            run_train: whether to train models, defaults to False.
-            run_find_best_configuration: whether to find the best model (ensemble) configurations, defaults to False.
-            run_predict_ensemble_postprocessing: whether to make predictions on test datasets, defaults to False.
+            run_convert_dataset: whether to convert datasets, defaults to True.
+            run_plan_and_process: whether to preprocess and analyze the dataset, defaults to True.
+            run_train: whether to train models, defaults to True.
+            run_find_best_configuration: whether to find the best model (ensemble) configurations, defaults to True.
+            run_predict_ensemble_postprocessing: whether to make predictions on test datasets, defaults to True.
         """
         if run_convert_dataset:
             self.convert_dataset()
 
         if run_plan_and_process:
             self.plan_and_process()
```

## monai/apps/nnunet/utils.py

```diff
@@ -43,23 +43,23 @@
         datalist_json: original data list .json (required by most monai tutorials).
         data_dir: raw data directory.
     """
     img = monai.transforms.LoadImage(image_only=True, ensure_channel_first=True, simple_keys=True)(
         os.path.join(data_dir, datalist_json["training"][0]["image"])
     )
     num_input_channels = img.size()[0] if img.dim() == 4 else 1
-    logger.warning(f"[info] num_input_channels: {num_input_channels}")
+    logger.info(f"num_input_channels: {num_input_channels}")
 
     num_foreground_classes = 0
     for _i in range(len(datalist_json["training"])):
         seg = monai.transforms.LoadImage(image_only=True, ensure_channel_first=True, simple_keys=True)(
             os.path.join(data_dir, datalist_json["training"][_i]["label"])
         )
         num_foreground_classes = max(num_foreground_classes, int(seg.max()))
-    logger.warning(f"[info] num_foreground_classes: {num_foreground_classes}")
+    logger.info(f"num_foreground_classes: {num_foreground_classes}")
 
     return num_input_channels, num_foreground_classes
 
 
 def create_new_data_copy(
     test_key: str, datalist_json: dict, data_dir: str, num_input_channels: int, output_datafolder: str
 ) -> None:
@@ -78,15 +78,15 @@
 
     for _key, _folder, _label_folder in list(
         zip(["training", test_key], ["imagesTr", "imagesTs"], ["labelsTr", "labelsTs"])
     ):
         if _key is None:
             continue
 
-        logger.warning(f"[info] converting data section: {_key}...")
+        logger.info(f"converting data section: {_key}...")
         for _k in tqdm(range(len(datalist_json[_key]))) if has_tqdm else range(len(datalist_json[_key])):
             orig_img_name = (
                 datalist_json[_key][_k]["image"]
                 if isinstance(datalist_json[_key][_k], dict)
                 else datalist_json[_key][_k]
             )
             img_name = f"case_{_index}"
```

## monai/auto3dseg/algo_gen.py

```diff
@@ -7,23 +7,26 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from __future__ import annotations
 
+from monai.config import PathLike
 from monai.transforms import Randomizable
 
 
 class Algo:
     """
     An algorithm in this context is loosely defined as a data processing pipeline consisting of multiple components
     such as image preprocessing, followed by deep learning model training and evaluation.
     """
 
+    template_path: PathLike | None = None
+
     def set_data_stats(self, *args, **kwargs):
         """Provide dataset (and summaries) so that the model creation can depend on the input datasets."""
         pass
 
     def train(self, *args, **kwargs):
         """Read training/validation data and output a model."""
         pass
```

## monai/auto3dseg/utils.py

```diff
@@ -7,28 +7,29 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from __future__ import annotations
 
+import logging
 import os
 import pickle
 import sys
-import warnings
 from copy import deepcopy
 from numbers import Number
 from typing import Any, cast
 
 import numpy as np
 import torch
 
 from monai.auto3dseg import Algo
 from monai.bundle.config_parser import ConfigParser
 from monai.bundle.utils import ID_SEP_KEY
+from monai.config import PathLike
 from monai.data.meta_tensor import MetaTensor
 from monai.transforms import CropForeground, ToCupy
 from monai.utils import min_version, optional_import
 
 __all__ = [
     "get_foreground_image",
     "get_foreground_label",
@@ -268,86 +269,106 @@
                 return verify_report_format(v[0], v_fmt[0])
             else:
                 return False
 
     return True
 
 
-def algo_to_pickle(algo: Algo, **algo_meta_data: Any) -> str:
+def algo_to_pickle(algo: Algo, template_path: PathLike | None = None, **algo_meta_data: Any) -> str:
     """
-    Export the Algo object to pickle file
+    Export the Algo object to pickle file.
 
     Args:
-        algo: Algo-like object
-        algo_meta_data: additional keyword to save into the dictionary. It may include template_path
-            which is used to instantiate the class. It may also include model training info
+        algo: Algo-like object.
+        template_path: a str path that is needed to be added to the sys.path to instantiate the class.
+        algo_meta_data: additional keyword to save into the dictionary, for example, model training info
             such as acc/best_metrics
 
     Returns:
         filename of the pickled Algo object
     """
-    data = {"algo_bytes": pickle.dumps(algo)}
+    data = {"algo_bytes": pickle.dumps(algo), "template_path": str(template_path)}
     pkl_filename = os.path.join(algo.get_output_path(), "algo_object.pkl")
     for k, v in algo_meta_data.items():
         data.update({k: v})
     data_bytes = pickle.dumps(data)
     with open(pkl_filename, "wb") as f_pi:
         f_pi.write(data_bytes)
     return pkl_filename
 
 
-def algo_from_pickle(pkl_filename: str, **kwargs: Any) -> Any:
+def algo_from_pickle(pkl_filename: str, template_path: PathLike | None = None, **kwargs: Any) -> Any:
     """
-    Import the Algo object from a pickle file
+    Import the Algo object from a pickle file.
 
     Args:
-        pkl_filename: name of the pickle file
-        algo_templates_dir: the algorithm script folder which is needed to instantiate the object.
-            If it is None, the function will use the internal ``'algo_templates_dir`` in the object
-            dict.
+        pkl_filename: the name of the pickle file.
+        template_path: a folder containing files to instantiate the Algo. Besides the `template_path`,
+        this function will also attempt to use the `template_path` saved in the pickle file and a directory
+        named `algorithm_templates` in the parent folder of the folder containing the pickle file.
 
     Returns:
-        algo: Algo-like object
+        algo: the Algo object saved in the pickle file.
+        algo_meta_data: additional keyword saved in the pickle file, for example, acc/best_metrics.
 
     Raises:
-        ValueError if the pkl_filename does not contain a dict, or the dict does not contain
-            ``template_path`` or ``algo_bytes``
+        ValueError if the pkl_filename does not contain a dict, or the dict does not contain `algo_bytes`.
+        ModuleNotFoundError if it is unable to instiante the Algo class.
+
     """
     with open(pkl_filename, "rb") as f_pi:
         data_bytes = f_pi.read()
     data = pickle.loads(data_bytes)
 
     if not isinstance(data, dict):
         raise ValueError(f"the data object is {data.__class__}. Dict is expected.")
 
     if "algo_bytes" not in data:
         raise ValueError(f"key [algo_bytes] not found in {data}. Unable to instantiate.")
 
     algo_bytes = data.pop("algo_bytes")
-    algo_meta_data = {}
+    algo_template_path = data.pop("template_path", None)
+
+    template_paths_candidates: list[str] = []
 
-    if "template_path" in kwargs:  # add template_path to sys.path
-        template_path = kwargs["template_path"]
-        if template_path is None:  # then load template_path from pickled data
-            if "template_path" not in data:
-                raise ValueError(f"key [template_path] not found in {data}")
-            template_path = data.pop("template_path")
-
-        if not os.path.isdir(template_path):
-            raise ValueError(f"Algorithm templates {template_path} is not a directory")
-        # Example of template path: "algorithm_templates/dints".
-        sys.path.insert(0, os.path.abspath(os.path.join(template_path, "..")))
-        algo_meta_data.update({"template_path": template_path})
+    if os.path.isdir(str(template_path)):
+        template_paths_candidates.append(os.path.abspath(str(template_path)))
+        template_paths_candidates.append(os.path.abspath(os.path.join(str(template_path), "..")))
+
+    if os.path.isdir(str(algo_template_path)):
+        template_paths_candidates.append(os.path.abspath(algo_template_path))
+        template_paths_candidates.append(os.path.abspath(os.path.join(algo_template_path, "..")))
 
-    algo = pickle.loads(algo_bytes)
     pkl_dir = os.path.dirname(pkl_filename)
-    if pkl_dir != algo.get_output_path():
-        warnings.warn(
-            f"{algo.get_output_path()} does not contain {pkl_filename}."
-            f"Now override the Algo output_path with: {pkl_dir}"
-        )
+    algo_template_path_fuzzy = os.path.join(pkl_dir, "..", "algorithm_templates")
+
+    if os.path.isdir(algo_template_path_fuzzy):
+        template_paths_candidates.append(os.path.abspath(algo_template_path_fuzzy))
+
+    if len(template_paths_candidates) == 0:
+        # no template_path provided or needed
+        algo = pickle.loads(algo_bytes)
+        algo.template_path = None
+    else:
+        for i, p in enumerate(template_paths_candidates):
+            try:
+                sys.path.append(p)
+                algo = pickle.loads(algo_bytes)
+                break
+            except ModuleNotFoundError as not_found_err:
+                logging.debug(f"Folder {p} doesn't contain the Algo templates for Algo instantiation.")
+                sys.path.pop()
+                if i == len(template_paths_candidates) - 1:
+                    raise ValueError(
+                        f"Failed to instantiate {pkl_filename} with {template_paths_candidates}"
+                    ) from not_found_err
+        algo.template_path = p
+
+    if os.path.abspath(pkl_dir) != os.path.abspath(algo.get_output_path()):
+        logging.debug(f"{algo.get_output_path()} is changed. Now override the Algo output_path with: {pkl_dir}.")
         algo.output_path = pkl_dir
 
+    algo_meta_data = {}
     for k, v in data.items():
         algo_meta_data.update({k: v})
 
     return algo, algo_meta_data
```

## monai/bundle/config_parser.py

```diff
@@ -341,26 +341,27 @@
                 sub_id = f"{id}{ID_SEP_KEY}{k}" if id != "" else k
                 config[k] = self._do_resolve(v, sub_id)
         if isinstance(config, str):
             config = self.resolve_relative_ids(id, config)
             if config.startswith(MACRO_KEY):
                 path, ids = ConfigParser.split_path_id(config[len(MACRO_KEY) :])
                 parser = ConfigParser(config=self.get() if not path else ConfigParser.load_config_file(path))
-                return parser[ids]
+                # deepcopy to ensure the macro replacement is independent config content
+                return deepcopy(parser[ids])
         return config
 
     def resolve_macro_and_relative_ids(self):
         """
         Recursively resolve `self.config` to replace the relative ids with absolute ids, for example,
         `@##A` means `A` in the upper level. and replace the macro tokens with target content,
         The macro tokens are marked as starting with "%", can be from another structured file, like:
         ``"%default_net"``, ``"%/data/config.json#net"``.
 
         """
-        self.set(self._do_resolve(config=deepcopy(self.get())))
+        self.set(self._do_resolve(config=self.get()))
 
     def _do_parse(self, config: Any, id: str = "") -> None:
         """
         Recursively parse the nested data in config source, add every item as `ConfigItem` to the resolver.
 
         Args:
             config: config source to parse.
@@ -371,22 +372,20 @@
 
         """
         if isinstance(config, (dict, list)):
             for k, v in enumerate(config) if isinstance(config, list) else config.items():
                 sub_id = f"{id}{ID_SEP_KEY}{k}" if id != "" else k
                 self._do_parse(config=v, id=sub_id)
 
-        # copy every config item to make them independent and add them to the resolver
-        item_conf = deepcopy(config)
-        if ConfigComponent.is_instantiable(item_conf):
-            self.ref_resolver.add_item(ConfigComponent(config=item_conf, id=id, locator=self.locator))
-        elif ConfigExpression.is_expression(item_conf):
-            self.ref_resolver.add_item(ConfigExpression(config=item_conf, id=id, globals=self.globals))
+        if ConfigComponent.is_instantiable(config):
+            self.ref_resolver.add_item(ConfigComponent(config=config, id=id, locator=self.locator))
+        elif ConfigExpression.is_expression(config):
+            self.ref_resolver.add_item(ConfigExpression(config=config, id=id, globals=self.globals))
         else:
-            self.ref_resolver.add_item(ConfigItem(config=item_conf, id=id))
+            self.ref_resolver.add_item(ConfigItem(config=config, id=id))
 
     @classmethod
     def load_config_file(cls, filepath: PathLike, **kwargs: Any) -> dict:
         """
         Load config file with specified file path (currently support JSON and YAML files).
 
         Args:
```

## monai/bundle/properties.py

```diff
@@ -155,14 +155,29 @@
         BundlePropertyConfig.ID: "bundle_root",
     },
     "device": {
         BundleProperty.DESC: "target device to execute the bundle workflow.",
         BundleProperty.REQUIRED: True,
         BundlePropertyConfig.ID: "device",
     },
+    "dataset_dir": {
+        BundleProperty.DESC: "directory path of the dataset.",
+        BundleProperty.REQUIRED: True,
+        BundlePropertyConfig.ID: "dataset_dir",
+    },
+    "dataset": {
+        BundleProperty.DESC: "PyTorch dataset object for the inference / evaluation logic.",
+        BundleProperty.REQUIRED: True,
+        BundlePropertyConfig.ID: "dataset",
+    },
+    "dataset_data": {
+        BundleProperty.DESC: "data source for the inference / evaluation dataset.",
+        BundleProperty.REQUIRED: True,
+        BundlePropertyConfig.ID: f"dataset{ID_SEP_KEY}data",
+    },
     "evaluator": {
         BundleProperty.DESC: "inference / evaluation workflow engine.",
         BundleProperty.REQUIRED: True,
         BundlePropertyConfig.ID: "evaluator",
     },
     "network_def": {
         BundleProperty.DESC: "network module for the inference.",
@@ -170,14 +185,20 @@
         BundlePropertyConfig.ID: "network_def",
     },
     "inferer": {
         BundleProperty.DESC: "MONAI Inferer object to execute the model computation in inference.",
         BundleProperty.REQUIRED: True,
         BundlePropertyConfig.ID: "inferer",
     },
+    "handlers": {
+        BundleProperty.DESC: "event-handlers for the inference / evaluation logic.",
+        BundleProperty.REQUIRED: False,
+        BundlePropertyConfig.ID: "handlers",
+        BundlePropertyConfig.REF_ID: f"evaluator{ID_SEP_KEY}val_handlers",
+    },
     "preprocessing": {
         BundleProperty.DESC: "preprocessing for the input data.",
         BundleProperty.REQUIRED: False,
         BundlePropertyConfig.ID: "preprocessing",
         BundlePropertyConfig.REF_ID: f"dataset{ID_SEP_KEY}transform",
     },
     "postprocessing": {
```

## monai/bundle/scripts.py

```diff
@@ -604,24 +604,30 @@
     Specify `config_file` to run monai bundle components and workflows.
 
     Typical usage examples:
 
     .. code-block:: bash
 
         # Execute this module as a CLI entry:
+        python -m monai.bundle run --meta_file <meta path> --config_file <config path>
+
+        # Execute with specified `run_id=training`:
         python -m monai.bundle run training --meta_file <meta path> --config_file <config path>
 
+        # Execute with all specified `run_id=runtest`, `init_id=inittest`, `final_id=finaltest`:
+        python -m monai.bundle run --run_id runtest --init_id inittest --final_id finaltest ...
+
         # Override config values at runtime by specifying the component id and its new value:
-        python -m monai.bundle run training --net#input_chns 1 ...
+        python -m monai.bundle run --net#input_chns 1 ...
 
         # Override config values with another config file `/path/to/another.json`:
-        python -m monai.bundle run evaluating --net %/path/to/another.json ...
+        python -m monai.bundle run --net %/path/to/another.json ...
 
         # Override config values with part content of another config file:
-        python -m monai.bundle run training --net %/data/other.json#net_arg ...
+        python -m monai.bundle run --net %/data/other.json#net_arg ...
 
         # Set default args of `run` in a JSON / YAML file, help to record and simplify the command line.
         # Other args still can override the default args at runtime:
         python -m monai.bundle run --args_file "/workspace/data/args.json" --config_file <config path>
 
     Args:
         run_id: ID name of the expected config expression to run, default to "run".
@@ -852,14 +858,15 @@
     net_id: str | None = None,
     meta_file: str | Sequence[str] | None = None,
     config_file: str | Sequence[str] | None = None,
     device: str | None = None,
     p: int | None = None,
     n: int | None = None,
     any: int | None = None,
+    extra_forward_args: dict | None = None,
     args_file: str | None = None,
     **override: Any,
 ) -> None:
     """
     Verify the input and output data shape and data type of network defined in the metadata.
     Will test with fake Tensor data according to the required data shape in `metadata`.
 
@@ -875,14 +882,16 @@
             if it is a list of file paths, the content of them will be merged.
         config_file: filepath of the config file to get network definition, if `None`, must be provided in `args_file`.
             if it is a list of file paths, the content of them will be merged.
         device: target device to run the network forward computation, if None, prefer to "cuda" if existing.
         p: power factor to generate fake data shape if dim of expected shape is "x**p", default to 1.
         n: multiply factor to generate fake data shape if dim of expected shape is "x*n", default to 1.
         any: specified size to generate fake data shape if dim of expected shape is "*", default to 1.
+        extra_forward_args: a dictionary that contains other args for the forward function of the network.
+            Default to an empty dictionary.
         args_file: a JSON or YAML file to provide default values for `net_id`, `meta_file`, `config_file`,
             `device`, `p`, `n`, `any`, and override pairs. so that the command line inputs can be simplified.
         override: id-value pairs to override or add the corresponding config content.
             e.g. ``--_meta#network_data_format#inputs#image#num_channels 3``.
 
     """
 
@@ -891,19 +900,28 @@
         net_id=net_id,
         meta_file=meta_file,
         config_file=config_file,
         device=device,
         p=p,
         n=n,
         any=any,
+        extra_forward_args=extra_forward_args,
         **override,
     )
     _log_input_summary(tag="verify_net_in_out", args=_args)
-    config_file_, meta_file_, net_id_, device_, p_, n_, any_ = _pop_args(
-        _args, "config_file", "meta_file", net_id="", device="cuda:0" if is_available() else "cpu", p=1, n=1, any=1
+    config_file_, meta_file_, net_id_, device_, p_, n_, any_, extra_forward_args_ = _pop_args(
+        _args,
+        "config_file",
+        "meta_file",
+        net_id="",
+        device="cuda:0" if is_available() else "cpu",
+        p=1,
+        n=1,
+        any=1,
+        extra_forward_args={},
     )
 
     parser = ConfigParser()
     parser.read_config(f=config_file_)
     parser.read_meta(f=meta_file_)
 
     # the rest key-values in the _args are to override config content
@@ -923,18 +941,18 @@
         test_data = torch.rand(*(1, input_channels, *spatial_shape), dtype=input_dtype, device=device_)
         if input_dtype == torch.float16:
             # fp16 can only be executed in gpu mode
             net.to("cuda")
             from torch.cuda.amp import autocast
 
             with autocast():
-                output = net(test_data.cuda())
+                output = net(test_data.cuda(), **extra_forward_args_)
             net.to(device_)
         else:
-            output = net(test_data)
+            output = net(test_data, **extra_forward_args_)
         if output.shape[1] != output_channels:
             raise ValueError(f"output channel number `{output.shape[1]}` doesn't match: `{output_channels}`.")
         if output.dtype != output_dtype:
             raise ValueError(f"dtype of output data `{output.dtype}` doesn't match: {output_dtype}.")
     logger.info("data shape of network is verified with no error.")
 
 
@@ -1458,15 +1476,15 @@
     with open(str(docs_dir / "README.md"), "w") as o:
         readme = """
         # Your Model Name
 
         Describe your model here and how to run it, for example using `inference.json`:
 
         ```
-        python -m monai.bundle run evaluating \
+        python -m monai.bundle run \
             --meta_file /path/to/bundle/configs/metadata.json \
             --config_file /path/to/bundle/configs/inference.json \
             --dataset_dir ./input \
             --bundle_root /path/to/bundle
         ```
         """
```

## monai/bundle/workflows.py

```diff
@@ -11,14 +11,15 @@
 
 from __future__ import annotations
 
 import os
 import time
 import warnings
 from abc import ABC, abstractmethod
+from copy import copy
 from logging.config import fileConfig
 from pathlib import Path
 from typing import Any, Sequence
 
 from monai.apps.utils import get_logger
 from monai.bundle.config_parser import ConfigParser
 from monai.bundle.properties import InferProperties, TrainProperties
@@ -49,18 +50,18 @@
 
     def __init__(self, workflow: str | None = None):
         if workflow is None:
             self.properties = None
             self.workflow = None
             return
         if workflow.lower() in self.supported_train_type:
-            self.properties = TrainProperties
+            self.properties = copy(TrainProperties)
             self.workflow = "train"
         elif workflow.lower() in self.supported_infer_type:
-            self.properties = InferProperties
+            self.properties = copy(InferProperties)
             self.workflow = "infer"
         else:
             raise ValueError(f"Unsupported workflow type: '{workflow}'.")
 
     @abstractmethod
     def initialize(self, *args: Any, **kwargs: Any) -> Any:
         """
@@ -125,14 +126,32 @@
     def get_workflow_type(self):
         """
         Get the workflow type, it can be `None`, "train", or "infer".
 
         """
         return self.workflow
 
+    def add_property(self, name: str, required: str, desc: str | None = None) -> None:
+        """
+        Besides the default predefined properties, some 3rd party aplications may need the bundle
+        definition to provide additonal properties for the specific use cases, if the bundlle can't
+        provide the property, means it can't work with the application.
+        This utility adds the property for the application requirements check and access.
+
+        Args:
+            name: the name of target property.
+            required: whether the property is "must-have".
+            desc: descriptions for the property.
+        """
+        if self.properties is None:
+            self.properties = {}
+        if name in self.properties:
+            warnings.warn(f"property '{name}' already exists in the properties list, overriding it.")
+        self.properties[name] = {BundleProperty.DESC: desc, BundleProperty.REQUIRED: required}
+
     def check_properties(self) -> list[str] | None:
         """
         Check whether the required properties are existing in the bundle workflow.
         If no workflow type specified, return None, otherwise, return a list of required but missing properties.
 
         """
         if self.properties is None:
@@ -312,14 +331,33 @@
         prop_id = self._get_prop_id(name, property)
         if prop_id is not None:
             self.parser[prop_id] = value
             # must parse the config again after changing the content
             self._is_initialized = False
             self.parser.ref_resolver.reset()
 
+    def add_property(  # type: ignore[override]
+        self, name: str, required: str, config_id: str, desc: str | None = None
+    ) -> None:
+        """
+        Besides the default predefined properties, some 3rd party aplications may need the bundle
+        definition to provide additonal properties for the specific use cases, if the bundlle can't
+        provide the property, means it can't work with the application.
+        This utility adds the property for the application requirements check and access.
+
+        Args:
+            name: the name of target property.
+            required: whether the property is "must-have".
+            config_id: the config ID of target property in the bundle definition.
+            desc: descriptions for the property.
+
+        """
+        super().add_property(name=name, required=required, desc=desc)
+        self.properties[name][BundlePropertyConfig.ID] = config_id  # type: ignore[index]
+
     def _check_optional_id(self, name: str, property: dict) -> bool:
         """
         If an optional property has reference in the config, check whether the property is existing.
         If `ValidationHandler` is defined for a training workflow, will check whether the optional properties
         "evaluator" and "val_interval" are existing.
 
         Args:
```

## monai/handlers/mlflow_handler.py

```diff
@@ -19,16 +19,18 @@
 
 import torch
 
 from monai.config import IgniteInfo
 from monai.utils import ensure_tuple, min_version, optional_import
 
 Events, _ = optional_import("ignite.engine", IgniteInfo.OPT_IMPORT_VERSION, min_version, "Events")
-mlflow, _ = optional_import("mlflow")
-mlflow.entities, _ = optional_import("mlflow.entities")
+mlflow, _ = optional_import("mlflow", descriptor="Please install mlflow before using MLFlowHandler.")
+mlflow.entities, _ = optional_import(
+    "mlflow.entities", descriptor="Please install mlflow.entities before using MLFlowHandler."
+)
 
 if TYPE_CHECKING:
     from ignite.engine import Engine
 else:
     Engine, _ = optional_import(
         "ignite.engine", IgniteInfo.OPT_IMPORT_VERSION, min_version, "Engine", as_type="decorator"
     )
@@ -72,29 +74,31 @@
             Must accept parameter "engine", use default logger if None.
         output_transform: a callable that is used to transform the
             ``ignite.engine.state.output`` into a scalar to track, or a dictionary of {key: scalar}.
             By default this value logging happens when every iteration completed.
             The default behavior is to track loss from output[0] as output is a decollated list
             and we replicated loss value for every item of the decollated list.
             `engine.state` and `output_transform` inherit from the ignite concept:
-            https://pytorch.org/ignite/concepts.html#state, explanation and usage example are in the tutorial:
+            https://pytorch-ignite.ai/concepts/03-state/, explanation and usage example are in the tutorial:
             https://github.com/Project-MONAI/tutorials/blob/master/modules/batch_output_transform.ipynb.
         global_epoch_transform: a callable that is used to customize global epoch number.
             For example, in evaluation, the evaluator engine might want to track synced epoch number
             with the trainer engine.
         state_attributes: expected attributes from `engine.state`, if provided, will extract them
             when epoch completed.
         tag_name: when iteration output is a scalar, `tag_name` is used to track, defaults to `'Loss'`.
-        experiment_name: name for an experiment, defaults to `default_experiment`.
-        run_name: name for run in an experiment.
-        experiment_param: a dict recording parameters which will not change through whole experiment,
+        experiment_name: the experiment name of MLflow, default to `'monai_experiment'`. An experiment can be
+            used to record several runs.
+        run_name: the run name in an experiment. A run can be used to record information about a workflow,
+            like the loss, metrics and so on.
+        experiment_param: a dict recording parameters which will not change through the whole workflow,
             like torch version, cuda version and so on.
-        artifacts: paths to images that need to be recorded after a whole run.
-        optimizer_param_names: parameters' name in optimizer that need to be record during running,
-            defaults to "lr".
+        artifacts: paths to images that need to be recorded after running the workflow.
+        optimizer_param_names: parameter names in the optimizer that need to be recorded during running the
+            workflow, default to `'lr'`.
         close_on_complete: whether to close the mlflow run in `complete` phase in workflow, default to False.
 
     For more details of MLFlow usage, please refer to: https://mlflow.org/docs/latest/index.html.
 
     """
 
     # parameters that are logged at the start of training
@@ -128,14 +132,15 @@
         self.tag_name = tag_name
         self.experiment_name = experiment_name
         self.run_name = run_name
         self.experiment_param = experiment_param
         self.artifacts = ensure_tuple(artifacts)
         self.optimizer_param_names = ensure_tuple(optimizer_param_names)
         self.client = mlflow.MlflowClient(tracking_uri=tracking_uri if tracking_uri else None)
+        self.run_finish_status = mlflow.entities.RunStatus.to_string(mlflow.entities.RunStatus.FINISHED)
         self.close_on_complete = close_on_complete
         self.experiment = None
         self.cur_run = None
 
     def _delete_exist_param_in_dict(self, param_dict: dict) -> None:
         """
         Delete parameters in given dict, if they are already logged by current mlflow run.
@@ -187,14 +192,16 @@
         if not self.experiment:
             raise ValueError(f"Failed to set experiment '{self.experiment_name}' as the active experiment")
 
         if not self.cur_run:
             run_name = f"run_{time.strftime('%Y%m%d_%H%M%S')}" if self.run_name is None else self.run_name
             runs = self.client.search_runs(self.experiment.experiment_id)
             runs = [r for r in runs if r.info.run_name == run_name or not self.run_name]
+            # runs marked as finish should not record info any more
+            runs = [r for r in runs if r.info.status != self.run_finish_status]
             if runs:
                 self.cur_run = self.client.get_run(runs[-1].info.run_id)  # pick latest active run
             else:
                 self.cur_run = self.client.create_run(experiment_id=self.experiment.experiment_id, run_name=run_name)
 
         if self.experiment_param:
             self._log_params(self.experiment_param)
@@ -260,16 +267,15 @@
 
     def close(self) -> None:
         """
         Stop current running logger of MLFlow.
 
         """
         if self.cur_run:
-            status = mlflow.entities.RunStatus.to_string(mlflow.entities.RunStatus.FINISHED)
-            self.client.set_terminated(self.cur_run.info.run_id, status)
+            self.client.set_terminated(self.cur_run.info.run_id, self.run_finish_status)
             self.cur_run = None
 
     def epoch_completed(self, engine: Engine) -> None:
         """
         Handler for train or validation/evaluation epoch completed Event.
         Track epoch level log, default values are from Ignite `engine.state.metrics` dict.
```

## monai/inferers/inferer.py

```diff
@@ -457,14 +457,15 @@
             args: optional args to be passed to ``network``.
             kwargs: optional keyword args to be passed to ``network``.
 
         """
 
         device = kwargs.pop("device", self.device)
         buffer_steps = kwargs.pop("buffer_steps", self.buffer_steps)
+        buffer_dim = kwargs.pop("buffer_dim", self.buffer_dim)
 
         if device is None and self.cpu_thresh is not None and inputs.shape[2:].numel() > self.cpu_thresh:
             device = "cpu"  # stitch in cpu memory if image is too large
 
         return sliding_window_inference(
             inputs,
             self.roi_size,
@@ -477,15 +478,15 @@
             self.cval,
             self.sw_device,
             device,
             self.progress,
             self.roi_weight_map,
             None,
             buffer_steps,
-            self.buffer_dim,
+            buffer_dim,
             *args,
             **kwargs,
         )
 
 
 class SlidingWindowInfererAdapt(SlidingWindowInferer):
     """
@@ -520,22 +521,29 @@
             return super().__call__(inputs, network, *args, **kwargs)
 
         skip_buffer = self.buffer_steps is not None and self.buffer_steps <= 0
         cpu_cond = self.cpu_thresh is not None and inputs.shape[2:].numel() > self.cpu_thresh
         gpu_stitching = inputs.is_cuda and not cpu_cond
         buffered_stitching = inputs.is_cuda and cpu_cond and not skip_buffer
         buffer_steps = max(1, self.buffer_steps) if self.buffer_steps is not None else 1
+        buffer_dim = -1
+
+        sh = list(inputs.shape[2:])
+        max_dim = sh.index(max(sh))
+        if inputs.shape[max_dim + 2] / inputs.shape[-1] >= 2:
+            buffer_dim = max_dim
 
         for _ in range(10):  # at most 10 trials
             try:
                 return super().__call__(
                     inputs,
                     network,
                     device=inputs.device if gpu_stitching else torch.device("cpu"),
                     buffer_steps=buffer_steps if buffered_stitching else None,
+                    buffer_dim=buffer_dim,
                     *args,
                     **kwargs,
                 )
             except RuntimeError as e:
                 if not gpu_stitching and not buffered_stitching or "OutOfMemoryError" not in str(type(e).__name__):
                     raise e
 
@@ -543,32 +551,31 @@
 
                 if gpu_stitching:  # if failed on gpu
                     gpu_stitching = False
                     self.cpu_thresh = inputs.shape[2:].numel() - 1  # update thresh
 
                     if skip_buffer:
                         buffered_stitching = False
-                        logger.warning(f"GPU stitching failed, attempting on CPU, image dim {inputs.shape}..")
+                        logger.warning(f"GPU stitching failed, attempting on CPU, image dim {inputs.shape}.")
 
                     else:
                         buffered_stitching = True
                         self.buffer_steps = buffer_steps
                         logger.warning(
-                            f"GPU stitching failed, attempting with buffer {buffer_steps}, image dim {inputs.shape}.."
+                            f"GPU stitching failed, buffer {buffer_steps} dim {buffer_dim}, image dim {inputs.shape}."
                         )
                 elif buffer_steps > 1:
                     buffer_steps = max(1, buffer_steps // 2)
                     self.buffer_steps = buffer_steps
                     logger.warning(
-                        f"GPU buffered stitching failed, image dim {inputs.shape} reducing buffer to {buffer_steps}"
+                        f"GPU buffered stitching failed, image dim {inputs.shape} reducing buffer to {buffer_steps}."
                     )
                 else:
                     buffered_stitching = False
-                    self.buffer_steps = 0  # disable future buffer attempts
-                    logger.warning(f"GPU buffered stitching failed, attempting on CPU, image dim {inputs.shape}")
+                    logger.warning(f"GPU buffered stitching failed, attempting on CPU, image dim {inputs.shape}.")
         raise RuntimeError(  # not possible to finish after the trials
             f"SlidingWindowInfererAdapt {skip_buffer} {cpu_cond} {gpu_stitching} {buffered_stitching} {buffer_steps}"
         )
 
 
 class SaliencyInferer(Inferer):
     """
```

## monai/networks/nets/dynunet.py

```diff
@@ -265,18 +265,19 @@
         else:
             self.filters = filters[: len(self.strides)]
 
     def forward(self, x):
         out = self.skip_layers(x)
         out = self.output_block(out)
         if self.training and self.deep_supervision:
-            out_all = [out]
-            for feature_map in self.heads:
-                out_all.append(interpolate(feature_map, out.shape[2:]))
-            return torch.stack(out_all, dim=1)
+            out_all = torch.zeros(out.shape[0], len(self.heads) + 1, *out.shape[1:], device=out.device, dtype=out.dtype)
+            out_all[:, 0] = out
+            for idx, feature_map in enumerate(self.heads):
+                out_all[:, idx + 1] = interpolate(feature_map, out.shape[2:])
+            return out_all
         return out
 
     def get_input_block(self):
         return self.conv_block(
             self.spatial_dims,
             self.in_channels,
             self.filters[0],
```

## monai/networks/nets/regunet.py

```diff
@@ -61,15 +61,15 @@
             in_channels: number of input channels
             num_channel_initial: number of initial channels
             depth: input is at level 0, bottom is at level depth.
             out_kernel_initializer: kernel initializer for the last layer
             out_activation: activation at the last layer
             out_channels: number of channels for the output
             extract_levels: list, which levels from net to extract. The maximum level must equal to ``depth``
-            pooling: for down-sampling, use non-parameterized pooling if true, otherwise use conv3d
+            pooling: for down-sampling, use non-parameterized pooling if true, otherwise use conv
             concat_skip: when up-sampling, concatenate skipped tensor if true, otherwise use addition
             encode_kernel_sizes: kernel size for down-sampling
         """
         super().__init__()
         if not extract_levels:
             extract_levels = (depth,)
         if max(extract_levels) != depth:
@@ -230,15 +230,30 @@
             outs.append(decoded)
 
         out = self.output_block(outs, image_size=image_size)
         return out
 
 
 class AffineHead(nn.Module):
-    def __init__(self, spatial_dims: int, image_size: list[int], decode_size: list[int], in_channels: int):
+    def __init__(
+        self,
+        spatial_dims: int,
+        image_size: list[int],
+        decode_size: list[int],
+        in_channels: int,
+        save_theta: bool = False,
+    ):
+        """
+        Args:
+            spatial_dims: number of spatial dimensions
+            image_size: output spatial size
+            decode_size: input spatial size (two or three integers depending on ``spatial_dims``)
+            in_channels: number of input channels
+            save_theta: whether to save the theta matrix estimation
+        """
         super().__init__()
         self.spatial_dims = spatial_dims
         if spatial_dims == 2:
             in_features = in_channels * decode_size[0] * decode_size[1]
             out_features = 6
             out_init = torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)
         elif spatial_dims == 3:
@@ -251,14 +266,17 @@
         self.fc = nn.Linear(in_features=in_features, out_features=out_features)
         self.grid = self.get_reference_grid(image_size)  # (spatial_dims, ...)
 
         # init weight/bias
         self.fc.weight.data.zero_()
         self.fc.bias.data.copy_(out_init)
 
+        self.save_theta = save_theta
+        self.theta = torch.Tensor()
+
     @staticmethod
     def get_reference_grid(image_size: tuple[int] | list[int]) -> torch.Tensor:
         mesh_points = [torch.arange(0, dim) for dim in image_size]
         grid = torch.stack(meshgrid_ij(*mesh_points), dim=0)  # (spatial_dims, ...)
         return grid.to(dtype=torch.float)
 
     def affine_transform(self, theta: torch.Tensor):
@@ -274,14 +292,16 @@
             raise ValueError(f"do not support spatial_dims={self.spatial_dims}")
         return grid_warped
 
     def forward(self, x: list[torch.Tensor], image_size: list[int]) -> torch.Tensor:
         f = x[0]
         self.grid = self.grid.to(device=f.device)
         theta = self.fc(f.reshape(f.shape[0], -1))
+        if self.save_theta:
+            self.theta = theta.detach()
         out: torch.Tensor = self.affine_transform(theta) - self.grid
         return out
 
 
 class GlobalNet(RegUNet):
     """
     Build GlobalNet for image registration.
@@ -301,24 +321,40 @@
         num_channel_initial: int,
         depth: int,
         out_kernel_initializer: str | None = "kaiming_uniform",
         out_activation: str | None = None,
         pooling: bool = True,
         concat_skip: bool = False,
         encode_kernel_sizes: int | list[int] = 3,
+        save_theta: bool = False,
     ):
+        """
+        Args:
+            image_size: output displacement field spatial size
+            spatial_dims: number of spatial dims
+            in_channels: number of input channels
+            num_channel_initial: number of initial channels
+            depth: input is at level 0, bottom is at level depth.
+            out_kernel_initializer: kernel initializer for the last layer
+            out_activation: activation at the last layer
+            pooling: for down-sampling, use non-parameterized pooling if true, otherwise use conv
+            concat_skip: when up-sampling, concatenate skipped tensor if true, otherwise use addition
+            encode_kernel_sizes: kernel size for down-sampling
+            save_theta: whether to save the theta matrix estimation
+        """
         for size in image_size:
             if size % (2**depth) != 0:
                 raise ValueError(
                     f"given depth {depth}, "
                     f"all input spatial dimension must be divisible by {2 ** depth}, "
                     f"got input of size {image_size}"
                 )
         self.image_size = image_size
         self.decode_size = [size // (2**depth) for size in image_size]
+        self.save_theta = save_theta
         super().__init__(
             spatial_dims=spatial_dims,
             in_channels=in_channels,
             num_channel_initial=num_channel_initial,
             depth=depth,
             out_kernel_initializer=out_kernel_initializer,
             out_activation=out_activation,
@@ -330,14 +366,15 @@
 
     def build_output_block(self):
         return AffineHead(
             spatial_dims=self.spatial_dims,
             image_size=self.image_size,
             decode_size=self.decode_size,
             in_channels=self.num_channels[-1],
+            save_theta=self.save_theta,
         )
 
 
 class AdditiveUpSampleBlock(nn.Module):
     def __init__(
         self,
         spatial_dims: int,
```

## monai/transforms/__init__.py

```diff
@@ -656,14 +656,15 @@
     rand_choice,
     remove_small_objects,
     rescale_array,
     rescale_array_int_max,
     rescale_instance_array,
     reset_ops_id,
     resize_center,
+    resolves_modes,
     sync_meta_info,
     weighted_patch_samples,
     zero_margins,
 )
 from .utils_pytorch_numpy_unification import (
     allclose,
     any_np_pt,
```

## monai/transforms/inverse.py

```diff
@@ -222,20 +222,26 @@
             else:
                 info[LazyAttr.AFFINE] = affine
             info[LazyAttr.AFFINE] = convert_to_tensor(info[LazyAttr.AFFINE], device=torch.device("cpu"))
             out_obj.push_pending_operation(info)
         else:
             if out_obj.pending_operations:
                 transform_name = info.get(TraceKeys.CLASS_NAME, "") if isinstance(info, dict) else ""
-                warnings.warn(
+                msg = (
                     f"Applying transform {transform_name} to a MetaTensor with pending operations "
                     "is not supported (as this eventually changes the ordering of applied_operations when the pending "
                     f"operations are executed). Please clear the pending operations before transform {transform_name}."
                     f"\nPending operations: {[x.get(TraceKeys.CLASS_NAME) for x in out_obj.pending_operations]}."
                 )
+                pend = out_obj.pending_operations[-1]
+                if not isinstance(pend.get(TraceKeys.EXTRA_INFO), dict):
+                    pend[TraceKeys.EXTRA_INFO] = dict(pend.get(TraceKeys.EXTRA_INFO, {}))
+                if not isinstance(info.get(TraceKeys.EXTRA_INFO), dict):
+                    info[TraceKeys.EXTRA_INFO] = dict(info.get(TraceKeys.EXTRA_INFO, {}))
+                info[TraceKeys.EXTRA_INFO]["warn"] = pend[TraceKeys.EXTRA_INFO]["warn"] = msg
             out_obj.push_applied_operation(info)
         if isinstance(data, Mapping):
             if not isinstance(data, dict):
                 data = dict(data)
             if isinstance(data_t, MetaTensor):
                 data[key] = data_t.copy_meta_from(out_obj)
             else:
@@ -251,14 +257,17 @@
         xform_id = transform.get(TraceKeys.ID, "")
         if xform_id == id(self):
             return
         # TraceKeys.NONE to skip the id check
         if xform_id == TraceKeys.NONE:
             return
         xform_name = transform.get(TraceKeys.CLASS_NAME, "")
+        warning_msg = transform.get(TraceKeys.EXTRA_INFO, {}).get("warn")
+        if warning_msg:
+            warnings.warn(warning_msg)
         # basic check if multiprocessing uses 'spawn' (objects get recreated so don't have same ID)
         if torch.multiprocessing.get_start_method() in ("spawn", None) and xform_name == self.__class__.__name__:
             return
         raise RuntimeError(
             f"Error {self.__class__.__name__} getting the most recently "
             f"applied invertible transform {xform_name} {xform_id} != {id(self)}."
         )
```

## monai/transforms/utils.py

```diff
@@ -12,15 +12,15 @@
 from __future__ import annotations
 
 import itertools
 import random
 import warnings
 from collections.abc import Callable, Hashable, Iterable, Mapping, Sequence
 from contextlib import contextmanager
-from functools import wraps
+from functools import lru_cache, wraps
 from inspect import getmembers, isclass
 from typing import Any
 
 import numpy as np
 import torch
 
 import monai
@@ -40,28 +40,32 @@
     searchsorted,
     unique,
     unravel_index,
     where,
 )
 from monai.utils import (
     GridSampleMode,
+    GridSamplePadMode,
     InterpolateMode,
+    NdimageMode,
     NumpyPadMode,
     PostFix,
     PytorchPadMode,
+    SplineMode,
     TraceKeys,
     ensure_tuple,
     ensure_tuple_rep,
     ensure_tuple_size,
     fall_back_tuple,
     get_equivalent_dtype,
     issequenceiterable,
     look_up_option,
     min_version,
     optional_import,
+    pytorch_after,
 )
 from monai.utils.enums import TransformBackends
 from monai.utils.type_conversion import convert_data_type, convert_to_cupy, convert_to_dst_type, convert_to_tensor
 
 measure, has_measure = optional_import("skimage.measure", "0.14.2", min_version)
 morphology, has_morphology = optional_import("skimage.morphology")
 ndimage, _ = optional_import("scipy.ndimage")
@@ -112,14 +116,15 @@
     "convert_pad_mode",
     "convert_to_contiguous",
     "get_unique_labels",
     "scale_affine",
     "attach_hook",
     "sync_meta_info",
     "reset_ops_id",
+    "resolves_modes",
 ]
 
 
 def rand_choice(prob: float = 0.5) -> bool:
     """
     Returns True if a randomly chosen number is less than or equal to `prob`, by default this is a 50/50 chance.
     """
@@ -400,15 +405,18 @@
         if channels > 1:
             label_flat = ravel(convert_data_type(label[c], dtype=bool)[0])
         else:
             label_flat = ravel(label == c)
         if img_flat is not None:
             label_flat = img_flat & label_flat
         # no need to save the indices in GPU, otherwise, still need to move to CPU at runtime when crop by indices
-        cls_indices: NdarrayOrTensor = convert_data_type(nonzero(label_flat), device=torch.device("cpu"))[0]
+        output_type = torch.Tensor if isinstance(label, monai.data.MetaTensor) else None
+        cls_indices: NdarrayOrTensor = convert_data_type(
+            nonzero(label_flat), output_type=output_type, device=torch.device("cpu")
+        )[0]
         if max_samples_per_class and len(cls_indices) > max_samples_per_class and len(cls_indices) > 1:
             sample_id = np.round(np.linspace(0, len(cls_indices) - 1, max_samples_per_class)).astype(int)
             indices.append(cls_indices[sample_id])
         else:
             indices.append(cls_indices)
 
     return indices
@@ -1839,9 +1847,128 @@
     mask2 = (~mask1) & (tmod < w * 2 * torch.pi)
     y[mask2] = 1
     mask3 = (~mask1) & (~mask2)
     y[mask3] = -1
     return y
 
 
+def _to_numpy_resample_interp_mode(interp_mode):
+    ret = look_up_option(str(interp_mode), SplineMode, default=None)
+    if ret is not None:
+        return int(ret)
+    _mapping = {
+        InterpolateMode.NEAREST: SplineMode.ZERO,
+        InterpolateMode.NEAREST_EXACT: SplineMode.ZERO,
+        InterpolateMode.LINEAR: SplineMode.ONE,
+        InterpolateMode.BILINEAR: SplineMode.ONE,
+        InterpolateMode.TRILINEAR: SplineMode.ONE,
+        InterpolateMode.BICUBIC: SplineMode.THREE,
+        InterpolateMode.AREA: SplineMode.ZERO,
+    }
+    ret = look_up_option(str(interp_mode), _mapping, default=None)
+    if ret is not None:
+        return ret
+    return look_up_option(str(interp_mode), list(_mapping) + list(SplineMode))  # for better error msg
+
+
+def _to_torch_resample_interp_mode(interp_mode):
+    ret = look_up_option(str(interp_mode), InterpolateMode, default=None)
+    if ret is not None:
+        return ret
+    _mapping = {
+        SplineMode.ZERO: InterpolateMode.NEAREST_EXACT if pytorch_after(1, 11) else InterpolateMode.NEAREST,
+        SplineMode.ONE: InterpolateMode.LINEAR,
+        SplineMode.THREE: InterpolateMode.BICUBIC,
+    }
+    ret = look_up_option(str(interp_mode), _mapping, default=None)
+    if ret is not None:
+        return ret
+    return look_up_option(str(interp_mode), list(_mapping) + list(InterpolateMode))
+
+
+def _to_numpy_resample_padding_mode(m):
+    ret = look_up_option(str(m), NdimageMode, default=None)
+    if ret is not None:
+        return ret
+    _mapping = {
+        GridSamplePadMode.ZEROS: NdimageMode.CONSTANT,
+        GridSamplePadMode.BORDER: NdimageMode.NEAREST,
+        GridSamplePadMode.REFLECTION: NdimageMode.REFLECT,
+    }
+    ret = look_up_option(str(m), _mapping, default=None)
+    if ret is not None:
+        return ret
+    return look_up_option(str(m), list(_mapping) + list(NdimageMode))
+
+
+def _to_torch_resample_padding_mode(m):
+    ret = look_up_option(str(m), GridSamplePadMode, default=None)
+    if ret is not None:
+        return ret
+    _mapping = {
+        NdimageMode.CONSTANT: GridSamplePadMode.ZEROS,
+        NdimageMode.GRID_CONSTANT: GridSamplePadMode.ZEROS,
+        NdimageMode.NEAREST: GridSamplePadMode.BORDER,
+        NdimageMode.REFLECT: GridSamplePadMode.REFLECTION,
+        NdimageMode.WRAP: GridSamplePadMode.REFLECTION,
+        NdimageMode.GRID_WRAP: GridSamplePadMode.REFLECTION,
+        NdimageMode.GRID_MIRROR: GridSamplePadMode.REFLECTION,
+    }
+    ret = look_up_option(str(m), _mapping, default=None)
+    if ret is not None:
+        return ret
+    return look_up_option(str(m), list(_mapping) + list(GridSamplePadMode))
+
+
+@lru_cache(None)
+def resolves_modes(
+    interp_mode: str | None = "constant", padding_mode="zeros", backend=TransformBackends.TORCH, **kwargs
+):
+    """
+    Automatically adjust the resampling interpolation mode and padding mode,
+    so that they are compatible with the corresponding API of the `backend`.
+    Depending on the availability of the backends, when there's no exact
+    equivalent, a similar mode is returned.
+
+    Args:
+        interp_mode: interpolation mode.
+        padding_mdoe: padding mode.
+        backend: optional backend of `TransformBackends`. If None, the backend will be decided from `interp_mode`.
+        kwargs: additional keyword arguments. currently support ``torch_interpolate_spatial_nd``, to provide
+            additional information to determine ``linear``, ``bilinear`` and ``trilinear``;
+            ``use_compiled`` to use MONAI's precompiled backend (pytorch c++ extensions), default to ``False``.
+    """
+    _interp_mode, _padding_mode, _kwargs = None, None, (kwargs or {}).copy()
+    if backend is None:  # infer backend
+        backend = (
+            TransformBackends.NUMPY
+            if look_up_option(str(interp_mode), SplineMode, default=None) is not None
+            else TransformBackends.TORCH
+        )
+    if backend == TransformBackends.NUMPY:
+        _interp_mode = _to_numpy_resample_interp_mode(interp_mode)
+        _padding_mode = _to_numpy_resample_padding_mode(padding_mode)
+        return backend, _interp_mode, _padding_mode, _kwargs
+    _interp_mode = _to_torch_resample_interp_mode(interp_mode)
+    _padding_mode = _to_torch_resample_padding_mode(padding_mode)
+    if str(_interp_mode).endswith("linear"):
+        nd = _kwargs.pop("torch_interpolate_spatial_nd", 2)
+        if nd == 1:
+            _interp_mode = InterpolateMode.LINEAR
+        elif nd == 3:
+            _interp_mode = InterpolateMode.TRILINEAR
+        else:
+            _interp_mode = InterpolateMode.BILINEAR  # torch grid_sample bilinear is trilinear in 3D
+    if not _kwargs.pop("use_compiled", False):
+        return backend, _interp_mode, _padding_mode, _kwargs
+    _padding_mode = 1 if _padding_mode == "reflection" else _padding_mode
+    if _interp_mode == "bicubic":
+        _interp_mode = 3
+    elif str(_interp_mode).endswith("linear"):
+        _interp_mode = 1
+    else:
+        _interp_mode = GridSampleMode(_interp_mode)
+    return backend, _interp_mode, _padding_mode, _kwargs
+
+
 if __name__ == "__main__":
     print_transform_backends()
```

## monai/transforms/lazy/functional.py

```diff
@@ -12,14 +12,15 @@
 from __future__ import annotations
 
 from typing import Any
 
 import torch
 
 from monai.data.meta_tensor import MetaTensor
+from monai.data.utils import to_affine_nd
 from monai.transforms.lazy.utils import (
     affine_from_pending,
     combine_transforms,
     is_compatible_apply_kwargs,
     kwargs_from_pending,
     resample,
 )
@@ -74,14 +75,17 @@
         data.clear_pending_operations()
     pending = [] if pending is None else pending
 
     if not pending:
         return data, []
 
     cumulative_xform = affine_from_pending(pending[0])
+    if cumulative_xform.shape[0] == 3:
+        cumulative_xform = to_affine_nd(3, cumulative_xform)
+
     cur_kwargs = kwargs_from_pending(pending[0])
     override_kwargs: dict[str, Any] = {}
     if "mode" in overrides:
         override_kwargs[LazyAttr.INTERP_MODE] = overrides["mode"]
     if "padding_mode" in overrides:
         override_kwargs[LazyAttr.PADDING_MODE] = overrides["padding_mode"]
     if "align_corners" in overrides:
@@ -96,14 +100,16 @@
         new_kwargs = kwargs_from_pending(p)
         if not is_compatible_apply_kwargs(cur_kwargs, new_kwargs):
             # carry out an intermediate resample here due to incompatibility between arguments
             _cur_kwargs = cur_kwargs.copy()
             _cur_kwargs.update(override_kwargs)
             data = resample(data.to(device), cumulative_xform, _cur_kwargs)
         next_matrix = affine_from_pending(p)
+        if next_matrix.shape[0] == 3:
+            next_matrix = to_affine_nd(3, next_matrix)
         cumulative_xform = combine_transforms(cumulative_xform, next_matrix)
         cur_kwargs.update(new_kwargs)
     cur_kwargs.update(override_kwargs)
     data = resample(data.to(device), cumulative_xform, cur_kwargs)
     if isinstance(data, MetaTensor):
         for p in pending:
             data.push_applied_operation(p)
```

## monai/transforms/spatial/array.py

```diff
@@ -11,15 +11,14 @@
 """
 A collection of "vanilla" transforms for spatial operations
 https://github.com/Project-MONAI/MONAI/wiki/MONAI_Design
 """
 
 from __future__ import annotations
 
-import functools
 import warnings
 from collections.abc import Callable
 from copy import deepcopy
 from itertools import zip_longest
 from typing import Any, Optional, Sequence, Tuple, Union, cast
 
 import numpy as np
@@ -50,24 +49,23 @@
     create_control_grid,
     create_grid,
     create_rotate,
     create_scale,
     create_shear,
     create_translate,
     map_spatial_axes,
+    resolves_modes,
     scale_affine,
 )
 from monai.transforms.utils_pytorch_numpy_unification import argsort, argwhere, linalg_inv, moveaxis
 from monai.utils import (
     GridSampleMode,
     GridSamplePadMode,
     InterpolateMode,
-    NdimageMode,
     NumpyPadMode,
-    SplineMode,
     convert_to_cupy,
     convert_to_dst_type,
     convert_to_numpy,
     convert_to_tensor,
     ensure_tuple,
     ensure_tuple_rep,
     ensure_tuple_size,
@@ -691,15 +689,15 @@
         align_corners: bool | None = None,
         anti_aliasing: bool = False,
         anti_aliasing_sigma: Sequence[float] | float | None = None,
         dtype: DtypeLike | torch.dtype = torch.float32,
     ) -> None:
         self.size_mode = look_up_option(size_mode, ["all", "longest"])
         self.spatial_size = spatial_size
-        self.mode: InterpolateMode = look_up_option(mode, InterpolateMode)
+        self.mode = mode
         self.align_corners = align_corners
         self.anti_aliasing = anti_aliasing
         self.anti_aliasing_sigma = anti_aliasing_sigma
         self.dtype = dtype
 
     def __call__(
         self,
@@ -755,15 +753,15 @@
         else:  # for the "longest" mode
             img_size = img.peek_pending_shape() if isinstance(img, MetaTensor) else img.shape[1:]
             if not isinstance(self.spatial_size, int):
                 raise ValueError("spatial_size must be an int number if size_mode is 'longest'.")
             scale = self.spatial_size / max(img_size)
             sp_size = tuple(int(round(s * scale)) for s in img_size)
 
-        _mode = look_up_option(self.mode if mode is None else mode, InterpolateMode)
+        _mode = self.mode if mode is None else mode
         _align_corners = self.align_corners if align_corners is None else align_corners
         _dtype = get_equivalent_dtype(dtype or self.dtype or img.dtype, torch.Tensor)
         return resize(  # type: ignore
             img,
             sp_size,
             _mode,
             _align_corners,
@@ -827,16 +825,16 @@
         mode: str = GridSampleMode.BILINEAR,
         padding_mode: str = GridSamplePadMode.BORDER,
         align_corners: bool = False,
         dtype: DtypeLike | torch.dtype = torch.float32,
     ) -> None:
         self.angle = angle
         self.keep_size = keep_size
-        self.mode: str = look_up_option(mode, GridSampleMode)
-        self.padding_mode: str = look_up_option(padding_mode, GridSamplePadMode)
+        self.mode: str = mode
+        self.padding_mode: str = padding_mode
         self.align_corners = align_corners
         self.dtype = dtype
 
     def __call__(
         self,
         img: torch.Tensor,
         mode: str | None = None,
@@ -863,16 +861,16 @@
 
         Raises:
             ValueError: When ``img`` spatially is not one of [2D, 3D].
 
         """
         img = convert_to_tensor(img, track_meta=get_track_meta())
         _dtype = get_equivalent_dtype(dtype or self.dtype or img.dtype, torch.Tensor)
-        _mode = look_up_option(mode or self.mode, GridSampleMode)
-        _padding_mode = look_up_option(padding_mode or self.padding_mode, GridSamplePadMode)
+        _mode = mode or self.mode
+        _padding_mode = padding_mode or self.padding_mode
         _align_corners = self.align_corners if align_corners is None else align_corners
         im_shape = img.peek_pending_shape() if isinstance(img, MetaTensor) else img.shape[1:]
         output_shape = im_shape if self.keep_size else None
         return rotate(  # type: ignore
             img, self.angle, output_shape, _mode, _padding_mode, _align_corners, _dtype, self.get_transform_info()
         )
 
@@ -884,18 +882,19 @@
         fwd_rot_mat = transform[TraceKeys.EXTRA_INFO]["rot_mat"]
         mode = transform[TraceKeys.EXTRA_INFO]["mode"]
         padding_mode = transform[TraceKeys.EXTRA_INFO]["padding_mode"]
         align_corners = transform[TraceKeys.EXTRA_INFO]["align_corners"]
         dtype = transform[TraceKeys.EXTRA_INFO]["dtype"]
         inv_rot_mat = linalg_inv(convert_to_numpy(fwd_rot_mat))
 
+        _, _m, _p, _ = resolves_modes(mode, padding_mode)
         xform = AffineTransform(
             normalized=False,
-            mode=mode,
-            padding_mode=padding_mode,
+            mode=_m,
+            padding_mode=_p,
             align_corners=False if align_corners == TraceKeys.NONE else align_corners,
             reverse_indexing=True,
         )
         img_t: torch.Tensor = convert_data_type(data, MetaTensor, dtype=dtype)[0]
         transform_t, *_ = convert_to_dst_type(inv_rot_mat, img_t)
         sp_size = transform[TraceKeys.ORIG_SIZE]
         out: torch.Tensor = xform(img_t.unsqueeze(0), transform_t, spatial_size=sp_size).float().squeeze(0)
@@ -949,15 +948,15 @@
         padding_mode: str = NumpyPadMode.EDGE,
         align_corners: bool | None = None,
         dtype: DtypeLike | torch.dtype = torch.float32,
         keep_size: bool = True,
         **kwargs,
     ) -> None:
         self.zoom = zoom
-        self.mode: InterpolateMode = InterpolateMode(mode)
+        self.mode = mode
         self.padding_mode = padding_mode
         self.align_corners = align_corners
         self.dtype = dtype
         self.keep_size = keep_size
         self.kwargs = kwargs
 
     def __call__(
@@ -987,15 +986,15 @@
                 See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html
             dtype: data type for resampling computation. Defaults to ``self.dtype``.
                 If None, use the data type of input data.
 
         """
         img = convert_to_tensor(img, track_meta=get_track_meta())
         _zoom = ensure_tuple_rep(self.zoom, img.ndim - 1)  # match the spatial image dim
-        _mode = look_up_option(self.mode if mode is None else mode, InterpolateMode).value
+        _mode = self.mode if mode is None else mode
         _padding_mode = padding_mode or self.padding_mode
         _align_corners = self.align_corners if align_corners is None else align_corners
         _dtype = get_equivalent_dtype(dtype or self.dtype or img.dtype, torch.Tensor)
         return zoom(  # type: ignore
             img, _zoom, self.keep_size, _mode, _padding_mode, _align_corners, _dtype, self.get_transform_info()
         )
 
@@ -1177,16 +1176,16 @@
         if len(self.range_y) == 1:
             self.range_y = tuple(sorted([-self.range_y[0], self.range_y[0]]))
         self.range_z = ensure_tuple(range_z)
         if len(self.range_z) == 1:
             self.range_z = tuple(sorted([-self.range_z[0], self.range_z[0]]))
 
         self.keep_size = keep_size
-        self.mode: str = look_up_option(mode, GridSampleMode)
-        self.padding_mode: str = look_up_option(padding_mode, GridSamplePadMode)
+        self.mode: str = mode
+        self.padding_mode: str = padding_mode
         self.align_corners = align_corners
         self.dtype = dtype
 
         self.x = 0.0
         self.y = 0.0
         self.z = 0.0
 
@@ -1227,16 +1226,16 @@
             self.randomize()
 
         if self._do_transform:
             ndim = len(img.peek_pending_shape() if isinstance(img, MetaTensor) else img.shape[1:])
             rotator = Rotate(
                 angle=self.x if ndim == 2 else (self.x, self.y, self.z),
                 keep_size=self.keep_size,
-                mode=look_up_option(mode or self.mode, GridSampleMode),
-                padding_mode=look_up_option(padding_mode or self.padding_mode, GridSamplePadMode),
+                mode=mode or self.mode,
+                padding_mode=padding_mode or self.padding_mode,
                 align_corners=self.align_corners if align_corners is None else align_corners,
                 dtype=dtype or self.dtype or img.dtype,
             )
             rotator.lazy_evaluation = self.lazy_evaluation
             out = rotator(img)
         else:
             out = convert_to_tensor(img, track_meta=get_track_meta(), dtype=torch.float32)
@@ -1402,15 +1401,15 @@
         RandomizableTransform.__init__(self, prob)
         self.min_zoom = ensure_tuple(min_zoom)
         self.max_zoom = ensure_tuple(max_zoom)
         if len(self.min_zoom) != len(self.max_zoom):
             raise ValueError(
                 f"min_zoom and max_zoom must have same length, got {len(self.min_zoom)} and {len(self.max_zoom)}."
             )
-        self.mode: InterpolateMode = look_up_option(mode, InterpolateMode)
+        self.mode = mode
         self.padding_mode = padding_mode
         self.align_corners = align_corners
         self.dtype = dtype
         self.keep_size = keep_size
         self.kwargs = kwargs
 
         self._zoom: Sequence[float] = [1.0]
@@ -1463,15 +1462,15 @@
 
         if not self._do_transform:
             out = convert_to_tensor(img, track_meta=get_track_meta(), dtype=torch.float32)
         else:
             xform = Zoom(
                 self._zoom,
                 keep_size=self.keep_size,
-                mode=look_up_option(mode or self.mode, InterpolateMode),
+                mode=mode or self.mode,
                 padding_mode=padding_mode or self.padding_mode,
                 align_corners=self.align_corners if align_corners is None else align_corners,
                 dtype=dtype or self.dtype,
                 **self.kwargs,
             )
             xform.lazy_evaluation = self.lazy_evaluation
             out = xform(img)
@@ -1811,43 +1810,14 @@
         self.mode = mode
         self.padding_mode = padding_mode
         self.norm_coords = norm_coords
         self.device = device
         self.align_corners = align_corners
         self.dtype = dtype
 
-    @staticmethod
-    @functools.lru_cache(None)
-    def resolve_modes(interp_mode, padding_mode):
-        """compute the backend and the corresponding mode for the given interpolation mode and padding mode."""
-        _interp_mode = None
-        _padding_mode = None
-        if look_up_option(str(interp_mode), SplineMode, default=None) is not None:
-            backend = TransformBackends.NUMPY
-        else:
-            backend = TransformBackends.TORCH
-
-        if (not USE_COMPILED) and (backend == TransformBackends.TORCH):
-            if str(interp_mode).lower().endswith("linear"):
-                _interp_mode = GridSampleMode("bilinear")
-            _interp_mode = GridSampleMode(interp_mode)
-            _padding_mode = GridSamplePadMode(padding_mode)
-        elif USE_COMPILED and backend == TransformBackends.TORCH:  # compiled is using torch backend param name
-            _padding_mode = 1 if padding_mode == "reflection" else padding_mode  # type: ignore
-            if interp_mode == "bicubic":
-                _interp_mode = 3  # type: ignore
-            elif interp_mode == "bilinear":
-                _interp_mode = 1  # type: ignore
-            else:
-                _interp_mode = GridSampleMode(interp_mode)
-        else:  # TransformBackends.NUMPY
-            _interp_mode = int(interp_mode)  # type: ignore
-            _padding_mode = look_up_option(padding_mode, NdimageMode)
-        return backend, _interp_mode, _padding_mode
-
     def __call__(
         self,
         img: torch.Tensor,
         grid: torch.Tensor | None = None,
         mode: str | int | None = None,
         padding_mode: str | None = None,
         dtype: DtypeLike = None,
@@ -1890,16 +1860,19 @@
             return img
 
         _device = img.device if isinstance(img, torch.Tensor) else self.device
         _dtype = dtype or self.dtype or img.dtype
         _align_corners = self.align_corners if align_corners is None else align_corners
         img_t, *_ = convert_data_type(img, torch.Tensor, dtype=_dtype, device=_device)
         sr = min(len(img_t.peek_pending_shape() if isinstance(img_t, MetaTensor) else img_t.shape[1:]), 3)
-        backend, _interp_mode, _padding_mode = Resample.resolve_modes(
-            self.mode if mode is None else mode, self.padding_mode if padding_mode is None else padding_mode
+        backend, _interp_mode, _padding_mode, _ = resolves_modes(
+            self.mode if mode is None else mode,
+            self.padding_mode if padding_mode is None else padding_mode,
+            backend=None,
+            use_compiled=USE_COMPILED,
         )
 
         if USE_COMPILED or backend == TransformBackends.NUMPY:
             grid_t, *_ = convert_to_dst_type(grid[:sr], img_t, dtype=grid.dtype, wrap_sequence=True)
             if isinstance(grid, torch.Tensor) and grid_t.data_ptr() == grid.data_ptr():
                 grid_t = grid_t.clone(memory_format=torch.contiguous_format)
             for i, dim in enumerate(img_t.shape[1 : 1 + sr]):
```

## monai/transforms/spatial/functional.py

```diff
@@ -28,15 +28,15 @@
 from monai.data.meta_obj import get_track_meta
 from monai.data.meta_tensor import MetaTensor
 from monai.data.utils import AFFINE_TOL, compute_shape_offset, to_affine_nd
 from monai.networks.layers import AffineTransform
 from monai.transforms.croppad.array import ResizeWithPadOrCrop
 from monai.transforms.intensity.array import GaussianSmooth
 from monai.transforms.inverse import TraceableTransform
-from monai.transforms.utils import create_rotate, create_translate, scale_affine
+from monai.transforms.utils import create_rotate, create_translate, resolves_modes, scale_affine
 from monai.transforms.utils_pytorch_numpy_unification import allclose
 from monai.utils import (
     LazyAttr,
     TraceKeys,
     convert_to_dst_type,
     convert_to_numpy,
     convert_to_tensor,
@@ -168,16 +168,17 @@
             image_only=True,
             dtype=dtype_pt,
             align_corners=align_corners,
         )
         with affine_xform.trace_transform(False):
             img = affine_xform(img, mode=mode, padding_mode=padding_mode)
     else:
+        _, _m, _p, _ = resolves_modes(mode, padding_mode)
         affine_xform = AffineTransform(  # type: ignore
-            normalized=False, mode=mode, padding_mode=padding_mode, align_corners=align_corners, reverse_indexing=True
+            normalized=False, mode=_m, padding_mode=_p, align_corners=align_corners, reverse_indexing=True
         )
         img = affine_xform(img.unsqueeze(0), theta=xform.to(img), spatial_size=spatial_size).squeeze(0)  # type: ignore
     if additional_dims:
         full_shape = (chns, *spatial_size, *additional_dims)
         img = img.reshape(full_shape)
     out = _maybe_new_metatensor(img, dtype=torch.float32)
     return out.copy_meta_from(meta_info) if isinstance(out, MetaTensor) else out  # type: ignore
@@ -327,16 +328,17 @@
         else:
             # if sigma is given, use the given value for downsampling axis
             anti_aliasing_sigma = list(ensure_tuple_rep(anti_aliasing_sigma, len(out_size)))
             for axis in range(len(out_size)):
                 anti_aliasing_sigma[axis] = anti_aliasing_sigma[axis] * int(factors[axis] > 1)
         anti_aliasing_filter = GaussianSmooth(sigma=anti_aliasing_sigma)
         img_ = convert_to_tensor(anti_aliasing_filter(img_), track_meta=False)
+    _, _m, _, _ = resolves_modes(mode, torch_interpolate_spatial_nd=len(img_.shape) - 1)
     resized = torch.nn.functional.interpolate(
-        input=img_.unsqueeze(0), size=out_size, mode=mode, align_corners=align_corners
+        input=img_.unsqueeze(0), size=out_size, mode=_m, align_corners=align_corners
     )
     out, *_ = convert_to_dst_type(resized.squeeze(0), out, dtype=torch.float32)
     return out.copy_meta_from(meta_info) if isinstance(out, MetaTensor) else out
 
 
 def rotate(img, angle, output_shape, mode, padding_mode, align_corners, dtype, transform_info):
     """
@@ -392,16 +394,17 @@
         orig_size=im_shape,
         transform_info=transform_info,
         lazy_evaluation=transform_info.get(TraceKeys.LAZY_EVALUATION, False),
     )
     out = _maybe_new_metatensor(img)
     if transform_info.get(TraceKeys.LAZY_EVALUATION, False):
         return out.copy_meta_from(meta_info) if isinstance(out, MetaTensor) else meta_info
+    _, _m, _p, _ = resolves_modes(mode, padding_mode)
     xform = AffineTransform(
-        normalized=False, mode=mode, padding_mode=padding_mode, align_corners=align_corners, reverse_indexing=True
+        normalized=False, mode=_m, padding_mode=_p, align_corners=align_corners, reverse_indexing=True
     )
     img_t = out.to(dtype)
     transform_t, *_ = convert_to_dst_type(transform, img_t)
     output: torch.Tensor = xform(img_t.unsqueeze(0), transform_t, spatial_size=tuple(int(i) for i in output_shape))
     output = output.float().squeeze(0)
     out, *_ = convert_to_dst_type(output, dst=out, dtype=torch.float32)
     return out.copy_meta_from(meta_info) if isinstance(out, MetaTensor) else out
@@ -464,19 +467,20 @@
         transform_info=transform_info,
         lazy_evaluation=transform_info.get(TraceKeys.LAZY_EVALUATION, False),
     )
     out = _maybe_new_metatensor(img)
     if transform_info.get(TraceKeys.LAZY_EVALUATION, False):
         return out.copy_meta_from(meta_info) if isinstance(out, MetaTensor) else meta_info
     img_t = out.to(dtype)
+    _, _m, _, _ = resolves_modes(mode, torch_interpolate_spatial_nd=len(img_t.shape) - 1)
     zoomed: NdarrayOrTensor = torch.nn.functional.interpolate(
         recompute_scale_factor=True,
         input=img_t.unsqueeze(0),
         scale_factor=list(scale_factor),
-        mode=mode,
+        mode=_m,
         align_corners=align_corners,
     ).squeeze(0)
     out, *_ = convert_to_dst_type(zoomed, dst=out, dtype=torch.float32)
     if isinstance(out, MetaTensor):
         out = out.copy_meta_from(meta_info)
     do_pad_crop = not np.allclose(output_size, zoomed.shape[1:])
     if do_pad_crop:
```

## Comparing `monai-1.2.0rc5.dist-info/LICENSE` & `monai-1.2.0rc6.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `monai-1.2.0rc5.dist-info/METADATA` & `monai-1.2.0rc6.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: monai
-Version: 1.2.0rc5
+Version: 1.2.0rc6
 Summary: AI Toolkit for Healthcare Imaging
 Home-page: https://monai.io/
 Author: MONAI Consortium
 Author-email: monai.contact@gmail.com
 License: Apache License 2.0
 Project-URL: Documentation, https://docs.monai.io/
 Project-URL: Bug Tracker, https://github.com/Project-MONAI/MONAI/issues
```

## Comparing `monai-1.2.0rc5.dist-info/RECORD` & `monai-1.2.0rc6.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,43 +1,43 @@
 monai/__init__.py,sha256=N68MC4_ibRXAO-vquIMx4tT--Er8hAHlKkmIrVBLsbY,2276
-monai/_version.py,sha256=YF1C3TbaZBviYayOAIDQ7H1qDqUZi8J7MKwvH52xUSw,500
+monai/_version.py,sha256=lrupaN5E4Wy0Oc0OKw9ubBGlkUBWKGz_wVZ3XdWEgtg,500
 monai/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 monai/_extensions/__init__.py,sha256=NEBPreRhQ8H9gVvgrLr_y52_TmqB96u_u4VQmeNT93I,642
 monai/_extensions/loader.py,sha256=7SiKw36q-nOzH8CRbBurFrz7GM40GCu7rc93Tm8XpnI,3643
 monai/_extensions/gmm/gmm.cpp,sha256=GLC_KOEFUlEB8fUs6vjeuZBxIqzuatZBDc-ZoAtQ6Xg,2931
 monai/_extensions/gmm/gmm.h,sha256=c9n8RLa4r9jTbqpY4cCHFm3ldVER_FN8pG2hQx3Lb3c,1760
 monai/_extensions/gmm/gmm_cpu.cpp,sha256=cyKn-7DjtVCPb9bnGb0bWUfagb3KUFX9rD6mI5BEXrs,1118
 monai/_extensions/gmm/gmm_cuda.cu,sha256=2CvNdiuc6JiEsdK31iiHJQytubn81fxIQim-At_-1HA,16213
 monai/_extensions/gmm/gmm_cuda_linalg.cuh,sha256=Glqg2oAcUFUXg-DVfpROkiv-DdXvvVdM1nyiFm8qlHY,3520
 monai/apps/__init__.py,sha256=VDIc3HB_uFbqKL1TS-OeRvryEMDfzm22KJRzwpkXsGo,908
 monai/apps/datasets.py,sha256=gA4z4WCfH4-lz66aZ3uvubopTteQjcxhg38nSOM01Sk,34568
 monai/apps/utils.py,sha256=lyUDb-HUvEb8xPAi0cwQcKEGaSrgKi8H9sikYNRyTL4,13505
-monai/apps/auto3dseg/__init__.py,sha256=bnhmTnu9QWJbYDbDDrlnpWLBNa6thxPMIvgIUOTgGZ4,993
+monai/apps/auto3dseg/__init__.py,sha256=DhUB2Ol0-iNAk1ZNmD1RkTODUOhdiibv8h9MgcLuF6s,1016
 monai/apps/auto3dseg/__main__.py,sha256=fCDhD8uhmJQKkKBxLO6hMJhEvZJRIsjTc1Ad3bYmNIY,1411
-monai/apps/auto3dseg/auto_runner.py,sha256=wua5mmnCAzGoTOF42zXj11syJvEADoMXEtXDUOQINCQ,37585
-monai/apps/auto3dseg/bundle_gen.py,sha256=qhBPV06QCbny0yd2tmowUnE0z5-AujMGWg3w7GCQJ1A,26014
-monai/apps/auto3dseg/data_analyzer.py,sha256=cVoPCOa2M2uIY8PVzCiHYgwt2ERSvpKFIDpA_UvvLH0,17319
-monai/apps/auto3dseg/ensemble_builder.py,sha256=WhgLtw8FfYW_-hoz5OkZz1OtYefcM0z4DZwHL-UhHzo,24328
-monai/apps/auto3dseg/hpo_gen.py,sha256=XnWjoSgkrZOS8W_AaSaXpShJjSxx5l0xaYDkJVlqewk,17523
-monai/apps/auto3dseg/transforms.py,sha256=eHueZBNFMgGa9Nz96JE7KxTH05mpMfWxaD1QHxQ-B98,3733
-monai/apps/auto3dseg/utils.py,sha256=JJJQy2GDVuwasqn8dzrgJwX0ZTwk660vKW7D-cxUweo,2917
+monai/apps/auto3dseg/auto_runner.py,sha256=Wpyu_QqkQFy9xKK-cF6SslW0sys4KaQbzqzFUOGJjuw,37011
+monai/apps/auto3dseg/bundle_gen.py,sha256=oCoL8cu56gE0tJ2EQQKY3EIyqD0JwA2BumnRlANz4jE,26063
+monai/apps/auto3dseg/data_analyzer.py,sha256=eee9yneQLLgwP1yZKRk2B7E3e-wjaCZw_6E5OKUBTug,17262
+monai/apps/auto3dseg/ensemble_builder.py,sha256=irSbcGwh1dSev7OBQK0_NmjSCEuJvhFmHb38g-8SZrQ,27516
+monai/apps/auto3dseg/hpo_gen.py,sha256=abKxbIew5sc8jo_A7FLIaainN2DHGbvAVz7LvO0G4pM,16619
+monai/apps/auto3dseg/transforms.py,sha256=iO4v9-dwQzvupJglX-H2HYuwUhmFdVgLbyh4BuDy7DY,3991
+monai/apps/auto3dseg/utils.py,sha256=7DPJbsL9YbhRdMZ6dEvCA_t_uLSSz7-WZSU2pMY4_qo,3138
 monai/apps/deepedit/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/apps/deepedit/interaction.py,sha256=h9zTmhHAmwndR315RknqXtLWYqyYGvdcmjP6EpRrzHg,4501
 monai/apps/deepedit/transforms.py,sha256=sgFUuMqMlxmvWmsx-lHajoUHVDMp9fLo7UiwG60IYU8,37435
 monai/apps/deepgrow/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/apps/deepgrow/dataset.py,sha256=W0wv1QujA4sZgrAcBS64dl3OBbDBM2cF4RK0fDCQnRU,10054
 monai/apps/deepgrow/interaction.py,sha256=l-wCmetMi4g-gcgMjA68firOX4RKvFm8WgefwiUFtTs,3739
 monai/apps/deepgrow/transforms.py,sha256=C38Q5ZdR98JWFyR5lNnhIaibL9zf14c111KDLaYvwB8,42011
 monai/apps/detection/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/apps/detection/metrics/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/apps/detection/metrics/coco.py,sha256=1yQJjd-9M0rUmPZlAle3B7IEqVNNAn53pDkSG1GJ3U8,26617
 monai/apps/detection/metrics/matching.py,sha256=GF4wgH5Let7GwW1SGwzfzz5BRnCVEhDe7_KR7zpLr44,17161
 monai/apps/detection/networks/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
-monai/apps/detection/networks/retinanet_detector.py,sha256=eR0hPip-ZeXUtDv_-y-aFGhe9kHnIdAJRqS1rEojJgQ,53221
-monai/apps/detection/networks/retinanet_network.py,sha256=p7VHhgyzhC47rX3TPvyH98_m5xWPt1kPxpyT9ciGpKA,18850
+monai/apps/detection/networks/retinanet_detector.py,sha256=-EcGvDJK13o7qqx6bUHtxEniIdCXriIzwty1o5pmG90,53640
+monai/apps/detection/networks/retinanet_network.py,sha256=JJESOQzII9F79Vbox3EWDn-6iXmUixSR2kG8klUgOMo,19136
 monai/apps/detection/transforms/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/apps/detection/transforms/array.py,sha256=MeDK8I3Q3KW13chF3XVdWcYoG7ZbiesokLVBLxMPPwI,24519
 monai/apps/detection/transforms/box_ops.py,sha256=17X76HXROCdk38-1c9wEuMBm4TM0nWCZfOmkkZLxxBI,17916
 monai/apps/detection/transforms/dictionary.py,sha256=GrghYnvDUnAuE8XC9pcv6eR2vlMOZ2pB9-wqCaOz5Jo,68979
 monai/apps/detection/utils/ATSS_matcher.py,sha256=6M9UwQ2sV5xOhgZuLujunOiJ9DlDXS7OcHfJoKGXu4M,13531
 monai/apps/detection/utils/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/apps/detection/utils/anchor_utils.py,sha256=w4SepYKRyyNn5a4irkkFmnHhUYifS-PsAaGb9xxzGlw,18681
@@ -46,17 +46,17 @@
 monai/apps/detection/utils/detector_utils.py,sha256=pU7bOzH-ay9Lnzu1aHCrIwlaGVf5xj13E7Somx_vFnk,10306
 monai/apps/detection/utils/hard_negative_sampler.py,sha256=PywdXkFIAdudmp3W8JWM_CcLC3BKWQh5x1y0tuuokcg,13890
 monai/apps/detection/utils/predict_utils.py,sha256=6j7U-7pLtbmgE6SXKR_MVImc67-M8WtzQkT89cCVsK8,5818
 monai/apps/mmars/__init__.py,sha256=BolpgEi9jNBgrOQd3Kwp-9QQLeWQwQtlN_MJkK1eu5s,726
 monai/apps/mmars/mmars.py,sha256=AYsx5FDmJ0dT0hAkWGYhM470aPIG23PYloHihDZfOKE,13115
 monai/apps/mmars/model_desc.py,sha256=k7WSMRuyQN8xPax8aUmGKiTNZmcVatdqPYCgxDih-x4,9996
 monai/apps/nnunet/__init__.py,sha256=gyqmg1fxPf3RF6LL25gnpMTfNS14uxweuJ93e4UzjB8,745
-monai/apps/nnunet/__main__.py,sha256=h5GTEHSZliEZ38_4WOtqujPa-C3M6if-0huNHmreaNI,2975
-monai/apps/nnunet/nnunetv2_runner.py,sha256=iqKvjUHH22jq0TmkHawRvMigEKBIDIirhpT3lbYDXQY,38587
-monai/apps/nnunet/utils.py,sha256=u1jVSAJeOmpCmXczqnGuU4jnSevsawwLbuFUUKB8XrI,6791
+monai/apps/nnunet/__main__.py,sha256=qrloBLymK98OPcaBKocrlF8io2h4mUuXJPFVLZT-XDo,832
+monai/apps/nnunet/nnunetv2_runner.py,sha256=vRL6Wb8sIDA5MbNFAFX3e7rS2mxM9nd2PCQUNN3r75g,47981
+monai/apps/nnunet/utils.py,sha256=OwLBcc0LZ_n7-ofE8EgkgmIHT23wq1xySCD6lphSjz0,6761
 monai/apps/nuclick/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/apps/nuclick/transforms.py,sha256=vCw2IpuMLLZ3C2NtBj59Kxjy1cHm4K3VCm7KMP71aug,24948
 monai/apps/pathology/__init__.py,sha256=SRBbxgPzZdtC22TpY1m0-Z3SSBfMig6xYVSdgOClgXg,1030
 monai/apps/pathology/utils.py,sha256=GtVzcr8mc1MsojpUx3tI7IYs7yhIo_HtnY0msv5pkwM,2860
 monai/apps/pathology/engines/__init__.py,sha256=sqR2PUjmFf46jRRQA8ZZ9umbQzuLGDpBaRWQNVA2r7Q,650
 monai/apps/pathology/engines/utils.py,sha256=Zr_DuWZ3qcIiNM7QjFSzgojeRPJV_UP5yGWIxrU5gI0,2397
 monai/apps/pathology/handlers/__init__.py,sha256=YRvZ5C6I56qvu1DTGROJV5Sq0ZF3t6f34vV3Vdeg9Hk,609
@@ -89,28 +89,28 @@
 monai/apps/reconstruction/transforms/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/apps/reconstruction/transforms/array.py,sha256=8qtDC5Exm9hvSwwbdFWV4ibiPxoxZYJX-8k5MYeb0eE,12240
 monai/apps/reconstruction/transforms/dictionary.py,sha256=1Mpp86Be4tzW55pVLhsmQCDZF9HVSMaigz6FzdNCVLY,15844
 monai/apps/tcia/__init__.py,sha256=W1IhsE1Z3DYAbEBTiJbnndPEIXwVI_n0OW5e2cMCUgE,765
 monai/apps/tcia/label_desc.py,sha256=B8l9mVmRzLysLmEIIYVeenly_68okCt461qeLQSxCJ8,1582
 monai/apps/tcia/utils.py,sha256=O-CGB_gF3MTh2BcdJw-KZ2luHvhB6HvEBqOwsRAa1HY,6152
 monai/auto3dseg/__init__.py,sha256=DbZC7wqx4zBNcguLQGu8bGmAiKnk9LvjtQDtwdwG19I,1164
-monai/auto3dseg/algo_gen.py,sha256=X8OrOGAEqbhJ06zbDWDCuZRs1HXjejt5CpW5qsWGkDk,4209
+monai/auto3dseg/algo_gen.py,sha256=_BscoAnUzQKRqz5jHvdsuCe3tTxq7PUQYPMLX0WuxCc,4286
 monai/auto3dseg/analyzer.py,sha256=5CImSIYLFvGJ0QsUUFAvjiWkWmwZk0OeckHbe6Rgeng,41223
 monai/auto3dseg/operations.py,sha256=1sNDWnz5Zs2-scpb1wotxar7yGYQ-VPI-_b2KnZqW9g,5110
 monai/auto3dseg/seg_summarizer.py,sha256=39zXmyRe9aNhXS9ZliDJsUEDqzU1OPtUG5tL1peJsx0,8725
-monai/auto3dseg/utils.py,sha256=QOapCrMbguQz_t5BsU_YAY4DWwjKA38i4sc-TWgzKJA,12193
+monai/auto3dseg/utils.py,sha256=cG-vBcGGBBEQlD6VDEb-_EqaXlh7SpdJS6Jd7PgBn9Y,13428
 monai/bundle/__init__.py,sha256=EFeyJGNju0ijBfKkobpdymGvbuAX_ZJWQd8sa6OWOSw,1341
 monai/bundle/__main__.py,sha256=EiWkpt3yIcT8uBKg99kQBWTYoFgSpHjR7GAvIKA9EME,926
 monai/bundle/config_item.py,sha256=sBQ_Kg93EFET_pZcdzdMatnrB2FD2bMvmg_LpupdwMA,16035
-monai/bundle/config_parser.py,sha256=tX8ZKDW43uWzCQ8EBUROu-U9wJItzF7_XzotLK-wt60,22410
-monai/bundle/properties.py,sha256=RaknletfzGrFLnwKMrrd3fFCeisRntZ7yKAN48zJ0MY,8811
+monai/bundle/config_parser.py,sha256=lop_lAck0HKyf4Gtob5pL3P27NM1ovDfvvsPq4hnDFA,22360
+monai/bundle/properties.py,sha256=eOyIdy4x2jbULRNn7UcpXSWo3FwQVCtCNHckga2gSvs,9672
 monai/bundle/reference_resolver.py,sha256=3KpSa-1n1hptY6N1C-3yt5pybTXB7QUVj8fNWWGvN2U,14353
-monai/bundle/scripts.py,sha256=__72s2fWFe2Lx1H15O9_UeD0PRywr5_Gd5DJCvMWuaQ,63940
+monai/bundle/scripts.py,sha256=L-nLaEt9aH25v0C8Wygzp6VTPJD1rkeJBpCQn3Rp3X4,64631
 monai/bundle/utils.py,sha256=Vou6ko4OjT1v8xOT99bZuFaAB-NPpTj-oZ4KjBJsYsY,8911
-monai/bundle/workflows.py,sha256=JxQKTCFAW3JgNz3NukRWpB39A7aE8aRgw9zQFGiiSGg,17082
+monai/bundle/workflows.py,sha256=iT3aNrX6YpS5nzNZWlMJgCevo-64LBSFHX7ufs9Tmow,19001
 monai/config/__init__.py,sha256=CN28CfTdsp301gv8YXfVvkbztCfbAqrLKrJi_C8oP9s,1048
 monai/config/deviceconfig.py,sha256=TifpuOAhccn1A3UCtInAnfciyGLHSNIen5ZRmUB8eKI,9913
 monai/config/type_definitions.py,sha256=0fAuI-_uX2Ac_33bgDVXKmBSl-fJNFcsOqBqYV16fhk,3485
 monai/data/__init__.py,sha256=9FhsQ7GHbTNAX25vLY1oR63b3q7jpJpVMfLigs5h0bs,5087
 monai/data/box_utils.py,sha256=YbG6lOoYwUGmwcNmoKzq2xnNTbYA4LMkHmfsqteopCg,50102
 monai/data/csv_saver.py,sha256=fcZF4kBNQnDFwQjV9TS4zjq_zqsv_u3QldxRprMC7zI,4952
 monai/data/dataloader.py,sha256=-n_LUfm3tlSHxh1at1xOmQkMrWQaKm3KBNo4A27PECE,3835
@@ -164,30 +164,30 @@
 monai/handlers/logfile_handler.py,sha256=9iUroCpfaP_YJu5mGHJ6CW53DoiYZ7F_XjhZwXw4a84,3931
 monai/handlers/lr_schedule_handler.py,sha256=jj-ukoR3p-m0LVs-AzPqn2On8GIj70PSIPNp9t-iiQY,3575
 monai/handlers/mean_dice.py,sha256=2jF0X1dfYjyPnYsd6hJX31yQQ9Y9Y6ZQxVPnPvv8kj4,3220
 monai/handlers/mean_iou.py,sha256=rmfSOTTZ10VXf0t9m4ct7mkjlGX6KeAu25hxnwwCl_A,2831
 monai/handlers/metric_logger.py,sha256=IEXGngnGh75Mxt1w6Nd4Tau8qHQjyZFLGzoePteH1jM,5477
 monai/handlers/metrics_reloaded_handler.py,sha256=6Xqg6Uocn5dJNBW6y-zU2o6sGvbWqN5oLB5F5vledl8,6168
 monai/handlers/metrics_saver.py,sha256=GPTaIeXi0noRyW2BQYQtazFfGyezmqSBAYWeAF-C5t0,8560
-monai/handlers/mlflow_handler.py,sha256=-0Dy8mtN8y0g0ViGcmD-vdRQs6yVSvYTlx4qp6O_Oa4,16326
+monai/handlers/mlflow_handler.py,sha256=HLYUPyG8FEw77_jlPJA--gsfo-UhpoLbRmaeQNw3Qoo,16845
 monai/handlers/nvtx_handlers.py,sha256=dBITb2hboynktwZNkRrlqM7STu7n3qXrdoC1-IogWc4,6819
 monai/handlers/panoptic_quality.py,sha256=Nz2dkMeaqB3s4dFzmW-vUl_guJWj34jRPq4nUkxntpQ,3637
 monai/handlers/parameter_scheduler.py,sha256=UE0Lww8ZYyXcHq9N4TXoWmJWSQaYTwpLlLHDeq2p_UY,7119
 monai/handlers/postprocessing.py,sha256=kKJ4eaCxEMcVYK-Q8zLGjY0HL07QnhkZZ1rSIa4bzFI,3285
 monai/handlers/probability_maps.py,sha256=bASiWiAAKpyOXEL2rZlfLUbKmfK5co6KgxhA-SvS_sU,5336
 monai/handlers/regression_metrics.py,sha256=7MQMmJsT2BgIomyCRJcBJuXk8-hDApPF0hPEw1Nfqe4,8422
 monai/handlers/roc_auc.py,sha256=gKNThJLsWKaNQI3t6sWn-UrD6yfb1Rw5KWaS648dmkw,2730
 monai/handlers/smartcache_handler.py,sha256=OA6v4EC2geH419eBKSAGSb-XNxO_qSPmJ2fkh7TOv-s,3051
 monai/handlers/stats_handler.py,sha256=35tkttd-LFKfE6bO9wPVOr5oHwiP2yEWXcZSSURtN9E,14251
 monai/handlers/surface_distance.py,sha256=jfuRpuzG7Ia-Ye0Izce1n4UPBghe1xtwBPwXx3fmODw,3313
 monai/handlers/tensorboard_handlers.py,sha256=FGd3KT7AoulwWOGbiJqDAyjMpZzuIyZZoNsatRdDoNE,23325
 monai/handlers/utils.py,sha256=PumzyMPAisJ_A-4RkgkNq31YfulhuI1UcOF9-Xj2_Dw,9855
 monai/handlers/validation_handler.py,sha256=xpRB3IB7pckzgTgdzTGb_gG6tIezeIvtmhJHPNYrpvY,3269
 monai/inferers/__init__.py,sha256=_MNRpyBTFuJQBAhWx3VAo8R3CAT7i-kcp8zp-CiXkSE,917
-monai/inferers/inferer.py,sha256=If9rhPEvNTc_vd6SlxiOMZKIAKOQo5HikwFqqIhWd2k,32038
+monai/inferers/inferer.py,sha256=-4nfQ_83Tsvqv49v5SsXyGrnq4fpch5pNIYvQspOkh8,32256
 monai/inferers/merger.py,sha256=onJ9OiCV9YOdsljd5YFZ3Dx6nSWv_gfCf7s5TSb6tHs,6393
 monai/inferers/splitter.py,sha256=dfU1AQWVvcbqt2hvZJqk_hFh6EtFrtgBoV7aId1ns7k,9397
 monai/inferers/utils.py,sha256=0wpfd6U351mNoli_M84B7eND_wHiGrYnZ-DHvrU8C14,20017
 monai/losses/__init__.py,sha256=9-QH3s0s1XACosZPhJz5LrDskiwjjcMMt9Rhr-LE4PU,1409
 monai/losses/contrastive.py,sha256=okmuiRPpwAhInBmk7wUFm0BogaxiP0EgqdV0lxIhbHY,3430
 monai/losses/deform.py,sha256=GJpIQMGFf97SMd-5PswsMFrvRokaO_NabgQn73KDfcw,4979
 monai/losses/dice.py,sha256=11wqc7MN5BfOh0STfO6YMGto9BxjmzOgnXBSPadH6Ds,46326
@@ -263,25 +263,25 @@
 monai/networks/nets/attentionunet.py,sha256=Mh_qbpgVuC_teCdZ50AtEu0Aa5yrtpVRO4BPDHTJ4-A,9202
 monai/networks/nets/autoencoder.py,sha256=pCWHM2nLBfDFEjZRC_SxDnNQET_v0lSIeGkPKNAD4Q0,12089
 monai/networks/nets/basic_unet.py,sha256=yS8JG6_V0GeTuFo8T2JPCiolAw6FzZt37ZIBt0wKtz0,10950
 monai/networks/nets/basic_unetplusplus.py,sha256=P5r6-Id-1OFLJey7jERAhlPvcPO3i2FBzFLR_bR9aaU,7960
 monai/networks/nets/classifier.py,sha256=U94OM91_pNT74wQV-_LOxAnbLvjuJvnorMK-xcE7HJE,6293
 monai/networks/nets/densenet.py,sha256=rq4BRlx3q3durEPbBRhWzhOa8o6x9wpkmL_95T8P-3I,15820
 monai/networks/nets/dints.py,sha256=42ehcT_zyoVTiOzMHDglTJ42hcslaw5PQZBvjKKPl9o,44771
-monai/networks/nets/dynunet.py,sha256=S2DX_tby7e5iCHL7q6X6f-vT6HwP6tbb2lRq9gHVJ24,18210
+monai/networks/nets/dynunet.py,sha256=snIiZ0hwkcHdE9O_ACsjI_MxMOPAR0e-drCklTtYmWo,18337
 monai/networks/nets/efficientnet.py,sha256=nmXG3D2UDUcj0Ce5TllVihtO8ZCgT1gDa03iI-x8gUQ,40643
 monai/networks/nets/flexible_unet.py,sha256=1FwOvDijaD-2V8nAaW2ibr5DYpl8Ule8LtPADH3TNb0,14147
 monai/networks/nets/fullyconnectednet.py,sha256=j5uo68qnYSxgH_sEMRh7s3QGNKFaJAIxmx8OixEv2Ig,7212
 monai/networks/nets/generator.py,sha256=q20EAl9N7Q56t78JiZaUEkPhYWyD02oqO0yekJCd9x0,6581
 monai/networks/nets/highresnet.py,sha256=CQTRix9HDRz_ey4jsnuHv-DWrqmH4rksTHXy_EVX5dM,8882
 monai/networks/nets/hovernet.py,sha256=SR46NPeUa7THwo5ic9hHPIlyl9GVbpx4aNqLUeyCogQ,28678
 monai/networks/nets/milmodel.py,sha256=tbyrLBibi5IVeMxe_erka8k9xw3KDtQOId5DwfSZU2o,9812
 monai/networks/nets/netadapter.py,sha256=JtcME9pcg8ud4jHKZKM9fE-8leP2PQXgUIfKBdB0wcA,6102
 monai/networks/nets/regressor.py,sha256=RDbBCppgOOBid-ISRNE9nDpRcKLMSdD_xFgOVKtGneU,6488
-monai/networks/nets/regunet.py,sha256=RZhLX6o0lqv3IEL2TOadTbypfpAj6yaibMzNb2gSwJQ,17189
+monai/networks/nets/regunet.py,sha256=oK3xMjpwZbCRzpElIHJJyfxQjp6IWVOOecEIUz-r0EU,18662
 monai/networks/nets/resnet.py,sha256=M7MVLoeiXxN4KXUq42HBCJshYqK37U34zHcQPMHDMm0,16785
 monai/networks/nets/segresnet.py,sha256=xNkSIvdk7kAyc3eVn-U_gGj8MoGVc5nklFKc_fkgOUs,13994
 monai/networks/nets/segresnet_ds.py,sha256=RRHTxsWrxI17282KtoFSEPJeBIcQIOdHeSHQpvUGFY8,15667
 monai/networks/nets/senet.py,sha256=gulqPMYmSABbMbN39NElGzSU1TKGviJas7EPTBaZ60A,19289
 monai/networks/nets/swin_unetr.py,sha256=kV5cSRWxtjTF25mOCSenxRsI0jyvIn1CBxttUCIdWSo,42000
 monai/networks/nets/torchvision_fc.py,sha256=3g5PD7C1MSkQ8xndhnVd0b3aN8zfshT8uiFS0OHyQaY,6309
 monai/networks/nets/transchex.py,sha256=TpdKj3nH-PWu_2S6JYPiAcjVld7TGPVyt52rKsbm9gY,16626
@@ -292,53 +292,53 @@
 monai/networks/nets/vitautoenc.py,sha256=AmnUQsUrm72z1cbadMYRD-V21ZuK3LxL2oKc10jP2nE,4817
 monai/networks/nets/vnet.py,sha256=6acwIN_NLyW1ANcT8C4Dr7RLCnBPJ2jxJlNbJvxHVdI,10011
 monai/optimizers/__init__.py,sha256=XUL7o9vSL7bZImpxVZqcc1c8MwUMrOZL4nJ-mjAA7yM,796
 monai/optimizers/lr_finder.py,sha256=Qn3rxLndSdU5mvB15GMDWXHUCPDV_LggDPlgWEvX70E,21952
 monai/optimizers/lr_scheduler.py,sha256=UJYzkhqxsYkyaisnCu0ba_kcAM6IL-lLskdDtHWOBnc,3652
 monai/optimizers/novograd.py,sha256=KOl3qiwsDg06rbZ48uLZI05kHxXUtwTfHp1P9Yn8Ot0,5661
 monai/optimizers/utils.py,sha256=vDgyMemHQAwAueZlnMIu4HuNJ5BSSEw_Csru2YnudJs,4131
-monai/transforms/__init__.py,sha256=xyRnpW54Ves5pvSABM_o2iQqAOV_CBepCotJw1Oq_xs,15248
+monai/transforms/__init__.py,sha256=-xWRF9HU8wLlG1u7wBJD6RNNzYNQvZRZSP1CVBRpl_k,15268
 monai/transforms/adaptors.py,sha256=IxmzJncOfEO2NPzuxP3Z41DZDqa1VLJ19Gz89ks_DyA,8946
 monai/transforms/compose.py,sha256=VYsWkSCfGEkguV9D0NQ3WVHhfZe-vTK-wMgy09_mmNY,40812
-monai/transforms/inverse.py,sha256=4MjvQNUW95xwVHdjRFRs3M8RpZvfLRosKBiED3MhHKs,18000
+monai/transforms/inverse.py,sha256=j9SJeiBAy9UvxQKDTRWk6ebxcSB6Vkk-293qU7lYVaA,18604
 monai/transforms/inverse_batch_transform.py,sha256=T1Kl7FQdE9omIm5SCCY9xnxGwaUxyfkgbJEtjB8LEuE,7054
 monai/transforms/nvtx.py,sha256=1EKEXZIhTUFKoIrJmd_fevwrHwo731dVFUFJQFiOk3w,3386
 monai/transforms/traits.py,sha256=HPAfD0ujRS0FYKH6bhwtKikW8Pebwrn9SoC4u7JChLg,2885
 monai/transforms/transform.py,sha256=c8B8bGgg55gpmXf3SBNUlvKyOim0s1rGTgv7RZsiZh4,18234
-monai/transforms/utils.py,sha256=zO0kp68hG91rqHtKr9aqQ8BB8L-Fx-G41q6E9ViI0P8,72516
+monai/transforms/utils.py,sha256=VZYsHElv-bSM3vJghOUVNb6M4tR9IpcfFUcGcw3icpU,77805
 monai/transforms/utils_create_transform_ims.py,sha256=v-MDiukqHkmCGpcRCSZDr14taMcoOuTDeZlL67Wr48E,31081
 monai/transforms/utils_pytorch_numpy_unification.py,sha256=BItSUa0J50ihOXNxxf21C9I-GenjWgoeaKx9EZ0f7Sk,18397
 monai/transforms/croppad/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/transforms/croppad/array.py,sha256=6xGjAscfgOkqh_88XUpQRrcsLUYdnaTr9LXfMiF7SEw,68231
 monai/transforms/croppad/batch.py,sha256=0JMO2XSwS9LuM1oiogoPl9HfEBpNbcw-OyrUEXFY5rM,6194
 monai/transforms/croppad/dictionary.py,sha256=8IHu2L6rU_RBgIOq_gd4TbKjOThuuXPzkSeAx7Lu9HA,54071
 monai/transforms/croppad/functional.py,sha256=XWJx8URCgkSLBUH3InD48QFcJ_YvAhVC4QNdjrs1c2c,12673
 monai/transforms/intensity/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/transforms/intensity/array.py,sha256=GnuSE0yc-YHCNn8BRGzVrMQ50b5rn-GUXHWjFQZH_Qs,98613
 monai/transforms/intensity/dictionary.py,sha256=-W9HVPA2VQUrqplmcI43Qo8EJXgMXARgcRmlhDasM0k,76501
 monai/transforms/io/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/transforms/io/array.py,sha256=7DUqkN5P2qYrUOA2wngcltYWAGmWCN45lr3nZMvrxw4,25430
 monai/transforms/io/dictionary.py,sha256=QKQSRnmHLPLZFnmaSERTAUUI3c0J9K4RwSsWv_rts00,17821
 monai/transforms/lazy/__init__.py,sha256=SvkdIlyzIDgO8oaGIrTr1YcLTjh2mZ510ySc9tEbRpk,699
-monai/transforms/lazy/functional.py,sha256=oeHyM_HcL8EhwhCXamEWYUZUlHAXUGKjSsZl1x4URJQ,5552
+monai/transforms/lazy/functional.py,sha256=T-ll0jhRTweOLRGAkBSwUv2eK7AQx9WPmu2_QhVrpeA,5788
 monai/transforms/lazy/utils.py,sha256=-vco90yMjoKPJb3x3j0arzenLhCvC5Igpl-aPugi_HE,9851
 monai/transforms/meta_utility/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/transforms/meta_utility/dictionary.py,sha256=YqbYeZOi4cFEmEPmrw2VIpOIwre6wxYB2UGZSrf-MoM,4896
 monai/transforms/post/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/transforms/post/array.py,sha256=Yb_qbOGoaQZFp7CpOA-4iZXvAxz3RtmDlhoPFNh3jWg,40858
 monai/transforms/post/dictionary.py,sha256=OaB9YzrJe0FztM9PETSXNNIihDiad1hnbNqnon7Mp2U,39905
 monai/transforms/signal/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/transforms/signal/array.py,sha256=gmTqVYcpsK74UaVbbIjpNFLS1emC_HrQR_AQL0H_GSE,16378
 monai/transforms/smooth_field/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/transforms/smooth_field/array.py,sha256=T_TfWUDpDi8rQMFuNI6VZNsWPPUUtXM6dkTwz0C34Ow,17833
 monai/transforms/smooth_field/dictionary.py,sha256=iU4V2VjSy2H1K03KgumMUr3cyZVWEJS0W-tgc6SZtP4,11194
 monai/transforms/spatial/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
-monai/transforms/spatial/array.py,sha256=Kb31oLKWr0Ccn2tKpirQta9_fM2EaMdfKggGkpLoPbI,168618
+monai/transforms/spatial/array.py,sha256=TC5NmiuEcupJ98S30fG9MgvYIPoWOelNbu4Yth9g2bg,166745
 monai/transforms/spatial/dictionary.py,sha256=4glbxbo4Cb0SbujSr9Fsk-OOLWR6yxiTQ_6LNcxll-s,107151
-monai/transforms/spatial/functional.py,sha256=Yr5Y93djwatUdoubmj_3I4_NnKF8QoF_X3FnYlTtCoU,31494
+monai/transforms/spatial/functional.py,sha256=5PdUj60RTlzBFrFasdamTf4m5vCCNYfsoXmKj0XgA1U,31773
 monai/transforms/utility/__init__.py,sha256=s9djSd6kvViPnFvMR11Dgd30Lv4oY6FaPJr4ZZJZLq0,573
 monai/transforms/utility/array.py,sha256=wu_XNG7odigV4bY4uND1tfLRRaJKRdWMH11PMA46f6M,70839
 monai/transforms/utility/dictionary.py,sha256=pxF8__rEeDIl7JHxGdr8Yn_SKE6Zd8jQM8swMi2LVPw,76330
 monai/utils/__init__.py,sha256=MUpVCcMoHgODXXUUg4kub-PL7HkUBjPIFW_O4HNUQj0,3380
 monai/utils/aliases.py,sha256=uBxkLudRfy3Rts9RZo4NDPGoq4e3Ymcaihk6lT92GFo,4096
 monai/utils/decorators.py,sha256=YRK5iEMdbc2INrWnBNDSMTaHge_0ezRf2b9yJGL-opg,3129
 monai/utils/deprecate_utils.py,sha256=gKeEV4MsI51qeQ5gci2me_C-0e-tDwa3VZzd3XPQqLk,14759
@@ -354,12 +354,12 @@
 monai/visualize/__init__.py,sha256=p7dv9-hRa9vAhlpHyk86yap9HgeDeJRO3pXmFhDx8Mc,1038
 monai/visualize/class_activation_maps.py,sha256=9BXlP-laeqz9rndM08DXZz97NzJcEsFPdC67k0B4CF8,16156
 monai/visualize/gradient_based.py,sha256=UInBLirXMr44xjxJRInlYW8onO3Zv58rDTWrZMDQzMM,6277
 monai/visualize/img2tensorboard.py,sha256=_p5olAefUs6t-y17z0TK32fKxNnUNXVkb0Op1SkfLMM,9200
 monai/visualize/occlusion_sensitivity.py,sha256=g3Au0X6LXoGKY9gPeuNWvNuFtynmPiUWgs_veCEIb_Q,18816
 monai/visualize/utils.py,sha256=xJbG68R-W17aqm0cpmMrypbZaiVfUaG9GWMFtKV7VUo,9966
 monai/visualize/visualizer.py,sha256=qckyaMZCbezYUwE20k5yc-Pb7UozVavMDbrmyQwfYHY,1377
-monai-1.2.0rc5.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-monai-1.2.0rc5.dist-info/METADATA,sha256=TpRsEXqK_Bhi8UepofvtzCtVX1Rm1mLTdhaQoGIkGq0,10172
-monai-1.2.0rc5.dist-info/WHEEL,sha256=akUX-06y5q3r-m-RPPS1ZNr1Qej84WXuMy5XU8CJZog,112
-monai-1.2.0rc5.dist-info/top_level.txt,sha256=UaNwRzLGORdus41Ip446s3bBfViLkdkDsXDo34J2P44,6
-monai-1.2.0rc5.dist-info/RECORD,,
+monai-1.2.0rc6.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+monai-1.2.0rc6.dist-info/METADATA,sha256=W7v2jGz_GVYZdO3Ytz5igUdxwYhv8encJWL5NJHPCwk,10172
+monai-1.2.0rc6.dist-info/WHEEL,sha256=ymdJFWkvDhkBXen1xLCddtEvaHYTfLtxeq_b4Er-JOU,112
+monai-1.2.0rc6.dist-info/top_level.txt,sha256=UaNwRzLGORdus41Ip446s3bBfViLkdkDsXDo34J2P44,6
+monai-1.2.0rc6.dist-info/RECORD,,
```

