# Comparing `tmp/ctlearn-0.6.1-py3-none-any.whl.zip` & `tmp/ctlearn-0.7.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,25 @@
-Zip file size: 39503 bytes, number of entries: 23
--rw-rw-r--  2.0 unx      186 b- defN 22-Jul-14 23:42 ctlearn/__init__.py
--rw-rw-r--  2.0 unx    16204 b- defN 22-Jul-14 23:42 ctlearn/build_irf.py
--rw-rw-r--  2.0 unx     8281 b- defN 22-Jul-14 23:42 ctlearn/data_loader.py
--rw-rw-r--  2.0 unx     9875 b- defN 22-Jul-15 22:43 ctlearn/output_handler.py
--rw-rw-r--  2.0 unx    29352 b- defN 22-Jul-14 23:42 ctlearn/run_model.py
--rw-rw-r--  2.0 unx    10107 b- defN 22-Jul-14 23:42 ctlearn/utils.py
--rw-rw-r--  2.0 unx     6114 b- defN 22-Jul-14 23:42 ctlearn/version.py
--rw-rw-r--  2.0 unx     1733 b- defN 22-Jul-14 23:42 ctlearn/default_config_files/CNNRNN.yml
--rw-rw-r--  2.0 unx     1678 b- defN 22-Jul-14 23:42 ctlearn/default_config_files/TRN.yml
--rw-rw-r--  2.0 unx     1672 b- defN 22-Jul-14 23:42 ctlearn/default_config_files/mergedTRN.yml
--rw-rw-r--  2.0 unx     2305 b- defN 22-Jul-14 23:42 ctlearn/default_models/attention.py
--rw-rw-r--  2.0 unx     4639 b- defN 22-Jul-15 22:43 ctlearn/default_models/basic.py
--rw-rw-r--  2.0 unx     3332 b- defN 22-Jul-14 23:42 ctlearn/default_models/cnn_rnn.py
--rw-rw-r--  2.0 unx     2659 b- defN 22-Jul-14 23:42 ctlearn/default_models/head.py
--rw-rw-r--  2.0 unx     6419 b- defN 22-Jul-14 23:42 ctlearn/default_models/resnet_engine.py
--rw-rw-r--  2.0 unx     2217 b- defN 22-Jul-14 23:42 ctlearn/default_models/single_cnn.py
--rw-rw-r--  2.0 unx     7949 b- defN 22-Jul-14 23:42 ctlearn/default_models/variable_input_model.py
--rw-rw-r--  2.0 unx     1518 b- defN 22-Jul-15 22:52 ctlearn-0.6.1.dist-info/LICENSE
--rw-rw-r--  2.0 unx    13182 b- defN 22-Jul-15 22:52 ctlearn-0.6.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 22-Jul-15 22:52 ctlearn-0.6.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       86 b- defN 22-Jul-15 22:52 ctlearn-0.6.1.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        8 b- defN 22-Jul-15 22:52 ctlearn-0.6.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1959 b- defN 22-Jul-15 22:52 ctlearn-0.6.1.dist-info/RECORD
-23 files, 131567 bytes uncompressed, 36321 bytes compressed:  72.4%
+Zip file size: 36997 bytes, number of entries: 23
+-rw-rw-r--  2.0 unx      186 b- defN 23-May-08 14:27 ctlearn/__init__.py
+-rw-rw-r--  2.0 unx    17396 b- defN 23-May-08 14:27 ctlearn/build_irf.py
+-rw-rw-r--  2.0 unx     8281 b- defN 23-May-08 14:27 ctlearn/data_loader.py
+-rw-rw-r--  2.0 unx    10257 b- defN 23-May-08 14:27 ctlearn/output_handler.py
+-rw-rw-r--  2.0 unx    29847 b- defN 23-May-08 14:27 ctlearn/run_model.py
+-rw-rw-r--  2.0 unx    10111 b- defN 23-May-08 14:27 ctlearn/utils.py
+-rw-rw-r--  2.0 unx     6114 b- defN 23-May-08 14:27 ctlearn/version.py
+-rw-rw-r--  2.0 unx     1798 b- defN 23-May-08 14:27 ctlearn/default_config_files/CNNRNN.yml
+-rw-rw-r--  2.0 unx     1729 b- defN 23-May-08 14:27 ctlearn/default_config_files/TRN.yml
+-rw-rw-r--  2.0 unx     1736 b- defN 23-May-08 14:27 ctlearn/default_config_files/mergedTRN.yml
+-rw-rw-r--  2.0 unx     2305 b- defN 23-May-08 14:27 ctlearn/default_models/attention.py
+-rw-rw-r--  2.0 unx     4639 b- defN 23-May-08 14:27 ctlearn/default_models/basic.py
+-rw-rw-r--  2.0 unx     3325 b- defN 23-May-08 14:27 ctlearn/default_models/cnn_rnn.py
+-rw-rw-r--  2.0 unx     2465 b- defN 23-May-08 14:27 ctlearn/default_models/head.py
+-rw-rw-r--  2.0 unx     6398 b- defN 23-May-08 14:27 ctlearn/default_models/resnet.py
+-rw-rw-r--  2.0 unx     2235 b- defN 23-May-08 14:27 ctlearn/default_models/single_cnn.py
+-rw-rw-r--  2.0 unx     7949 b- defN 23-May-08 14:27 ctlearn/default_models/variable_input_model.py
+-rw-rw-r--  2.0 unx     1518 b- defN 23-May-08 14:35 ctlearn-0.7.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     6925 b- defN 23-May-08 14:35 ctlearn-0.7.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-08 14:35 ctlearn-0.7.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       86 b- defN 23-May-08 14:35 ctlearn-0.7.0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        8 b- defN 23-May-08 14:35 ctlearn-0.7.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     1952 b- defN 23-May-08 14:35 ctlearn-0.7.0.dist-info/RECORD
+23 files, 127352 bytes uncompressed, 33829 bytes compressed:  73.4%
```

## zipnote {}

```diff
@@ -36,35 +36,35 @@
 
 Filename: ctlearn/default_models/cnn_rnn.py
 Comment: 
 
 Filename: ctlearn/default_models/head.py
 Comment: 
 
-Filename: ctlearn/default_models/resnet_engine.py
+Filename: ctlearn/default_models/resnet.py
 Comment: 
 
 Filename: ctlearn/default_models/single_cnn.py
 Comment: 
 
 Filename: ctlearn/default_models/variable_input_model.py
 Comment: 
 
-Filename: ctlearn-0.6.1.dist-info/LICENSE
+Filename: ctlearn-0.7.0.dist-info/LICENSE
 Comment: 
 
-Filename: ctlearn-0.6.1.dist-info/METADATA
+Filename: ctlearn-0.7.0.dist-info/METADATA
 Comment: 
 
-Filename: ctlearn-0.6.1.dist-info/WHEEL
+Filename: ctlearn-0.7.0.dist-info/WHEEL
 Comment: 
 
-Filename: ctlearn-0.6.1.dist-info/entry_points.txt
+Filename: ctlearn-0.7.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: ctlearn-0.6.1.dist-info/top_level.txt
+Filename: ctlearn-0.7.0.dist-info/top_level.txt
 Comment: 
 
-Filename: ctlearn-0.6.1.dist-info/RECORD
+Filename: ctlearn-0.7.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ctlearn/build_irf.py

```diff
@@ -1,9 +1,9 @@
 """
-Build IRFS and sensitivity from CTLearn DL2-like files using pyirf.
+Build IRFs and sensitivity curves from CTLearn DL2-like files using pyirf.
 Edited from pyirf examples (Credits Noethe et al.):
 https://github.com/cta-observatory/pyirf/blob/master/examples/calculate_eventdisplay_irfs.py
 """
 import argparse
 import glob
 import logging
 import operator
@@ -53,118 +53,128 @@
 )
 
 
 log = logging.getLogger("pyirf")
 
 # Map the particle ids to the particle information
 particles = {
-    1: {
+    0: {
         "name": "gamma",
         "target_spectrum": CRAB_HEGRA,
         "mc_header": pd.DataFrame(),
         "events": QTable(),
     },
-    0: {
+    101: {
         "name": "proton",
         "target_spectrum": IRFDOC_PROTON_SPECTRUM,
         "mc_header": pd.DataFrame(),
         "events": QTable(),
     },
-    2: {
+    1: {
         "name": "electron",
         "target_spectrum": IRFDOC_ELECTRON_SPECTRUM,
         "mc_header": pd.DataFrame(),
         "events": QTable(),
     },
 }
 
+# Map column names
+name_mapping = {
+    "gammaness": "gh_score",
+    "source_alt": "true_alt",
+    "source_az": "true_az",
+}
 # Map units
 unit_mapping = {
     "true_energy": u.TeV,
     "reco_energy": u.TeV,
     "pointing_alt": u.rad,
     "pointing_az": u.rad,
     "true_alt": u.rad,
     "true_az": u.rad,
     "reco_alt": u.rad,
     "reco_az": u.rad,
 }
 
 
 def main():
-
     parser = argparse.ArgumentParser(
         description=(
-            "Build IRFS and sensitivity from CTLearn DL2-like files using pyirf."
+            "Build IRFs and sensitivity curves from CTLearn DL2-like files using pyirf."
         )
     )
     parser.add_argument(
         "--input",
         "-i",
-        help="Input directories",
+        help="Input directories; default is ./",
         default=["./"],
         nargs="+",
     )
     parser.add_argument(
         "--pattern",
         "-p",
-        help="Pattern to mask unwanted files from the data input directory",
+        help="Pattern to mask unwanted files from the data input directory; default is *.h5",
         default=["*.h5"],
         nargs="+",
     )
     parser.add_argument(
         "--output",
         "-o",
-        help="Output file",
-        default="pyirf.fits.gz",
+        help="Output file; default is ./pyirf.fits.gz",
+        default="./pyirf.fits.gz",
     )
     parser.add_argument(
         "--energy_range",
         "-e",
-        help="Energy range in TeV",
-        default = [0.03, 30.0],
+        help="Energy range in TeV; default is [0.02, 20.0]",
+        default=[0.02, 20.0],
         nargs="+",
         type=float,
     )
     parser.add_argument(
         "--theta_range",
         "-t",
-        help="Theta cut range in deg",
-        default = [0.05, 0.3],
+        help="Theta cut range in deg; default is [0.05, 0.3]",
+        default=[0.05, 0.3],
         nargs="+",
         type=float,
     )
     parser.add_argument(
         "--obstime",
-        help="Observation time in hours",
+        help="Observation time in hours; default is 50",
         default=50,
     )
     parser.add_argument(
         "--alpha",
-        help="Scaling between on and off region",
+        help="Scaling between on and off region; default is 0.2",
         default=0.2,
     )
     parser.add_argument(
-        "--max_bg_radius",
-        help="Maximum background radius in deg",
+        "--fov_offset_min",
+        help="Minimum distance from the fov center for background events to be taken into account; default is 0.0",
+        default=0.0,
+    )
+    parser.add_argument(
+        "--fov_offset_max",
+        help="Maximum distance from the fov center in deg for background events to be taken into account; default is 1.0",
         default=1.0,
     )
     parser.add_argument(
         "--max_gh_cut_eff",
-        help="Maximum gamma/hadron cut efficiency",
+        help="Maximum gamma/hadron cut efficiency; default is 0.9",
         default=0.9,
     )
     parser.add_argument(
         "--gh_cut_eff_step",
-        help="Gamma/hadron cut efficiency step",
+        help="Gamma/hadron cut efficiency step; default is 0.01",
         default=0.01,
     )
     parser.add_argument(
         "--init_gh_cut_eff",
-        help="Initial gamma/hadron cut efficiency",
+        help="Initial gamma/hadron cut efficiency; default is 0.4",
         default=0.4,
     )
     parser.add_argument(
         "--quality_cuts",
         "-c",
         help="String of the quality cuts",
         type=str,
@@ -193,101 +203,114 @@
 
     # scaling between on and off region.
     # (Default) Make off region 5 times larger than on region for better
     # background statistics
     ALPHA = args.alpha
 
     # Radius to use for calculating bg rate
-    MAX_BG_RADIUS = args.max_bg_radius * u.deg
+    FOV_OFFSET_MIN = args.fov_offset_min * u.deg
+    FOV_OFFSET_MAX = args.fov_offset_max * u.deg
     MAX_GH_CUT_EFFICIENCY = args.max_gh_cut_eff
     GH_CUT_EFFICIENCY_STEP = args.gh_cut_eff_step
 
     # gh cut used for first calculation of the binned theta cuts
     INITIAL_GH_CUT_EFFICENCY = args.init_gh_cut_eff
 
     MIN_ENERGY = args.energy_range[0] * u.TeV
     MAX_ENERGY = args.energy_range[-1] * u.TeV
 
     MIN_THETA_CUT = args.theta_range[0] * u.deg
     MAX_THETA_CUT = args.theta_range[-1] * u.deg
 
+    global_tel_ids = []
+    n_showers_factor = 1
     for input in args.input:
         abs_file_dir = os.path.abspath(input)
         for pattern in args.pattern:
             files = glob.glob(os.path.join(abs_file_dir, pattern))
             if not files:
                 continue
 
             for file in np.sort(files):
+                tel_ids = []
                 with pd.HDFStore(file, mode="r") as f:
                     file_keys = list(f.keys())
                     events = f["/dl2/reco"]
+                    events = events.rename(columns=name_mapping)
                     particle_type = int(events["true_shower_primary_id"][0])
                     drop_cols = ["event_id", "obs_id", "true_shower_primary_id"]
                     for k in [key for key in file_keys if key.startswith("/dl1b/")]:
+                        tel_ids_string = k.split("/")[-1].replace("tel_", "")
+                        n_showers_factor = len(tel_ids_string.split("_"))
+                        tel_ids.append(int(tel_ids_string))
                         parameters = f[k].rename(
-                            lambda x: f'{k.split("/")[-1]}_' + x, axis="columns"
+                            lambda x: f"tel_{int(tel_ids_string)}_" + x, axis="columns"
                         )
                         drop_cols.extend(parameters.keys())
                         events = pd.concat([events, parameters], axis=1)
 
+                    if not global_tel_ids:
+                        global_tel_ids = tel_ids
+                    else:
+                        if global_tel_ids != tel_ids:
+                            raise ValueError(
+                                f"Tel ids inconsistent. '{global_tel_ids}' is not equal to '{tel_ids}' from '{file}'."
+                            )
+
                     # Apply quality cuts
                     mask = None
                     if args.quality_cuts:
                         mask = args.quality_cuts
 
                     if args.size_cut:
                         for s, size in enumerate(args.size_cut):
                             if mask:
-                                mask += f"& tel_{s+1}_hillas_intensity > {size} "
+                                mask += f"& tel_{global_tel_ids[s]}_hillas_intensity > {size} "
                             else:
-                                mask = f"tel_{s+1}_hillas_intensity > {size} "
+                                mask = f"tel_{global_tel_ids[s]}_hillas_intensity > {size} "
                     if args.leakage_cut:
                         for l, leakage in enumerate(args.leakage_cut):
                             if mask:
-                                mask += f"& tel_{l+1}_leakage_intensity_width_2 > {leakage} "
+                                mask += f"& tel_{global_tel_ids[l]}_leakage_intensity_width_2 < {leakage} "
                             else:
-                                mask = (
-                                    f"tel_{l+1}_leakage_intensity_width_2 > {leakage} "
-                                )
+                                mask = f"tel_{global_tel_ids[l]}_leakage_intensity_width_2 < {leakage} "
                     if mask:
                         events.query(mask, inplace=True)
                     events = events.drop(drop_cols, axis=1)
                     events = table.QTable.from_pandas(events)
                     for k, v in unit_mapping.items():
                         events[k] *= v
 
                     particles[particle_type]["events"] = table.vstack(
                         [particles[particle_type]["events"], events]
                     )
 
                     # Sims info
                     mc_header = f["/info/mc_header"]
+
                     # Check if ringwobbles then set the viewcone radius to zero
-                    if (
-                        particles[particle_type]["name"] == "gamma"
-                        and np.around(mc_header["max_viewcone_radius"][0], decimals=1)
-                        == 0.4
-                    ):
-                        mc_header["max_viewcone_radius"][0] = 0
                     particles[particle_type]["mc_header"] = pd.concat(
                         [particles[particle_type]["mc_header"], mc_header],
                         ignore_index=True,
                     )
 
     for particle_type, p in particles.items():
         log.info(f'Simulated {p["name"]} Events:')
 
         simulation_info = SimulatedEventsInfo(
-            n_showers=int(p["mc_header"]["num_showers"].sum()),
+            n_showers=int(n_showers_factor * p["mc_header"]["n_showers"].sum()),
             energy_min=u.Quantity(p["mc_header"]["energy_range_min"].min(), u.TeV),
             energy_max=u.Quantity(p["mc_header"]["energy_range_max"].max(), u.TeV),
             spectral_index=p["mc_header"]["spectral_index"][0],
             max_impact=u.Quantity(p["mc_header"]["max_scatter_range"].max(), u.m),
-            viewcone=u.Quantity(p["mc_header"]["max_viewcone_radius"][0], u.deg),
+            viewcone=u.Quantity(
+                p["mc_header"]["max_viewcone_radius"][0]
+                - p["mc_header"]["min_viewcone_radius"][0],
+                u.deg,
+            ),
         )
         p["simulation_info"] = simulation_info
         p["simulated_spectrum"] = PowerLaw.from_simulation(simulation_info, T_OBS)
         p["events"]["weight"] = MaskedColumn(
             data=calculate_event_weights(
                 p["events"]["true_energy"],
                 p["target_spectrum"],
@@ -303,31 +326,27 @@
             p["events"],
             assumed_source_az=p["events"]["true_az"],
             assumed_source_alt=p["events"]["true_alt"],
         )
         log.info(simulation_info)
         log.info("")
 
-    gammas = particles[1]["events"]
+    gammas = particles[0]["events"]
     # background table composed of both electrons and protons
-    background = table.vstack([particles[0]["events"], particles[2]["events"]])
+    background = table.vstack([particles[101]["events"], particles[1]["events"]])
 
     INITIAL_GH_CUT = np.quantile(gammas["gh_score"], (1 - INITIAL_GH_CUT_EFFICENCY))
     log.info(f"Using fixed G/H cut of {INITIAL_GH_CUT} to calculate theta cuts")
 
     # event display uses much finer bins for the theta cut than
     # for the sensitivity
-    theta_bins = add_overflow_bins(
-        create_bins_per_decade(MIN_ENERGY, MAX_ENERGY, 50)
-    )
+    theta_bins = add_overflow_bins(create_bins_per_decade(MIN_ENERGY, MAX_ENERGY, 50))
     # same bins as event display uses
     sensitivity_bins = add_overflow_bins(
-        create_bins_per_decade(
-            MIN_ENERGY, MAX_ENERGY, bins_per_decade=5
-        )
+        create_bins_per_decade(MIN_ENERGY, MAX_ENERGY, bins_per_decade=5)
     )
 
     # theta cut is 68 percent containmente of the gammas
     # for now with a fixed global, unoptimized score cut
     # the cut is calculated in the same bins as the sensitivity,
     # but then interpolated to 10x the resolution.
     mask_theta_cuts = gammas["gh_score"] >= INITIAL_GH_CUT
@@ -367,15 +386,16 @@
         gammas,
         background,
         reco_energy_bins=sensitivity_bins,
         gh_cut_efficiencies=gh_cut_efficiencies,
         op=operator.ge,
         theta_cuts=theta_cuts,
         alpha=ALPHA,
-        background_radius=MAX_BG_RADIUS,
+        fov_offset_min=FOV_OFFSET_MIN,
+        fov_offset_max=FOV_OFFSET_MAX,
     )
 
     # now that we have the optimized gh cuts, we recalculate the theta
     # cut as 68 percent containment on the events surviving these cuts.
     log.info("Recalculating theta cut for optimized GH Cuts")
     for tab in (gammas, background):
         tab["selected_gh"] = evaluate_binned_cut(
@@ -384,15 +404,15 @@
 
     gammas["selected_theta"] = evaluate_binned_cut(
         gammas["theta"], gammas["reco_energy"], theta_cuts, operator.le
     )
     gammas["selected"] = gammas["selected_theta"] & gammas["selected_gh"]
 
     # scale relative sensitivity by Crab flux to get the flux sensitivity
-    spectrum = particles[1]["target_spectrum"]
+    spectrum = particles[0]["target_spectrum"]
     sensitivity["flux_sensitivity"] = sensitivity["relative_sensitivity"] * spectrum(
         sensitivity["reco_energy_center"]
     )
 
     log.info("Calculating IRFs")
     hdus = [
         fits.PrimaryHDU(),
@@ -418,15 +438,15 @@
     fov_offset_bins = [0, 0.5] * u.deg
     source_offset_bins = np.arange(0, 1 + 1e-4, 1e-3) * u.deg
     energy_migration_bins = np.geomspace(0.2, 5, 200)
 
     for label, mask in masks.items():
         effective_area = effective_area_per_energy(
             gammas[mask],
-            particles[1]["simulation_info"],
+            particles[0]["simulation_info"],
             true_energy_bins=true_energy_bins,
         )
         hdus.append(
             create_aeff2d_hdu(
                 effective_area[..., np.newaxis],  # add one dimension for FOV offset
                 true_energy_bins,
                 fov_offset_bins,
```

## ctlearn/output_handler.py

```diff
@@ -58,53 +58,57 @@
                 ),
                 axis=0,
             )
         reco["event_id"] = event_id
     if data.obs_pos:
         obs_id = data.obs_list[data.batch_size :]
         if rest_data:
-            reco["obs_id"] = np.concatenate(
+            obs_id = np.concatenate(
                 (
                     obs_id,
                     rest_data.obs_list[rest_data.batch_size :],
                 ),
                 axis=0,
             )
+        reco["obs_id"] = obs_id
 
     # Store the timestamp
     if data.mjd_pos:
         mjd = data.mjd_list[data.batch_size :]
         if rest_data:
-            reco["mjd"] = np.concatenate(
+            mjd = np.concatenate(
                 (
                     mjd,
                     rest_data.mjd_list[rest_data.batch_size :],
                 ),
                 axis=0,
             )
+        reco["mjd"] = mjd
     if data.milli_pos:
         milli_sec = data.milli_list[data.batch_size :]
         if rest_data:
-            reco["milli_sec"] = np.concatenate(
+            milli_sec = np.concatenate(
                 (
                     milli_sec,
                     rest_data.milli_list[rest_data.batch_size :],
                 ),
                 axis=0,
             )
+        reco["milli_sec"] = milli_sec
     if data.nano_pos:
         nano_sec = data.nano_list[data.batch_size :]
         if rest_data:
-            reco["nano_sec"] = np.concatenate(
+            nano_sec = np.concatenate(
                 (
                     nano_sec,
                     rest_data.nano_list[rest_data.batch_size :],
                 ),
                 axis=0,
             )
+        reco["nano_sec"] = nano_sec
 
     # Store pointings
     if data.pon_pos:
         pointing_alt = np.array(data.pointing)[data.batch_size :, 0]
         pointing_az = np.array(data.pointing)[data.batch_size :, 1]
         if rest_data:
             pointing_alt = np.concatenate(
@@ -117,57 +121,61 @@
             pointing_az = np.concatenate(
                 (
                     pointing_az,
                     np.array(rest_data.pointing)[rest_data.batch_size :, 1],
                 ),
                 axis=0,
             )
+
     else:
         pointing_alt = np.array([reader.pointing[0]] * len(reader))
         pointing_az = np.array([reader.pointing[1]] * len(reader))
     reco["pointing_alt"] = pointing_alt
     reco["pointing_az"] = pointing_az
 
     # Store predictions and simulation values
     # Gamma/hadron classification
     if data.prt_pos:
         true_shower_primary_id = data.prt_labels[data.batch_size :]
         if rest_data:
-            reco["true_shower_primary_id"] = np.concatenate(
+            true_shower_primary_id = np.concatenate(
                 (
                     true_shower_primary_id,
                     rest_data.prt_labels[rest_data.batch_size :],
                 ),
                 axis=0,
             )
+        reco["true_shower_primary_id"] = true_shower_primary_id
     if "particletype" in tasks:
         for n, name in enumerate(data.class_names):
             reco[name + "ness"] = np.array(predictions[:, n])
     # Energy regression
     if data.enr_pos:
         if data.energy_unit == "log(TeV)":
             true_energy = np.power(10, data.enr_labels[data.batch_size :])
             if rest_data:
-                reco["true_energy"] = np.concatenate(
+                true_energy = np.concatenate(
                     (
                         true_energy,
                         np.power(10, rest_data.enr_labels[rest_data.batch_size :]),
                     ),
                     axis=0,
                 )
+            reco["true_energy"] = true_energy
         else:
             true_energy = data.enr_labels[data.batch_size :]
             if rest_data:
-                reco["true_energy"] = np.concatenate(
+                true_energy = np.concatenate(
                     (
                         true_energy,
                         rest_data.enr_labels[rest_data.batch_size :],
                     ),
                     axis=0,
                 )
+            reco["true_energy"] = true_energy
     if "energy" in tasks:
         if data.energy_unit == "log(TeV)" or np.min(predictions) < 0.0:
             reco["reco_energy"] = np.power(10, predictions)[:, 0]
         else:
             reco["reco_energy"] = np.array(predictions)[:, 0]
     # Arrival direction regression
     if data.drc_pos:
@@ -214,49 +222,51 @@
             h5file, key=f"/info/mc_header", mode="a"
         )
 
     # Store the selected Hillas parameters (dl1b)
     if reader.parameter_list:
         tel_counter = 0
         if reader.mode == "mono":
-            tel_type = list(reader.telescopes.keys())[0]
+            tel_type = list(reader.selected_telescopes.keys())[0]
             tel_ids = "tel"
-            for tel_id in reader.telescopes[tel_type]:
+            for tel_id in reader.selected_telescopes[tel_type]:
                 tel_ids += f"_{tel_id}"
             parameters = {}
             for p, parameter in enumerate(reader.parameter_list):
                 parameter_list = np.array(data.parameter_list)[data.batch_size :, p]
                 if rest_data:
-                    parameters[parameter] = np.concatenate(
+                    parameter_list = np.concatenate(
                         (
                             parameter_list,
                             np.array(rest_data.parameter_list)[
                                 rest_data.batch_size :, p
                             ],
                         ),
                         axis=0,
                     )
+                parameters[parameter] = parameter_list
             pd.DataFrame(data=parameters).to_hdf(
                 h5file, key=f"/dl1b/{tel_type}/{tel_ids}", mode="a"
             )
         else:
-            for tel_type in reader.telescopes:
-                for t, tel_id in enumerate(reader.telescopes[tel_type]):
+            for tel_type in reader.selected_telescopes:
+                for t, tel_id in enumerate(reader.selected_telescopes[tel_type]):
                     parameters = {}
                     for p, parameter in enumerate(reader.parameter_list):
                         parameter_list = np.array(data.parameter_list)[
                             data.batch_size :, tel_counter + t, p
                         ]
                         if rest_data:
-                            parameters[parameter] = np.concatenate(
+                            parameter_list = np.concatenate(
                                 (
                                     parameter_list,
                                     np.array(rest_data.parameter_list)[
                                         rest_data.batch_size :, tel_counter + t, p
                                     ],
                                 ),
                                 axis=0,
                             )
+                        parameters[parameter] = parameter_list
                     pd.DataFrame(data=parameters).to_hdf(
                         h5file, key=f"/dl1b/{tel_type}/tel_{tel_id}", mode="a"
                     )
-                tel_counter += len(reader.telescopes[tel_type])
+                tel_counter += len(reader.selected_telescopes[tel_type])
```

## ctlearn/run_model.py

```diff
@@ -659,14 +659,18 @@
                             config["Data"]["selected_telescope_ids"] = args.allowed_tels
                         if parameter_selection:
                             config["Data"]["parameter_selection"] = parameter_selection
                         if args.multiplicity_cut:
                             config["Data"]["multiplicity_selection"] = {
                                 "Subarray": args.multiplicity_cut
                             }
+                        if args.batch_size:
+                            if "Input" not in config:
+                                config["Input"] = {}
+                            config["Input"]["batch_size_per_worker"] = args.batch_size
                         if args.output:
                             config["Logging"] = {}
                             config["Logging"]["model_directory"] = args.output
                         if args.pretrained_weights:
                             config["Model"][
                                 "pretrained_weights"
                             ] = args.pretrained_weights
@@ -680,14 +684,15 @@
                         if "Prediction" not in config:
                             config["Prediction"] = {}
 
                         prediction_file = (
                             file.split("/")[-1]
                             .replace("_S_", "_E_")
                             .replace("dl1", "dl2")
+                            .replace("DL1", "DL2")
                         )
                         prediction_path = file.replace(f'{file.split("/")[-1]}', "")
                         if args.prediction_directory:
                             prediction_path = args.prediction_directory
                         prediction_file = f"{prediction_path}/{prediction_file}"
                         config["Prediction"]["prediction_file_lists"] = {
                             prediction_file: file
@@ -716,14 +721,18 @@
                     config["Data"]["selected_telescope_ids"] = args.allowed_tels
                 if parameter_selection:
                     config["Data"]["parameter_selection"] = parameter_selection
                 if args.multiplicity_cut:
                     config["Data"]["multiplicity_selection"] = {
                         "Subarray": args.multiplicity_cut
                     }
+                if args.batch_size:
+                    if "Input" not in config:
+                        config["Input"] = {}
+                    config["Input"]["batch_size_per_worker"] = args.batch_size
                 if args.output:
                     config["Logging"] = {}
                     config["Logging"]["model_directory"] = args.output
                 if args.pretrained_weights:
                     config["Model"]["pretrained_weights"] = args.pretrained_weights
                     config["Model"]["trainable_backbone"] = False
```

## ctlearn/utils.py

```diff
@@ -163,16 +163,16 @@
                     },
                 }
             )
     else:
         if "parameter_list" not in config["Data"] and mode == "predict":
             config["Data"]["parameter_list"] = [
                 "hillas_intensity",
-                "hillas_x",
-                "hillas_y",
+                "hillas_fov_lon",
+                "hillas_fov_lat",
                 "hillas_r",
                 "hillas_phi",
                 "hillas_length",
                 "hillas_length_uncertainty",
                 "hillas_width",
                 "hillas_width_uncertainty",
                 "hillas_psi",
@@ -184,18 +184,18 @@
                 "leakage_pixels_width_1",
                 "leakage_pixels_width_2",
                 "leakage_intensity_width_1",
                 "leakage_intensity_width_2",
                 "concentration_cog",
                 "concentration_core",
                 "concentration_pixel",
-                "morphology_num_pixels",
-                "morphology_num_islands",
-                "morphology_num_medium_islands",
-                "morphology_num_large_islands",
+                "morphology_n_pixels",
+                "morphology_n_islands",
+                "morphology_n_medium_islands",
+                "morphology_n_large_islands",
                 "intensity_max",
                 "intensity_min",
                 "intensity_mean",
                 "intensity_std",
                 "intensity_skewness",
                 "intensity_kurtosis",
                 "peak_time_max",
```

## ctlearn/default_config_files/CNNRNN.yml

```diff
@@ -4,29 +4,31 @@
     mapping_settings:
         mapping_method:
             'LSTCam': 'bilinear_interpolation'
             'FlashCam': 'bilinear_interpolation'
             'NectarCam': 'bilinear_interpolation'
             'CHEC': 'oversampling'
             'SCTCam': 'oversampling'
+            'LSTSiPMCam': 'bilinear_interpolation'
             'MAGICCam': 'bilinear_interpolation'
         padding:
             'LSTCam': 2
             'FlashCam': 2
             'NectarCam': 2
             'CHEC': 0
-            'SCTCam': 0             
+            'SCTCam': 0
+            'LSTSiPMCam': 2
             'MAGICCam': 2
 Input:
     batch_size_per_worker: 16
     concat_telescopes: false
 Model:
     name: 'CNNRNN'
     backbone: {module: 'cnn_rnn', function: 'cnn_rnn_model'}
-    network: {module: 'basic', function: 'conv_block'}
+    engine: {module: 'basic', function: 'conv_block'}
     head: {module: 'head', function: 'standard_head'}
 Model Parameters:
     attention: {mechanism: 'Squeeze-and-Excitation', ratio: 16}
     basic:
         conv_block:
             layers:
                 - {filters: 32, kernel_size: 3, number: 1}
```

## ctlearn/default_config_files/TRN.yml

```diff
@@ -4,33 +4,35 @@
     mapping_settings:
         mapping_method:
             'LSTCam': 'bilinear_interpolation'
             'FlashCam': 'bilinear_interpolation'
             'NectarCam': 'bilinear_interpolation'
             'CHEC': 'oversampling'
             'SCTCam': 'oversampling'
+            'LSTSiPMCam': 'bilinear_interpolation'
             'MAGICCam': 'bilinear_interpolation'
         padding:
             'LSTCam': 2
             'FlashCam': 2
             'NectarCam': 2
             'CHEC': 0
-            'SCTCam': 0             
+            'SCTCam': 0
+            'LSTSiPMCam': 2
             'MAGICCam': 2
 Input:
     batch_size_per_worker: 64
     concat_telescopes: false
 Model:
     name: 'ThinResNet'
     backbone: {module: 'single_cnn', function: 'single_cnn_model'}
-    network: {module: 'resnet_engine', function: 'stacked_res_blocks'}
+    engine: {module: 'resnet', function: 'stacked_res_blocks'}
     head: {module: 'head', function: 'standard_head'}
 Model Parameters:
     attention: {mechanism: 'Squeeze-and-Excitation', ratio: 16}
-    resnet_engine:
+    resnet:
         stacked_res_blocks:
             residual_block: 'bottleneck'
             architecture:
                 - {filters: 48, blocks: 2}
                 - {filters: 96, blocks: 3}
                 - {filters: 128, blocks: 3}
                 - {filters: 256, blocks: 3}
```

## ctlearn/default_config_files/mergedTRN.yml

```diff
@@ -4,33 +4,35 @@
     mapping_settings:
         mapping_method:
             'LSTCam': 'bilinear_interpolation'
             'FlashCam': 'bilinear_interpolation'
             'NectarCam': 'bilinear_interpolation'
             'CHEC': 'oversampling'
             'SCTCam': 'oversampling'
+            'LSTSiPMCam': 'bilinear_interpolation'
             'MAGICCam': 'bilinear_interpolation'
         padding:
             'LSTCam': 2
             'FlashCam': 2
             'NectarCam': 2
             'CHEC': 0
             'SCTCam': 0
+            'LSTSiPMCam': 2
             'MAGICCam': 2
 Input:
     batch_size_per_worker: 64
     concat_telescopes: true
 Model:
     name: 'ThinResNet'
     backbone: {module: 'single_cnn', function: 'single_cnn_model'}
-    network: {module: 'resnet_engine', function: 'stacked_res_blocks'}
+    engine: {module: 'resnet', function: 'stacked_res_blocks'}
     head: {module: 'head', function: 'standard_head'}
 Model Parameters:
     attention: {mechanism: 'Squeeze-and-Excitation', ratio: 16}
-    resnet_engine:
+    resnet:
         stacked_res_blocks:
             residual_block: 'bottleneck'
             architecture:
                 - {filters: 48, blocks: 2}
                 - {filters: 96, blocks: 3}
                 - {filters: 128, blocks: 3}
                 - {filters: 256, blocks: 3}
```

## ctlearn/default_models/cnn_rnn.py

```diff
@@ -17,36 +17,36 @@
     # Unlike standard dropout, this zeroing-out procedure is performed both at
     # training and test time since it encodes meaningful aspects of the data.
     # The telescope outputs are then stacked into input for the array-level
     # network, either into 1D feature vectors or into 3D convolutional
     # feature maps, depending on the requirements of the network head.
     # The array-level processing is then performed by the network head. The
     # logits are returned and fed into a classifier/regressor.
-    network_name = model_params.get("name", "CNNRNN")
+    backbone_name = model_params.get("name", "CNNRNN")
     trainable_backbone = model_params.get("trainable_backbone", True)
     pretrained_weights = model_params.get("pretrained_weights", None)
     if pretrained_weights:
         loaded_model = tf.keras.models.load_model(pretrained_weights)
         for layer in loaded_model.layers:
             if layer.name.endswith("_block"):
                 model = loaded_model.get_layer(layer.name)
                 model.trainable = trainable_backbone
     else:
         sys.path.append(model_params["model_directory"])
-        network_module = importlib.import_module(model_params["network"]["module"])
-        network = getattr(network_module, model_params["network"]["function"])
+        engine_module = importlib.import_module(model_params["engine"]["module"])
+        engine = getattr(engine_module, model_params["engine"]["function"])
         network_input = tf.keras.Input(shape=data.singleimg_shape, name=f"images")
-        network_output = network(
-            network_input, params=model_params, name=model_params["network"]["function"]
+        engine_output = engine(
+            network_input, params=model_params, name=model_params["engine"]["function"]
         )
         output = tf.keras.layers.GlobalAveragePooling2D(
-            name=network_name + "_global_avgpool"
-        )(network_output)
+            name=backbone_name + "_global_avgpool"
+        )(engine_output)
         model = tf.keras.Model(
-            network_input, output, name=model_params["network"]["function"]
+            network_input, output, name=model_params["engine"]["function"]
         )
 
     telescope_data = tf.keras.Input(shape=data.img_shape, name=f"images")
     telescope_triggers = tf.keras.Input(shape=(*data.trg_shape, 1), name=f"triggers")
 
     output = tf.keras.layers.TimeDistributed(model)(telescope_data)
     dropout = tf.keras.layers.TimeDistributed(tf.keras.layers.Dropout(rate=0.2))(output)
@@ -66,11 +66,11 @@
 
     fc2 = tf.keras.layers.Dense(
         units=512, kernel_regularizer=tf.keras.regularizers.L2(l2=0.004), name="fc2"
     )(dropout_1)
     dropout_2 = tf.keras.layers.Dropout(rate=dropout_rate)(fc2)
 
     cnnrnn_model = tf.keras.Model(
-        [telescope_data, telescope_triggers], dropout_2, name=network_name
+        [telescope_data, telescope_triggers], dropout_2, name=backbone_name
     )
 
     return cnnrnn_model, [telescope_data, telescope_triggers]
```

## ctlearn/default_models/head.py

```diff
@@ -21,18 +21,15 @@
         logits["particletype"] = tf.keras.layers.Softmax(name="particletype")(logit)
         losses["particletype"] = tf.keras.losses.CategoricalCrossentropy(
             reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE
         )
         loss_weights["particletype"] = standard_head_settings["particletype"]["weight"]
         metrics["particletype"] = [
             tf.keras.metrics.CategoricalAccuracy(name="accuracy"),
-            tf.keras.metrics.Precision(name="precision"),
-            tf.keras.metrics.Recall(name="recall"),
             tf.keras.metrics.AUC(name="auc"),
-            tf.keras.metrics.AUC(name="prc", curve="PR"),  # precision-recall curve
         ]
     if "energy" in tasks:
         logits["energy"] = fully_connect(
             inputs,
             standard_head_settings["energy"]["fc_head"],
             expected_logits_dimension=1,
             name="energy",
```

## ctlearn/default_models/single_cnn.py

```diff
@@ -4,49 +4,49 @@
 import tensorflow as tf
 
 
 def single_cnn_model(data, model_params):
 
     # Load neural network model
     network_input = tf.keras.Input(shape=data.img_shape, name=f"images")
-    network_name = model_params.get("name", "CNN") + "_block"
+    backbone_name = model_params.get("name", "CNN") + "_block"
     trainable_backbone = model_params.get("trainable_backbone", True)
     pretrained_weights = model_params.get("pretrained_weights", None)
     if pretrained_weights:
         loaded_model = tf.keras.models.load_model(pretrained_weights)
         for layer in loaded_model.layers:
             if layer.name.endswith("_block"):
                 model = loaded_model.get_layer(layer.name)
                 model.trainable = trainable_backbone
     else:
         sys.path.append(model_params["model_directory"])
-        network_module = importlib.import_module(model_params["network"]["module"])
-        network = getattr(network_module, model_params["network"]["function"])
+        engine_module = importlib.import_module(model_params["engine"]["module"])
+        engine = getattr(engine_module, model_params["engine"]["function"])
 
         # The original ResNet implementation use this padding, but we pad the images in the ImageMapper.
         # x = tf.pad(telescope_data, tf.constant([[3, 3], [3, 3]]), name='conv1_pad')
         init_layer = model_params.get("init_layer", False)
         if init_layer:
             network_input = tf.keras.layers.Conv2D(
                 filters=init_layer["filters"],
                 kernel_size=init_layer["kernel_size"],
                 strides=init_layer["strides"],
-                name=network_name + "_conv1_conv",
+                name=backbone_name + "_conv1_conv",
             )(network_input)
         # x = tf.pad(x, tf.constant([[1, 1], [1, 1]]), name='pool1_pad')
         init_max_pool = model_params.get("init_max_pool", False)
         if init_max_pool:
             network_input = tf.keras.layers.MaxPool2D(
                 pool_size=init_max_pool["size"],
                 strides=init_max_pool["strides"],
-                name=network_name + "_pool1_pool",
+                name=backbone_name + "_pool1_pool",
             )(network_input)
 
-        network_output = network(network_input, params=model_params, name=network_name)
+        engine_output = engine(network_input, params=model_params, name=backbone_name)
 
         output = tf.keras.layers.GlobalAveragePooling2D(
-            name=network_name + "_global_avgpool"
-        )(network_output)
+            name=backbone_name + "_global_avgpool"
+        )(engine_output)
 
-        model = tf.keras.Model(network_input, output, name=network_name)
+        singlecnn_model = tf.keras.Model(network_input, output, name=backbone_name)
 
-    return model, [network_input]
+    return singlecnn_model, [network_input]
```

## Comparing `ctlearn/default_models/resnet_engine.py` & `ctlearn/default_models/resnet.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,24 +11,24 @@
     Arguments:
       input_shape: input tensor shape.
       params: config parameters for the ResNet engine.
     Returns:
       Output tensor for the ResNet architecture.
     """
     # Get custom hyperparameters
-    residual_block = params["resnet_engine"]["stacked_res_blocks"].get(
+    residual_block = params["resnet"]["stacked_res_blocks"].get(
         "residual_block", "bottleneck"
     )
     filters_list = [
         layer["filters"]
-        for layer in params["resnet_engine"]["stacked_res_blocks"]["architecture"]
+        for layer in params["resnet"]["stacked_res_blocks"]["architecture"]
     ]
     blocks_list = [
         layer["blocks"]
-        for layer in params["resnet_engine"]["stacked_res_blocks"]["architecture"]
+        for layer in params["resnet"]["stacked_res_blocks"]["architecture"]
     ]
     attention = params.get("attention", None)
 
     x = stack_fn(
         inputs,
         filters_list[0],
         blocks_list[0],
```

## Comparing `ctlearn-0.6.1.dist-info/LICENSE` & `ctlearn-0.7.0.dist-info/LICENSE`

 * *Files identical despite different names*

