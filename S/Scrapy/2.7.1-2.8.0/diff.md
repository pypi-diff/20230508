# Comparing `tmp/Scrapy-2.7.1.tar.gz` & `tmp/Scrapy-2.8.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "Scrapy-2.7.1.tar", last modified: Wed Nov  2 11:18:23 2022, max compression
+gzip compressed data, was "Scrapy-2.8.0.tar", last modified: Thu Feb  2 04:54:56 2023, max compression
```

## Comparing `Scrapy-2.7.1.tar` & `Scrapy-2.8.0.tar`

### file list

```diff
@@ -1,543 +1,543 @@
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/
--rw-r--r--   0 runner    (1001) docker     (121)     1284 2022-11-02 11:18:03.000000 Scrapy-2.7.1/AUTHORS
--rw-r--r--   0 runner    (1001) docker     (121)     1517 2022-11-02 11:18:03.000000 Scrapy-2.7.1/LICENSE
--rw-r--r--   0 runner    (1001) docker     (121)      486 2022-11-02 11:18:03.000000 Scrapy-2.7.1/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (121)       18 2022-11-02 11:18:03.000000 Scrapy-2.7.1/NEWS
--rw-r--r--   0 runner    (1001) docker     (121)     4525 2022-11-02 11:18:23.573532 Scrapy-2.7.1/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)     3095 2022-11-02 11:18:03.000000 Scrapy-2.7.1/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.533532 Scrapy-2.7.1/Scrapy.egg-info/
--rw-r--r--   0 runner    (1001) docker     (121)     4525 2022-11-02 11:18:23.000000 Scrapy-2.7.1/Scrapy.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)    15437 2022-11-02 11:18:23.000000 Scrapy-2.7.1/Scrapy.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2022-11-02 11:18:23.000000 Scrapy-2.7.1/Scrapy.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (121)       50 2022-11-02 11:18:23.000000 Scrapy-2.7.1/Scrapy.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2022-11-02 11:18:23.000000 Scrapy-2.7.1/Scrapy.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (121)      393 2022-11-02 11:18:23.000000 Scrapy-2.7.1/Scrapy.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (121)        7 2022-11-02 11:18:23.000000 Scrapy-2.7.1/Scrapy.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (121)       80 2022-11-02 11:18:03.000000 Scrapy-2.7.1/codecov.yml
--rw-r--r--   0 runner    (1001) docker     (121)     2163 2022-11-02 11:18:03.000000 Scrapy-2.7.1/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.533532 Scrapy-2.7.1/docs/
--rw-r--r--   0 runner    (1001) docker     (121)     2812 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/Makefile
--rw-r--r--   0 runner    (1001) docker     (121)     1581 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.533532 Scrapy-2.7.1/docs/_ext/
--rw-r--r--   0 runner    (1001) docker     (121)     4533 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/_ext/scrapydocs.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.533532 Scrapy-2.7.1/docs/_static/
--rw-r--r--   0 runner    (1001) docker     (121)      360 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/_static/custom.css
--rw-r--r--   0 runner    (1001) docker     (121)      667 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/_static/selectors-sample1.html
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.533532 Scrapy-2.7.1/docs/_templates/
--rw-r--r--   0 runner    (1001) docker     (121)      224 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/_templates/layout.html
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.533532 Scrapy-2.7.1/docs/_tests/
--rw-r--r--   0 runner    (1001) docker     (121)    11053 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/_tests/quotes.html
--rw-r--r--   0 runner    (1001) docker     (121)    11053 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/_tests/quotes1.html
--rw-r--r--   0 runner    (1001) docker     (121)    10945 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/conf.py
--rw-r--r--   0 runner    (1001) docker     (121)      939 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/conftest.py
--rw-r--r--   0 runner    (1001) docker     (121)    11558 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/contributing.rst
--rw-r--r--   0 runner    (1001) docker     (121)    15893 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/faq.rst
--rw-r--r--   0 runner    (1001) docker     (121)     6788 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.533532 Scrapy-2.7.1/docs/intro/
--rw-r--r--   0 runner    (1001) docker     (121)      745 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/intro/examples.rst
--rw-r--r--   0 runner    (1001) docker     (121)    10323 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/intro/install.rst
--rw-r--r--   0 runner    (1001) docker     (121)     6648 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/intro/overview.rst
--rw-r--r--   0 runner    (1001) docker     (121)    29874 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/intro/tutorial.rst
--rw-r--r--   0 runner    (1001) docker     (121)   236500 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/news.rst
--rw-r--r--   0 runner    (1001) docker     (121)       88 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.537532 Scrapy-2.7.1/docs/topics/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.537532 Scrapy-2.7.1/docs/topics/_images/
--rw-r--r--   0 runner    (1001) docker     (121)    53922 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/_images/inspector_01.png
--rw-r--r--   0 runner    (1001) docker     (121)    10720 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/_images/network_01.png
--rw-r--r--   0 runner    (1001) docker     (121)    82702 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/_images/network_02.png
--rw-r--r--   0 runner    (1001) docker     (121)    45506 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/_images/network_03.png
--rw-r--r--   0 runner    (1001) docker     (121)    19653 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/_images/scrapy_architecture.odg
--rw-r--r--   0 runner    (1001) docker     (121)    92558 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/_images/scrapy_architecture.png
--rw-r--r--   0 runner    (1001) docker     (121)    53978 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/_images/scrapy_architecture_02.png
--rw-r--r--   0 runner    (1001) docker     (121)     8302 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/api.rst
--rw-r--r--   0 runner    (1001) docker     (121)     6114 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/architecture.rst
--rw-r--r--   0 runner    (1001) docker     (121)     4265 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/asyncio.rst
--rw-r--r--   0 runner    (1001) docker     (121)     5722 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/autothrottle.rst
--rw-r--r--   0 runner    (1001) docker     (121)     5257 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/benchmarking.rst
--rw-r--r--   0 runner    (1001) docker     (121)     8228 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/broad-crawls.rst
--rw-r--r--   0 runner    (1001) docker     (121)    18057 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/commands.rst
--rw-r--r--   0 runner    (1001) docker     (121)     2722 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/components.rst
--rw-r--r--   0 runner    (1001) docker     (121)     4856 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/contracts.rst
--rw-r--r--   0 runner    (1001) docker     (121)     8413 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/coroutines.rst
--rw-r--r--   0 runner    (1001) docker     (121)     4797 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/debug.rst
--rw-r--r--   0 runner    (1001) docker     (121)     2263 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/deploy.rst
--rw-r--r--   0 runner    (1001) docker     (121)    13738 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/developer-tools.rst
--rw-r--r--   0 runner    (1001) docker     (121)      194 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/djangoitem.rst
--rw-r--r--   0 runner    (1001) docker     (121)    39774 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/downloader-middleware.rst
--rw-r--r--   0 runner    (1001) docker     (121)    11937 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/dynamic-content.rst
--rw-r--r--   0 runner    (1001) docker     (121)     4973 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/email.rst
--rw-r--r--   0 runner    (1001) docker     (121)     3230 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/exceptions.rst
--rw-r--r--   0 runner    (1001) docker     (121)    15472 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/exporters.rst
--rw-r--r--   0 runner    (1001) docker     (121)    12253 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/extensions.rst
--rw-r--r--   0 runner    (1001) docker     (121)    22767 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/feed-exports.rst
--rw-r--r--   0 runner    (1001) docker     (121)     8510 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/item-pipeline.rst
--rw-r--r--   0 runner    (1001) docker     (121)    11212 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/items.rst
--rw-r--r--   0 runner    (1001) docker     (121)     2881 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/jobs.rst
--rw-r--r--   0 runner    (1001) docker     (121)    11579 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/leaks.rst
--rw-r--r--   0 runner    (1001) docker     (121)     6803 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/link-extractors.rst
--rw-r--r--   0 runner    (1001) docker     (121)    16589 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/loaders.rst
--rw-r--r--   0 runner    (1001) docker     (121)    10783 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/logging.rst
--rw-r--r--   0 runner    (1001) docker     (121)    25911 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/media-pipeline.rst
--rw-r--r--   0 runner    (1001) docker     (121)    10367 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/practices.rst
--rw-r--r--   0 runner    (1001) docker     (121)    47790 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/request-response.rst
--rw-r--r--   0 runner    (1001) docker     (121)      761 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/scheduler.rst
--rw-r--r--   0 runner    (1001) docker     (121)      188 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/scrapyd.rst
--rw-r--r--   0 runner    (1001) docker     (121)    34983 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/selectors.rst
--rw-r--r--   0 runner    (1001) docker     (121)    49064 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/settings.rst
--rw-r--r--   0 runner    (1001) docker     (121)    10884 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/shell.rst
--rw-r--r--   0 runner    (1001) docker     (121)    16192 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/signals.rst
--rw-r--r--   0 runner    (1001) docker     (121)    18936 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/spider-middleware.rst
--rw-r--r--   0 runner    (1001) docker     (121)    32293 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/spiders.rst
--rw-r--r--   0 runner    (1001) docker     (121)     3537 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/stats.rst
--rw-r--r--   0 runner    (1001) docker     (121)     7477 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/topics/telnetconsole.rst
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.537532 Scrapy-2.7.1/docs/utils/
--rwxr-xr-x   0 runner    (1001) docker     (121)     1969 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/utils/linkfix.py
--rw-r--r--   0 runner    (1001) docker     (121)     2161 2022-11-02 11:18:03.000000 Scrapy-2.7.1/docs/versioning.rst
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.537532 Scrapy-2.7.1/extras/
--rwxr-xr-x   0 runner    (1001) docker     (121)      259 2022-11-02 11:18:03.000000 Scrapy-2.7.1/extras/coverage-report.sh
--rwxr-xr-x   0 runner    (1001) docker     (121)     1587 2022-11-02 11:18:03.000000 Scrapy-2.7.1/extras/qps-bench-server.py
--rw-r--r--   0 runner    (1001) docker     (121)     1503 2022-11-02 11:18:03.000000 Scrapy-2.7.1/extras/qpsclient.py
--rw-r--r--   0 runner    (1001) docker     (121)     2010 2022-11-02 11:18:03.000000 Scrapy-2.7.1/extras/scrapy.1
--rw-r--r--   0 runner    (1001) docker     (121)      698 2022-11-02 11:18:03.000000 Scrapy-2.7.1/extras/scrapy_bash_completion
--rw-r--r--   0 runner    (1001) docker     (121)     7008 2022-11-02 11:18:03.000000 Scrapy-2.7.1/extras/scrapy_zsh_completion
--rw-r--r--   0 runner    (1001) docker     (121)      938 2022-11-02 11:18:03.000000 Scrapy-2.7.1/pytest.ini
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.541532 Scrapy-2.7.1/scrapy/
--rw-r--r--   0 runner    (1001) docker     (121)        6 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/VERSION
--rw-r--r--   0 runner    (1001) docker     (121)     1044 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)       77 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/__main__.py
--rw-r--r--   0 runner    (1001) docker     (121)     5652 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/cmdline.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.541532 Scrapy-2.7.1/scrapy/commands/
--rw-r--r--   0 runner    (1001) docker     (121)     6634 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1614 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/bench.py
--rw-r--r--   0 runner    (1001) docker     (121)     3454 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/check.py
--rw-r--r--   0 runner    (1001) docker     (121)     1011 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/crawl.py
--rw-r--r--   0 runner    (1001) docker     (121)     1043 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/edit.py
--rw-r--r--   0 runner    (1001) docker     (121)     2690 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/fetch.py
--rw-r--r--   0 runner    (1001) docker     (121)     5993 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/genspider.py
--rw-r--r--   0 runner    (1001) docker     (121)      337 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/list.py
--rw-r--r--   0 runner    (1001) docker     (121)    10567 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/parse.py
--rw-r--r--   0 runner    (1001) docker     (121)     1771 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/runspider.py
--rw-r--r--   0 runner    (1001) docker     (121)     1803 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/settings.py
--rw-r--r--   0 runner    (1001) docker     (121)     2806 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/shell.py
--rw-r--r--   0 runner    (1001) docker     (121)     4040 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/startproject.py
--rw-r--r--   0 runner    (1001) docker     (121)      959 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/version.py
--rw-r--r--   0 runner    (1001) docker     (121)      552 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/commands/view.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.541532 Scrapy-2.7.1/scrapy/contracts/
--rw-r--r--   0 runner    (1001) docker     (121)     6122 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/contracts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2966 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/contracts/default.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.545532 Scrapy-2.7.1/scrapy/core/
--rw-r--r--   0 runner    (1001) docker     (121)       51 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.545532 Scrapy-2.7.1/scrapy/core/downloader/
--rw-r--r--   0 runner    (1001) docker     (121)     7390 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     5957 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/contextfactory.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.545532 Scrapy-2.7.1/scrapy/core/downloader/handlers/
--rw-r--r--   0 runner    (1001) docker     (121)     2881 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/handlers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      693 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/handlers/datauri.py
--rw-r--r--   0 runner    (1001) docker     (121)      480 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/handlers/file.py
--rw-r--r--   0 runner    (1001) docker     (121)     4603 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/handlers/ftp.py
--rw-r--r--   0 runner    (1001) docker     (121)      178 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/handlers/http.py
--rw-r--r--   0 runner    (1001) docker     (121)     1346 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/handlers/http10.py
--rw-r--r--   0 runner    (1001) docker     (121)    24193 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/handlers/http11.py
--rw-r--r--   0 runner    (1001) docker     (121)     4975 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/handlers/http2.py
--rw-r--r--   0 runner    (1001) docker     (121)     2949 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/handlers/s3.py
--rw-r--r--   0 runner    (1001) docker     (121)     3777 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/middleware.py
--rw-r--r--   0 runner    (1001) docker     (121)     3058 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/tls.py
--rw-r--r--   0 runner    (1001) docker     (121)     7966 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/downloader/webclient.py
--rw-r--r--   0 runner    (1001) docker     (121)    17444 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.545532 Scrapy-2.7.1/scrapy/core/http2/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/http2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     5877 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/http2/agent.py
--rw-r--r--   0 runner    (1001) docker     (121)    16522 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/http2/protocol.py
--rw-r--r--   0 runner    (1001) docker     (121)    18681 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/http2/stream.py
--rw-r--r--   0 runner    (1001) docker     (121)    13911 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/scheduler.py
--rw-r--r--   0 runner    (1001) docker     (121)    12826 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/scraper.py
--rw-r--r--   0 runner    (1001) docker     (121)    13967 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/core/spidermw.py
--rw-r--r--   0 runner    (1001) docker     (121)    14389 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/crawler.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.545532 Scrapy-2.7.1/scrapy/downloadermiddlewares/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3280 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/ajaxcrawl.py
--rw-r--r--   0 runner    (1001) docker     (121)     5095 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/cookies.py
--rw-r--r--   0 runner    (1001) docker     (121)     2730 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/decompression.py
--rw-r--r--   0 runner    (1001) docker     (121)      560 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/defaultheaders.py
--rw-r--r--   0 runner    (1001) docker     (121)      701 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/downloadtimeout.py
--rw-r--r--   0 runner    (1001) docker     (121)     1990 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/httpauth.py
--rw-r--r--   0 runner    (1001) docker     (121)     5479 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/httpcache.py
--rw-r--r--   0 runner    (1001) docker     (121)     3871 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/httpcompression.py
--rw-r--r--   0 runner    (1001) docker     (121)     3171 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/httpproxy.py
--rw-r--r--   0 runner    (1001) docker     (121)     5173 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/redirect.py
--rw-r--r--   0 runner    (1001) docker     (121)     6602 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/retry.py
--rw-r--r--   0 runner    (1001) docker     (121)     4168 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/robotstxt.py
--rw-r--r--   0 runner    (1001) docker     (121)     2045 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/stats.py
--rw-r--r--   0 runner    (1001) docker     (121)      741 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/downloadermiddlewares/useragent.py
--rw-r--r--   0 runner    (1001) docker     (121)     4392 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/dupefilters.py
--rw-r--r--   0 runner    (1001) docker     (121)     1977 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (121)    12741 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/exporters.py
--rw-r--r--   0 runner    (1001) docker     (121)      397 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extension.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/extensions/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2626 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/closespider.py
--rw-r--r--   0 runner    (1001) docker     (121)     1813 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/corestats.py
--rw-r--r--   0 runner    (1001) docker     (121)     1859 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/debug.py
--rw-r--r--   0 runner    (1001) docker     (121)    21436 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/feedexport.py
--rw-r--r--   0 runner    (1001) docker     (121)    14535 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/httpcache.py
--rw-r--r--   0 runner    (1001) docker     (121)     1746 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/logstats.py
--rw-r--r--   0 runner    (1001) docker     (121)      917 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/memdebug.py
--rw-r--r--   0 runner    (1001) docker     (121)     4930 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/memusage.py
--rw-r--r--   0 runner    (1001) docker     (121)     4869 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/postprocessing.py
--rw-r--r--   0 runner    (1001) docker     (121)     1136 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/spiderstate.py
--rw-r--r--   0 runner    (1001) docker     (121)     1229 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/statsmailer.py
--rw-r--r--   0 runner    (1001) docker     (121)     4067 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/telnet.py
--rw-r--r--   0 runner    (1001) docker     (121)     3578 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/extensions/throttle.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/http/
--rw-r--r--   0 runner    (1001) docker     (121)      602 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      239 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/common.py
--rw-r--r--   0 runner    (1001) docker     (121)     5720 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/cookies.py
--rw-r--r--   0 runner    (1001) docker     (121)     2847 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/headers.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/http/request/
--rw-r--r--   0 runner    (1001) docker     (121)     8252 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/request/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     8690 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/request/form.py
--rw-r--r--   0 runner    (1001) docker     (121)     2094 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/request/json_request.py
--rw-r--r--   0 runner    (1001) docker     (121)     1087 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/request/rpc.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/http/response/
--rw-r--r--   0 runner    (1001) docker     (121)     7074 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/response/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      300 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/response/html.py
--rw-r--r--   0 runner    (1001) docker     (121)     9519 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/response/text.py
--rw-r--r--   0 runner    (1001) docker     (121)      297 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/http/response/xml.py
--rw-r--r--   0 runner    (1001) docker     (121)      558 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/interfaces.py
--rw-r--r--   0 runner    (1001) docker     (121)     3832 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/item.py
--rw-r--r--   0 runner    (1001) docker     (121)     1867 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/link.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/linkextractors/
--rw-r--r--   0 runner    (1001) docker     (121)     4880 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/linkextractors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     5472 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/linkextractors/lxmlhtml.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/loader/
--rw-r--r--   0 runner    (1001) docker     (121)     3491 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/loader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      657 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/loader/common.py
--rw-r--r--   0 runner    (1001) docker     (121)      633 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/loader/processors.py
--rw-r--r--   0 runner    (1001) docker     (121)     5330 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/logformatter.py
--rw-r--r--   0 runner    (1001) docker     (121)     5156 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/mail.py
--rw-r--r--   0 runner    (1001) docker     (121)     3155 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/middleware.py
--rw-r--r--   0 runner    (1001) docker     (121)    20392 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/mime.types
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/pipelines/
--rw-r--r--   0 runner    (1001) docker     (121)      793 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/pipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    20050 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/pipelines/files.py
--rw-r--r--   0 runner    (1001) docker     (121)     7237 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/pipelines/images.py
--rw-r--r--   0 runner    (1001) docker     (121)     9989 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/pipelines/media.py
--rw-r--r--   0 runner    (1001) docker     (121)     7735 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/pqueues.py
--rw-r--r--   0 runner    (1001) docker     (121)     4088 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/resolver.py
--rw-r--r--   0 runner    (1001) docker     (121)     4886 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/responsetypes.py
--rw-r--r--   0 runner    (1001) docker     (121)     4202 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/robotstxt.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/selector/
--rw-r--r--   0 runner    (1001) docker     (121)       98 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/selector/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2659 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/selector/unified.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/settings/
--rw-r--r--   0 runner    (1001) docker     (121)    16983 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/settings/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     9359 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/settings/default_settings.py
--rw-r--r--   0 runner    (1001) docker     (121)     6933 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/shell.py
--rw-r--r--   0 runner    (1001) docker     (121)     2391 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/signalmanager.py
--rw-r--r--   0 runner    (1001) docker     (121)      815 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/signals.py
--rw-r--r--   0 runner    (1001) docker     (121)     2929 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spiderloader.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/spidermiddlewares/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spidermiddlewares/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2221 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spidermiddlewares/depth.py
--rw-r--r--   0 runner    (1001) docker     (121)     1905 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spidermiddlewares/httperror.py
--rw-r--r--   0 runner    (1001) docker     (121)     3134 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spidermiddlewares/offsite.py
--rw-r--r--   0 runner    (1001) docker     (121)    13843 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spidermiddlewares/referer.py
--rw-r--r--   0 runner    (1001) docker     (121)     1309 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spidermiddlewares/urllength.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/spiders/
--rw-r--r--   0 runner    (1001) docker     (121)     2979 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spiders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4660 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spiders/crawl.py
--rw-r--r--   0 runner    (1001) docker     (121)     5469 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spiders/feed.py
--rw-r--r--   0 runner    (1001) docker     (121)     1337 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spiders/init.py
--rw-r--r--   0 runner    (1001) docker     (121)     3500 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/spiders/sitemap.py
--rw-r--r--   0 runner    (1001) docker     (121)     5925 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/squeues.py
--rw-r--r--   0 runner    (1001) docker     (121)     2093 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/statscollectors.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.529532 Scrapy-2.7.1/scrapy/templates/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.549532 Scrapy-2.7.1/scrapy/templates/project/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.553532 Scrapy-2.7.1/scrapy/templates/project/module/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/project/module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      270 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/project/module/items.py.tmpl
--rw-r--r--   0 runner    (1001) docker     (121)     3664 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/project/module/middlewares.py.tmpl
--rw-r--r--   0 runner    (1001) docker     (121)      368 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/project/module/pipelines.py.tmpl
--rw-r--r--   0 runner    (1001) docker     (121)     3331 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/project/module/settings.py.tmpl
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.553532 Scrapy-2.7.1/scrapy/templates/project/module/spiders/
--rw-r--r--   0 runner    (1001) docker     (121)      161 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/project/module/spiders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      273 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/project/scrapy.cfg
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.553532 Scrapy-2.7.1/scrapy/templates/spiders/
--rw-r--r--   0 runner    (1001) docker     (121)      184 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/spiders/basic.tmpl
--rw-r--r--   0 runner    (1001) docker     (121)      633 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/spiders/crawl.tmpl
--rw-r--r--   0 runner    (1001) docker     (121)      547 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/spiders/csvfeed.tmpl
--rw-r--r--   0 runner    (1001) docker     (121)      541 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/templates/spiders/xmlfeed.tmpl
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.557532 Scrapy-2.7.1/scrapy/utils/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      509 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/asyncgen.py
--rw-r--r--   0 runner    (1001) docker     (121)     1295 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/benchserver.py
--rw-r--r--   0 runner    (1001) docker     (121)     1040 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/boto.py
--rw-r--r--   0 runner    (1001) docker     (121)     7973 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/conf.py
--rw-r--r--   0 runner    (1001) docker     (121)     3432 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/console.py
--rw-r--r--   0 runner    (1001) docker     (121)     3316 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/curl.py
--rw-r--r--   0 runner    (1001) docker     (121)     3349 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/datatypes.py
--rw-r--r--   0 runner    (1001) docker     (121)     1270 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/decorators.py
--rw-r--r--   0 runner    (1001) docker     (121)    13903 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/defer.py
--rw-r--r--   0 runner    (1001) docker     (121)     6378 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/deprecate.py
--rw-r--r--   0 runner    (1001) docker     (121)     1319 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/display.py
--rw-r--r--   0 runner    (1001) docker     (121)     1302 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/engine.py
--rw-r--r--   0 runner    (1001) docker     (121)     1233 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/ftp.py
--rw-r--r--   0 runner    (1001) docker     (121)     1353 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/gz.py
--rw-r--r--   0 runner    (1001) docker     (121)      705 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/httpobj.py
--rw-r--r--   0 runner    (1001) docker     (121)     5389 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/iterators.py
--rw-r--r--   0 runner    (1001) docker     (121)      250 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/job.py
--rw-r--r--   0 runner    (1001) docker     (121)     7120 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/log.py
--rw-r--r--   0 runner    (1001) docker     (121)     9311 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (121)      943 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/ossignal.py
--rw-r--r--   0 runner    (1001) docker     (121)     2853 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/project.py
--rw-r--r--   0 runner    (1001) docker     (121)    10720 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/python.py
--rw-r--r--   0 runner    (1001) docker     (121)     4341 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/reactor.py
--rw-r--r--   0 runner    (1001) docker     (121)      680 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/reqser.py
--rw-r--r--   0 runner    (1001) docker     (121)    13704 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/request.py
--rw-r--r--   0 runner    (1001) docker     (121)     3503 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/response.py
--rw-r--r--   0 runner    (1001) docker     (121)     1171 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/serialize.py
--rw-r--r--   0 runner    (1001) docker     (121)     3107 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/signal.py
--rw-r--r--   0 runner    (1001) docker     (121)     1501 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/sitemap.py
--rw-r--r--   0 runner    (1001) docker     (121)     2195 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/spider.py
--rw-r--r--   0 runner    (1001) docker     (121)     2422 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/ssl.py
--rw-r--r--   0 runner    (1001) docker     (121)      833 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/template.py
--rw-r--r--   0 runner    (1001) docker     (121)     3708 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/test.py
--rw-r--r--   0 runner    (1001) docker     (121)     1487 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/testproc.py
--rw-r--r--   0 runner    (1001) docker     (121)     1542 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/testsite.py
--rw-r--r--   0 runner    (1001) docker     (121)     2019 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/trackref.py
--rw-r--r--   0 runner    (1001) docker     (121)     5558 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/url.py
--rw-r--r--   0 runner    (1001) docker     (121)      850 2022-11-02 11:18:03.000000 Scrapy-2.7.1/scrapy/utils/versions.py
--rw-r--r--   0 runner    (1001) docker     (121)     1575 2022-11-02 11:18:23.577532 Scrapy-2.7.1/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (121)     3232 2022-11-02 11:18:03.000000 Scrapy-2.7.1/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.565532 Scrapy-2.7.1/tests/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.569532 Scrapy-2.7.1/tests/CrawlerProcess/
--rw-r--r--   0 runner    (1001) docker     (121)      380 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/asyncio_custom_loop.py
--rw-r--r--   0 runner    (1001) docker     (121)     1147 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/asyncio_deferred_signal.py
--rw-r--r--   0 runner    (1001) docker     (121)      340 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/asyncio_enabled_no_reactor.py
--rw-r--r--   0 runner    (1001) docker     (121)      624 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/asyncio_enabled_reactor.py
--rw-r--r--   0 runner    (1001) docker     (121)      665 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/asyncio_enabled_reactor_different_loop.py
--rw-r--r--   0 runner    (1001) docker     (121)      721 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/asyncio_enabled_reactor_same_loop.py
--rw-r--r--   0 runner    (1001) docker     (121)      856 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/caching_hostname_resolver.py
--rw-r--r--   0 runner    (1001) docker     (121)      521 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/caching_hostname_resolver_ipv6.py
--rw-r--r--   0 runner    (1001) docker     (121)      423 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/default_name_resolver.py
--rw-r--r--   0 runner    (1001) docker     (121)      291 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/multi.py
--rw-r--r--   0 runner    (1001) docker     (121)      310 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/reactor_default.py
--rw-r--r--   0 runner    (1001) docker     (121)      382 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/reactor_default_twisted_reactor_select.py
--rw-r--r--   0 runner    (1001) docker     (121)      326 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/reactor_select.py
--rw-r--r--   0 runner    (1001) docker     (121)      552 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/reactor_select_subclass_twisted_reactor_select.py
--rw-r--r--   0 runner    (1001) docker     (121)      398 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/reactor_select_twisted_reactor_select.py
--rw-r--r--   0 runner    (1001) docker     (121)      259 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/simple.py
--rw-r--r--   0 runner    (1001) docker     (121)      304 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/twisted_reactor_asyncio.py
--rw-r--r--   0 runner    (1001) docker     (121)      326 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/twisted_reactor_custom_settings.py
--rw-r--r--   0 runner    (1001) docker     (121)      538 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/twisted_reactor_custom_settings_conflict.py
--rw-r--r--   0 runner    (1001) docker     (121)      557 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/twisted_reactor_custom_settings_same.py
--rw-r--r--   0 runner    (1001) docker     (121)      281 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/twisted_reactor_poll.py
--rw-r--r--   0 runner    (1001) docker     (121)      290 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerProcess/twisted_reactor_select.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.569532 Scrapy-2.7.1/tests/CrawlerRunner/
--rw-r--r--   0 runner    (1001) docker     (121)     1740 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/CrawlerRunner/ip_address.py
--rw-r--r--   0 runner    (1001) docker     (121)     1433 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      632 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/ftpserver.py
--rw-r--r--   0 runner    (1001) docker     (121)      103 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/ignores.txt
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.569532 Scrapy-2.7.1/tests/keys/
--rw-r--r--   0 runner    (1001) docker     (121)     1921 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1562 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/example-com.cert.pem
--rw-r--r--   0 runner    (1001) docker     (121)     3126 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/example-com.conf
--rw-r--r--   0 runner    (1001) docker     (121)      933 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/example-com.gen.README
--rw-r--r--   0 runner    (1001) docker     (121)     1704 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/example-com.key.pem
--rw-r--r--   0 runner    (1001) docker     (121)      945 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/localhost-ip.gen.README
--rw-r--r--   0 runner    (1001) docker     (121)      945 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/localhost.gen.README
--rw-r--r--   0 runner    (1001) docker     (121)     1176 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/localhost.ip.crt
--rw-r--r--   0 runner    (1001) docker     (121)     1704 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/localhost.ip.key
--rw-r--r--   0 runner    (1001) docker     (121)     3022 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/keys/mitmproxy-ca.pem
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.569532 Scrapy-2.7.1/tests/mocks/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/mocks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      442 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/mocks/dummydbm.py
--rw-r--r--   0 runner    (1001) docker     (121)    12195 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/mockserver.py
--rw-r--r--   0 runner    (1001) docker     (121)      291 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/pipelines.py
--rw-r--r--   0 runner    (1001) docker     (121)      457 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.529532 Scrapy-2.7.1/tests/sample_data/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.569532 Scrapy-2.7.1/tests/sample_data/compressed/
--rw-r--r--   0 runner    (1001) docker     (121)    20480 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.tar
--rw-r--r--   0 runner    (1001) docker     (121)     9950 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.xml
--rw-r--r--   0 runner    (1001) docker     (121)     1430 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.xml.bz2
--rw-r--r--   0 runner    (1001) docker     (121)     1131 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.xml.gz
--rw-r--r--   0 runner    (1001) docker     (121)     1260 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.zip
--rw-r--r--   0 runner    (1001) docker     (121)     4027 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/html-br.bin
--rw-r--r--   0 runner    (1001) docker     (121)     8037 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/html-gzip.bin
--rw-r--r--   0 runner    (1001) docker     (121)     8021 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/html-rawdeflate.bin
--rw-r--r--   0 runner    (1001) docker     (121)     8027 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/html-zlibdeflate.bin
--rw-r--r--   0 runner    (1001) docker     (121)     8066 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/html-zstd-static-content-size.bin
--rw-r--r--   0 runner    (1001) docker     (121)     8063 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/html-zstd-static-no-content-size.bin
--rw-r--r--   0 runner    (1001) docker     (121)     8047 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/html-zstd-streaming-no-content-size.bin
--rw-r--r--   0 runner    (1001) docker     (121)     1930 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/truncated-crc-error-short.gz
--rw-r--r--   0 runner    (1001) docker     (121)     5766 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/truncated-crc-error.gz
--rw-r--r--   0 runner    (1001) docker     (121)    16782 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/unexpected-eof-output.txt
--rw-r--r--   0 runner    (1001) docker     (121)     5134 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/compressed/unexpected-eof.gz
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/sample_data/feeds/
--rw-r--r--   0 runner    (1001) docker     (121)     9950 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/feeds/feed-sample1.xml
--rw-r--r--   0 runner    (1001) docker     (121)     6388 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/feeds/feed-sample2.xml
--rw-r--r--   0 runner    (1001) docker     (121)       81 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/feeds/feed-sample3.csv
--rw-r--r--   0 runner    (1001) docker     (121)       45 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/feeds/feed-sample4.csv
--rw-r--r--   0 runner    (1001) docker     (121)       47 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/feeds/feed-sample5.csv
--rw-r--r--   0 runner    (1001) docker     (121)      101 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/feeds/feed-sample6.csv
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/sample_data/link_extractor/
--rw-r--r--   0 runner    (1001) docker     (121)      777 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/link_extractor/linkextractor.html
--rw-r--r--   0 runner    (1001) docker     (121)      585 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/link_extractor/linkextractor_latin1.html
--rw-r--r--   0 runner    (1001) docker     (121)      740 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/link_extractor/linkextractor_no_href.html
--rw-r--r--   0 runner    (1001) docker     (121)      390 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/link_extractor/linkextractor_noenc.html
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/sample_data/test_site/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.529532 Scrapy-2.7.1/tests/sample_data/test_site/files/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/sample_data/test_site/files/images/
--rw-r--r--   0 runner    (1001) docker     (121)    11155 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/test_site/files/images/python-logo-master-v3-TM-flattened.png
--rw-r--r--   0 runner    (1001) docker     (121)     3243 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/test_site/files/images/python-powered-h-50x65.png
--rw-r--r--   0 runner    (1001) docker     (121)     2710 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/test_site/files/images/scrapy.png
--rw-r--r--   0 runner    (1001) docker     (121)      311 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/test_site/index.html
--rw-r--r--   0 runner    (1001) docker     (121)      225 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/test_site/item1.html
--rw-r--r--   0 runner    (1001) docker     (121)      209 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/sample_data/test_site/item2.html
--rw-r--r--   0 runner    (1001) docker     (121)    14604 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/spiders.py
--rw-r--r--   0 runner    (1001) docker     (121)     2351 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_closespider.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_cmdline/
--rw-r--r--   0 runner    (1001) docker     (121)     2698 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      310 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline/extensions.py
--rw-r--r--   0 runner    (1001) docker     (121)      223 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline/settings.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/
--rw-r--r--   0 runner    (1001) docker     (121)      629 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)       42 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/scrapy.cfg
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/test_spider/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/test_spider/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      311 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/test_spider/pipelines.py
--rw-r--r--   0 runner    (1001) docker     (121)       66 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/test_spider/settings.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      260 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/exception.py
--rw-r--r--   0 runner    (1001) docker     (121)      245 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/normal.py
--rw-r--r--   0 runner    (1001) docker     (121)     2810 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_command_check.py
--rw-r--r--   0 runner    (1001) docker     (121)     1221 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_command_fetch.py
--rw-r--r--   0 runner    (1001) docker     (121)    10750 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_command_parse.py
--rw-r--r--   0 runner    (1001) docker     (121)     4500 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_command_shell.py
--rw-r--r--   0 runner    (1001) docker     (121)     1077 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_command_version.py
--rw-r--r--   0 runner    (1001) docker     (121)    35207 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_commands.py
--rw-r--r--   0 runner    (1001) docker     (121)    13010 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_contracts.py
--rw-r--r--   0 runner    (1001) docker     (121)      301 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_core_downloader.py
--rw-r--r--   0 runner    (1001) docker     (121)    26142 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_crawl.py
--rw-r--r--   0 runner    (1001) docker     (121)    21078 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_crawler.py
--rw-r--r--   0 runner    (1001) docker     (121)     1579 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_dependencies.py
--rw-r--r--   0 runner    (1001) docker     (121)    48107 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloader_handlers.py
--rw-r--r--   0 runner    (1001) docker     (121)     9964 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloader_handlers_http2.py
--rw-r--r--   0 runner    (1001) docker     (121)     9153 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware.py
--rw-r--r--   0 runner    (1001) docker     (121)     2497 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_ajaxcrawlable.py
--rw-r--r--   0 runner    (1001) docker     (121)    27540 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_cookies.py
--rw-r--r--   0 runner    (1001) docker     (121)     1857 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_decompression.py
--rw-r--r--   0 runner    (1001) docker     (121)     1398 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_defaultheaders.py
--rw-r--r--   0 runner    (1001) docker     (121)     1702 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_downloadtimeout.py
--rw-r--r--   0 runner    (1001) docker     (121)     4159 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_httpauth.py
--rw-r--r--   0 runner    (1001) docker     (121)    24952 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_httpcache.py
--rw-r--r--   0 runner    (1001) docker     (121)    16355 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_httpcompression.py
--rw-r--r--   0 runner    (1001) docker     (121)    19799 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_httpproxy.py
--rw-r--r--   0 runner    (1001) docker     (121)    14720 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_redirect.py
--rw-r--r--   0 runner    (1001) docker     (121)    21761 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_retry.py
--rw-r--r--   0 runner    (1001) docker     (121)     8679 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_robotstxt.py
--rw-r--r--   0 runner    (1001) docker     (121)     2695 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_stats.py
--rw-r--r--   0 runner    (1001) docker     (121)     2228 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_downloadermiddleware_useragent.py
--rw-r--r--   0 runner    (1001) docker     (121)     9361 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_dupefilters.py
--rw-r--r--   0 runner    (1001) docker     (121)    19396 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_engine.py
--rw-r--r--   0 runner    (1001) docker     (121)     2752 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_engine_stop_download_bytes.py
--rw-r--r--   0 runner    (1001) docker     (121)     2471 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_engine_stop_download_headers.py
--rw-r--r--   0 runner    (1001) docker     (121)    21634 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_exporters.py
--rw-r--r--   0 runner    (1001) docker     (121)     1845 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_extension_telnet.py
--rw-r--r--   0 runner    (1001) docker     (121)    97228 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_feedexport.py
--rw-r--r--   0 runner    (1001) docker     (121)    23814 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_http2_client_protocol.py
--rw-r--r--   0 runner    (1001) docker     (121)     2593 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_http_cookies.py
--rw-r--r--   0 runner    (1001) docker     (121)     6379 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_http_headers.py
--rw-r--r--   0 runner    (1001) docker     (121)    63066 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_http_request.py
--rw-r--r--   0 runner    (1001) docker     (121)    37464 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_http_response.py
--rw-r--r--   0 runner    (1001) docker     (121)     8870 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_item.py
--rw-r--r--   0 runner    (1001) docker     (121)     1791 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_link.py
--rw-r--r--   0 runner    (1001) docker     (121)    27068 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_linkextractors.py
--rw-r--r--   0 runner    (1001) docker     (121)    20706 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_loader.py
--rw-r--r--   0 runner    (1001) docker     (121)    26431 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_loader_deprecated.py
--rw-r--r--   0 runner    (1001) docker     (121)     8662 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_logformatter.py
--rw-r--r--   0 runner    (1001) docker     (121)     4782 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_mail.py
--rw-r--r--   0 runner    (1001) docker     (121)     2310 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_middleware.py
--rw-r--r--   0 runner    (1001) docker     (121)     7504 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_pipeline_crawl.py
--rw-r--r--   0 runner    (1001) docker     (121)    23146 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_pipeline_files.py
--rw-r--r--   0 runner    (1001) docker     (121)    17728 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_pipeline_images.py
--rw-r--r--   0 runner    (1001) docker     (121)    22522 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_pipeline_media.py
--rw-r--r--   0 runner    (1001) docker     (121)     3891 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_pipelines.py
--rw-r--r--   0 runner    (1001) docker     (121)     6043 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_pqueues.py
--rw-r--r--   0 runner    (1001) docker     (121)     3923 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_proxy_connect.py
--rw-r--r--   0 runner    (1001) docker     (121)     7251 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_request_attribute_binding.py
--rw-r--r--   0 runner    (1001) docker     (121)     7190 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_request_cb_kwargs.py
--rw-r--r--   0 runner    (1001) docker     (121)     8054 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_request_dict.py
--rw-r--r--   0 runner    (1001) docker     (121)     1993 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_request_left.py
--rw-r--r--   0 runner    (1001) docker     (121)     4434 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_responsetypes.py
--rw-r--r--   0 runner    (1001) docker     (121)     6949 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_robotstxt_interface.py
--rw-r--r--   0 runner    (1001) docker     (121)    10859 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_scheduler.py
--rw-r--r--   0 runner    (1001) docker     (121)     5418 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_scheduler_base.py
--rw-r--r--   0 runner    (1001) docker     (121)     3815 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_selector.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_settings/
--rw-r--r--   0 runner    (1001) docker     (121)    17949 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_settings/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)       55 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_settings/default_settings.py
--rw-r--r--   0 runner    (1001) docker     (121)     1336 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_signals.py
--rw-r--r--   0 runner    (1001) docker     (121)    24662 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spider.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_spiderloader/
--rw-r--r--   0 runner    (1001) docker     (121)     7774 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spiderloader/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_spiderloader/test_spiders/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spiderloader/test_spiders/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_spiderloader/test_spiders/nested/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spiderloader/test_spiders/nested/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      235 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spiderloader/test_spiders/nested/spider4.py
--rw-r--r--   0 runner    (1001) docker     (121)      112 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spiderloader/test_spiders/spider0.py
--rw-r--r--   0 runner    (1001) docker     (121)      133 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spiderloader/test_spiders/spider1.py
--rw-r--r--   0 runner    (1001) docker     (121)      133 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spiderloader/test_spiders/spider2.py
--rw-r--r--   0 runner    (1001) docker     (121)      235 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spiderloader/test_spiders/spider3.py
--rw-r--r--   0 runner    (1001) docker     (121)    20312 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spidermiddleware.py
--rw-r--r--   0 runner    (1001) docker     (121)     1343 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spidermiddleware_depth.py
--rw-r--r--   0 runner    (1001) docker     (121)     9047 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spidermiddleware_httperror.py
--rw-r--r--   0 runner    (1001) docker     (121)     3562 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spidermiddleware_offsite.py
--rw-r--r--   0 runner    (1001) docker     (121)    18627 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spidermiddleware_output_chain.py
--rw-r--r--   0 runner    (1001) docker     (121)    39335 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spidermiddleware_referer.py
--rw-r--r--   0 runner    (1001) docker     (121)     1452 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spidermiddleware_urllength.py
--rw-r--r--   0 runner    (1001) docker     (121)     1418 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_spiderstate.py
--rw-r--r--   0 runner    (1001) docker     (121)     5233 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_squeues.py
--rw-r--r--   0 runner    (1001) docker     (121)     7452 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_squeues_request.py
--rw-r--r--   0 runner    (1001) docker     (121)     3997 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_stats.py
--rw-r--r--   0 runner    (1001) docker     (121)      866 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_toplevel.py
--rw-r--r--   0 runner    (1001) docker     (121)      363 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_urlparse_monkeypatches.py
--rw-r--r--   0 runner    (1001) docker     (121)      663 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_asyncgen.py
--rw-r--r--   0 runner    (1001) docker     (121)      659 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_asyncio.py
--rw-r--r--   0 runner    (1001) docker     (121)     8329 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_conf.py
--rw-r--r--   0 runner    (1001) docker     (121)     1174 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_console.py
--rw-r--r--   0 runner    (1001) docker     (121)     9955 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_curl.py
--rw-r--r--   0 runner    (1001) docker     (121)     9399 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_datatypes.py
--rw-r--r--   0 runner    (1001) docker     (121)     7615 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_defer.py
--rw-r--r--   0 runner    (1001) docker     (121)    10403 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_deprecate.py
--rw-r--r--   0 runner    (1001) docker     (121)     2915 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_display.py
--rw-r--r--   0 runner    (1001) docker     (121)     2040 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_gz.py
--rw-r--r--   0 runner    (1001) docker     (121)      686 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_httpobj.py
--rw-r--r--   0 runner    (1001) docker     (121)    20424 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_iterators.py
--rw-r--r--   0 runner    (1001) docker     (121)     3591 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_log.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_utils_misc/
--rw-r--r--   0 runner    (1001) docker     (121)     7200 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_misc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2231 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_misc/test.egg
--rw-r--r--   0 runner    (1001) docker     (121)     8145 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_misc/test_return_with_argument_inside_generator.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_utils_misc/test_walk_modules/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_misc/test_walk_modules/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:23.573532 Scrapy-2.7.1/tests/test_utils_misc/test_walk_modules/mod/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_misc/test_walk_modules/mod/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_misc/test_walk_modules/mod/mod0.py
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_misc/test_walk_modules/mod1.py
--rw-r--r--   0 runner    (1001) docker     (121)     2933 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_project.py
--rw-r--r--   0 runner    (1001) docker     (121)     9025 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_python.py
--rw-r--r--   0 runner    (1001) docker     (121)    25142 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_request.py
--rw-r--r--   0 runner    (1001) docker     (121)     6205 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_response.py
--rw-r--r--   0 runner    (1001) docker     (121)     2457 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_serialize.py
--rw-r--r--   0 runner    (1001) docker     (121)     3511 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_signal.py
--rw-r--r--   0 runner    (1001) docker     (121)     8170 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_sitemap.py
--rw-r--r--   0 runner    (1001) docker     (121)      964 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_spider.py
--rw-r--r--   0 runner    (1001) docker     (121)     1295 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_template.py
--rw-r--r--   0 runner    (1001) docker     (121)     2098 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_trackref.py
--rw-r--r--   0 runner    (1001) docker     (121)    20291 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_utils_url.py
--rw-r--r--   0 runner    (1001) docker     (121)    16308 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/test_webclient.py
--rw-r--r--   0 runner    (1001) docker     (121)      412 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tests/upper-constraints.txt
--rw-r--r--   0 runner    (1001) docker     (121)     4886 2022-11-02 11:18:03.000000 Scrapy-2.7.1/tox.ini
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.330201 Scrapy-2.8.0/
+-rw-r--r--   0 runner    (1001) docker     (123)     1284 2023-02-02 04:54:40.000000 Scrapy-2.8.0/AUTHORS
+-rw-r--r--   0 runner    (1001) docker     (123)     1517 2023-02-02 04:54:40.000000 Scrapy-2.8.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)      486 2023-02-02 04:54:40.000000 Scrapy-2.8.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)       18 2023-02-02 04:54:40.000000 Scrapy-2.8.0/NEWS
+-rw-r--r--   0 runner    (1001) docker     (123)     4562 2023-02-02 04:54:56.330201 Scrapy-2.8.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     3095 2023-02-02 04:54:40.000000 Scrapy-2.8.0/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.250197 Scrapy-2.8.0/Scrapy.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     4562 2023-02-02 04:54:56.000000 Scrapy-2.8.0/Scrapy.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    15437 2023-02-02 04:54:56.000000 Scrapy-2.8.0/Scrapy.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-02-02 04:54:56.000000 Scrapy-2.8.0/Scrapy.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       50 2023-02-02 04:54:56.000000 Scrapy-2.8.0/Scrapy.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-02-02 04:54:56.000000 Scrapy-2.8.0/Scrapy.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)      395 2023-02-02 04:54:56.000000 Scrapy-2.8.0/Scrapy.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-02-02 04:54:56.000000 Scrapy-2.8.0/Scrapy.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       80 2023-02-02 04:54:40.000000 Scrapy-2.8.0/codecov.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2209 2023-02-02 04:54:40.000000 Scrapy-2.8.0/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.254197 Scrapy-2.8.0/docs/
+-rw-r--r--   0 runner    (1001) docker     (123)     2832 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.254197 Scrapy-2.8.0/docs/_ext/
+-rw-r--r--   0 runner    (1001) docker     (123)     4516 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/_ext/scrapydocs.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.254197 Scrapy-2.8.0/docs/_static/
+-rw-r--r--   0 runner    (1001) docker     (123)      360 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/_static/custom.css
+-rw-r--r--   0 runner    (1001) docker     (123)      667 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/_static/selectors-sample1.html
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.254197 Scrapy-2.8.0/docs/_templates/
+-rw-r--r--   0 runner    (1001) docker     (123)      224 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/_templates/layout.html
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.254197 Scrapy-2.8.0/docs/_tests/
+-rw-r--r--   0 runner    (1001) docker     (123)    11053 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/_tests/quotes.html
+-rw-r--r--   0 runner    (1001) docker     (123)    11053 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/_tests/quotes1.html
+-rw-r--r--   0 runner    (1001) docker     (123)    10943 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      921 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12493 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/contributing.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    15893 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/faq.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6788 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.258197 Scrapy-2.8.0/docs/intro/
+-rw-r--r--   0 runner    (1001) docker     (123)      745 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/intro/examples.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    10323 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/intro/install.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6648 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/intro/overview.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    29876 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/intro/tutorial.rst
+-rw-r--r--   0 runner    (1001) docker     (123)   243245 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/news.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       88 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.270198 Scrapy-2.8.0/docs/topics/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.270198 Scrapy-2.8.0/docs/topics/_images/
+-rw-r--r--   0 runner    (1001) docker     (123)    53922 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/_images/inspector_01.png
+-rw-r--r--   0 runner    (1001) docker     (123)    10720 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/_images/network_01.png
+-rw-r--r--   0 runner    (1001) docker     (123)    82702 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/_images/network_02.png
+-rw-r--r--   0 runner    (1001) docker     (123)    45506 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/_images/network_03.png
+-rw-r--r--   0 runner    (1001) docker     (123)    19653 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/_images/scrapy_architecture.odg
+-rw-r--r--   0 runner    (1001) docker     (123)    92558 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/_images/scrapy_architecture.png
+-rw-r--r--   0 runner    (1001) docker     (123)    53978 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/_images/scrapy_architecture_02.png
+-rw-r--r--   0 runner    (1001) docker     (123)     8302 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/api.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6114 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/architecture.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4265 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/asyncio.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     5722 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/autothrottle.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     5257 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/benchmarking.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     8229 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/broad-crawls.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    18057 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/commands.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2722 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/components.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4856 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/contracts.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     8413 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/coroutines.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     5487 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/debug.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2263 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/deploy.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    13738 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/developer-tools.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      194 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/djangoitem.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    39774 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/downloader-middleware.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    11937 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/dynamic-content.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     4973 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/email.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     3230 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/exceptions.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    15472 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/exporters.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    12253 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/extensions.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    22908 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/feed-exports.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     8610 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/item-pipeline.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    11212 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/items.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     2881 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/jobs.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    11579 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/leaks.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     6803 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/link-extractors.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    16589 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/loaders.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    10783 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/logging.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    25932 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/media-pipeline.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    10367 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/practices.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    48009 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/request-response.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      761 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/scheduler.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      188 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/scrapyd.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    34983 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/selectors.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    49593 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/settings.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    10884 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/shell.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    16192 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/signals.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    18936 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/spider-middleware.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    32291 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/spiders.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     3537 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/stats.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     7477 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/topics/telnetconsole.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.274198 Scrapy-2.8.0/docs/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)     1977 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/utils/linkfix.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2161 2023-02-02 04:54:40.000000 Scrapy-2.8.0/docs/versioning.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.274198 Scrapy-2.8.0/extras/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      259 2023-02-02 04:54:40.000000 Scrapy-2.8.0/extras/coverage-report.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1618 2023-02-02 04:54:40.000000 Scrapy-2.8.0/extras/qps-bench-server.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1508 2023-02-02 04:54:40.000000 Scrapy-2.8.0/extras/qpsclient.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-02-02 04:54:40.000000 Scrapy-2.8.0/extras/scrapy.1
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-02 04:54:40.000000 Scrapy-2.8.0/extras/scrapy_bash_completion
+-rw-r--r--   0 runner    (1001) docker     (123)     7008 2023-02-02 04:54:40.000000 Scrapy-2.8.0/extras/scrapy_zsh_completion
+-rw-r--r--   0 runner    (1001) docker     (123)      938 2023-02-02 04:54:40.000000 Scrapy-2.8.0/pytest.ini
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.282198 Scrapy-2.8.0/scrapy/
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/VERSION
+-rw-r--r--   0 runner    (1001) docker     (123)     1071 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)       77 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5640 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/cmdline.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.282198 Scrapy-2.8.0/scrapy/commands/
+-rw-r--r--   0 runner    (1001) docker     (123)     7019 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1578 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/bench.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3572 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/check.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1079 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/crawl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1061 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/edit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2798 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/fetch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6434 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/genspider.py
+-rw-r--r--   0 runner    (1001) docker     (123)      337 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/list.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11075 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/parse.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1826 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/runspider.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1897 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/settings.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2858 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/shell.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4267 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/startproject.py
+-rw-r--r--   0 runner    (1001) docker     (123)      978 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/version.py
+-rw-r--r--   0 runner    (1001) docker     (123)      576 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/commands/view.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.282198 Scrapy-2.8.0/scrapy/contracts/
+-rw-r--r--   0 runner    (1001) docker     (123)     6119 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/contracts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2946 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/contracts/default.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.282198 Scrapy-2.8.0/scrapy/core/
+-rw-r--r--   0 runner    (1001) docker     (123)       51 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.282198 Scrapy-2.8.0/scrapy/core/downloader/
+-rw-r--r--   0 runner    (1001) docker     (123)     7335 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6205 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/contextfactory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.286198 Scrapy-2.8.0/scrapy/core/downloader/handlers/
+-rw-r--r--   0 runner    (1001) docker     (123)     2947 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/handlers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      675 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/handlers/datauri.py
+-rw-r--r--   0 runner    (1001) docker     (123)      479 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/handlers/file.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4638 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/handlers/ftp.py
+-rw-r--r--   0 runner    (1001) docker     (123)      178 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/handlers/http.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1351 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/handlers/http10.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24231 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/handlers/http11.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4410 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/handlers/http2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3009 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/handlers/s3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3954 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/middleware.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3051 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/tls.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8080 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/downloader/webclient.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18171 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.286198 Scrapy-2.8.0/scrapy/core/http2/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/http2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5903 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/http2/agent.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16717 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/http2/protocol.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19140 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/http2/stream.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14001 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13706 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/scraper.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14333 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/core/spidermw.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14693 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/crawler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.286198 Scrapy-2.8.0/scrapy/downloadermiddlewares/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3291 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/ajaxcrawl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5208 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/cookies.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2778 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/decompression.py
+-rw-r--r--   0 runner    (1001) docker     (123)      559 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/defaultheaders.py
+-rw-r--r--   0 runner    (1001) docker     (123)      700 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/downloadtimeout.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/httpauth.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5567 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/httpcache.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3979 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/httpcompression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3103 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/httpproxy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5204 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/redirect.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6574 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/retry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4397 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/robotstxt.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2170 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/stats.py
+-rw-r--r--   0 runner    (1001) docker     (123)      741 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/downloadermiddlewares/useragent.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4467 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/dupefilters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1984 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12770 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/exporters.py
+-rw-r--r--   0 runner    (1001) docker     (123)      397 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extension.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/extensions/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2609 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/closespider.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1834 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/corestats.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1877 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/debug.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22072 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/feedexport.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14727 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/httpcache.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1804 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/logstats.py
+-rw-r--r--   0 runner    (1001) docker     (123)      968 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/memdebug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5310 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/memusage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4993 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/postprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1164 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/spiderstate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/statsmailer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4028 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/telnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3705 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/extensions/throttle.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/http/
+-rw-r--r--   0 runner    (1001) docker     (123)      600 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      240 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5758 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/cookies.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2868 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/headers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/http/request/
+-rw-r--r--   0 runner    (1001) docker     (123)     9318 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/request/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8730 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/request/form.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2115 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/request/json_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1085 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/request/rpc.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/http/response/
+-rw-r--r--   0 runner    (1001) docker     (123)     7266 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/response/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      300 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/response/html.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9653 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/response/text.py
+-rw-r--r--   0 runner    (1001) docker     (123)      297 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/http/response/xml.py
+-rw-r--r--   0 runner    (1001) docker     (123)      557 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/interfaces.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3823 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/item.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1891 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/link.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/linkextractors/
+-rw-r--r--   0 runner    (1001) docker     (123)     1408 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/linkextractors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8241 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/linkextractors/lxmlhtml.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/loader/
+-rw-r--r--   0 runner    (1001) docker     (123)     3491 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/loader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      658 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/loader/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)      632 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/loader/processors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5336 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/logformatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6099 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/mail.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3253 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/middleware.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20392 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/mime.types
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/pipelines/
+-rw-r--r--   0 runner    (1001) docker     (123)      798 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/pipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20548 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/pipelines/files.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8889 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/pipelines/images.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10396 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/pipelines/media.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7712 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/pqueues.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4149 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/resolver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4897 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/responsetypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4227 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/robotstxt.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/selector/
+-rw-r--r--   0 runner    (1001) docker     (123)       98 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/selector/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2662 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/selector/unified.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/settings/
+-rw-r--r--   0 runner    (1001) docker     (123)    16481 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/settings/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9342 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/settings/default_settings.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7579 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/shell.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2391 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/signalmanager.py
+-rw-r--r--   0 runner    (1001) docker     (123)      815 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/signals.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2963 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spiderloader.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.290199 Scrapy-2.8.0/scrapy/spidermiddlewares/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spidermiddlewares/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2159 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spidermiddlewares/depth.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1950 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spidermiddlewares/httperror.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3223 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spidermiddlewares/offsite.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13893 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spidermiddlewares/referer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1339 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spidermiddlewares/urllength.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.294199 Scrapy-2.8.0/scrapy/spiders/
+-rw-r--r--   0 runner    (1001) docker     (123)     3092 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spiders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4772 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spiders/crawl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5583 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spiders/feed.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1337 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spiders/init.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3543 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/spiders/sitemap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6044 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/squeues.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2113 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/statscollectors.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.238196 Scrapy-2.8.0/scrapy/templates/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.294199 Scrapy-2.8.0/scrapy/templates/project/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.294199 Scrapy-2.8.0/scrapy/templates/project/module/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/project/module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      270 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/project/module/items.py.tmpl
+-rw-r--r--   0 runner    (1001) docker     (123)     3664 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/project/module/middlewares.py.tmpl
+-rw-r--r--   0 runner    (1001) docker     (123)      368 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/project/module/pipelines.py.tmpl
+-rw-r--r--   0 runner    (1001) docker     (123)     3364 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/project/module/settings.py.tmpl
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.294199 Scrapy-2.8.0/scrapy/templates/project/module/spiders/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/project/module/spiders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      273 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/project/scrapy.cfg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.294199 Scrapy-2.8.0/scrapy/templates/spiders/
+-rw-r--r--   0 runner    (1001) docker     (123)      184 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/spiders/basic.tmpl
+-rw-r--r--   0 runner    (1001) docker     (123)      619 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/spiders/crawl.tmpl
+-rw-r--r--   0 runner    (1001) docker     (123)      545 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/spiders/csvfeed.tmpl
+-rw-r--r--   0 runner    (1001) docker     (123)      543 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/templates/spiders/xmlfeed.tmpl
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.298199 Scrapy-2.8.0/scrapy/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      507 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/asyncgen.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1300 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/benchserver.py
+-rw-r--r--   0 runner    (1001) docker     (123)      172 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/boto.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8088 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/console.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3323 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/curl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3351 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/datatypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1275 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13863 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/defer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6369 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/deprecate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/display.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1302 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1221 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/ftp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1040 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/gz.py
+-rw-r--r--   0 runner    (1001) docker     (123)      712 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/httpobj.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5431 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/iterators.py
+-rw-r--r--   0 runner    (1001) docker     (123)      277 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/job.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7174 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/log.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9277 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (123)      943 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/ossignal.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2569 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/project.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9590 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/python.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5819 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/reactor.py
+-rw-r--r--   0 runner    (1001) docker     (123)      708 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/reqser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13698 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3537 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1137 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/serialize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3297 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/signal.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1523 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/sitemap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2173 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/spider.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2095 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/ssl.py
+-rw-r--r--   0 runner    (1001) docker     (123)      888 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/template.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3735 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1487 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/testproc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1611 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/testsite.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/trackref.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5595 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/url.py
+-rw-r--r--   0 runner    (1001) docker     (123)      850 2023-02-02 04:54:40.000000 Scrapy-2.8.0/scrapy/utils/versions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1575 2023-02-02 04:54:56.330201 Scrapy-2.8.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     3276 2023-02-02 04:54:40.000000 Scrapy-2.8.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.310200 Scrapy-2.8.0/tests/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.314200 Scrapy-2.8.0/tests/CrawlerProcess/
+-rw-r--r--   0 runner    (1001) docker     (123)      399 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/asyncio_custom_loop.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1173 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/asyncio_deferred_signal.py
+-rw-r--r--   0 runner    (1001) docker     (123)      354 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/asyncio_enabled_no_reactor.py
+-rw-r--r--   0 runner    (1001) docker     (123)      639 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/asyncio_enabled_reactor.py
+-rw-r--r--   0 runner    (1001) docker     (123)      684 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/asyncio_enabled_reactor_different_loop.py
+-rw-r--r--   0 runner    (1001) docker     (123)      739 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/asyncio_enabled_reactor_same_loop.py
+-rw-r--r--   0 runner    (1001) docker     (123)      913 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/caching_hostname_resolver.py
+-rw-r--r--   0 runner    (1001) docker     (123)      548 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/caching_hostname_resolver_ipv6.py
+-rw-r--r--   0 runner    (1001) docker     (123)      424 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/default_name_resolver.py
+-rw-r--r--   0 runner    (1001) docker     (123)      291 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/multi.py
+-rw-r--r--   0 runner    (1001) docker     (123)      311 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/reactor_default.py
+-rw-r--r--   0 runner    (1001) docker     (123)      397 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/reactor_default_twisted_reactor_select.py
+-rw-r--r--   0 runner    (1001) docker     (123)      328 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/reactor_select.py
+-rw-r--r--   0 runner    (1001) docker     (123)      567 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/reactor_select_subclass_twisted_reactor_select.py
+-rw-r--r--   0 runner    (1001) docker     (123)      414 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/reactor_select_twisted_reactor_select.py
+-rw-r--r--   0 runner    (1001) docker     (123)      259 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/simple.py
+-rw-r--r--   0 runner    (1001) docker     (123)      318 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/twisted_reactor_asyncio.py
+-rw-r--r--   0 runner    (1001) docker     (123)      326 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/twisted_reactor_custom_settings.py
+-rw-r--r--   0 runner    (1001) docker     (123)      538 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/twisted_reactor_custom_settings_conflict.py
+-rw-r--r--   0 runner    (1001) docker     (123)      557 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/twisted_reactor_custom_settings_same.py
+-rw-r--r--   0 runner    (1001) docker     (123)      295 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/twisted_reactor_poll.py
+-rw-r--r--   0 runner    (1001) docker     (123)      304 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerProcess/twisted_reactor_select.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.314200 Scrapy-2.8.0/tests/CrawlerRunner/
+-rw-r--r--   0 runner    (1001) docker     (123)     1803 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/CrawlerRunner/ip_address.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1287 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      632 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/ftpserver.py
+-rw-r--r--   0 runner    (1001) docker     (123)      103 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/ignores.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.314200 Scrapy-2.8.0/tests/keys/
+-rw-r--r--   0 runner    (1001) docker     (123)     1833 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1562 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/example-com.cert.pem
+-rw-r--r--   0 runner    (1001) docker     (123)     3126 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/example-com.conf
+-rw-r--r--   0 runner    (1001) docker     (123)      933 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/example-com.gen.README
+-rw-r--r--   0 runner    (1001) docker     (123)     1704 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/example-com.key.pem
+-rw-r--r--   0 runner    (1001) docker     (123)      945 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/localhost-ip.gen.README
+-rw-r--r--   0 runner    (1001) docker     (123)      945 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/localhost.gen.README
+-rw-r--r--   0 runner    (1001) docker     (123)     1176 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/localhost.ip.crt
+-rw-r--r--   0 runner    (1001) docker     (123)     1704 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/localhost.ip.key
+-rw-r--r--   0 runner    (1001) docker     (123)     3022 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/keys/mitmproxy-ca.pem
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.314200 Scrapy-2.8.0/tests/mocks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/mocks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      443 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/mocks/dummydbm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12241 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/mockserver.py
+-rw-r--r--   0 runner    (1001) docker     (123)      289 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/pipelines.py
+-rw-r--r--   0 runner    (1001) docker     (123)      457 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.242196 Scrapy-2.8.0/tests/sample_data/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.318200 Scrapy-2.8.0/tests/sample_data/compressed/
+-rw-r--r--   0 runner    (1001) docker     (123)    20480 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.tar
+-rw-r--r--   0 runner    (1001) docker     (123)     9950 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1430 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.xml.bz2
+-rw-r--r--   0 runner    (1001) docker     (123)     1131 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.xml.gz
+-rw-r--r--   0 runner    (1001) docker     (123)     1260 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.zip
+-rw-r--r--   0 runner    (1001) docker     (123)     4027 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/html-br.bin
+-rw-r--r--   0 runner    (1001) docker     (123)     8037 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/html-gzip.bin
+-rw-r--r--   0 runner    (1001) docker     (123)     8021 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/html-rawdeflate.bin
+-rw-r--r--   0 runner    (1001) docker     (123)     8027 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/html-zlibdeflate.bin
+-rw-r--r--   0 runner    (1001) docker     (123)     8066 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/html-zstd-static-content-size.bin
+-rw-r--r--   0 runner    (1001) docker     (123)     8063 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/html-zstd-static-no-content-size.bin
+-rw-r--r--   0 runner    (1001) docker     (123)     8047 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/html-zstd-streaming-no-content-size.bin
+-rw-r--r--   0 runner    (1001) docker     (123)     1930 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/truncated-crc-error-short.gz
+-rw-r--r--   0 runner    (1001) docker     (123)     5766 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/truncated-crc-error.gz
+-rw-r--r--   0 runner    (1001) docker     (123)    16782 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/unexpected-eof-output.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     5134 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/compressed/unexpected-eof.gz
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.322200 Scrapy-2.8.0/tests/sample_data/feeds/
+-rw-r--r--   0 runner    (1001) docker     (123)     9950 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/feeds/feed-sample1.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     6388 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/feeds/feed-sample2.xml
+-rw-r--r--   0 runner    (1001) docker     (123)       81 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/feeds/feed-sample3.csv
+-rw-r--r--   0 runner    (1001) docker     (123)       45 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/feeds/feed-sample4.csv
+-rw-r--r--   0 runner    (1001) docker     (123)       47 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/feeds/feed-sample5.csv
+-rw-r--r--   0 runner    (1001) docker     (123)      101 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/feeds/feed-sample6.csv
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.322200 Scrapy-2.8.0/tests/sample_data/link_extractor/
+-rw-r--r--   0 runner    (1001) docker     (123)      830 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/link_extractor/linkextractor.html
+-rw-r--r--   0 runner    (1001) docker     (123)      585 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/link_extractor/linkextractor_latin1.html
+-rw-r--r--   0 runner    (1001) docker     (123)      740 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/link_extractor/linkextractor_no_href.html
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/link_extractor/linkextractor_noenc.html
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.322200 Scrapy-2.8.0/tests/sample_data/test_site/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.242196 Scrapy-2.8.0/tests/sample_data/test_site/files/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.322200 Scrapy-2.8.0/tests/sample_data/test_site/files/images/
+-rw-r--r--   0 runner    (1001) docker     (123)    11155 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/test_site/files/images/python-logo-master-v3-TM-flattened.png
+-rw-r--r--   0 runner    (1001) docker     (123)     3243 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/test_site/files/images/python-powered-h-50x65.png
+-rw-r--r--   0 runner    (1001) docker     (123)     2710 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/test_site/files/images/scrapy.png
+-rw-r--r--   0 runner    (1001) docker     (123)      311 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/test_site/index.html
+-rw-r--r--   0 runner    (1001) docker     (123)      225 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/test_site/item1.html
+-rw-r--r--   0 runner    (1001) docker     (123)      209 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/sample_data/test_site/item2.html
+-rw-r--r--   0 runner    (1001) docker     (123)    15185 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/spiders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2351 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_closespider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.322200 Scrapy-2.8.0/tests/test_cmdline/
+-rw-r--r--   0 runner    (1001) docker     (123)     2540 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      309 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline/extensions.py
+-rw-r--r--   0 runner    (1001) docker     (123)      223 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline/settings.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.322200 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)      632 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)       42 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/scrapy.cfg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.322200 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/test_spider/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/test_spider/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      309 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/test_spider/pipelines.py
+-rw-r--r--   0 runner    (1001) docker     (123)       66 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/test_spider/settings.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.326200 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      238 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/exception.py
+-rw-r--r--   0 runner    (1001) docker     (123)      223 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/test_spider/spiders/normal.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2764 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_command_check.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1243 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_command_fetch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10868 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_command_parse.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5083 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_command_shell.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_command_version.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36027 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_commands.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13176 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_contracts.py
+-rw-r--r--   0 runner    (1001) docker     (123)      322 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_core_downloader.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27521 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_crawl.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21639 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_crawler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1506 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_dependencies.py
+-rw-r--r--   0 runner    (1001) docker     (123)    48356 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloader_handlers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9271 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloader_handlers_http2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9309 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2541 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_ajaxcrawlable.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28200 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_cookies.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1858 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_decompression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1397 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_defaultheaders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1701 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_downloadtimeout.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4108 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_httpauth.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25835 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_httpcache.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16529 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_httpcompression.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19944 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_httpproxy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15457 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_redirect.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21799 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_retry.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10222 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_robotstxt.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2665 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_stats.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2227 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_downloadermiddleware_useragent.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9493 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_dupefilters.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19529 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2966 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_engine_stop_download_bytes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2709 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_engine_stop_download_headers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21912 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_exporters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1833 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_extension_telnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)   102299 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_feedexport.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24027 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_http2_client_protocol.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2546 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_http_cookies.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6251 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_http_headers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    65265 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_http_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)    39197 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_http_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8991 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_item.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1900 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_link.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32026 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_linkextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21141 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26618 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_loader_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8854 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_logformatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5761 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_mail.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2339 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_middleware.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7644 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_pipeline_crawl.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24216 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_pipeline_files.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25450 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_pipeline_images.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22768 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_pipeline_media.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3893 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_pipelines.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6200 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_pqueues.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3934 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_proxy_connect.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7644 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_request_attribute_binding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7476 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_request_cb_kwargs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8146 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_request_dict.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1931 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_request_left.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4880 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_responsetypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7177 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_robotstxt_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10588 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5463 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_scheduler_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3996 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_selector.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.326200 Scrapy-2.8.0/tests/test_settings/
+-rw-r--r--   0 runner    (1001) docker     (123)    18018 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_settings/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)       54 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_settings/default_settings.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1339 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_signals.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25207 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.326200 Scrapy-2.8.0/tests/test_spiderloader/
+-rw-r--r--   0 runner    (1001) docker     (123)     7779 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spiderloader/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.326200 Scrapy-2.8.0/tests/test_spiderloader/test_spiders/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spiderloader/test_spiders/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.326200 Scrapy-2.8.0/tests/test_spiderloader/test_spiders/nested/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spiderloader/test_spiders/nested/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      235 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spiderloader/test_spiders/nested/spider4.py
+-rw-r--r--   0 runner    (1001) docker     (123)      112 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spiderloader/test_spiders/spider0.py
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spiderloader/test_spiders/spider1.py
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spiderloader/test_spiders/spider2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      235 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spiderloader/test_spiders/spider3.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19916 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spidermiddleware.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spidermiddleware_depth.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9253 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spidermiddleware_httperror.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3685 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spidermiddleware_offsite.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19049 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spidermiddleware_output_chain.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43427 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spidermiddleware_referer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spidermiddleware_urllength.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1437 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_spiderstate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5230 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_squeues.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7600 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_squeues_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3996 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_stats.py
+-rw-r--r--   0 runner    (1001) docker     (123)      869 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_toplevel.py
+-rw-r--r--   0 runner    (1001) docker     (123)      362 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_urlparse_monkeypatches.py
+-rw-r--r--   0 runner    (1001) docker     (123)      663 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_asyncgen.py
+-rw-r--r--   0 runner    (1001) docker     (123)      835 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_asyncio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8659 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1176 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_console.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9845 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_curl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9415 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_datatypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7634 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_defer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10172 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_deprecate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_display.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1909 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_gz.py
+-rw-r--r--   0 runner    (1001) docker     (123)      685 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_httpobj.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20658 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_iterators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3576 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_log.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.326200 Scrapy-2.8.0/tests/test_utils_misc/
+-rw-r--r--   0 runner    (1001) docker     (123)     7039 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_misc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2231 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_misc/test.egg
+-rw-r--r--   0 runner    (1001) docker     (123)     8263 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_misc/test_return_with_argument_inside_generator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.326200 Scrapy-2.8.0/tests/test_utils_misc/test_walk_modules/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_misc/test_walk_modules/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:56.330201 Scrapy-2.8.0/tests/test_utils_misc/test_walk_modules/mod/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_misc/test_walk_modules/mod/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_misc/test_walk_modules/mod/mod0.py
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_misc/test_walk_modules/mod1.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2598 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_project.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8061 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_python.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25041 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_request.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6752 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_response.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2480 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_serialize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3538 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_signal.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9009 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_sitemap.py
+-rw-r--r--   0 runner    (1001) docker     (123)      964 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_spider.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1183 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_template.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2152 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_trackref.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22553 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_utils_url.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17290 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/test_webclient.py
+-rw-r--r--   0 runner    (1001) docker     (123)      412 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tests/upper-constraints.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     4936 2023-02-02 04:54:40.000000 Scrapy-2.8.0/tox.ini
```

### Comparing `Scrapy-2.7.1/AUTHORS` & `Scrapy-2.8.0/AUTHORS`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/LICENSE` & `Scrapy-2.8.0/LICENSE`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/PKG-INFO` & `Scrapy-2.8.0/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 Metadata-Version: 2.1
 Name: Scrapy
-Version: 2.7.1
+Version: 2.8.0
 Summary: A high-level Web Crawling and Web Scraping framework
 Home-page: https://scrapy.org
 Author: Scrapy developers
+Author-email: pablo@pablohoffman.com
 Maintainer: Pablo Hoffman
 Maintainer-email: pablo@pablohoffman.com
 License: BSD
 Project-URL: Documentation, https://docs.scrapy.org/
 Project-URL: Source, https://github.com/scrapy/scrapy
 Project-URL: Tracker, https://github.com/scrapy/scrapy/issues
 Classifier: Framework :: Scrapy
```

### Comparing `Scrapy-2.7.1/README.rst` & `Scrapy-2.8.0/README.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/Scrapy.egg-info/PKG-INFO` & `Scrapy-2.8.0/Scrapy.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 Metadata-Version: 2.1
 Name: Scrapy
-Version: 2.7.1
+Version: 2.8.0
 Summary: A high-level Web Crawling and Web Scraping framework
 Home-page: https://scrapy.org
 Author: Scrapy developers
+Author-email: pablo@pablohoffman.com
 Maintainer: Pablo Hoffman
 Maintainer-email: pablo@pablohoffman.com
 License: BSD
 Project-URL: Documentation, https://docs.scrapy.org/
 Project-URL: Source, https://github.com/scrapy/scrapy
 Project-URL: Tracker, https://github.com/scrapy/scrapy/issues
 Classifier: Framework :: Scrapy
```

### Comparing `Scrapy-2.7.1/Scrapy.egg-info/SOURCES.txt` & `Scrapy-2.8.0/Scrapy.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/conftest.py` & `Scrapy-2.8.0/conftest.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,40 +1,39 @@
 from pathlib import Path
 
 import pytest
 from twisted.web.http import H2_ENABLED
 
 from scrapy.utils.reactor import install_reactor
-
 from tests.keys import generate_keys
 
 
 def _py_files(folder):
-    return (str(p) for p in Path(folder).rglob('*.py'))
+    return (str(p) for p in Path(folder).rglob("*.py"))
 
 
 collect_ignore = [
     # not a test, but looks like a test
     "scrapy/utils/testsite.py",
     # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess
     *_py_files("tests/CrawlerProcess"),
     # contains scripts to be run by tests/test_crawler.py::CrawlerRunnerSubprocess
     *_py_files("tests/CrawlerRunner"),
 ]
 
-with open('tests/ignores.txt') as reader:
+with Path("tests/ignores.txt").open(encoding="utf-8") as reader:
     for line in reader:
         file_path = line.strip()
-        if file_path and file_path[0] != '#':
+        if file_path and file_path[0] != "#":
             collect_ignore.append(file_path)
 
 if not H2_ENABLED:
     collect_ignore.extend(
         (
-            'scrapy/core/downloader/handlers/http2.py',
+            "scrapy/core/downloader/handlers/http2.py",
             *_py_files("scrapy/core/http2"),
         )
     )
 
 
 @pytest.fixture()
 def chdir(tmpdir):
@@ -46,33 +45,36 @@
     parser.addoption(
         "--reactor",
         default="default",
         choices=["default", "asyncio"],
     )
 
 
-@pytest.fixture(scope='class')
+@pytest.fixture(scope="class")
 def reactor_pytest(request):
     if not request.cls:
         # doctests
         return
     request.cls.reactor_pytest = request.config.getoption("--reactor")
     return request.cls.reactor_pytest
 
 
 @pytest.fixture(autouse=True)
 def only_asyncio(request, reactor_pytest):
-    if request.node.get_closest_marker('only_asyncio') and reactor_pytest != 'asyncio':
-        pytest.skip('This test is only run with --reactor=asyncio')
+    if request.node.get_closest_marker("only_asyncio") and reactor_pytest != "asyncio":
+        pytest.skip("This test is only run with --reactor=asyncio")
 
 
 @pytest.fixture(autouse=True)
 def only_not_asyncio(request, reactor_pytest):
-    if request.node.get_closest_marker('only_not_asyncio') and reactor_pytest == 'asyncio':
-        pytest.skip('This test is only run without --reactor=asyncio')
+    if (
+        request.node.get_closest_marker("only_not_asyncio")
+        and reactor_pytest == "asyncio"
+    ):
+        pytest.skip("This test is only run without --reactor=asyncio")
 
 
 def pytest_configure(config):
     if config.getoption("--reactor") == "asyncio":
         install_reactor("twisted.internet.asyncioreactor.AsyncioSelectorReactor")
```

### Comparing `Scrapy-2.7.1/docs/Makefile` & `Scrapy-2.8.0/docs/Makefile`

 * *Files 2% similar despite different names*

```diff
@@ -82,15 +82,15 @@
 	@echo "Building finished; now copy build/pydoc-topics/pydoc_topics.py " \
 	      "into the Lib/ directory"
 
 coverage: BUILDER = coverage
 coverage: build
 
 htmlview: html
-	 $(PYTHON) -c "import webbrowser, os; webbrowser.open('file://' + \
-	 os.path.realpath('build/html/index.html'))"
+	 $(PYTHON) -c "import webbrowser; from pathlib import Path; \
+	 webbrowser.open('file://' + Path('build/html/index.html').resolve())"
 
 clean:
 	-rm -rf build/*
 
 watch: htmlview
 	watchmedo shell-command -p '*.rst' -c 'make html' -R -D
```

### Comparing `Scrapy-2.7.1/docs/README.rst` & `Scrapy-2.8.0/docs/README.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/_ext/scrapydocs.py` & `Scrapy-2.8.0/docs/_ext/scrapydocs.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,84 +1,93 @@
-from docutils.parsers.rst.roles import set_classes
+from operator import itemgetter
+
 from docutils import nodes
 from docutils.parsers.rst import Directive
+from docutils.parsers.rst.roles import set_classes
 from sphinx.util.nodes import make_refnode
-from operator import itemgetter
 
 
 class settingslist_node(nodes.General, nodes.Element):
     pass
 
 
 class SettingsListDirective(Directive):
     def run(self):
-        return [settingslist_node('')]
+        return [settingslist_node("")]
 
 
 def is_setting_index(node):
-    if node.tagname == 'index' and node['entries']:
+    if node.tagname == "index" and node["entries"]:
         # index entries for setting directives look like:
         # [('pair', 'SETTING_NAME; setting', 'std:setting-SETTING_NAME', '')]
-        entry_type, info, refid = node['entries'][0][:3]
-        return entry_type == 'pair' and info.endswith('; setting')
+        entry_type, info, refid = node["entries"][0][:3]
+        return entry_type == "pair" and info.endswith("; setting")
     return False
 
 
 def get_setting_target(node):
     # target nodes are placed next to the node in the doc tree
     return node.parent[node.parent.index(node) + 1]
 
 
 def get_setting_name_and_refid(node):
     """Extract setting name from directive index node"""
-    entry_type, info, refid = node['entries'][0][:3]
-    return info.replace('; setting', ''), refid
+    entry_type, info, refid = node["entries"][0][:3]
+    return info.replace("; setting", ""), refid
 
 
 def collect_scrapy_settings_refs(app, doctree):
     env = app.builder.env
 
-    if not hasattr(env, 'scrapy_all_settings'):
+    if not hasattr(env, "scrapy_all_settings"):
         env.scrapy_all_settings = []
 
     for node in doctree.traverse(is_setting_index):
         targetnode = get_setting_target(node)
         assert isinstance(targetnode, nodes.target), "Next node is not a target"
 
         setting_name, refid = get_setting_name_and_refid(node)
 
-        env.scrapy_all_settings.append({
-            'docname': env.docname,
-            'setting_name': setting_name,
-            'refid': refid,
-        })
+        env.scrapy_all_settings.append(
+            {
+                "docname": env.docname,
+                "setting_name": setting_name,
+                "refid": refid,
+            }
+        )
 
 
 def make_setting_element(setting_data, app, fromdocname):
-    refnode = make_refnode(app.builder, fromdocname,
-                           todocname=setting_data['docname'],
-                           targetid=setting_data['refid'],
-                           child=nodes.Text(setting_data['setting_name']))
+    refnode = make_refnode(
+        app.builder,
+        fromdocname,
+        todocname=setting_data["docname"],
+        targetid=setting_data["refid"],
+        child=nodes.Text(setting_data["setting_name"]),
+    )
     p = nodes.paragraph()
     p += refnode
 
     item = nodes.list_item()
     item += p
     return item
 
 
 def replace_settingslist_nodes(app, doctree, fromdocname):
     env = app.builder.env
 
     for node in doctree.traverse(settingslist_node):
         settings_list = nodes.bullet_list()
-        settings_list.extend([make_setting_element(d, app, fromdocname)
-                              for d in sorted(env.scrapy_all_settings,
-                                              key=itemgetter('setting_name'))
-                              if fromdocname != d['docname']])
+        settings_list.extend(
+            [
+                make_setting_element(d, app, fromdocname)
+                for d in sorted(env.scrapy_all_settings, key=itemgetter("setting_name"))
+                if fromdocname != d["docname"]
+            ]
+        )
         node.replace_self(settings_list)
 
 
 def setup(app):
     app.add_crossref_type(
         directivename="setting",
         rolename="setting",
@@ -95,45 +104,45 @@
         indextemplate="pair: %s; command",
     )
     app.add_crossref_type(
         directivename="reqmeta",
         rolename="reqmeta",
         indextemplate="pair: %s; reqmeta",
     )
-    app.add_role('source', source_role)
-    app.add_role('commit', commit_role)
-    app.add_role('issue', issue_role)
-    app.add_role('rev', rev_role)
+    app.add_role("source", source_role)
+    app.add_role("commit", commit_role)
+    app.add_role("issue", issue_role)
+    app.add_role("rev", rev_role)
 
     app.add_node(settingslist_node)
-    app.add_directive('settingslist', SettingsListDirective)
+    app.add_directive("settingslist", SettingsListDirective)
 
-    app.connect('doctree-read', collect_scrapy_settings_refs)
-    app.connect('doctree-resolved', replace_settingslist_nodes)
+    app.connect("doctree-read", collect_scrapy_settings_refs)
+    app.connect("doctree-resolved", replace_settingslist_nodes)
 
 
 def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):
-    ref = 'https://github.com/scrapy/scrapy/blob/master/' + text
+    ref = "https://github.com/scrapy/scrapy/blob/master/" + text
     set_classes(options)
     node = nodes.reference(rawtext, text, refuri=ref, **options)
     return [node], []
 
 
 def issue_role(name, rawtext, text, lineno, inliner, options={}, content=[]):
-    ref = 'https://github.com/scrapy/scrapy/issues/' + text
+    ref = "https://github.com/scrapy/scrapy/issues/" + text
     set_classes(options)
-    node = nodes.reference(rawtext, 'issue ' + text, refuri=ref, **options)
+    node = nodes.reference(rawtext, "issue " + text, refuri=ref, **options)
     return [node], []
 
 
 def commit_role(name, rawtext, text, lineno, inliner, options={}, content=[]):
-    ref = 'https://github.com/scrapy/scrapy/commit/' + text
+    ref = "https://github.com/scrapy/scrapy/commit/" + text
     set_classes(options)
-    node = nodes.reference(rawtext, 'commit ' + text, refuri=ref, **options)
+    node = nodes.reference(rawtext, "commit " + text, refuri=ref, **options)
     return [node], []
 
 
 def rev_role(name, rawtext, text, lineno, inliner, options={}, content=[]):
-    ref = 'http://hg.scrapy.org/scrapy/changeset/' + text
+    ref = "http://hg.scrapy.org/scrapy/changeset/" + text
     set_classes(options)
-    node = nodes.reference(rawtext, 'r' + text, refuri=ref, **options)
+    node = nodes.reference(rawtext, "r" + text, refuri=ref, **options)
     return [node], []
```

### Comparing `Scrapy-2.7.1/docs/_static/selectors-sample1.html` & `Scrapy-2.8.0/docs/_static/selectors-sample1.html`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/_tests/quotes.html` & `Scrapy-2.8.0/docs/_tests/quotes.html`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/_tests/quotes1.html` & `Scrapy-2.8.0/docs/_tests/quotes1.html`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/conf.py` & `Scrapy-2.8.0/docs/conf.py`

 * *Files 11% similar despite different names*

```diff
@@ -7,298 +7,290 @@
 # that aren't pickleable (module imports are okay, they're removed automatically).
 #
 # All configuration values have a default; values that are commented out
 # serve to show the default.
 
 import sys
 from datetime import datetime
-from os import path
+from pathlib import Path
 
 # If your extensions are in another directory, add it here. If the directory
-# is relative to the documentation root, use os.path.abspath to make it
-# absolute, like shown here.
-sys.path.append(path.join(path.dirname(__file__), "_ext"))
-sys.path.insert(0, path.dirname(path.dirname(__file__)))
+# is relative to the documentation root, use Path.absolute to make it absolute.
+sys.path.append(str(Path(__file__).parent / "_ext"))
+sys.path.insert(0, str(Path(__file__).parent.parent))
 
 
 # General configuration
 # ---------------------
 
 # Add any Sphinx extension module names here, as strings. They can be extensions
 # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
 extensions = [
-    'hoverxref.extension',
-    'notfound.extension',
-    'scrapydocs',
-    'sphinx.ext.autodoc',
-    'sphinx.ext.coverage',
-    'sphinx.ext.intersphinx',
-    'sphinx.ext.viewcode',
+    "hoverxref.extension",
+    "notfound.extension",
+    "scrapydocs",
+    "sphinx.ext.autodoc",
+    "sphinx.ext.coverage",
+    "sphinx.ext.intersphinx",
+    "sphinx.ext.viewcode",
 ]
 
 # Add any paths that contain templates here, relative to this directory.
-templates_path = ['_templates']
+templates_path = ["_templates"]
 
 # The suffix of source filenames.
-source_suffix = '.rst'
+source_suffix = ".rst"
 
 # The encoding of source files.
-#source_encoding = 'utf-8'
+# source_encoding = 'utf-8'
 
 # The master toctree document.
-master_doc = 'index'
+master_doc = "index"
 
 # General information about the project.
-project = 'Scrapy'
-copyright = f'2008{datetime.now().year}, Scrapy developers'
+project = "Scrapy"
+copyright = f"2008{datetime.now().year}, Scrapy developers"
 
 # The version info for the project you're documenting, acts as replacement for
 # |version| and |release|, also used in various other places throughout the
 # built documents.
 #
 # The short X.Y version.
 try:
     import scrapy
-    version = '.'.join(map(str, scrapy.version_info[:2]))
+
+    version = ".".join(map(str, scrapy.version_info[:2]))
     release = scrapy.__version__
 except ImportError:
-    version = ''
-    release = ''
+    version = ""
+    release = ""
 
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
-language = 'en'
+language = "en"
 
 # There are two options for replacing |today|: either, you set today to some
 # non-false value, then it is used:
-#today = ''
+# today = ''
 # Else, today_fmt is used as the format for a strftime call.
-#today_fmt = '%B %d, %Y'
+# today_fmt = '%B %d, %Y'
 
 # List of documents that shouldn't be included in the build.
-#unused_docs = []
+# unused_docs = []
 
-exclude_patterns = ['build']
+exclude_patterns = ["build"]
 
 # List of directories, relative to source directory, that shouldn't be searched
 # for source files.
-exclude_trees = ['.build']
+exclude_trees = [".build"]
 
 # The reST default role (used for this markup: `text`) to use for all documents.
-#default_role = None
+# default_role = None
 
 # If true, '()' will be appended to :func: etc. cross-reference text.
-#add_function_parentheses = True
+# add_function_parentheses = True
 
 # If true, the current module name will be prepended to all description
 # unit titles (such as .. function::).
-#add_module_names = True
+# add_module_names = True
 
 # If true, sectionauthor and moduleauthor directives will be shown in the
 # output. They are ignored by default.
-#show_authors = False
+# show_authors = False
 
 # The name of the Pygments (syntax highlighting) style to use.
-pygments_style = 'sphinx'
+pygments_style = "sphinx"
 
 # List of Sphinx warnings that will not be raised
-suppress_warnings = ['epub.unknown_project_files']
+suppress_warnings = ["epub.unknown_project_files"]
 
 
 # Options for HTML output
 # -----------------------
 
 # The theme to use for HTML and HTML Help pages.  See the documentation for
 # a list of builtin themes.
-html_theme = 'sphinx_rtd_theme'
+html_theme = "sphinx_rtd_theme"
 
 # Theme options are theme-specific and customize the look and feel of a theme
 # further.  For a list of options available for each theme, see the
 # documentation.
-#html_theme_options = {}
+# html_theme_options = {}
 
 # Add any paths that contain custom themes here, relative to this directory.
 # Add path to the RTD explicitly to robustify builds (otherwise might
 # fail in a clean Debian build env)
 import sphinx_rtd_theme
+
 html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]
 
 # The style sheet to use for HTML and HTML Help pages. A file of that name
 # must exist either in Sphinx' static/ path, or in one of the custom paths
 # given in html_static_path.
 # html_style = 'scrapydoc.css'
 
 # The name for this set of Sphinx documents.  If None, it defaults to
 # "<project> v<release> documentation".
-#html_title = None
+# html_title = None
 
 # A shorter title for the navigation bar.  Default is the same as html_title.
-#html_short_title = None
+# html_short_title = None
 
 # The name of an image file (relative to this directory) to place at the top
 # of the sidebar.
-#html_logo = None
+# html_logo = None
 
 # The name of an image file (within the static path) to use as favicon of the
 # docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
 # pixels large.
-#html_favicon = None
+# html_favicon = None
 
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named "default.css" will overwrite the builtin "default.css".
-html_static_path = ['_static']
+html_static_path = ["_static"]
 
 # If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
 # using the given strftime format.
-html_last_updated_fmt = '%b %d, %Y'
+html_last_updated_fmt = "%b %d, %Y"
 
 # Custom sidebar templates, maps document names to template names.
-#html_sidebars = {}
+# html_sidebars = {}
 
 # Additional templates that should be rendered to pages, maps page names to
 # template names.
-#html_additional_pages = {}
+# html_additional_pages = {}
 
 # If false, no module index is generated.
-#html_use_modindex = True
+# html_use_modindex = True
 
 # If false, no index is generated.
-#html_use_index = True
+# html_use_index = True
 
 # If true, the index is split into individual pages for each letter.
-#html_split_index = False
+# html_split_index = False
 
 # If true, the reST sources are included in the HTML build as _sources/<name>.
 html_copy_source = True
 
 # If true, an OpenSearch description file will be output, and all pages will
 # contain a <link> tag referring to it.  The value of this option must be the
 # base URL from which the finished HTML is served.
-#html_use_opensearch = ''
+# html_use_opensearch = ''
 
 # If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
-#html_file_suffix = ''
+# html_file_suffix = ''
 
 # Output file base name for HTML help builder.
-htmlhelp_basename = 'Scrapydoc'
+htmlhelp_basename = "Scrapydoc"
 
 html_css_files = [
-    'custom.css',
+    "custom.css",
 ]
 
 
 # Options for LaTeX output
 # ------------------------
 
 # The paper size ('letter' or 'a4').
-#latex_paper_size = 'letter'
+# latex_paper_size = 'letter'
 
 # The font size ('10pt', '11pt' or '12pt').
-#latex_font_size = '10pt'
+# latex_font_size = '10pt'
 
 # Grouping the document tree into LaTeX files. List of tuples
 # (source start file, target name, title, author, document class [howto/manual]).
 latex_documents = [
-  ('index', 'Scrapy.tex', 'Scrapy Documentation',
-   'Scrapy developers', 'manual'),
+    ("index", "Scrapy.tex", "Scrapy Documentation", "Scrapy developers", "manual"),
 ]
 
 # The name of an image file (relative to this directory) to place at the top of
 # the title page.
-#latex_logo = None
+# latex_logo = None
 
 # For "manual" documents, if this is true, then toplevel headings are parts,
 # not chapters.
-#latex_use_parts = False
+# latex_use_parts = False
 
 # Additional stuff for the LaTeX preamble.
-#latex_preamble = ''
+# latex_preamble = ''
 
 # Documents to append as an appendix to all manuals.
-#latex_appendices = []
+# latex_appendices = []
 
 # If false, no module index is generated.
-#latex_use_modindex = True
+# latex_use_modindex = True
 
 
 # Options for the linkcheck builder
 # ---------------------------------
 
 # A list of regular expressions that match URIs that should not be checked when
 # doing a linkcheck build.
 linkcheck_ignore = [
-    'http://localhost:\d+', 'http://hg.scrapy.org',
-    'http://directory.google.com/'
+    "http://localhost:\d+",
+    "http://hg.scrapy.org",
+    "http://directory.google.com/",
 ]
 
 
 # Options for the Coverage extension
 # ----------------------------------
 coverage_ignore_pyobjects = [
     # Contracts add_pre_hook and add_post_hook are not documented because
     # they should be transparent to contract developers, for whom pre_hook and
     # post_hook should be the actual concern.
-    r'\bContract\.add_(pre|post)_hook$',
-
+    r"\bContract\.add_(pre|post)_hook$",
     # ContractsManager is an internal class, developers are not expected to
     # interact with it directly in any way.
-    r'\bContractsManager\b$',
-
+    r"\bContractsManager\b$",
     # For default contracts we only want to document their general purpose in
     # their __init__ method, the methods they reimplement to achieve that purpose
     # should be irrelevant to developers using those contracts.
-    r'\w+Contract\.(adjust_request_args|(pre|post)_process)$',
-
+    r"\w+Contract\.(adjust_request_args|(pre|post)_process)$",
     # Methods of downloader middlewares are not documented, only the classes
     # themselves, since downloader middlewares are controlled through Scrapy
     # settings.
-    r'^scrapy\.downloadermiddlewares\.\w*?\.(\w*?Middleware|DownloaderStats)\.',
-
+    r"^scrapy\.downloadermiddlewares\.\w*?\.(\w*?Middleware|DownloaderStats)\.",
     # Base classes of downloader middlewares are implementation details that
     # are not meant for users.
-    r'^scrapy\.downloadermiddlewares\.\w*?\.Base\w*?Middleware',
-
+    r"^scrapy\.downloadermiddlewares\.\w*?\.Base\w*?Middleware",
     # Private exception used by the command-line interface implementation.
-    r'^scrapy\.exceptions\.UsageError',
-
+    r"^scrapy\.exceptions\.UsageError",
     # Methods of BaseItemExporter subclasses are only documented in
     # BaseItemExporter.
-    r'^scrapy\.exporters\.(?!BaseItemExporter\b)\w*?\.',
-
+    r"^scrapy\.exporters\.(?!BaseItemExporter\b)\w*?\.",
     # Extension behavior is only modified through settings. Methods of
     # extension classes, as well as helper functions, are implementation
     # details that are not documented.
-    r'^scrapy\.extensions\.[a-z]\w*?\.[A-Z]\w*?\.',  # methods
-    r'^scrapy\.extensions\.[a-z]\w*?\.[a-z]',  # helper functions
-
+    r"^scrapy\.extensions\.[a-z]\w*?\.[A-Z]\w*?\.",  # methods
+    r"^scrapy\.extensions\.[a-z]\w*?\.[a-z]",  # helper functions
     # Never documented before, and deprecated now.
-    r'^scrapy\.linkextractors\.FilteringLinkExtractor$',
-
+    r"^scrapy\.linkextractors\.FilteringLinkExtractor$",
     # Implementation detail of LxmlLinkExtractor
-    r'^scrapy\.linkextractors\.lxmlhtml\.LxmlParserLinkExtractor',
+    r"^scrapy\.linkextractors\.lxmlhtml\.LxmlParserLinkExtractor",
 ]
 
 
 # Options for the InterSphinx extension
 # -------------------------------------
 
 intersphinx_mapping = {
-    'attrs': ('https://www.attrs.org/en/stable/', None),
-    'coverage': ('https://coverage.readthedocs.io/en/stable', None),
-    'cryptography' : ('https://cryptography.io/en/latest/', None),
-    'cssselect': ('https://cssselect.readthedocs.io/en/latest', None),
-    'itemloaders': ('https://itemloaders.readthedocs.io/en/latest/', None),
-    'pytest': ('https://docs.pytest.org/en/latest', None),
-    'python': ('https://docs.python.org/3', None),
-    'sphinx': ('https://www.sphinx-doc.org/en/master', None),
-    'tox': ('https://tox.wiki/en/latest/', None),
-    'twisted': ('https://docs.twisted.org/en/stable/', None),
-    'twistedapi': ('https://docs.twisted.org/en/stable/api/', None),
-    'w3lib': ('https://w3lib.readthedocs.io/en/latest', None),
+    "attrs": ("https://www.attrs.org/en/stable/", None),
+    "coverage": ("https://coverage.readthedocs.io/en/stable", None),
+    "cryptography": ("https://cryptography.io/en/latest/", None),
+    "cssselect": ("https://cssselect.readthedocs.io/en/latest", None),
+    "itemloaders": ("https://itemloaders.readthedocs.io/en/latest/", None),
+    "pytest": ("https://docs.pytest.org/en/latest", None),
+    "python": ("https://docs.python.org/3", None),
+    "sphinx": ("https://www.sphinx-doc.org/en/master", None),
+    "tox": ("https://tox.wiki/en/latest/", None),
+    "twisted": ("https://docs.twisted.org/en/stable/", None),
+    "twistedapi": ("https://docs.twisted.org/en/stable/api/", None),
+    "w3lib": ("https://w3lib.readthedocs.io/en/latest", None),
 }
 intersphinx_disabled_reftypes = []
 
 
 # Options for sphinx-hoverxref options
 # ------------------------------------
 
@@ -310,20 +302,20 @@
     "hoverxref": "tooltip",
     "mod": "tooltip",
     "ref": "tooltip",
     "reqmeta": "tooltip",
     "setting": "tooltip",
     "signal": "tooltip",
 }
-hoverxref_roles = ['command', 'reqmeta', 'setting', 'signal']
+hoverxref_roles = ["command", "reqmeta", "setting", "signal"]
 
 
 def setup(app):
-    app.connect('autodoc-skip-member', maybe_skip_member)
+    app.connect("autodoc-skip-member", maybe_skip_member)
 
 
 def maybe_skip_member(app, what, name, obj, skip, options):
     if not skip:
         # autodocs was generating a text "alias of" for the following members
         # https://github.com/sphinx-doc/sphinx/issues/4422
-        return name in {'default_item_class', 'default_selector_class'}
+        return name in {"default_item_class", "default_selector_class"}
     return skip
```

### Comparing `Scrapy-2.7.1/docs/contributing.rst` & `Scrapy-2.8.0/docs/contributing.rst`

 * *Files 10% similar despite different names*

```diff
@@ -45,15 +45,15 @@
   well-known question
 
 * if you have a general question about Scrapy usage, please ask it at
   `Stack Overflow <https://stackoverflow.com/questions/tagged/scrapy>`__
   (use "scrapy" tag).
 
 * check the `open issues`_ to see if the issue has already been reported. If it
-  has, don't dismiss the report, but check the ticket history and comments. If 
+  has, don't dismiss the report, but check the ticket history and comments. If
   you have additional useful information, please leave a comment, or consider
   :ref:`sending a pull request <writing-patches>` with a fix.
 
 * search the `scrapy-users`_ list and `Scrapy subreddit`_ to see if it has
   been discussed there, or if you're not sure if what you're seeing is a bug.
   You can also ask in the ``#scrapy`` IRC channel.
 
@@ -165,24 +165,51 @@
 
 Coding style
 ============
 
 Please follow these coding conventions when writing code for inclusion in
 Scrapy:
 
-* Unless otherwise specified, follow :pep:`8`.
-
-* It's OK to use lines longer than 79 chars if it improves the code
-  readability.
+* We use `black <https://black.readthedocs.io/en/stable/>`_ for code formatting.
+  There is a hook in the pre-commit config
+  that will automatically format your code before every commit. You can also
+  run black manually with ``tox -e black``.
 
 * Don't put your name in the code you contribute; git provides enough
   metadata to identify author of the code.
   See https://help.github.com/en/github/using-git/setting-your-username-in-git for
   setup instructions.
 
+.. _scrapy-pre-commit:
+
+Pre-commit
+==========
+
+We use `pre-commit`_ to automatically address simple code issues before every
+commit.
+
+.. _pre-commit: https://pre-commit.com/
+
+After your create a local clone of your fork of the Scrapy repository:
+
+#.  `Install pre-commit <https://pre-commit.com/#installation>`_.
+
+#.  On the root of your local clone of the Scrapy repository, run the following
+    command:
+
+    .. code-block:: bash
+
+       pre-commit install
+
+Now pre-commit will check your changes every time you create a Git commit. Upon
+finding issues, pre-commit aborts your commit, and either fixes those issues
+automatically, or only reports them to you. If it fixes those issues
+automatically, creating your commit again should succeed. Otherwise, you may
+need to address the corresponding issues manually first.
+
 .. _documentation-policies:
 
 Documentation policies
 ======================
 
 For reference documentation of API members (classes, methods, etc.) use
 docstrings and make sure that the Sphinx documentation uses the
```

### Comparing `Scrapy-2.7.1/docs/faq.rst` & `Scrapy-2.8.0/docs/faq.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/index.rst` & `Scrapy-2.8.0/docs/index.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/intro/examples.rst` & `Scrapy-2.8.0/docs/intro/examples.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/intro/install.rst` & `Scrapy-2.8.0/docs/intro/install.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/intro/overview.rst` & `Scrapy-2.8.0/docs/intro/overview.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/intro/tutorial.rst` & `Scrapy-2.8.0/docs/intro/tutorial.rst`

 * *Files 1% similar despite different names*

```diff
@@ -81,14 +81,16 @@
 :class:`~scrapy.Spider` and define the initial requests to make,
 optionally how to follow links in the pages, and how to parse the downloaded
 page content to extract data.
 
 This is the code for our first Spider. Save it in a file named
 ``quotes_spider.py`` under the ``tutorial/spiders`` directory in your project::
 
+    from pathlib import Path
+
     import scrapy
 
 
     class QuotesSpider(scrapy.Spider):
         name = "quotes"
 
         def start_requests(self):
@@ -98,16 +100,15 @@
             ]
             for url in urls:
                 yield scrapy.Request(url=url, callback=self.parse)
 
         def parse(self, response):
             page = response.url.split("/")[-2]
             filename = f'quotes-{page}.html'
-            with open(filename, 'wb') as f:
-                f.write(response.body)
+            Path(filename).write_bytes(response.body)
             self.log(f'Saved file {filename}')
 
 
 As you can see, our Spider subclasses :class:`scrapy.Spider <scrapy.Spider>`
 and defines some attributes and methods:
 
 * :attr:`~scrapy.Spider.name`: identifies the Spider. It must be
@@ -174,29 +175,30 @@
 Instead of implementing a :meth:`~scrapy.Spider.start_requests` method
 that generates :class:`scrapy.Request <scrapy.Request>` objects from URLs,
 you can just define a :attr:`~scrapy.Spider.start_urls` class attribute
 with a list of URLs. This list will then be used by the default implementation
 of :meth:`~scrapy.Spider.start_requests` to create the initial requests
 for your spider::
 
+    from pathlib import Path
+
     import scrapy
 
 
     class QuotesSpider(scrapy.Spider):
         name = "quotes"
         start_urls = [
             'https://quotes.toscrape.com/page/1/',
             'https://quotes.toscrape.com/page/2/',
         ]
 
         def parse(self, response):
             page = response.url.split("/")[-2]
             filename = f'quotes-{page}.html'
-            with open(filename, 'wb') as f:
-                f.write(response.body)
+            Path(filename).write_bytes(response.body)
 
 The :meth:`~scrapy.Spider.parse` method will be called to handle each
 of the requests for those URLs, even though we haven't explicitly told Scrapy
 to do so. This happens because :meth:`~scrapy.Spider.parse` is Scrapy's
 default callback method, which is called for requests without an explicitly
 assigned callback.
```

### Comparing `Scrapy-2.7.1/docs/news.rst` & `Scrapy-2.8.0/docs/news.rst`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,228 @@
 .. _news:
 
 Release notes
 =============
 
+.. _release-2.8.0:
+
+Scrapy 2.8.0 (2023-02-02)
+-------------------------
+
+This is a maintenance release, with minor features, bug fixes, and cleanups.
+
+Deprecation removals
+~~~~~~~~~~~~~~~~~~~~
+
+-   The ``scrapy.utils.gz.read1`` function, deprecated in Scrapy 2.0, has now
+    been removed. Use the :meth:`~io.BufferedIOBase.read1` method of
+    :class:`~gzip.GzipFile` instead.
+    (:issue:`5719`)
+
+-   The ``scrapy.utils.python.to_native_str`` function, deprecated in Scrapy
+    2.0, has now been removed. Use :func:`scrapy.utils.python.to_unicode`
+    instead.
+    (:issue:`5719`)
+
+-   The ``scrapy.utils.python.MutableChain.next`` method, deprecated in Scrapy
+    2.0, has now been removed. Use
+    :meth:`~scrapy.utils.python.MutableChain.__next__` instead.
+    (:issue:`5719`)
+
+-   The ``scrapy.linkextractors.FilteringLinkExtractor`` class, deprecated
+    in Scrapy 2.0, has now been removed. Use
+    :class:`LinkExtractor <scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor>`
+    instead.
+    (:issue:`5720`)
+
+-   Support for using environment variables prefixed with ``SCRAPY_`` to
+    override settings, deprecated in Scrapy 2.0, has now been removed.
+    (:issue:`5724`)
+
+-   Support for the ``noconnect`` query string argument in proxy URLs,
+    deprecated in Scrapy 2.0, has now been removed. We expect proxies that used
+    to need it to work fine without it.
+    (:issue:`5731`)
+
+-   The ``scrapy.utils.python.retry_on_eintr`` function, deprecated in Scrapy
+    2.3, has now been removed.
+    (:issue:`5719`)
+
+-   The ``scrapy.utils.python.WeakKeyCache`` class, deprecated in Scrapy 2.4,
+    has now been removed.
+    (:issue:`5719`)
+
+
+Deprecations
+~~~~~~~~~~~~
+
+-   :exc:`scrapy.pipelines.images.NoimagesDrop` is now deprecated.
+    (:issue:`5368`, :issue:`5489`)
+
+-   :meth:`ImagesPipeline.convert_image
+    <scrapy.pipelines.images.ImagesPipeline.convert_image>` must now accept a
+    ``response_body`` parameter.
+    (:issue:`3055`, :issue:`3689`, :issue:`4753`)
+
+
+New features
+~~~~~~~~~~~~
+
+-   Applied black_ coding style to files generated with the
+    :command:`genspider` and :command:`startproject` commands.
+    (:issue:`5809`, :issue:`5814`)
+
+    .. _black: https://black.readthedocs.io/en/stable/
+
+-   :setting:`FEED_EXPORT_ENCODING` is now set to ``"utf-8"`` in the
+    ``settings.py`` file that the :command:`startproject` command generates.
+    With this value, JSON exports wont force the use of escape sequences for
+    non-ASCII characters.
+    (:issue:`5797`, :issue:`5800`)
+
+-   The :class:`~scrapy.extensions.memusage.MemoryUsage` extension now logs the
+    peak memory usage during checks, and the binary unit MiB is now used to
+    avoid confusion.
+    (:issue:`5717`, :issue:`5722`, :issue:`5727`)
+
+-   The ``callback`` parameter of :class:`~scrapy.http.Request` can now be set
+    to :func:`scrapy.http.request.NO_CALLBACK`, to distinguish it from
+    ``None``, as the latter indicates that the default spider callback
+    (:meth:`~scrapy.Spider.parse`) is to be used.
+    (:issue:`5798`)
+
+
+Bug fixes
+~~~~~~~~~
+
+-   Enabled unsafe legacy SSL renegotiation to fix access to some outdated
+    websites.
+    (:issue:`5491`, :issue:`5790`)
+
+-   Fixed STARTTLS-based email delivery not working with Twisted 21.2.0 and
+    better.
+    (:issue:`5386`, :issue:`5406`)
+
+-   Fixed the :meth:`finish_exporting` method of :ref:`item exporters
+    <topics-exporters>` not being called for empty files.
+    (:issue:`5537`, :issue:`5758`)
+
+-   Fixed HTTP/2 responses getting only the last value for a header when
+    multiple headers with the same name are received.
+    (:issue:`5777`)
+
+-   Fixed an exception raised by the :command:`shell` command on some cases
+    when :ref:`using asyncio <using-asyncio>`.
+    (:issue:`5740`, :issue:`5742`, :issue:`5748`, :issue:`5759`, :issue:`5760`,
+    :issue:`5771`)
+
+-   When using :class:`~scrapy.spiders.CrawlSpider`, callback keyword arguments
+    (``cb_kwargs``) added to a request in the ``process_request`` callback of a
+    :class:`~scrapy.spiders.Rule` will no longer be ignored.
+    (:issue:`5699`)
+
+-   The :ref:`images pipeline <images-pipeline>` no longer re-encodes JPEG
+    files.
+    (:issue:`3055`, :issue:`3689`, :issue:`4753`)
+
+-   Fixed the handling of transparent WebP images by the :ref:`images pipeline
+    <images-pipeline>`.
+    (:issue:`3072`, :issue:`5766`, :issue:`5767`)
+
+-   :func:`scrapy.shell.inspect_response` no longer inhibits ``SIGINT``
+    (Ctrl+C).
+    (:issue:`2918`)
+
+-   :class:`LinkExtractor <scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor>`
+    with ``unique=False`` no longer filters out links that have identical URL
+    *and* text.
+    (:issue:`3798`, :issue:`3799`, :issue:`4695`, :issue:`5458`)
+
+-   :class:`~scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware` now
+    ignores URL protocols that do not support ``robots.txt`` (``data://``,
+    ``file://``).
+    (:issue:`5807`)
+
+-   Silenced the ``filelock`` debug log messages introduced in Scrapy 2.6.
+    (:issue:`5753`, :issue:`5754`)
+
+-   Fixed the output of ``scrapy -h`` showing an unintended ``**commands**``
+    line.
+    (:issue:`5709`, :issue:`5711`, :issue:`5712`)
+
+-   Made the active project indication in the output of :ref:`commands
+    <topics-commands>` more clear.
+    (:issue:`5715`)
+
+
+Documentation
+~~~~~~~~~~~~~
+
+-   Documented how to :ref:`debug spiders from Visual Studio Code
+    <debug-vscode>`.
+    (:issue:`5721`)
+
+-   Documented how :setting:`DOWNLOAD_DELAY` affects per-domain concurrency.
+    (:issue:`5083`, :issue:`5540`)
+
+-   Improved consistency.
+    (:issue:`5761`)
+
+-   Fixed typos.
+    (:issue:`5714`, :issue:`5744`, :issue:`5764`)
+
+
+Quality assurance
+~~~~~~~~~~~~~~~~~
+
+-   Applied :ref:`black coding style <coding-style>`, sorted import statements,
+    and introduced :ref:`pre-commit <scrapy-pre-commit>`.
+    (:issue:`4654`, :issue:`4658`, :issue:`5734`, :issue:`5737`, :issue:`5806`,
+    :issue:`5810`)
+
+-   Switched from :mod:`os.path` to :mod:`pathlib`.
+    (:issue:`4916`, :issue:`4497`, :issue:`5682`)
+
+-   Addressed many issues reported by Pylint.
+    (:issue:`5677`)
+
+-   Improved code readability.
+    (:issue:`5736`)
+
+-   Improved package metadata.
+    (:issue:`5768`)
+
+-   Removed direct invocations of ``setup.py``.
+    (:issue:`5774`, :issue:`5776`)
+
+-   Removed unnecessary :class:`~collections.OrderedDict` usages.
+    (:issue:`5795`)
+
+-   Removed unnecessary ``__str__`` definitions.
+    (:issue:`5150`)
+
+-   Removed obsolete code and comments.
+    (:issue:`5725`, :issue:`5729`, :issue:`5730`, :issue:`5732`)
+
+-   Fixed test and CI issues.
+    (:issue:`5749`, :issue:`5750`, :issue:`5756`, :issue:`5762`, :issue:`5765`,
+    :issue:`5780`, :issue:`5781`, :issue:`5782`, :issue:`5783`, :issue:`5785`,
+    :issue:`5786`)
+
+
 .. _release-2.7.1:
 
 Scrapy 2.7.1 (2022-11-02)
 -------------------------
 
 New features
 ~~~~~~~~~~~~
 
 -   Relaxed the restriction introduced in 2.6.2 so that the
-    ``Proxy-Authentication`` header can again be set explicitly, as long as the
+    ``Proxy-Authorization`` header can again be set explicitly, as long as the
     proxy URL in the :reqmeta:`proxy` metadata has no other credentials, and
     for as long as that proxy URL remains the same; this restores compatibility
     with scrapy-zyte-smartproxy 2.1.0 and older (:issue:`5626`).
 
 Bug fixes
 ~~~~~~~~~
 
@@ -277,28 +483,28 @@
 
 **Security bug fix:**
 
 -   When :class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware`
     processes a request with :reqmeta:`proxy` metadata, and that
     :reqmeta:`proxy` metadata includes proxy credentials,
     :class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` sets
-    the ``Proxy-Authentication`` header, but only if that header is not already
+    the ``Proxy-Authorization`` header, but only if that header is not already
     set.
 
     There are third-party proxy-rotation downloader middlewares that set
     different :reqmeta:`proxy` metadata every time they process a request.
 
     Because of request retries and redirects, the same request can be processed
     by downloader middlewares more than once, including both
     :class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` and
     any third-party proxy-rotation downloader middleware.
 
     These third-party proxy-rotation downloader middlewares could change the
     :reqmeta:`proxy` metadata of a request to a new value, but fail to remove
-    the ``Proxy-Authentication`` header from the previous value of the
+    the ``Proxy-Authorization`` header from the previous value of the
     :reqmeta:`proxy` metadata, causing the credentials of one proxy to be sent
     to a different proxy.
 
     To prevent the unintended leaking of proxy credentials, the behavior of
     :class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` is now
     as follows when processing a request:
 
@@ -2246,28 +2452,28 @@
 
 **Security bug fix:**
 
 -   When :class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware`
     processes a request with :reqmeta:`proxy` metadata, and that
     :reqmeta:`proxy` metadata includes proxy credentials,
     :class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` sets
-    the ``Proxy-Authentication`` header, but only if that header is not already
+    the ``Proxy-Authorization`` header, but only if that header is not already
     set.
 
     There are third-party proxy-rotation downloader middlewares that set
     different :reqmeta:`proxy` metadata every time they process a request.
 
     Because of request retries and redirects, the same request can be processed
     by downloader middlewares more than once, including both
     :class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` and
     any third-party proxy-rotation downloader middleware.
 
     These third-party proxy-rotation downloader middlewares could change the
     :reqmeta:`proxy` metadata of a request to a new value, but fail to remove
-    the ``Proxy-Authentication`` header from the previous value of the
+    the ``Proxy-Authorization`` header from the previous value of the
     :reqmeta:`proxy` metadata, causing the credentials of one proxy to be sent
     to a different proxy.
 
     To prevent the unintended leaking of proxy credentials, the behavior of
     :class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` is now
     as follows when processing a request:
 
@@ -4696,15 +4902,15 @@
 - Fix wrong checks on subclassing of deprecated classes. closes #581 (:commit:`46d98d6`)
 - Docs: 4-space indent for final spider example (:commit:`13846de`)
 - Fix HtmlParserLinkExtractor and tests after #485 merge (:commit:`368a946`)
 - BaseSgmlLinkExtractor: Fixed the missing space when the link has an inner tag (:commit:`b566388`)
 - BaseSgmlLinkExtractor: Added unit test of a link with an inner tag (:commit:`c1cb418`)
 - BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set current_link=None when the end tag match the opening tag (:commit:`7e4d627`)
 - Fix tests for Travis-CI build (:commit:`76c7e20`)
-- replace unencodable codepoints with html entities. fixes #562 and #285 (:commit:`5f87b17`)
+- replace unencodeable codepoints with html entities. fixes #562 and #285 (:commit:`5f87b17`)
 - RegexLinkExtractor: encode URL unicode value when creating Links (:commit:`d0ee545`)
 - Updated the tutorial crawl output with latest output. (:commit:`8da65de`)
 - Updated shell docs with the crawler reference and fixed the actual shell output. (:commit:`875b9ab`)
 - PEP8 minor edits. (:commit:`f89efaf`)
 - Expose current crawler in the Scrapy shell. (:commit:`5349cec`)
 - Unused re import and PEP8 minor edits. (:commit:`387f414`)
 - Ignore None's values when using the ItemLoader. (:commit:`0632546`)
@@ -4721,15 +4927,15 @@
 
 Enhancements
 ~~~~~~~~~~~~
 
 - [**Backward incompatible**] Switched HTTPCacheMiddleware backend to filesystem (:issue:`541`)
   To restore old backend set ``HTTPCACHE_STORAGE`` to ``scrapy.contrib.httpcache.DbmCacheStorage``
 - Proxy \https:// urls using CONNECT method (:issue:`392`, :issue:`397`)
-- Add a middleware to crawl ajax crawleable pages as defined by google (:issue:`343`)
+- Add a middleware to crawl ajax crawlable pages as defined by google (:issue:`343`)
 - Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (:issue:`510`, :issue:`519`)
 - Selectors register EXSLT namespaces by default (:issue:`472`)
 - Unify item loaders similar to selectors renaming (:issue:`461`)
 - Make ``RFPDupeFilter`` class easily subclassable (:issue:`533`)
 - Improve test coverage and forthcoming Python 3 support (:issue:`525`)
 - Promote startup info on settings and middleware to INFO level (:issue:`520`)
 - Support partials in ``get_func_args`` util (:issue:`506`, issue:`504`)
@@ -4901,15 +5107,15 @@
 - minor updates to 0.18 release notes (:commit:`c45e5f1`)
 - fix contributors list format (:commit:`0b60031`)
 
 Scrapy 0.18.0 (released 2013-08-09)
 -----------------------------------
 
 - Lot of improvements to testsuite run using Tox, including a way to test on pypi
-- Handle GET parameters for AJAX crawleable urls (:commit:`3fe2a32`)
+- Handle GET parameters for AJAX crawlable urls (:commit:`3fe2a32`)
 - Use lxml recover option to parse sitemaps (:issue:`347`)
 - Bugfix cookie merging by hostname and not by netloc (:issue:`352`)
 - Support disabling ``HttpCompressionMiddleware`` using a flag setting (:issue:`359`)
 - Support xml namespaces using ``iternodes`` parser in ``XMLFeedSpider`` (:issue:`12`)
 - Support ``dont_cache`` request meta flag (:issue:`19`)
 - Bugfix ``scrapy.utils.gz.gunzip`` broken by changes in python 2.7.4 (:commit:`4dc76e`)
 - Bugfix url encoding on ``SgmlLinkExtractor`` (:issue:`24`)
@@ -4935,16 +5141,16 @@
 - Add ``ftp://`` scheme downloader handler (:issue:`329`)
 - Added downloader benchmark webserver and spider tools :ref:`benchmarking`
 - Moved persistent (on disk) queues to a separate project (queuelib_) which Scrapy now depends on
 - Add Scrapy commands using external libraries (:issue:`260`)
 - Added ``--pdb`` option to ``scrapy`` command line tool
 - Added :meth:`XPathSelector.remove_namespaces <scrapy.selector.Selector.remove_namespaces>` which allows to remove all namespaces from XML documents for convenience (to work with namespace-less XPaths). Documented in :ref:`topics-selectors`.
 - Several improvements to spider contracts
-- New default middleware named MetaRefreshMiddldeware that handles meta-refresh html tag redirections,
-- MetaRefreshMiddldeware and RedirectMiddleware have different priorities to address #62
+- New default middleware named MetaRefreshMiddleware that handles meta-refresh html tag redirections,
+- MetaRefreshMiddleware and RedirectMiddleware have different priorities to address #62
 - added from_crawler method to spiders
 - added system tests with mock server
 - more improvements to macOS compatibility (thanks Alex Cepoi)
 - several more cleanups to singletons and multi-spider support (thanks Nicolas Ramirez)
 - support custom download slots
 - added --spider option to "shell" command.
 - log overridden settings when Scrapy starts
@@ -5078,15 +5284,15 @@
 - removed per-spider settings (to be replaced by instantiating multiple crawler objects)
 - ``USER_AGENT`` spider attribute will no longer work, use ``user_agent`` attribute instead
 - ``DOWNLOAD_TIMEOUT`` spider attribute will no longer work, use ``download_timeout`` attribute instead
 - removed ``ENCODING_ALIASES`` setting, as encoding auto-detection has been moved to the `w3lib`_ library
 - promoted :ref:`topics-djangoitem` to main contrib
 - LogFormatter method now return dicts(instead of strings) to support lazy formatting (:issue:`164`, :commit:`dcef7b0`)
 - downloader handlers (:setting:`DOWNLOAD_HANDLERS` setting) now receive settings as the first argument of the ``__init__`` method
-- replaced memory usage acounting with (more portable) `resource`_ module, removed ``scrapy.utils.memory`` module
+- replaced memory usage accounting with (more portable) `resource`_ module, removed ``scrapy.utils.memory`` module
 - removed signal: ``scrapy.mail.mail_sent``
 - removed ``TRACK_REFS`` setting, now :ref:`trackrefs <topics-leaks-trackrefs>` is always enabled
 - DBM is now the default storage backend for HTTP cache middleware
 - number of log messages (per level) are now tracked through Scrapy stats (stat name: ``log_count/LEVEL``)
 - number received responses are now tracked through Scrapy stats (stat name: ``response_received_count``)
 - removed ``scrapy.log.started`` attribute
 
@@ -5144,15 +5350,15 @@
 
 Scrapy 0.14
 -----------
 
 New features and settings
 ~~~~~~~~~~~~~~~~~~~~~~~~~
 
-- Support for `AJAX crawleable urls`_
+- Support for `AJAX crawlable urls`_
 - New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (:rev:`2737`)
 - added ``-o`` option to ``scrapy crawl``, a shortcut for dumping scraped items into a file (or standard output using ``-``)
 - Added support for passing custom settings to Scrapyd ``schedule.json`` api (:rev:`2779`, :rev:`2783`)
 - New ``ChunkedTransferMiddleware`` (enabled by default) to support `chunked transfer encoding`_ (:rev:`2769`)
 - Add boto 2.0 support for S3 downloader handler (:rev:`2763`)
 - Added `marshal`_ to formats supported by feed exports (:rev:`2744`)
 - In request errbacks, offending requests are now received in ``failure.request`` attribute (:rev:`2738`)
@@ -5404,26 +5610,26 @@
    - Its settings were also renamed:
       - ``CLOSEDOMAIN_TIMEOUT`` to ``CLOSESPIDER_TIMEOUT``
       - ``CLOSEDOMAIN_ITEMCOUNT`` to ``CLOSESPIDER_ITEMCOUNT``
 - Removed deprecated ``SCRAPYSETTINGS_MODULE`` environment variable - use ``SCRAPY_SETTINGS_MODULE`` instead (:rev:`1840`)
 - Renamed setting: ``REQUESTS_PER_DOMAIN`` to ``CONCURRENT_REQUESTS_PER_SPIDER`` (:rev:`1830`, :rev:`1844`)
 - Renamed setting: ``CONCURRENT_DOMAINS`` to ``CONCURRENT_SPIDERS`` (:rev:`1830`)
 - Refactored HTTP Cache middleware
-- HTTP Cache middleware has been heavilty refactored, retaining the same functionality except for the domain sectorization which was removed. (:rev:`1843` )
+- HTTP Cache middleware has been heavily refactored, retaining the same functionality except for the domain sectorization which was removed. (:rev:`1843` )
 - Renamed exception: ``DontCloseDomain`` to ``DontCloseSpider`` (:rev:`1859` | #120)
 - Renamed extension: ``DelayedCloseDomain`` to ``SpiderCloseDelay`` (:rev:`1861` | #121)
 - Removed obsolete ``scrapy.utils.markup.remove_escape_chars`` function - use ``scrapy.utils.markup.replace_escape_chars`` instead (:rev:`1865`)
 
 Scrapy 0.7
 ----------
 
 First release of Scrapy.
 
 
-.. _AJAX crawleable urls: https://developers.google.com/search/docs/ajax-crawling/docs/getting-started?csw=1
+.. _AJAX crawlable urls: https://developers.google.com/search/docs/ajax-crawling/docs/getting-started?csw=1
 .. _botocore: https://github.com/boto/botocore
 .. _chunked transfer encoding: https://en.wikipedia.org/wiki/Chunked_transfer_encoding
 .. _ClientForm: http://wwwsearch.sourceforge.net/old/ClientForm/
 .. _Creating a pull request: https://help.github.com/en/articles/creating-a-pull-request
 .. _cryptography: https://cryptography.io/en/latest/
 .. _docstrings: https://docs.python.org/3/glossary.html#term-docstring
 .. _KeyboardInterrupt: https://docs.python.org/3/library/exceptions.html#KeyboardInterrupt
```

### Comparing `Scrapy-2.7.1/docs/topics/_images/inspector_01.png` & `Scrapy-2.8.0/docs/topics/_images/inspector_01.png`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/_images/network_01.png` & `Scrapy-2.8.0/docs/topics/_images/network_01.png`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/_images/network_02.png` & `Scrapy-2.8.0/docs/topics/_images/network_02.png`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/_images/network_03.png` & `Scrapy-2.8.0/docs/topics/_images/network_03.png`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/_images/scrapy_architecture.odg` & `Scrapy-2.8.0/docs/topics/_images/scrapy_architecture.odg`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/_images/scrapy_architecture.png` & `Scrapy-2.8.0/docs/topics/_images/scrapy_architecture.png`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/_images/scrapy_architecture_02.png` & `Scrapy-2.8.0/docs/topics/_images/scrapy_architecture_02.png`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/api.rst` & `Scrapy-2.8.0/docs/topics/api.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/architecture.rst` & `Scrapy-2.8.0/docs/topics/architecture.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/asyncio.rst` & `Scrapy-2.8.0/docs/topics/asyncio.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/autothrottle.rst` & `Scrapy-2.8.0/docs/topics/autothrottle.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/benchmarking.rst` & `Scrapy-2.8.0/docs/topics/benchmarking.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/broad-crawls.rst` & `Scrapy-2.8.0/docs/topics/broad-crawls.rst`

 * *Files 1% similar despite different names*

```diff
@@ -64,15 +64,15 @@
 
 .. note:: The scheduler priority queue :ref:`recommended for broad crawls
           <broad-crawls-scheduler-priority-queue>` does not support
           :setting:`CONCURRENT_REQUESTS_PER_IP`.
 
 The default global concurrency limit in Scrapy is not suitable for crawling
 many different domains in parallel, so you will want to increase it. How much
-to increase it will depend on how much CPU and memory you crawler will have
+to increase it will depend on how much CPU and memory your crawler will have
 available.
 
 A good starting point is ``100``::
 
     CONCURRENT_REQUESTS = 100
 
 But the best way to find out is by doing some trials and identifying at what
```

### Comparing `Scrapy-2.7.1/docs/topics/commands.rst` & `Scrapy-2.8.0/docs/topics/commands.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/components.rst` & `Scrapy-2.8.0/docs/topics/components.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/contracts.rst` & `Scrapy-2.8.0/docs/topics/contracts.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/coroutines.rst` & `Scrapy-2.8.0/docs/topics/coroutines.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/debug.rst` & `Scrapy-2.8.0/docs/topics/debug.rst`

 * *Files 5% similar despite different names*

```diff
@@ -15,15 +15,15 @@
         start_urls = (
             'http://example.com/page1',
             'http://example.com/page2',
             )
 
         def parse(self, response):
             # <processing code not shown>
-            # collect `item_urls` 
+            # collect `item_urls`
             for item_url in item_urls:
                 yield scrapy.Request(item_url, self.parse_item)
 
         def parse_item(self, response):
             # <processing code not shown>
             item = MyItem()
             # populate `item` fields
@@ -146,7 +146,37 @@
             return item
         else:
             self.logger.warning('No item received for %s', response.url)
 
 For more information, check the :ref:`topics-logging` section.
 
 .. _base tag: https://www.w3schools.com/tags/tag_base.asp
+
+.. _debug-vscode:
+
+Visual Studio Code
+==================
+
+.. highlight:: json
+
+To debug spiders with Visual Studio Code you can use the following ``launch.json``::
+
+    {
+        "version": "0.1.0",
+        "configurations": [
+            {
+                "name": "Python: Launch Scrapy Spider",
+                "type": "python",
+                "request": "launch",
+                "module": "scrapy",
+                "args": [
+                    "runspider",
+                    "${file}"
+                ],
+                "console": "integratedTerminal"
+            }
+        ]
+    }
+
+
+Also, make sure you enable "User Uncaught Exceptions", to catch exceptions in
+your Scrapy spider.
```

### Comparing `Scrapy-2.7.1/docs/topics/deploy.rst` & `Scrapy-2.8.0/docs/topics/deploy.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/developer-tools.rst` & `Scrapy-2.8.0/docs/topics/developer-tools.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/downloader-middleware.rst` & `Scrapy-2.8.0/docs/topics/downloader-middleware.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/dynamic-content.rst` & `Scrapy-2.8.0/docs/topics/dynamic-content.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/email.rst` & `Scrapy-2.8.0/docs/topics/email.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/exceptions.rst` & `Scrapy-2.8.0/docs/topics/exceptions.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/exporters.rst` & `Scrapy-2.8.0/docs/topics/exporters.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/extensions.rst` & `Scrapy-2.8.0/docs/topics/extensions.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/feed-exports.rst` & `Scrapy-2.8.0/docs/topics/feed-exports.rst`

 * *Files 1% similar despite different names*

```diff
@@ -511,14 +511,18 @@
 The encoding to be used for the feed.
 
 If unset or set to ``None`` (default) it uses UTF-8 for everything except JSON output,
 which uses safe numeric encoding (``\uXXXX`` sequences) for historic reasons.
 
 Use ``utf-8`` if you want UTF-8 for JSON too.
 
+.. versionchanged:: 2.8
+   The :command:`startproject` command now sets this setting to
+   ``utf-8`` in the generated ``settings.py`` file.
+
 .. setting:: FEED_EXPORT_FIELDS
 
 FEED_EXPORT_FIELDS
 ------------------
 
 Default: ``None``
```

### Comparing `Scrapy-2.7.1/docs/topics/item-pipeline.rst` & `Scrapy-2.8.0/docs/topics/item-pipeline.rst`

 * *Files 3% similar despite different names*

```diff
@@ -182,44 +182,47 @@
 render a screenshot of the item URL. After the request response is downloaded,
 the item pipeline saves the screenshot to a file and adds the filename to the
 item.
 
 ::
 
     import hashlib
+    from pathlib import Path
     from urllib.parse import quote
 
     import scrapy
     from itemadapter import ItemAdapter
+    from scrapy.http.request import NO_CALLBACK
     from scrapy.utils.defer import maybe_deferred_to_future
 
 
     class ScreenshotPipeline:
         """Pipeline that uses Splash to render screenshot of
         every Scrapy item."""
 
         SPLASH_URL = "http://localhost:8050/render.png?url={}"
 
         async def process_item(self, item, spider):
             adapter = ItemAdapter(item)
             encoded_item_url = quote(adapter["url"])
             screenshot_url = self.SPLASH_URL.format(encoded_item_url)
-            request = scrapy.Request(screenshot_url)
-            response = await maybe_deferred_to_future(spider.crawler.engine.download(request, spider))
+            request = scrapy.Request(screenshot_url, callback=NO_CALLBACK)
+            response = await maybe_deferred_to_future(
+                spider.crawler.engine.download(request, spider)
+            )
 
             if response.status != 200:
                 # Error happened, return item.
                 return item
 
             # Save screenshot to file, filename will be hash of url.
             url = adapter["url"]
             url_hash = hashlib.md5(url.encode("utf8")).hexdigest()
             filename = f"{url_hash}.png"
-            with open(filename, "wb") as f:
-                f.write(response.body)
+            Path(filename).write_bytes(response.body)
 
             # Store filename in item.
             adapter["screenshot_filename"] = filename
             return item
 
 .. _Splash: https://splash.readthedocs.io/en/stable/
```

### Comparing `Scrapy-2.7.1/docs/topics/items.rst` & `Scrapy-2.8.0/docs/topics/items.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/jobs.rst` & `Scrapy-2.8.0/docs/topics/jobs.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/leaks.rst` & `Scrapy-2.8.0/docs/topics/leaks.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/link-extractors.rst` & `Scrapy-2.8.0/docs/topics/link-extractors.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/loaders.rst` & `Scrapy-2.8.0/docs/topics/loaders.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/logging.rst` & `Scrapy-2.8.0/docs/topics/logging.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/media-pipeline.rst` & `Scrapy-2.8.0/docs/topics/media-pipeline.rst`

 * *Files 1% similar despite different names*

```diff
@@ -152,15 +152,14 @@
   00b08510e4_front.jpg
 
 By overriding ``file_path`` like this:
 
 .. code-block:: python
 
   import hashlib
-  from os.path import splitext
 
   def file_path(self, request, response=None, info=None, *, item=None):
       image_url_hash = hashlib.shake_256(request.url.encode()).hexdigest(5)
       image_perspective = request.url.split('/')[-2]
       image_filename = f'{image_url_hash}_{image_perspective}.jpg'
 
       return image_filename
@@ -494,23 +493,23 @@
       You can override this method to customize the download path of each file.
 
       For example, if file URLs end like regular paths (e.g.
       ``https://example.com/a/b/c/foo.png``), you can use the following
       approach to download all files into the ``files`` folder with their
       original filenames (e.g. ``files/foo.png``)::
 
-        import os
+        from pathlib import PurePosixPath
         from urllib.parse import urlparse
 
         from scrapy.pipelines.files import FilesPipeline
 
         class MyFilesPipeline(FilesPipeline):
 
             def file_path(self, request, response=None, info=None, *, item=None):
-                return 'files/' + os.path.basename(urlparse(request.url).path)
+                return 'files/' + PurePosixPath(urlparse(request.url).path).name
 
       Similarly, you can use the ``item`` to determine the file path based on some item 
       property.
       
       By default the :meth:`file_path` method returns
       ``full/<request URL hash>.<extension>``.
 
@@ -633,23 +632,23 @@
       You can override this method to customize the download path of each file.
 
       For example, if file URLs end like regular paths (e.g.
       ``https://example.com/a/b/c/foo.png``), you can use the following
       approach to download all files into the ``files`` folder with their
       original filenames (e.g. ``files/foo.png``)::
 
-        import os
+        from pathlib import PurePosixPath
         from urllib.parse import urlparse
 
         from scrapy.pipelines.images import ImagesPipeline
 
         class MyImagesPipeline(ImagesPipeline):
 
             def file_path(self, request, response=None, info=None, *, item=None):
-                return 'files/' + os.path.basename(urlparse(request.url).path)
+                return 'files/' + PurePosixPath(urlparse(request.url).path).name
 
       Similarly, you can use the ``item`` to determine the file path based on some item 
       property.
       
       By default the :meth:`file_path` method returns
       ``full/<request URL hash>.<extension>``.
```

### Comparing `Scrapy-2.7.1/docs/topics/practices.rst` & `Scrapy-2.8.0/docs/topics/practices.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/request-response.rst` & `Scrapy-2.8.0/docs/topics/request-response.rst`

 * *Files 1% similar despite different names*

```diff
@@ -28,19 +28,28 @@
 
     :param url: the URL of this request
 
         If the URL is invalid, a :exc:`ValueError` exception is raised.
     :type url: str
 
     :param callback: the function that will be called with the response of this
-       request (once it's downloaded) as its first parameter. For more information
-       see :ref:`topics-request-response-ref-request-callback-arguments` below.
-       If a Request doesn't specify a callback, the spider's
-       :meth:`~scrapy.Spider.parse` method will be used.
-       Note that if exceptions are raised during processing, errback is called instead.
+       request (once it's downloaded) as its first parameter.
+
+       In addition to a function, the following values are supported:
+
+       -   ``None`` (default), which indicates that the spider's
+           :meth:`~scrapy.Spider.parse` method must be used.
+
+       -   :func:`~scrapy.http.request.NO_CALLBACK`
+
+       For more information, see
+       :ref:`topics-request-response-ref-request-callback-arguments`.
+
+       .. note:: If exceptions are raised during processing, ``errback`` is
+                 called instead.
 
     :type callback: collections.abc.Callable
 
     :param method: the HTTP method of this request. Defaults to ``'GET'``.
     :type method: str
 
     :param meta: the initial values for the :attr:`Request.meta` attribute. If
@@ -65,24 +74,32 @@
 
     :type headers: dict
 
     :param cookies: the request cookies. These can be sent in two forms.
 
         1. Using a dict::
 
-            request_with_cookies = Request(url="http://www.example.com",
-                                           cookies={'currency': 'USD', 'country': 'UY'})
+            request_with_cookies = Request(
+                url="http://www.example.com",
+                cookies={'currency': 'USD', 'country': 'UY'},
+            )
 
         2. Using a list of dicts::
 
-            request_with_cookies = Request(url="http://www.example.com",
-                                           cookies=[{'name': 'currency',
-                                                    'value': 'USD',
-                                                    'domain': 'example.com',
-                                                    'path': '/currency'}])
+            request_with_cookies = Request(
+                url="http://www.example.com",
+                cookies=[
+                    {
+                        'name': 'currency',
+                        'value': 'USD',
+                        'domain': 'example.com',
+                        'path': '/currency',
+                    },
+                ],
+            )
 
         The latter form allows for customizing the ``domain`` and ``path``
         attributes of the cookie. This is only useful if the cookies are saved
         for later requests.
 
         .. reqmeta:: dont_merge_cookies
 
@@ -224,14 +241,16 @@
 
     .. automethod:: to_dict
 
 
 Other functions related to requests
 -----------------------------------
 
+.. autofunction:: scrapy.http.request.NO_CALLBACK
+
 .. autofunction:: scrapy.utils.request.request_from_dict
 
 
 .. _topics-request-response-ref-request-callback-arguments:
 
 Passing additional data to callback functions
 ---------------------------------------------
```

### Comparing `Scrapy-2.7.1/docs/topics/scheduler.rst` & `Scrapy-2.8.0/docs/topics/scheduler.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/selectors.rst` & `Scrapy-2.8.0/docs/topics/selectors.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/settings.rst` & `Scrapy-2.8.0/docs/topics/settings.rst`

 * *Files 1% similar despite different names*

```diff
@@ -632,32 +632,49 @@
 .. setting:: DOWNLOAD_DELAY
 
 DOWNLOAD_DELAY
 --------------
 
 Default: ``0``
 
-The amount of time (in secs) that the downloader should wait before downloading
-consecutive pages from the same website. This can be used to throttle the
-crawling speed to avoid hitting servers too hard. Decimal numbers are
-supported.  Example::
+Minimum seconds to wait between 2 consecutive requests to the same domain.
 
-    DOWNLOAD_DELAY = 0.25    # 250 ms of delay
+Use :setting:`DOWNLOAD_DELAY` to throttle your crawling speed, to avoid hitting
+servers too hard.
+
+Decimal numbers are supported. For example, to send a maximum of 4 requests
+every 10 seconds::
+
+    DOWNLOAD_DELAY = 2.5
 
 This setting is also affected by the :setting:`RANDOMIZE_DOWNLOAD_DELAY`
-setting (which is enabled by default). By default, Scrapy doesn't wait a fixed
-amount of time between requests, but uses a random interval between 0.5 * :setting:`DOWNLOAD_DELAY` and 1.5 * :setting:`DOWNLOAD_DELAY`.
+setting, which is enabled by default.
 
 When :setting:`CONCURRENT_REQUESTS_PER_IP` is non-zero, delays are enforced
-per ip address instead of per domain.
+per IP address instead of per domain.
+
+Note that :setting:`DOWNLOAD_DELAY` can lower the effective per-domain
+concurrency below :setting:`CONCURRENT_REQUESTS_PER_DOMAIN`. If the response
+time of a domain is lower than :setting:`DOWNLOAD_DELAY`, the effective
+concurrency for that domain is 1. When testing throttling configurations, it
+usually makes sense to lower :setting:`CONCURRENT_REQUESTS_PER_DOMAIN` first,
+and only increase :setting:`DOWNLOAD_DELAY` once
+:setting:`CONCURRENT_REQUESTS_PER_DOMAIN` is 1 but a higher throttling is
+desired.
 
 .. _spider-download_delay-attribute:
 
-You can also change this setting per spider by setting ``download_delay``
-spider attribute.
+.. note::
+
+    This delay can be set per spider using :attr:`download_delay` spider attribute.
+
+It is also possible to change this setting per domain, although it requires
+non-trivial code. See the implementation of the :ref:`AutoThrottle
+<topics-autothrottle>` extension for an example.
+
 
 .. setting:: DOWNLOAD_HANDLERS
 
 DOWNLOAD_HANDLERS
 -----------------
 
 Default: ``{}``
```

### Comparing `Scrapy-2.7.1/docs/topics/shell.rst` & `Scrapy-2.8.0/docs/topics/shell.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/signals.rst` & `Scrapy-2.8.0/docs/topics/signals.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/spider-middleware.rst` & `Scrapy-2.8.0/docs/topics/spider-middleware.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/spiders.rst` & `Scrapy-2.8.0/docs/topics/spiders.rst`

 * *Files 0% similar despite different names*

```diff
@@ -95,15 +95,15 @@
 
       For a list of available built-in settings see:
       :ref:`topics-settings-ref`.
 
    .. attribute:: crawler
 
       This attribute is set by the :meth:`from_crawler` class method after
-      initializating the class, and links to the
+      initializing the class, and links to the
       :class:`~scrapy.crawler.Crawler` object to which this spider instance is
       bound.
 
       Crawlers encapsulate a lot of components in the project for their single
       entry access (such as extensions, middlewares, signals managers, etc).
       See :ref:`topics-api-crawler` to know more about them.
```

### Comparing `Scrapy-2.7.1/docs/topics/stats.rst` & `Scrapy-2.8.0/docs/topics/stats.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/topics/telnetconsole.rst` & `Scrapy-2.8.0/docs/topics/telnetconsole.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/docs/versioning.rst` & `Scrapy-2.8.0/docs/versioning.rst`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/extras/qps-bench-server.py` & `Scrapy-2.8.0/extras/qps-bench-server.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 #!/usr/bin/env python
-from time import time
 from collections import deque
-from twisted.web.server import Site, NOT_DONE_YET
-from twisted.web.resource import Resource
+from time import time
+
 from twisted.internet import reactor
+from twisted.web.resource import Resource
+from twisted.web.server import NOT_DONE_YET, Site
 
 
 class Root(Resource):
-
     def __init__(self):
         Resource.__init__(self)
         self.concurrent = 0
         self.tail = deque(maxlen=100)
         self._reset_stats()
 
     def _reset_stats(self):
@@ -22,34 +22,36 @@
         return self
 
     def render(self, request):
         now = time()
         delta = now - self.lasttime
 
         # reset stats on high iter-request times caused by client restarts
-        if delta > 3: # seconds
+        if delta > 3:  # seconds
             self._reset_stats()
-            return ''
+            return ""
 
         self.tail.appendleft(delta)
         self.lasttime = now
         self.concurrent += 1
 
         if now - self.lastmark >= 3:
             self.lastmark = now
             qps = len(self.tail) / sum(self.tail)
-            print(f'samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}')
+            print(
+                f"samplesize={len(self.tail)} concurrent={self.concurrent} qps={qps:0.2f}"
+            )
 
-        if 'latency' in request.args:
-            latency = float(request.args['latency'][0])
+        if "latency" in request.args:
+            latency = float(request.args["latency"][0])
             reactor.callLater(latency, self._finish, request)
             return NOT_DONE_YET
 
         self.concurrent -= 1
-        return ''
+        return ""
 
     def _finish(self, request):
         self.concurrent -= 1
         if not request.finished and not request._disconnected:
             request.finish()
```

### Comparing `Scrapy-2.7.1/extras/qpsclient.py` & `Scrapy-2.8.0/extras/qpsclient.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 """
-A spider that generate light requests to meassure QPS throughput
+A spider that generate light requests to measure QPS throughput
 
 usage:
 
-    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0 --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3
+    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0
+     --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3
 
 """
 
-from scrapy.spiders import Spider
 from scrapy.http import Request
+from scrapy.spiders import Spider
 
 
 class QPSSpider(Spider):
 
-    name = 'qps'
-    benchurl = 'http://localhost:8880/'
+    name = "qps"
+    benchurl = "http://localhost:8880/"
 
     # Max concurrency is limited by global CONCURRENT_REQUESTS setting
     max_concurrent_requests = 8
     # Requests per second goal
-    qps = None # same as: 1 / download_delay
+    qps = None  # same as: 1 / download_delay
     download_delay = None
     # time in seconds to delay server responses
     latency = None
     # number of slots to create
     slots = 1
 
     def __init__(self, *a, **kw):
@@ -33,19 +34,19 @@
             self.download_delay = 1 / self.qps
         elif self.download_delay is not None:
             self.download_delay = float(self.download_delay)
 
     def start_requests(self):
         url = self.benchurl
         if self.latency is not None:
-            url += f'?latency={self.latency}'
+            url += f"?latency={self.latency}"
 
         slots = int(self.slots)
         if slots > 1:
-            urls = [url.replace('localhost', f'127.0.0.{x + 1}') for x in range(slots)]
+            urls = [url.replace("localhost", f"127.0.0.{x + 1}") for x in range(slots)]
         else:
             urls = [url]
 
         idx = 0
         while True:
             url = urls[idx % len(urls)]
             yield Request(url, dont_filter=True)
```

### Comparing `Scrapy-2.7.1/extras/scrapy.1` & `Scrapy-2.8.0/extras/scrapy.1`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/extras/scrapy_bash_completion` & `Scrapy-2.8.0/extras/scrapy_bash_completion`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/extras/scrapy_zsh_completion` & `Scrapy-2.8.0/extras/scrapy_zsh_completion`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/pytest.ini` & `Scrapy-2.8.0/pytest.ini`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/scrapy/__init__.py` & `Scrapy-2.8.0/scrapy/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -5,38 +5,44 @@
 import pkgutil
 import sys
 import warnings
 
 from twisted import version as _txv
 
 # Declare top-level shortcuts
-from scrapy.spiders import Spider
-from scrapy.http import Request, FormRequest
+from scrapy.http import FormRequest, Request
+from scrapy.item import Field, Item
 from scrapy.selector import Selector
-from scrapy.item import Item, Field
-
+from scrapy.spiders import Spider
 
 __all__ = [
-    '__version__', 'version_info', 'twisted_version', 'Spider',
-    'Request', 'FormRequest', 'Selector', 'Item', 'Field',
+    "__version__",
+    "version_info",
+    "twisted_version",
+    "Spider",
+    "Request",
+    "FormRequest",
+    "Selector",
+    "Item",
+    "Field",
 ]
 
 
 # Scrapy and Twisted versions
 __version__ = (pkgutil.get_data(__package__, "VERSION") or b"").decode("ascii").strip()
-version_info = tuple(int(v) if v.isdigit() else v for v in __version__.split('.'))
+version_info = tuple(int(v) if v.isdigit() else v for v in __version__.split("."))
 twisted_version = (_txv.major, _txv.minor, _txv.micro)
 
 
 # Check minimum required Python version
 if sys.version_info < (3, 7):
     print(f"Scrapy {__version__} requires Python 3.7+")
     sys.exit(1)
 
 
 # Ignore noisy twisted deprecation warnings
-warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')
+warnings.filterwarnings("ignore", category=DeprecationWarning, module="twisted")
 
 
 del pkgutil
 del sys
 del warnings
```

### Comparing `Scrapy-2.7.1/scrapy/cmdline.py` & `Scrapy-2.8.0/scrapy/cmdline.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,88 +1,90 @@
-import sys
-import os
 import argparse
 import cProfile
 import inspect
+import os
+import sys
+
 import pkg_resources
 
 import scrapy
+from scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter
 from scrapy.crawler import CrawlerProcess
-from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter
 from scrapy.exceptions import UsageError
 from scrapy.utils.misc import walk_modules
-from scrapy.utils.project import inside_project, get_project_settings
+from scrapy.utils.project import get_project_settings, inside_project
 from scrapy.utils.python import garbage_collect
 
 
 class ScrapyArgumentParser(argparse.ArgumentParser):
     def _parse_optional(self, arg_string):
         # if starts with -: it means that is a parameter not a argument
-        if arg_string[:2] == '-:':
+        if arg_string[:2] == "-:":
             return None
 
         return super()._parse_optional(arg_string)
 
 
 def _iter_command_classes(module_name):
-    # TODO: add `name` attribute to commands and and merge this function with
+    # TODO: add `name` attribute to commands and merge this function with
     # scrapy.utils.spider.iter_spider_classes
     for module in walk_modules(module_name):
         for obj in vars(module).values():
             if (
                 inspect.isclass(obj)
                 and issubclass(obj, ScrapyCommand)
                 and obj.__module__ == module.__name__
-                and not obj == ScrapyCommand
+                and obj not in (ScrapyCommand, BaseRunSpiderCommand)
             ):
                 yield obj
 
 
 def _get_commands_from_module(module, inproject):
     d = {}
     for cmd in _iter_command_classes(module):
         if inproject or not cmd.requires_project:
-            cmdname = cmd.__module__.split('.')[-1]
+            cmdname = cmd.__module__.split(".")[-1]
             d[cmdname] = cmd()
     return d
 
 
-def _get_commands_from_entry_points(inproject, group='scrapy.commands'):
+def _get_commands_from_entry_points(inproject, group="scrapy.commands"):
     cmds = {}
     for entry_point in pkg_resources.iter_entry_points(group):
         obj = entry_point.load()
         if inspect.isclass(obj):
             cmds[entry_point.name] = obj()
         else:
             raise Exception(f"Invalid entry point {entry_point.name}")
     return cmds
 
 
 def _get_commands_dict(settings, inproject):
-    cmds = _get_commands_from_module('scrapy.commands', inproject)
+    cmds = _get_commands_from_module("scrapy.commands", inproject)
     cmds.update(_get_commands_from_entry_points(inproject))
-    cmds_module = settings['COMMANDS_MODULE']
+    cmds_module = settings["COMMANDS_MODULE"]
     if cmds_module:
         cmds.update(_get_commands_from_module(cmds_module, inproject))
     return cmds
 
 
 def _pop_command_name(argv):
     i = 0
     for arg in argv[1:]:
-        if not arg.startswith('-'):
+        if not arg.startswith("-"):
             del argv[i]
             return arg
         i += 1
 
 
 def _print_header(settings, inproject):
     version = scrapy.__version__
     if inproject:
-        print(f"Scrapy {version} - project: {settings['BOT_NAME']}\n")
+        print(f"Scrapy {version} - active project: {settings['BOT_NAME']}\n")
+
     else:
         print(f"Scrapy {version} - no active project\n")
 
 
 def _print_commands(settings, inproject):
     _print_header(settings, inproject)
     print("Usage:")
@@ -119,36 +121,38 @@
     if argv is None:
         argv = sys.argv
 
     if settings is None:
         settings = get_project_settings()
         # set EDITOR from environment if available
         try:
-            editor = os.environ['EDITOR']
+            editor = os.environ["EDITOR"]
         except KeyError:
             pass
         else:
-            settings['EDITOR'] = editor
+            settings["EDITOR"] = editor
 
     inproject = inside_project()
     cmds = _get_commands_dict(settings, inproject)
     cmdname = _pop_command_name(argv)
     if not cmdname:
         _print_commands(settings, inproject)
         sys.exit(0)
     elif cmdname not in cmds:
         _print_unknown_command(settings, cmdname, inproject)
         sys.exit(2)
 
     cmd = cmds[cmdname]
-    parser = ScrapyArgumentParser(formatter_class=ScrapyHelpFormatter,
-                                  usage=f"scrapy {cmdname} {cmd.syntax()}",
-                                  conflict_handler='resolve',
-                                  description=cmd.long_desc())
-    settings.setdict(cmd.default_settings, priority='command')
+    parser = ScrapyArgumentParser(
+        formatter_class=ScrapyHelpFormatter,
+        usage=f"scrapy {cmdname} {cmd.syntax()}",
+        conflict_handler="resolve",
+        description=cmd.long_desc(),
+    )
+    settings.setdict(cmd.default_settings, priority="command")
     cmd.settings = settings
     cmd.add_options(parser)
     opts, args = parser.parse_known_args(args=argv[1:])
     _run_print_help(parser, cmd.process_options, args, opts)
 
     cmd.crawler_process = CrawlerProcess(settings)
     _run_print_help(parser, _run_command, cmd, args, opts)
@@ -163,20 +167,20 @@
 
 
 def _run_command_profiled(cmd, args, opts):
     if opts.profile:
         sys.stderr.write(f"scrapy: writing cProfile stats to {opts.profile!r}\n")
     loc = locals()
     p = cProfile.Profile()
-    p.runctx('cmd.run(args, opts)', globals(), loc)
+    p.runctx("cmd.run(args, opts)", globals(), loc)
     if opts.profile:
         p.dump_stats(opts.profile)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     try:
         execute()
     finally:
         # Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect() on exit:
         # http://doc.pypy.org/en/latest/cpython_differences.html
         # ?highlight=gc.collect#differences-related-to-garbage-collection-strategies
         garbage_collect()
```

### Comparing `Scrapy-2.7.1/scrapy/commands/__init__.py` & `Scrapy-2.8.0/scrapy/commands/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,35 +1,37 @@
 """
 Base class for Scrapy commands
 """
-import os
 import argparse
-from typing import Any, Dict
+import os
+from pathlib import Path
+from typing import Any, Dict, Optional
 
 from twisted.python import failure
 
-from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli
+from scrapy.crawler import CrawlerProcess
 from scrapy.exceptions import UsageError
+from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli
 
 
 class ScrapyCommand:
 
     requires_project = False
-    crawler_process = None
+    crawler_process: Optional[CrawlerProcess] = None
 
     # default settings to be used for this command instead of global defaults
     default_settings: Dict[str, Any] = {}
 
     exitcode = 0
 
-    def __init__(self):
-        self.settings = None  # set in scrapy.cmdline
+    def __init__(self) -> None:
+        self.settings: Any = None  # set in scrapy.cmdline
 
     def set_crawler(self, crawler):
-        if hasattr(self, '_crawler'):
+        if hasattr(self, "_crawler"):
             raise RuntimeError("crawler already set")
         self._crawler = crawler
 
     def syntax(self):
         """
         Command syntax (preferably one-line). Do not include command name.
         """
@@ -55,50 +57,66 @@
         """
         return self.long_desc()
 
     def add_options(self, parser):
         """
         Populate option parse with options available for this command
         """
-        group = parser.add_argument_group(title='Global Options')
-        group.add_argument("--logfile", metavar="FILE",
-                           help="log file. if omitted stderr will be used")
-        group.add_argument("-L", "--loglevel", metavar="LEVEL", default=None,
-                           help=f"log level (default: {self.settings['LOG_LEVEL']})")
-        group.add_argument("--nolog", action="store_true",
-                           help="disable logging completely")
-        group.add_argument("--profile", metavar="FILE", default=None,
-                           help="write python cProfile stats to FILE")
-        group.add_argument("--pidfile", metavar="FILE",
-                           help="write process ID to FILE")
-        group.add_argument("-s", "--set", action="append", default=[], metavar="NAME=VALUE",
-                           help="set/override setting (may be repeated)")
+        group = parser.add_argument_group(title="Global Options")
+        group.add_argument(
+            "--logfile", metavar="FILE", help="log file. if omitted stderr will be used"
+        )
+        group.add_argument(
+            "-L",
+            "--loglevel",
+            metavar="LEVEL",
+            default=None,
+            help=f"log level (default: {self.settings['LOG_LEVEL']})",
+        )
+        group.add_argument(
+            "--nolog", action="store_true", help="disable logging completely"
+        )
+        group.add_argument(
+            "--profile",
+            metavar="FILE",
+            default=None,
+            help="write python cProfile stats to FILE",
+        )
+        group.add_argument("--pidfile", metavar="FILE", help="write process ID to FILE")
+        group.add_argument(
+            "-s",
+            "--set",
+            action="append",
+            default=[],
+            metavar="NAME=VALUE",
+            help="set/override setting (may be repeated)",
+        )
         group.add_argument("--pdb", action="store_true", help="enable pdb on failure")
 
     def process_options(self, args, opts):
         try:
-            self.settings.setdict(arglist_to_dict(opts.set),
-                                  priority='cmdline')
+            self.settings.setdict(arglist_to_dict(opts.set), priority="cmdline")
         except ValueError:
             raise UsageError("Invalid -s value, use -s NAME=VALUE", print_help=False)
 
         if opts.logfile:
-            self.settings.set('LOG_ENABLED', True, priority='cmdline')
-            self.settings.set('LOG_FILE', opts.logfile, priority='cmdline')
+            self.settings.set("LOG_ENABLED", True, priority="cmdline")
+            self.settings.set("LOG_FILE", opts.logfile, priority="cmdline")
 
         if opts.loglevel:
-            self.settings.set('LOG_ENABLED', True, priority='cmdline')
-            self.settings.set('LOG_LEVEL', opts.loglevel, priority='cmdline')
+            self.settings.set("LOG_ENABLED", True, priority="cmdline")
+            self.settings.set("LOG_LEVEL", opts.loglevel, priority="cmdline")
 
         if opts.nolog:
-            self.settings.set('LOG_ENABLED', False, priority='cmdline')
+            self.settings.set("LOG_ENABLED", False, priority="cmdline")
 
         if opts.pidfile:
-            with open(opts.pidfile, "w") as f:
-                f.write(str(os.getpid()) + os.linesep)
+            Path(opts.pidfile).write_text(
+                str(os.getpid()) + os.linesep, encoding="utf-8"
+            )
 
         if opts.pdb:
             failure.startDebugMode()
 
     def run(self, args, opts):
         """
         Entry point for running commands
@@ -106,61 +124,89 @@
         raise NotImplementedError
 
 
 class BaseRunSpiderCommand(ScrapyCommand):
     """
     Common class used to share functionality between the crawl, parse and runspider commands
     """
+
     def add_options(self, parser):
         ScrapyCommand.add_options(self, parser)
-        parser.add_argument("-a", dest="spargs", action="append", default=[], metavar="NAME=VALUE",
-                            help="set spider argument (may be repeated)")
-        parser.add_argument("-o", "--output", metavar="FILE", action="append",
-                            help="append scraped items to the end of FILE (use - for stdout),"
-                                 " to define format set a colon at the end of the output URI (i.e. -o FILE:FORMAT)")
-        parser.add_argument("-O", "--overwrite-output", metavar="FILE", action="append",
-                            help="dump scraped items into FILE, overwriting any existing file,"
-                                 " to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)")
-        parser.add_argument("-t", "--output-format", metavar="FORMAT",
-                            help="format to use for dumping items")
+        parser.add_argument(
+            "-a",
+            dest="spargs",
+            action="append",
+            default=[],
+            metavar="NAME=VALUE",
+            help="set spider argument (may be repeated)",
+        )
+        parser.add_argument(
+            "-o",
+            "--output",
+            metavar="FILE",
+            action="append",
+            help="append scraped items to the end of FILE (use - for stdout),"
+            " to define format set a colon at the end of the output URI (i.e. -o FILE:FORMAT)",
+        )
+        parser.add_argument(
+            "-O",
+            "--overwrite-output",
+            metavar="FILE",
+            action="append",
+            help="dump scraped items into FILE, overwriting any existing file,"
+            " to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)",
+        )
+        parser.add_argument(
+            "-t",
+            "--output-format",
+            metavar="FORMAT",
+            help="format to use for dumping items",
+        )
 
     def process_options(self, args, opts):
         ScrapyCommand.process_options(self, args, opts)
         try:
             opts.spargs = arglist_to_dict(opts.spargs)
         except ValueError:
             raise UsageError("Invalid -a value, use -a NAME=VALUE", print_help=False)
         if opts.output or opts.overwrite_output:
             feeds = feed_process_params_from_cli(
                 self.settings,
                 opts.output,
                 opts.output_format,
                 opts.overwrite_output,
             )
-            self.settings.set('FEEDS', feeds, priority='cmdline')
+            self.settings.set("FEEDS", feeds, priority="cmdline")
 
 
 class ScrapyHelpFormatter(argparse.HelpFormatter):
     """
     Help Formatter for scrapy command line help messages.
     """
+
     def __init__(self, prog, indent_increment=2, max_help_position=24, width=None):
-        super().__init__(prog, indent_increment=indent_increment,
-                         max_help_position=max_help_position, width=width)
+        super().__init__(
+            prog,
+            indent_increment=indent_increment,
+            max_help_position=max_help_position,
+            width=width,
+        )
 
     def _join_parts(self, part_strings):
         parts = self.format_part_strings(part_strings)
         return super()._join_parts(parts)
 
     def format_part_strings(self, part_strings):
         """
         Underline and title case command line help message headers.
         """
         if part_strings and part_strings[0].startswith("usage: "):
-            part_strings[0] = "Usage\n=====\n  " + part_strings[0][len('usage: '):]
-        headings = [i for i in range(len(part_strings)) if part_strings[i].endswith(':\n')]
+            part_strings[0] = "Usage\n=====\n  " + part_strings[0][len("usage: ") :]
+        headings = [
+            i for i in range(len(part_strings)) if part_strings[i].endswith(":\n")
+        ]
         for index in headings[::-1]:
-            char = '-' if "Global Options" in part_strings[index] else '='
+            char = "-" if "Global Options" in part_strings[index] else "="
             part_strings[index] = part_strings[index][:-2].title()
-            underline = ''.join(["\n", (char * len(part_strings[index])), "\n"])
+            underline = "".join(["\n", (char * len(part_strings[index])), "\n"])
             part_strings.insert(index + 1, underline)
         return part_strings
```

### Comparing `Scrapy-2.7.1/scrapy/commands/bench.py` & `Scrapy-2.8.0/scrapy/commands/bench.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,58 +1,58 @@
+import subprocess
 import sys
 import time
-import subprocess
 from urllib.parse import urlencode
 
 import scrapy
 from scrapy.commands import ScrapyCommand
 from scrapy.linkextractors import LinkExtractor
 
 
 class Command(ScrapyCommand):
 
     default_settings = {
-        'LOG_LEVEL': 'INFO',
-        'LOGSTATS_INTERVAL': 1,
-        'CLOSESPIDER_TIMEOUT': 10,
+        "LOG_LEVEL": "INFO",
+        "LOGSTATS_INTERVAL": 1,
+        "CLOSESPIDER_TIMEOUT": 10,
     }
 
     def short_desc(self):
         return "Run quick benchmark test"
 
     def run(self, args, opts):
         with _BenchServer():
             self.crawler_process.crawl(_BenchSpider, total=100000)
             self.crawler_process.start()
 
 
 class _BenchServer:
-
     def __enter__(self):
         from scrapy.utils.test import get_testenv
-        pargs = [sys.executable, '-u', '-m', 'scrapy.utils.benchserver']
-        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE,
-                                     env=get_testenv())
+
+        pargs = [sys.executable, "-u", "-m", "scrapy.utils.benchserver"]
+        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE, env=get_testenv())
         self.proc.stdout.readline()
 
     def __exit__(self, exc_type, exc_value, traceback):
         self.proc.kill()
         self.proc.wait()
         time.sleep(0.2)
 
 
 class _BenchSpider(scrapy.Spider):
     """A spider that follows all links"""
-    name = 'follow'
+
+    name = "follow"
     total = 10000
     show = 20
-    baseurl = 'http://localhost:8998'
+    baseurl = "http://localhost:8998"
     link_extractor = LinkExtractor()
 
     def start_requests(self):
-        qargs = {'total': self.total, 'show': self.show}
-        url = f'{self.baseurl}?{urlencode(qargs, doseq=True)}'
+        qargs = {"total": self.total, "show": self.show}
+        url = f"{self.baseurl}?{urlencode(qargs, doseq=True)}"
         return [scrapy.Request(url, dont_filter=True)]
 
     def parse(self, response):
         for link in self.link_extractor.extract_links(response):
             yield scrapy.Request(link.url, callback=self.parse)
```

### Comparing `Scrapy-2.7.1/scrapy/commands/check.py` & `Scrapy-2.8.0/scrapy/commands/check.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 import time
 from collections import defaultdict
-from unittest import TextTestRunner, TextTestResult as _TextTestResult
+from unittest import TextTestResult as _TextTestResult
+from unittest import TextTestRunner
 
 from scrapy.commands import ScrapyCommand
 from scrapy.contracts import ContractsManager
-from scrapy.utils.misc import load_object, set_environ
 from scrapy.utils.conf import build_component_list
+from scrapy.utils.misc import load_object, set_environ
 
 
 class TextTestResult(_TextTestResult):
     def printSummary(self, start, stop):
         write = self.stream.write
         writeln = self.stream.writeln
 
@@ -35,42 +36,53 @@
             writeln(f" ({', '.join(infos)})")
         else:
             write("\n")
 
 
 class Command(ScrapyCommand):
     requires_project = True
-    default_settings = {'LOG_ENABLED': False}
+    default_settings = {"LOG_ENABLED": False}
 
     def syntax(self):
         return "[options] <spider>"
 
     def short_desc(self):
         return "Check spider contracts"
 
     def add_options(self, parser):
         ScrapyCommand.add_options(self, parser)
-        parser.add_argument("-l", "--list", dest="list", action="store_true",
-                            help="only list contracts, without checking them")
-        parser.add_argument("-v", "--verbose", dest="verbose", default=False, action='store_true',
-                            help="print contract tests for all spiders")
+        parser.add_argument(
+            "-l",
+            "--list",
+            dest="list",
+            action="store_true",
+            help="only list contracts, without checking them",
+        )
+        parser.add_argument(
+            "-v",
+            "--verbose",
+            dest="verbose",
+            default=False,
+            action="store_true",
+            help="print contract tests for all spiders",
+        )
 
     def run(self, args, opts):
         # load contracts
-        contracts = build_component_list(self.settings.getwithbase('SPIDER_CONTRACTS'))
+        contracts = build_component_list(self.settings.getwithbase("SPIDER_CONTRACTS"))
         conman = ContractsManager(load_object(c) for c in contracts)
         runner = TextTestRunner(verbosity=2 if opts.verbose else 1)
         result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)
 
         # contract requests
         contract_reqs = defaultdict(list)
 
         spider_loader = self.crawler_process.spider_loader
 
-        with set_environ(SCRAPY_CHECK='true'):
+        with set_environ(SCRAPY_CHECK="true"):
             for spidername in args or spider_loader.list():
                 spidercls = spider_loader.load(spidername)
                 spidercls.start_requests = lambda s: conman.from_spider(s, result)
 
                 tested_methods = conman.tested_methods_from_spidercls(spidercls)
                 if opts.list:
                     for method in tested_methods:
@@ -81,15 +93,15 @@
             # start checks
             if opts.list:
                 for spider, methods in sorted(contract_reqs.items()):
                     if not methods and not opts.verbose:
                         continue
                     print(spider)
                     for method in sorted(methods):
-                        print(f'  * {method}')
+                        print(f"  * {method}")
             else:
                 start = time.time()
                 self.crawler_process.start()
                 stop = time.time()
 
                 result.printErrors()
                 result.printSummary(start, stop)
```

### Comparing `Scrapy-2.7.1/scrapy/commands/crawl.py` & `Scrapy-2.8.0/scrapy/commands/crawl.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,22 +12,27 @@
     def short_desc(self):
         return "Run a spider"
 
     def run(self, args, opts):
         if len(args) < 1:
             raise UsageError()
         elif len(args) > 1:
-            raise UsageError("running 'scrapy crawl' with more than one spider is not supported")
+            raise UsageError(
+                "running 'scrapy crawl' with more than one spider is not supported"
+            )
         spname = args[0]
 
         crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)
 
-        if getattr(crawl_defer, 'result', None) is not None and issubclass(crawl_defer.result.type, Exception):
+        if getattr(crawl_defer, "result", None) is not None and issubclass(
+            crawl_defer.result.type, Exception
+        ):
             self.exitcode = 1
         else:
             self.crawler_process.start()
 
             if (
                 self.crawler_process.bootstrap_failed
-                or hasattr(self.crawler_process, 'has_exception') and self.crawler_process.has_exception
+                or hasattr(self.crawler_process, "has_exception")
+                and self.crawler_process.has_exception
             ):
                 self.exitcode = 1
```

### Comparing `Scrapy-2.7.1/scrapy/commands/fetch.py` & `Scrapy-2.8.0/scrapy/commands/fetch.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 import sys
+
 from w3lib.url import is_url
 
 from scrapy.commands import ScrapyCommand
-from scrapy.http import Request
 from scrapy.exceptions import UsageError
+from scrapy.http import Request
 from scrapy.utils.datatypes import SequenceExclude
-from scrapy.utils.spider import spidercls_for_request, DefaultSpider
+from scrapy.utils.spider import DefaultSpider, spidercls_for_request
 
 
 class Command(ScrapyCommand):
 
     requires_project = False
 
     def syntax(self):
@@ -23,46 +24,59 @@
             "Fetch a URL using the Scrapy downloader and print its content"
             " to stdout. You may want to use --nolog to disable logging"
         )
 
     def add_options(self, parser):
         ScrapyCommand.add_options(self, parser)
         parser.add_argument("--spider", dest="spider", help="use this spider")
-        parser.add_argument("--headers", dest="headers", action="store_true",
-                            help="print response HTTP headers instead of body")
-        parser.add_argument("--no-redirect", dest="no_redirect", action="store_true", default=False,
-                            help="do not handle HTTP 3xx status codes and print response as-is")
+        parser.add_argument(
+            "--headers",
+            dest="headers",
+            action="store_true",
+            help="print response HTTP headers instead of body",
+        )
+        parser.add_argument(
+            "--no-redirect",
+            dest="no_redirect",
+            action="store_true",
+            default=False,
+            help="do not handle HTTP 3xx status codes and print response as-is",
+        )
 
     def _print_headers(self, headers, prefix):
         for key, values in headers.items():
             for value in values:
-                self._print_bytes(prefix + b' ' + key + b': ' + value)
+                self._print_bytes(prefix + b" " + key + b": " + value)
 
     def _print_response(self, response, opts):
         if opts.headers:
-            self._print_headers(response.request.headers, b'>')
-            print('>')
-            self._print_headers(response.headers, b'<')
+            self._print_headers(response.request.headers, b">")
+            print(">")
+            self._print_headers(response.headers, b"<")
         else:
             self._print_bytes(response.body)
 
     def _print_bytes(self, bytes_):
-        sys.stdout.buffer.write(bytes_ + b'\n')
+        sys.stdout.buffer.write(bytes_ + b"\n")
 
     def run(self, args, opts):
         if len(args) != 1 or not is_url(args[0]):
             raise UsageError()
-        request = Request(args[0], callback=self._print_response,
-                          cb_kwargs={"opts": opts}, dont_filter=True)
+        request = Request(
+            args[0],
+            callback=self._print_response,
+            cb_kwargs={"opts": opts},
+            dont_filter=True,
+        )
         # by default, let the framework handle redirects,
         # i.e. command handles all codes expect 3xx
         if not opts.no_redirect:
-            request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))
+            request.meta["handle_httpstatus_list"] = SequenceExclude(range(300, 400))
         else:
-            request.meta['handle_httpstatus_all'] = True
+            request.meta["handle_httpstatus_all"] = True
 
         spidercls = DefaultSpider
         spider_loader = self.crawler_process.spider_loader
         if opts.spider:
             spidercls = spider_loader.load(opts.spider)
         else:
             spidercls = spidercls_for_request(spider_loader, request, spidercls)
```

### Comparing `Scrapy-2.7.1/scrapy/commands/genspider.py` & `Scrapy-2.8.0/scrapy/commands/genspider.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,159 +1,193 @@
 import os
 import shutil
 import string
-
 from importlib import import_module
-from os.path import join, dirname, abspath, exists, splitext
+from pathlib import Path
+from typing import Optional, cast
 from urllib.parse import urlparse
 
 import scrapy
 from scrapy.commands import ScrapyCommand
-from scrapy.utils.template import render_templatefile, string_camelcase
 from scrapy.exceptions import UsageError
+from scrapy.utils.template import render_templatefile, string_camelcase
 
 
 def sanitize_module_name(module_name):
     """Sanitize the given module name, by replacing dashes and points
     with underscores and prefixing it with a letter if it doesn't start
     with one
     """
-    module_name = module_name.replace('-', '_').replace('.', '_')
+    module_name = module_name.replace("-", "_").replace(".", "_")
     if module_name[0] not in string.ascii_letters:
         module_name = "a" + module_name
     return module_name
 
 
 def extract_domain(url):
     """Extract domain name from URL string"""
     o = urlparse(url)
-    if o.scheme == '' and o.netloc == '':
+    if o.scheme == "" and o.netloc == "":
         o = urlparse("//" + url.lstrip("/"))
     return o.netloc
 
 
 class Command(ScrapyCommand):
 
     requires_project = False
-    default_settings = {'LOG_ENABLED': False}
+    default_settings = {"LOG_ENABLED": False}
 
     def syntax(self):
         return "[options] <name> <domain>"
 
     def short_desc(self):
         return "Generate new spider using pre-defined templates"
 
     def add_options(self, parser):
         ScrapyCommand.add_options(self, parser)
-        parser.add_argument("-l", "--list", dest="list", action="store_true",
-                            help="List available templates")
-        parser.add_argument("-e", "--edit", dest="edit", action="store_true",
-                            help="Edit spider after creating it")
-        parser.add_argument("-d", "--dump", dest="dump", metavar="TEMPLATE",
-                            help="Dump template to standard output")
-        parser.add_argument("-t", "--template", dest="template", default="basic",
-                            help="Uses a custom template.")
-        parser.add_argument("--force", dest="force", action="store_true",
-                            help="If the spider already exists, overwrite it with the template")
+        parser.add_argument(
+            "-l",
+            "--list",
+            dest="list",
+            action="store_true",
+            help="List available templates",
+        )
+        parser.add_argument(
+            "-e",
+            "--edit",
+            dest="edit",
+            action="store_true",
+            help="Edit spider after creating it",
+        )
+        parser.add_argument(
+            "-d",
+            "--dump",
+            dest="dump",
+            metavar="TEMPLATE",
+            help="Dump template to standard output",
+        )
+        parser.add_argument(
+            "-t",
+            "--template",
+            dest="template",
+            default="basic",
+            help="Uses a custom template.",
+        )
+        parser.add_argument(
+            "--force",
+            dest="force",
+            action="store_true",
+            help="If the spider already exists, overwrite it with the template",
+        )
 
     def run(self, args, opts):
         if opts.list:
             self._list_templates()
             return
         if opts.dump:
             template_file = self._find_template(opts.dump)
             if template_file:
-                with open(template_file, "r") as f:
-                    print(f.read())
+                print(template_file.read_text(encoding="utf-8"))
             return
         if len(args) != 2:
             raise UsageError()
 
         name, url = args[0:2]
         domain = extract_domain(url)
         module = sanitize_module_name(name)
 
-        if self.settings.get('BOT_NAME') == module:
+        if self.settings.get("BOT_NAME") == module:
             print("Cannot create a spider with the same name as your project")
             return
 
         if not opts.force and self._spider_exists(name):
             return
 
         template_file = self._find_template(opts.template)
         if template_file:
             self._genspider(module, name, domain, opts.template, template_file)
             if opts.edit:
                 self.exitcode = os.system(f'scrapy edit "{name}"')
 
     def _genspider(self, module, name, domain, template_name, template_file):
         """Generate the spider module, based on the given template"""
-        capitalized_module = ''.join(s.capitalize() for s in module.split('_'))
+        capitalized_module = "".join(s.capitalize() for s in module.split("_"))
         tvars = {
-            'project_name': self.settings.get('BOT_NAME'),
-            'ProjectName': string_camelcase(self.settings.get('BOT_NAME')),
-            'module': module,
-            'name': name,
-            'domain': domain,
-            'classname': f'{capitalized_module}Spider'
+            "project_name": self.settings.get("BOT_NAME"),
+            "ProjectName": string_camelcase(self.settings.get("BOT_NAME")),
+            "module": module,
+            "name": name,
+            "domain": domain,
+            "classname": f"{capitalized_module}Spider",
         }
-        if self.settings.get('NEWSPIDER_MODULE'):
-            spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])
-            spiders_dir = abspath(dirname(spiders_module.__file__))
+        if self.settings.get("NEWSPIDER_MODULE"):
+            spiders_module = import_module(self.settings["NEWSPIDER_MODULE"])
+            spiders_dir = Path(spiders_module.__file__).parent.resolve()
         else:
             spiders_module = None
-            spiders_dir = "."
-        spider_file = f"{join(spiders_dir, module)}.py"
+            spiders_dir = Path(".")
+        spider_file = f"{spiders_dir / module}.py"
         shutil.copyfile(template_file, spider_file)
         render_templatefile(spider_file, **tvars)
-        print(f"Created spider {name!r} using template {template_name!r} ",
-              end=('' if spiders_module else '\n'))
+        print(
+            f"Created spider {name!r} using template {template_name!r} ",
+            end=("" if spiders_module else "\n"),
+        )
         if spiders_module:
             print(f"in module:\n  {spiders_module.__name__}.{module}")
 
-    def _find_template(self, template):
-        template_file = join(self.templates_dir, f'{template}.tmpl')
-        if exists(template_file):
+    def _find_template(self, template: str) -> Optional[Path]:
+        template_file = Path(self.templates_dir, f"{template}.tmpl")
+        if template_file.exists():
             return template_file
         print(f"Unable to find template: {template}\n")
         print('Use "scrapy genspider --list" to see all available templates.')
+        return None
 
     def _list_templates(self):
         print("Available templates:")
-        for filename in sorted(os.listdir(self.templates_dir)):
-            if filename.endswith('.tmpl'):
-                print(f"  {splitext(filename)[0]}")
+        for file in sorted(Path(self.templates_dir).iterdir()):
+            if file.suffix == ".tmpl":
+                print(f"  {file.stem}")
 
-    def _spider_exists(self, name):
-        if not self.settings.get('NEWSPIDER_MODULE'):
+    def _spider_exists(self, name: str) -> bool:
+        if not self.settings.get("NEWSPIDER_MODULE"):
             # if run as a standalone command and file with same filename already exists
-            if exists(name + ".py"):
-                print(f"{abspath(name + '.py')} already exists")
+            path = Path(name + ".py")
+            if path.exists():
+                print(f"{path.resolve()} already exists")
                 return True
             return False
 
+        assert (
+            self.crawler_process is not None
+        ), "crawler_process must be set before calling run"
+
         try:
             spidercls = self.crawler_process.spider_loader.load(name)
         except KeyError:
             pass
         else:
             # if spider with same name exists
             print(f"Spider {name!r} already exists in module:")
             print(f"  {spidercls.__module__}")
             return True
 
         # a file with the same name exists in the target directory
-        spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])
-        spiders_dir = dirname(spiders_module.__file__)
-        spiders_dir_abs = abspath(spiders_dir)
-        if exists(join(spiders_dir_abs, name + ".py")):
-            print(f"{join(spiders_dir_abs, (name + '.py'))} already exists")
+        spiders_module = import_module(self.settings["NEWSPIDER_MODULE"])
+        spiders_dir = Path(cast(str, spiders_module.__file__)).parent
+        spiders_dir_abs = spiders_dir.resolve()
+        path = spiders_dir_abs / (name + ".py")
+        if path.exists():
+            print(f"{path} already exists")
             return True
 
         return False
 
     @property
-    def templates_dir(self):
-        return join(
-            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),
-            'spiders'
+    def templates_dir(self) -> str:
+        return str(
+            Path(
+                self.settings["TEMPLATES_DIR"] or Path(scrapy.__path__[0], "templates"),
+                "spiders",
+            )
         )
```

### Comparing `Scrapy-2.7.1/scrapy/commands/parse.py` & `Scrapy-2.8.0/scrapy/commands/parse.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,20 @@
 import json
 import logging
 from typing import Dict
 
-from itemadapter import is_item, ItemAdapter
-from w3lib.url import is_url
-
+from itemadapter import ItemAdapter, is_item
 from twisted.internet.defer import maybeDeferred
+from w3lib.url import is_url
 
 from scrapy.commands import BaseRunSpiderCommand
+from scrapy.exceptions import UsageError
 from scrapy.http import Request
 from scrapy.utils import display
 from scrapy.utils.spider import iterate_spider_output, spidercls_for_request
-from scrapy.exceptions import UsageError
-
 
 logger = logging.getLogger(__name__)
 
 
 class Command(BaseRunSpiderCommand):
     requires_project = True
 
@@ -30,36 +28,80 @@
         return "[options] <url>"
 
     def short_desc(self):
         return "Parse URL (using its spider) and print the results"
 
     def add_options(self, parser):
         BaseRunSpiderCommand.add_options(self, parser)
-        parser.add_argument("--spider", dest="spider", default=None,
-                            help="use this spider without looking for one")
-        parser.add_argument("--pipelines", action="store_true",
-                            help="process items through pipelines")
-        parser.add_argument("--nolinks", dest="nolinks", action="store_true",
-                            help="don't show links to follow (extracted requests)")
-        parser.add_argument("--noitems", dest="noitems", action="store_true",
-                            help="don't show scraped items")
-        parser.add_argument("--nocolour", dest="nocolour", action="store_true",
-                            help="avoid using pygments to colorize the output")
-        parser.add_argument("-r", "--rules", dest="rules", action="store_true",
-                            help="use CrawlSpider rules to discover the callback")
-        parser.add_argument("-c", "--callback", dest="callback",
-                            help="use this callback for parsing, instead looking for a callback")
-        parser.add_argument("-m", "--meta", dest="meta",
-                            help="inject extra meta into the Request, it must be a valid raw json string")
-        parser.add_argument("--cbkwargs", dest="cbkwargs",
-                            help="inject extra callback kwargs into the Request, it must be a valid raw json string")
-        parser.add_argument("-d", "--depth", dest="depth", type=int, default=1,
-                            help="maximum depth for parsing requests [default: %(default)s]")
-        parser.add_argument("-v", "--verbose", dest="verbose", action="store_true",
-                            help="print each depth level one by one")
+        parser.add_argument(
+            "--spider",
+            dest="spider",
+            default=None,
+            help="use this spider without looking for one",
+        )
+        parser.add_argument(
+            "--pipelines", action="store_true", help="process items through pipelines"
+        )
+        parser.add_argument(
+            "--nolinks",
+            dest="nolinks",
+            action="store_true",
+            help="don't show links to follow (extracted requests)",
+        )
+        parser.add_argument(
+            "--noitems",
+            dest="noitems",
+            action="store_true",
+            help="don't show scraped items",
+        )
+        parser.add_argument(
+            "--nocolour",
+            dest="nocolour",
+            action="store_true",
+            help="avoid using pygments to colorize the output",
+        )
+        parser.add_argument(
+            "-r",
+            "--rules",
+            dest="rules",
+            action="store_true",
+            help="use CrawlSpider rules to discover the callback",
+        )
+        parser.add_argument(
+            "-c",
+            "--callback",
+            dest="callback",
+            help="use this callback for parsing, instead looking for a callback",
+        )
+        parser.add_argument(
+            "-m",
+            "--meta",
+            dest="meta",
+            help="inject extra meta into the Request, it must be a valid raw json string",
+        )
+        parser.add_argument(
+            "--cbkwargs",
+            dest="cbkwargs",
+            help="inject extra callback kwargs into the Request, it must be a valid raw json string",
+        )
+        parser.add_argument(
+            "-d",
+            "--depth",
+            dest="depth",
+            type=int,
+            default=1,
+            help="maximum depth for parsing requests [default: %(default)s]",
+        )
+        parser.add_argument(
+            "-v",
+            "--verbose",
+            dest="verbose",
+            action="store_true",
+            help="print each depth level one by one",
+        )
 
     @property
     def max_level(self):
         max_items, max_requests = 0, 0
         if self.items:
             max_items = max(self.items)
         if self.requests:
@@ -96,21 +138,21 @@
         display.pprint(requests, colorize=colour)
 
     def print_results(self, opts):
         colour = not opts.nocolour
 
         if opts.verbose:
             for level in range(1, self.max_level + 1):
-                print(f'\n>>> DEPTH LEVEL: {level} <<<')
+                print(f"\n>>> DEPTH LEVEL: {level} <<<")
                 if not opts.noitems:
                     self.print_items(level, colour)
                 if not opts.nolinks:
                     self.print_requests(level, colour)
         else:
-            print(f'\n>>> STATUS DEPTH LEVEL {self.max_level} <<<')
+            print(f"\n>>> STATUS DEPTH LEVEL {self.max_level} <<<")
             if not opts.noitems:
                 self.print_items(colour=colour)
             if not opts.nolinks:
                 self.print_requests(colour=colour)
 
     def _get_items_and_requests(self, spider_output, opts, depth, spider, callback):
         items, requests = [], []
@@ -123,141 +165,154 @@
 
     def run_callback(self, response, callback, cb_kwargs=None):
         cb_kwargs = cb_kwargs or {}
         d = maybeDeferred(iterate_spider_output, callback(response, **cb_kwargs))
         return d
 
     def get_callback_from_rules(self, spider, response):
-        if getattr(spider, 'rules', None):
+        if getattr(spider, "rules", None):
             for rule in spider.rules:
                 if rule.link_extractor.matches(response.url):
                     return rule.callback or "parse"
         else:
-            logger.error('No CrawlSpider rules found in spider %(spider)r, '
-                         'please specify a callback to use for parsing',
-                         {'spider': spider.name})
+            logger.error(
+                "No CrawlSpider rules found in spider %(spider)r, "
+                "please specify a callback to use for parsing",
+                {"spider": spider.name},
+            )
 
     def set_spidercls(self, url, opts):
         spider_loader = self.crawler_process.spider_loader
         if opts.spider:
             try:
                 self.spidercls = spider_loader.load(opts.spider)
             except KeyError:
-                logger.error('Unable to find spider: %(spider)s',
-                             {'spider': opts.spider})
+                logger.error(
+                    "Unable to find spider: %(spider)s", {"spider": opts.spider}
+                )
         else:
             self.spidercls = spidercls_for_request(spider_loader, Request(url))
             if not self.spidercls:
-                logger.error('Unable to find spider for: %(url)s', {'url': url})
+                logger.error("Unable to find spider for: %(url)s", {"url": url})
 
         def _start_requests(spider):
             yield self.prepare_request(spider, Request(url), opts)
+
         if self.spidercls:
             self.spidercls.start_requests = _start_requests
 
     def start_parsing(self, url, opts):
         self.crawler_process.crawl(self.spidercls, **opts.spargs)
         self.pcrawler = list(self.crawler_process.crawlers)[0]
         self.crawler_process.start()
 
         if not self.first_response:
-            logger.error('No response downloaded for: %(url)s',
-                         {'url': url})
+            logger.error("No response downloaded for: %(url)s", {"url": url})
 
     def scraped_data(self, args):
         items, requests, opts, depth, spider, callback = args
         if opts.pipelines:
             itemproc = self.pcrawler.engine.scraper.itemproc
             for item in items:
                 itemproc.process_item(item, spider)
         self.add_items(depth, items)
         self.add_requests(depth, requests)
 
         scraped_data = items if opts.output else []
         if depth < opts.depth:
             for req in requests:
-                req.meta['_depth'] = depth + 1
-                req.meta['_callback'] = req.callback
+                req.meta["_depth"] = depth + 1
+                req.meta["_callback"] = req.callback
                 req.callback = callback
             scraped_data += requests
 
         return scraped_data
 
     def prepare_request(self, spider, request, opts):
         def callback(response, **cb_kwargs):
             # memorize first request
             if not self.first_response:
                 self.first_response = response
 
             # determine real callback
-            cb = response.meta['_callback']
+            cb = response.meta["_callback"]
             if not cb:
                 if opts.callback:
                     cb = opts.callback
                 elif opts.rules and self.first_response == response:
                     cb = self.get_callback_from_rules(spider, response)
 
                     if not cb:
-                        logger.error('Cannot find a rule that matches %(url)r in spider: %(spider)s',
-                                     {'url': response.url, 'spider': spider.name})
+                        logger.error(
+                            "Cannot find a rule that matches %(url)r in spider: %(spider)s",
+                            {"url": response.url, "spider": spider.name},
+                        )
                         return
                 else:
-                    cb = 'parse'
+                    cb = "parse"
 
             if not callable(cb):
                 cb_method = getattr(spider, cb, None)
                 if callable(cb_method):
                     cb = cb_method
                 else:
-                    logger.error('Cannot find callback %(callback)r in spider: %(spider)s',
-                                 {'callback': cb, 'spider': spider.name})
+                    logger.error(
+                        "Cannot find callback %(callback)r in spider: %(spider)s",
+                        {"callback": cb, "spider": spider.name},
+                    )
                     return
 
             # parse items and requests
-            depth = response.meta['_depth']
+            depth = response.meta["_depth"]
 
             d = self.run_callback(response, cb, cb_kwargs)
             d.addCallback(self._get_items_and_requests, opts, depth, spider, callback)
             d.addCallback(self.scraped_data)
             return d
 
         # update request meta if any extra meta was passed through the --meta/-m opts.
         if opts.meta:
             request.meta.update(opts.meta)
 
         # update cb_kwargs if any extra values were was passed through the --cbkwargs option.
         if opts.cbkwargs:
             request.cb_kwargs.update(opts.cbkwargs)
 
-        request.meta['_depth'] = 1
-        request.meta['_callback'] = request.callback
+        request.meta["_depth"] = 1
+        request.meta["_callback"] = request.callback
         request.callback = callback
         return request
 
     def process_options(self, args, opts):
         BaseRunSpiderCommand.process_options(self, args, opts)
 
         self.process_request_meta(opts)
         self.process_request_cb_kwargs(opts)
 
     def process_request_meta(self, opts):
         if opts.meta:
             try:
                 opts.meta = json.loads(opts.meta)
             except ValueError:
-                raise UsageError("Invalid -m/--meta value, pass a valid json string to -m or --meta. "
-                                 "Example: --meta='{\"foo\" : \"bar\"}'", print_help=False)
+                raise UsageError(
+                    "Invalid -m/--meta value, pass a valid json string to -m or --meta. "
+                    'Example: --meta=\'{"foo" : "bar"}\'',
+                    print_help=False,
+                )
 
     def process_request_cb_kwargs(self, opts):
         if opts.cbkwargs:
             try:
                 opts.cbkwargs = json.loads(opts.cbkwargs)
             except ValueError:
-                raise UsageError("Invalid --cbkwargs value, pass a valid json string to --cbkwargs. "
-                                 "Example: --cbkwargs='{\"foo\" : \"bar\"}'", print_help=False)
+                raise UsageError(
+                    "Invalid --cbkwargs value, pass a valid json string to --cbkwargs. "
+                    'Example: --cbkwargs=\'{"foo" : "bar"}\'',
+                    print_help=False,
+                )
 
     def run(self, args, opts):
         # parse arguments
         if not len(args) == 1 or not is_url(args[0]):
             raise UsageError()
         else:
             url = args[0]
```

### Comparing `Scrapy-2.7.1/scrapy/commands/settings.py` & `Scrapy-2.8.0/scrapy/commands/settings.py`

 * *Files 15% similar despite different names*

```diff
@@ -3,35 +3,51 @@
 from scrapy.commands import ScrapyCommand
 from scrapy.settings import BaseSettings
 
 
 class Command(ScrapyCommand):
 
     requires_project = False
-    default_settings = {'LOG_ENABLED': False,
-                        'SPIDER_LOADER_WARN_ONLY': True}
+    default_settings = {"LOG_ENABLED": False, "SPIDER_LOADER_WARN_ONLY": True}
 
     def syntax(self):
         return "[options]"
 
     def short_desc(self):
         return "Get settings values"
 
     def add_options(self, parser):
         ScrapyCommand.add_options(self, parser)
-        parser.add_argument("--get", dest="get", metavar="SETTING",
-                            help="print raw setting value")
-        parser.add_argument("--getbool", dest="getbool", metavar="SETTING",
-                            help="print setting value, interpreted as a boolean")
-        parser.add_argument("--getint", dest="getint", metavar="SETTING",
-                            help="print setting value, interpreted as an integer")
-        parser.add_argument("--getfloat", dest="getfloat", metavar="SETTING",
-                            help="print setting value, interpreted as a float")
-        parser.add_argument("--getlist", dest="getlist", metavar="SETTING",
-                            help="print setting value, interpreted as a list")
+        parser.add_argument(
+            "--get", dest="get", metavar="SETTING", help="print raw setting value"
+        )
+        parser.add_argument(
+            "--getbool",
+            dest="getbool",
+            metavar="SETTING",
+            help="print setting value, interpreted as a boolean",
+        )
+        parser.add_argument(
+            "--getint",
+            dest="getint",
+            metavar="SETTING",
+            help="print setting value, interpreted as an integer",
+        )
+        parser.add_argument(
+            "--getfloat",
+            dest="getfloat",
+            metavar="SETTING",
+            help="print setting value, interpreted as a float",
+        )
+        parser.add_argument(
+            "--getlist",
+            dest="getlist",
+            metavar="SETTING",
+            help="print setting value, interpreted as a list",
+        )
 
     def run(self, args, opts):
         settings = self.crawler_process.settings
         if opts.get:
             s = settings.get(opts.get)
             if isinstance(s, BaseSettings):
                 print(json.dumps(s.copy_to_dict()))
```

### Comparing `Scrapy-2.7.1/scrapy/commands/shell.py` & `Scrapy-2.8.0/scrapy/commands/shell.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,45 +4,54 @@
 See documentation in docs/topics/shell.rst
 """
 from threading import Thread
 
 from scrapy.commands import ScrapyCommand
 from scrapy.http import Request
 from scrapy.shell import Shell
-from scrapy.utils.spider import spidercls_for_request, DefaultSpider
+from scrapy.utils.spider import DefaultSpider, spidercls_for_request
 from scrapy.utils.url import guess_scheme
 
 
 class Command(ScrapyCommand):
 
     requires_project = False
     default_settings = {
-        'KEEP_ALIVE': True,
-        'LOGSTATS_INTERVAL': 0,
-        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
+        "KEEP_ALIVE": True,
+        "LOGSTATS_INTERVAL": 0,
+        "DUPEFILTER_CLASS": "scrapy.dupefilters.BaseDupeFilter",
     }
 
     def syntax(self):
         return "[url|file]"
 
     def short_desc(self):
         return "Interactive scraping console"
 
     def long_desc(self):
-        return ("Interactive console for scraping the given url or file. "
-                "Use ./file.html syntax or full path for local file.")
+        return (
+            "Interactive console for scraping the given url or file. "
+            "Use ./file.html syntax or full path for local file."
+        )
 
     def add_options(self, parser):
         ScrapyCommand.add_options(self, parser)
-        parser.add_argument("-c", dest="code",
-                            help="evaluate the code in the shell, print the result and exit")
-        parser.add_argument("--spider", dest="spider",
-                            help="use this spider")
-        parser.add_argument("--no-redirect", dest="no_redirect", action="store_true", default=False,
-                            help="do not handle HTTP 3xx status codes and print response as-is")
+        parser.add_argument(
+            "-c",
+            dest="code",
+            help="evaluate the code in the shell, print the result and exit",
+        )
+        parser.add_argument("--spider", dest="spider", help="use this spider")
+        parser.add_argument(
+            "--no-redirect",
+            dest="no_redirect",
+            action="store_true",
+            default=False,
+            help="do not handle HTTP 3xx status codes and print response as-is",
+        )
 
     def update_vars(self, vars):
         """You can use this function to update the Scrapy objects that will be
         available in the shell
         """
         pass
 
@@ -54,27 +63,30 @@
 
         spider_loader = self.crawler_process.spider_loader
 
         spidercls = DefaultSpider
         if opts.spider:
             spidercls = spider_loader.load(opts.spider)
         elif url:
-            spidercls = spidercls_for_request(spider_loader, Request(url),
-                                              spidercls, log_multiple=True)
+            spidercls = spidercls_for_request(
+                spider_loader, Request(url), spidercls, log_multiple=True
+            )
 
         # The crawler is created this way since the Shell manually handles the
         # crawling engine, so the set up in the crawl method won't work
         crawler = self.crawler_process._create_crawler(spidercls)
         # The Shell class needs a persistent engine in the crawler
         crawler.engine = crawler._create_engine()
         crawler.engine.start()
 
         self._start_crawler_thread()
 
         shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)
         shell.start(url=url, redirect=not opts.no_redirect)
 
     def _start_crawler_thread(self):
-        t = Thread(target=self.crawler_process.start,
-                   kwargs={'stop_after_crawl': False, 'install_signal_handlers': False})
+        t = Thread(
+            target=self.crawler_process.start,
+            kwargs={"stop_after_crawl": False, "install_signal_handlers": False},
+        )
         t.daemon = True
         t.start()
```

### Comparing `Scrapy-2.7.1/scrapy/commands/startproject.py` & `Scrapy-2.8.0/scrapy/commands/startproject.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,125 +1,139 @@
-import re
 import os
+import re
 import string
 from importlib.util import find_spec
-from os.path import join, exists, abspath
-from shutil import ignore_patterns, move, copy2, copystat
+from pathlib import Path
+from shutil import copy2, copystat, ignore_patterns, move
 from stat import S_IWUSR as OWNER_WRITE_PERMISSION
 
 import scrapy
 from scrapy.commands import ScrapyCommand
-from scrapy.utils.template import render_templatefile, string_camelcase
 from scrapy.exceptions import UsageError
-
+from scrapy.utils.template import render_templatefile, string_camelcase
 
 TEMPLATES_TO_RENDER = (
-    ('scrapy.cfg',),
-    ('${project_name}', 'settings.py.tmpl'),
-    ('${project_name}', 'items.py.tmpl'),
-    ('${project_name}', 'pipelines.py.tmpl'),
-    ('${project_name}', 'middlewares.py.tmpl'),
+    ("scrapy.cfg",),
+    ("${project_name}", "settings.py.tmpl"),
+    ("${project_name}", "items.py.tmpl"),
+    ("${project_name}", "pipelines.py.tmpl"),
+    ("${project_name}", "middlewares.py.tmpl"),
 )
 
-IGNORE = ignore_patterns('*.pyc', '__pycache__', '.svn')
+IGNORE = ignore_patterns("*.pyc", "__pycache__", ".svn")
 
 
 def _make_writable(path):
     current_permissions = os.stat(path).st_mode
     os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)
 
 
 class Command(ScrapyCommand):
 
     requires_project = False
-    default_settings = {'LOG_ENABLED': False,
-                        'SPIDER_LOADER_WARN_ONLY': True}
+    default_settings = {"LOG_ENABLED": False, "SPIDER_LOADER_WARN_ONLY": True}
 
     def syntax(self):
         return "<project_name> [project_dir]"
 
     def short_desc(self):
         return "Create new project"
 
     def _is_valid_name(self, project_name):
         def _module_exists(module_name):
             spec = find_spec(module_name)
             return spec is not None and spec.loader is not None
 
-        if not re.search(r'^[_a-zA-Z]\w*$', project_name):
-            print('Error: Project names must begin with a letter and contain'
-                  ' only\nletters, numbers and underscores')
+        if not re.search(r"^[_a-zA-Z]\w*$", project_name):
+            print(
+                "Error: Project names must begin with a letter and contain"
+                " only\nletters, numbers and underscores"
+            )
         elif _module_exists(project_name):
-            print(f'Error: Module {project_name!r} already exists')
+            print(f"Error: Module {project_name!r} already exists")
         else:
             return True
         return False
 
-    def _copytree(self, src, dst):
+    def _copytree(self, src: Path, dst: Path):
         """
         Since the original function always creates the directory, to resolve
         the issue a new function had to be created. It's a simple copy and
         was reduced for this case.
 
         More info at:
         https://github.com/scrapy/scrapy/pull/2005
         """
         ignore = IGNORE
-        names = os.listdir(src)
+        names = [x.name for x in src.iterdir()]
         ignored_names = ignore(src, names)
 
-        if not os.path.exists(dst):
-            os.makedirs(dst)
+        if not dst.exists():
+            dst.mkdir(parents=True)
 
         for name in names:
             if name in ignored_names:
                 continue
 
-            srcname = os.path.join(src, name)
-            dstname = os.path.join(dst, name)
-            if os.path.isdir(srcname):
+            srcname = src / name
+            dstname = dst / name
+            if srcname.is_dir():
                 self._copytree(srcname, dstname)
             else:
                 copy2(srcname, dstname)
                 _make_writable(dstname)
 
         copystat(src, dst)
         _make_writable(dst)
 
     def run(self, args, opts):
         if len(args) not in (1, 2):
             raise UsageError()
 
         project_name = args[0]
-        project_dir = args[0]
 
         if len(args) == 2:
-            project_dir = args[1]
+            project_dir = Path(args[1])
+        else:
+            project_dir = Path(args[0])
 
-        if exists(join(project_dir, 'scrapy.cfg')):
+        if (project_dir / "scrapy.cfg").exists():
             self.exitcode = 1
-            print(f'Error: scrapy.cfg already exists in {abspath(project_dir)}')
+            print(f"Error: scrapy.cfg already exists in {project_dir.resolve()}")
             return
 
         if not self._is_valid_name(project_name):
             self.exitcode = 1
             return
 
-        self._copytree(self.templates_dir, abspath(project_dir))
-        move(join(project_dir, 'module'), join(project_dir, project_name))
+        self._copytree(Path(self.templates_dir), project_dir.resolve())
+        move(project_dir / "module", project_dir / project_name)
         for paths in TEMPLATES_TO_RENDER:
-            path = join(*paths)
-            tplfile = join(project_dir, string.Template(path).substitute(project_name=project_name))
-            render_templatefile(tplfile, project_name=project_name, ProjectName=string_camelcase(project_name))
-        print(f"New Scrapy project '{project_name}', using template directory "
-              f"'{self.templates_dir}', created in:")
-        print(f"    {abspath(project_dir)}\n")
+            tplfile = Path(
+                project_dir,
+                *(
+                    string.Template(s).substitute(project_name=project_name)
+                    for s in paths
+                ),
+            )
+            render_templatefile(
+                tplfile,
+                project_name=project_name,
+                ProjectName=string_camelcase(project_name),
+            )
+        print(
+            f"New Scrapy project '{project_name}', using template directory "
+            f"'{self.templates_dir}', created in:"
+        )
+        print(f"    {project_dir.resolve()}\n")
         print("You can start your first spider with:")
         print(f"    cd {project_dir}")
         print("    scrapy genspider example example.com")
 
     @property
-    def templates_dir(self):
-        return join(
-            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),
-            'project'
+    def templates_dir(self) -> str:
+        return str(
+            Path(
+                self.settings["TEMPLATES_DIR"] or Path(scrapy.__path__[0], "templates"),
+                "project",
+            )
         )
```

### Comparing `Scrapy-2.7.1/scrapy/commands/version.py` & `Scrapy-2.8.0/scrapy/commands/version.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,27 +1,31 @@
 import scrapy
 from scrapy.commands import ScrapyCommand
 from scrapy.utils.versions import scrapy_components_versions
 
 
 class Command(ScrapyCommand):
 
-    default_settings = {'LOG_ENABLED': False,
-                        'SPIDER_LOADER_WARN_ONLY': True}
+    default_settings = {"LOG_ENABLED": False, "SPIDER_LOADER_WARN_ONLY": True}
 
     def syntax(self):
         return "[-v]"
 
     def short_desc(self):
         return "Print Scrapy version"
 
     def add_options(self, parser):
         ScrapyCommand.add_options(self, parser)
-        parser.add_argument("--verbose", "-v", dest="verbose", action="store_true",
-                            help="also display twisted/python/platform info (useful for bug reports)")
+        parser.add_argument(
+            "--verbose",
+            "-v",
+            dest="verbose",
+            action="store_true",
+            help="also display twisted/python/platform info (useful for bug reports)",
+        )
 
     def run(self, args, opts):
         if opts.verbose:
             versions = scrapy_components_versions()
             width = max(len(n) for (n, _) in versions)
             for name, version in versions:
                 print(f"{name:<{width}} : {version}")
```

### Comparing `Scrapy-2.7.1/scrapy/commands/view.py` & `Scrapy-2.8.0/scrapy/commands/view.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 import argparse
+
 from scrapy.commands import fetch
 from scrapy.utils.response import open_in_browser
 
 
 class Command(fetch.Command):
-
     def short_desc(self):
         return "Open URL in browser, as seen by Scrapy"
 
     def long_desc(self):
-        return "Fetch a URL using the Scrapy downloader and show its contents in a browser"
+        return (
+            "Fetch a URL using the Scrapy downloader and show its contents in a browser"
+        )
 
     def add_options(self, parser):
         super().add_options(parser)
-        parser.add_argument('--headers', help=argparse.SUPPRESS)
+        parser.add_argument("--headers", help=argparse.SUPPRESS)
 
     def _print_response(self, response, opts):
         open_in_browser(response)
```

### Comparing `Scrapy-2.7.1/scrapy/contracts/__init__.py` & `Scrapy-2.8.0/scrapy/contracts/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,24 +7,25 @@
 
 from scrapy.http import Request
 from scrapy.utils.python import get_spec
 from scrapy.utils.spider import iterate_spider_output
 
 
 class Contract:
-    """ Abstract class for contracts """
+    """Abstract class for contracts"""
+
     request_cls = None
 
     def __init__(self, method, *args):
-        self.testcase_pre = _create_testcase(method, f'@{self.name} pre-hook')
-        self.testcase_post = _create_testcase(method, f'@{self.name} post-hook')
+        self.testcase_pre = _create_testcase(method, f"@{self.name} pre-hook")
+        self.testcase_post = _create_testcase(method, f"@{self.name} post-hook")
         self.args = args
 
     def add_pre_hook(self, request, results):
-        if hasattr(self, 'pre_process'):
+        if hasattr(self, "pre_process"):
             cb = request.callback
 
             @wraps(cb)
             def wrapper(response, **cb_kwargs):
                 try:
                     results.startTest(self.testcase_pre)
                     self.pre_process(response)
@@ -39,15 +40,15 @@
                     return list(iterate_spider_output(cb(response, **cb_kwargs)))
 
             request.callback = wrapper
 
         return request
 
     def add_post_hook(self, request, results):
-        if hasattr(self, 'post_process'):
+        if hasattr(self, "post_process"):
             cb = request.callback
 
             @wraps(cb)
             def wrapper(response, **cb_kwargs):
                 output = list(iterate_spider_output(cb(response, **cb_kwargs)))
                 try:
                     results.startTest(self.testcase_post)
@@ -84,33 +85,33 @@
             if callable(value) and value.__doc__ and is_method(value.__doc__):
                 methods.append(key)
 
         return methods
 
     def extract_contracts(self, method):
         contracts = []
-        for line in method.__doc__.split('\n'):
+        for line in method.__doc__.split("\n"):
             line = line.strip()
 
-            if line.startswith('@'):
-                name, args = re.match(r'@(\w+)\s*(.*)', line).groups()
-                args = re.split(r'\s+', args)
+            if line.startswith("@"):
+                name, args = re.match(r"@(\w+)\s*(.*)", line).groups()
+                args = re.split(r"\s+", args)
 
                 contracts.append(self.contracts[name](method, *args))
 
         return contracts
 
     def from_spider(self, spider, results):
         requests = []
         for method in self.tested_methods_from_spidercls(type(spider)):
             bound_method = spider.__getattribute__(method)
             try:
                 requests.append(self.from_method(bound_method, results))
             except Exception:
-                case = _create_testcase(bound_method, 'contract')
+                case = _create_testcase(bound_method, "contract")
                 results.addError(case, sys.exc_info())
 
         return requests
 
     def from_method(self, method, results):
         contracts = self.extract_contracts(method)
         if contracts:
@@ -120,21 +121,21 @@
                     request_cls = contract.request_cls
 
             # calculate request args
             args, kwargs = get_spec(request_cls.__init__)
 
             # Don't filter requests to allow
             # testing different callbacks on the same URL.
-            kwargs['dont_filter'] = True
-            kwargs['callback'] = method
+            kwargs["dont_filter"] = True
+            kwargs["callback"] = method
 
             for contract in contracts:
                 kwargs = contract.adjust_request_args(kwargs)
 
-            args.remove('self')
+            args.remove("self")
 
             # check if all positional arguments are defined in kwargs
             if set(args).issubset(set(kwargs)):
                 request = request_cls(**kwargs)
 
                 # execute pre and post hooks in order
                 for contract in reversed(contracts):
@@ -142,39 +143,39 @@
                 for contract in contracts:
                     request = contract.add_post_hook(request, results)
 
                 self._clean_req(request, method, results)
                 return request
 
     def _clean_req(self, request, method, results):
-        """ stop the request from returning objects and records any errors """
+        """stop the request from returning objects and records any errors"""
 
         cb = request.callback
 
         @wraps(cb)
         def cb_wrapper(response, **cb_kwargs):
             try:
                 output = cb(response, **cb_kwargs)
                 output = list(iterate_spider_output(output))
             except Exception:
-                case = _create_testcase(method, 'callback')
+                case = _create_testcase(method, "callback")
                 results.addError(case, sys.exc_info())
 
         def eb_wrapper(failure):
-            case = _create_testcase(method, 'errback')
+            case = _create_testcase(method, "errback")
             exc_info = failure.type, failure.value, failure.getTracebackObject()
             results.addError(case, exc_info)
 
         request.callback = cb_wrapper
         request.errback = eb_wrapper
 
 
 def _create_testcase(method, desc):
     spider = method.__self__.name
 
     class ContractTestCase(TestCase):
         def __str__(_self):
             return f"[{spider}] {method.__name__} ({desc})"
 
-    name = f'{spider}_{method.__name__}'
+    name = f"{spider}_{method.__name__}"
     setattr(ContractTestCase, name, lambda x: x)
     return ContractTestCase(name)
```

### Comparing `Scrapy-2.7.1/scrapy/contracts/default.py` & `Scrapy-2.8.0/scrapy/contracts/default.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,62 +1,62 @@
 import json
 
-from itemadapter import is_item, ItemAdapter
+from itemadapter import ItemAdapter, is_item
 
 from scrapy.contracts import Contract
 from scrapy.exceptions import ContractFail
 from scrapy.http import Request
 
 
 # contracts
 class UrlContract(Contract):
-    """ Contract to set the url of the request (mandatory)
-        @url http://scrapy.org
+    """Contract to set the url of the request (mandatory)
+    @url http://scrapy.org
     """
 
-    name = 'url'
+    name = "url"
 
     def adjust_request_args(self, args):
-        args['url'] = self.args[0]
+        args["url"] = self.args[0]
         return args
 
 
 class CallbackKeywordArgumentsContract(Contract):
-    """ Contract to set the keyword arguments for the request.
-        The value should be a JSON-encoded dictionary, e.g.:
+    """Contract to set the keyword arguments for the request.
+    The value should be a JSON-encoded dictionary, e.g.:
 
-        @cb_kwargs {"arg1": "some value"}
+    @cb_kwargs {"arg1": "some value"}
     """
 
-    name = 'cb_kwargs'
+    name = "cb_kwargs"
 
     def adjust_request_args(self, args):
-        args['cb_kwargs'] = json.loads(' '.join(self.args))
+        args["cb_kwargs"] = json.loads(" ".join(self.args))
         return args
 
 
 class ReturnsContract(Contract):
-    """ Contract to check the output of a callback
+    """Contract to check the output of a callback
 
-        general form:
-        @returns request(s)/item(s) [min=1 [max]]
+    general form:
+    @returns request(s)/item(s) [min=1 [max]]
 
-        e.g.:
-        @returns request
-        @returns request 2
-        @returns request 2 10
-        @returns request 0 10
+    e.g.:
+    @returns request
+    @returns request 2
+    @returns request 2 10
+    @returns request 0 10
     """
 
-    name = 'returns'
+    name = "returns"
     object_type_verifiers = {
-        'request': lambda x: isinstance(x, Request),
-        'requests': lambda x: isinstance(x, Request),
-        'item': is_item,
-        'items': is_item,
+        "request": lambda x: isinstance(x, Request),
+        "requests": lambda x: isinstance(x, Request),
+        "item": is_item,
+        "items": is_item,
     }
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
         if len(self.args) not in [1, 2, 3]:
             raise ValueError(
@@ -69,39 +69,41 @@
             self.min_bound = int(self.args[1])
         except IndexError:
             self.min_bound = 1
 
         try:
             self.max_bound = int(self.args[2])
         except IndexError:
-            self.max_bound = float('inf')
+            self.max_bound = float("inf")
 
     def post_process(self, output):
         occurrences = 0
         for x in output:
             if self.obj_type_verifier(x):
                 occurrences += 1
 
-        assertion = (self.min_bound <= occurrences <= self.max_bound)
+        assertion = self.min_bound <= occurrences <= self.max_bound
 
         if not assertion:
             if self.min_bound == self.max_bound:
                 expected = self.min_bound
             else:
-                expected = f'{self.min_bound}..{self.max_bound}'
+                expected = f"{self.min_bound}..{self.max_bound}"
 
-            raise ContractFail(f"Returned {occurrences} {self.obj_name}, expected {expected}")
+            raise ContractFail(
+                f"Returned {occurrences} {self.obj_name}, expected {expected}"
+            )
 
 
 class ScrapesContract(Contract):
-    """ Contract to check presence of fields in scraped items
-        @scrapes page_name page_body
+    """Contract to check presence of fields in scraped items
+    @scrapes page_name page_body
     """
 
-    name = 'scrapes'
+    name = "scrapes"
 
     def post_process(self, output):
         for x in output:
             if is_item(x):
                 missing = [arg for arg in self.args if arg not in ItemAdapter(x)]
                 if missing:
                     missing_fields = ", ".join(missing)
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/__init__.py` & `Scrapy-2.8.0/scrapy/core/downloader/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 import random
-from time import time
-from datetime import datetime
 from collections import deque
+from datetime import datetime
+from time import time
 
 from twisted.internet import defer, task
 
-from scrapy.utils.defer import mustbe_deferred
-from scrapy.utils.httpobj import urlparse_cached
-from scrapy.resolver import dnscache
 from scrapy import signals
-from scrapy.core.downloader.middleware import DownloaderMiddlewareManager
 from scrapy.core.downloader.handlers import DownloadHandlers
+from scrapy.core.downloader.middleware import DownloaderMiddlewareManager
+from scrapy.resolver import dnscache
+from scrapy.utils.defer import mustbe_deferred
+from scrapy.utils.httpobj import urlparse_cached
 
 
 class Slot:
     """Downloader slot"""
 
     def __init__(self, concurrency, delay, randomize_delay):
         self.concurrency = concurrency
@@ -37,53 +37,55 @@
 
     def close(self):
         if self.latercall and self.latercall.active():
             self.latercall.cancel()
 
     def __repr__(self):
         cls_name = self.__class__.__name__
-        return (f"{cls_name}(concurrency={self.concurrency!r}, "
-                f"delay={self.delay:.2f}, "
-                f"randomize_delay={self.randomize_delay!r})")
+        return (
+            f"{cls_name}(concurrency={self.concurrency!r}, "
+            f"delay={self.delay:.2f}, "
+            f"randomize_delay={self.randomize_delay!r})"
+        )
 
     def __str__(self):
         return (
             f"<downloader.Slot concurrency={self.concurrency!r} "
             f"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} "
             f"len(active)={len(self.active)} len(queue)={len(self.queue)} "
             f"len(transferring)={len(self.transferring)} "
             f"lastseen={datetime.fromtimestamp(self.lastseen).isoformat()}>"
         )
 
 
 def _get_concurrency_delay(concurrency, spider, settings):
-    delay = settings.getfloat('DOWNLOAD_DELAY')
-    if hasattr(spider, 'download_delay'):
+    delay = settings.getfloat("DOWNLOAD_DELAY")
+    if hasattr(spider, "download_delay"):
         delay = spider.download_delay
 
-    if hasattr(spider, 'max_concurrent_requests'):
+    if hasattr(spider, "max_concurrent_requests"):
         concurrency = spider.max_concurrent_requests
 
     return concurrency, delay
 
 
 class Downloader:
 
-    DOWNLOAD_SLOT = 'download_slot'
+    DOWNLOAD_SLOT = "download_slot"
 
     def __init__(self, crawler):
         self.settings = crawler.settings
         self.signals = crawler.signals
         self.slots = {}
         self.active = set()
         self.handlers = DownloadHandlers(crawler)
-        self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')
-        self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')
-        self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')
-        self.randomize_delay = self.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY')
+        self.total_concurrency = self.settings.getint("CONCURRENT_REQUESTS")
+        self.domain_concurrency = self.settings.getint("CONCURRENT_REQUESTS_PER_DOMAIN")
+        self.ip_concurrency = self.settings.getint("CONCURRENT_REQUESTS_PER_IP")
+        self.randomize_delay = self.settings.getbool("RANDOMIZE_DOWNLOAD_DELAY")
         self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
         self._slot_gc_loop = task.LoopingCall(self._slot_gc)
         self._slot_gc_loop.start(60)
 
     def fetch(self, request, spider):
         def _deactivate(response):
             self.active.remove(request)
@@ -95,59 +97,64 @@
 
     def needs_backout(self):
         return len(self.active) >= self.total_concurrency
 
     def _get_slot(self, request, spider):
         key = self._get_slot_key(request, spider)
         if key not in self.slots:
-            conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency
+            conc = (
+                self.ip_concurrency if self.ip_concurrency else self.domain_concurrency
+            )
             conc, delay = _get_concurrency_delay(conc, spider, self.settings)
             self.slots[key] = Slot(conc, delay, self.randomize_delay)
 
         return key, self.slots[key]
 
     def _get_slot_key(self, request, spider):
         if self.DOWNLOAD_SLOT in request.meta:
             return request.meta[self.DOWNLOAD_SLOT]
 
-        key = urlparse_cached(request).hostname or ''
+        key = urlparse_cached(request).hostname or ""
         if self.ip_concurrency:
             key = dnscache.get(key, key)
 
         return key
 
     def _enqueue_request(self, request, spider):
         key, slot = self._get_slot(request, spider)
         request.meta[self.DOWNLOAD_SLOT] = key
 
         def _deactivate(response):
             slot.active.remove(request)
             return response
 
         slot.active.add(request)
-        self.signals.send_catch_log(signal=signals.request_reached_downloader,
-                                    request=request,
-                                    spider=spider)
+        self.signals.send_catch_log(
+            signal=signals.request_reached_downloader, request=request, spider=spider
+        )
         deferred = defer.Deferred().addBoth(_deactivate)
         slot.queue.append((request, deferred))
         self._process_queue(spider, slot)
         return deferred
 
     def _process_queue(self, spider, slot):
         from twisted.internet import reactor
+
         if slot.latercall and slot.latercall.active():
             return
 
         # Delay queue processing if a download_delay is configured
         now = time()
         delay = slot.download_delay()
         if delay:
             penalty = delay - now + slot.lastseen
             if penalty > 0:
-                slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot)
+                slot.latercall = reactor.callLater(
+                    penalty, self._process_queue, spider, slot
+                )
                 return
 
         # Process enqueued requests if there are free slots to transfer for this slot
         while slot.queue and slot.free_transfer_slots() > 0:
             slot.lastseen = now
             request, deferred = slot.queue.popleft()
             dfd = self._download(slot, request, spider)
@@ -162,33 +169,36 @@
 
         # 1. Create the download deferred
         dfd = mustbe_deferred(self.handlers.download_request, request, spider)
 
         # 2. Notify response_downloaded listeners about the recent download
         # before querying queue for next request
         def _downloaded(response):
-            self.signals.send_catch_log(signal=signals.response_downloaded,
-                                        response=response,
-                                        request=request,
-                                        spider=spider)
+            self.signals.send_catch_log(
+                signal=signals.response_downloaded,
+                response=response,
+                request=request,
+                spider=spider,
+            )
             return response
+
         dfd.addCallback(_downloaded)
 
         # 3. After response arrives, remove the request from transferring
         # state to free up the transferring slot so it can be used by the
         # following requests (perhaps those which came from the downloader
         # middleware itself)
         slot.transferring.add(request)
 
         def finish_transferring(_):
             slot.transferring.remove(request)
             self._process_queue(spider, slot)
-            self.signals.send_catch_log(signal=signals.request_left_downloader,
-                                        request=request,
-                                        spider=spider)
+            self.signals.send_catch_log(
+                signal=signals.request_left_downloader, request=request, spider=spider
+            )
             return _
 
         return dfd.addBoth(finish_transferring)
 
     def close(self):
         self._slot_gc_loop.stop()
         for slot in self.slots.values():
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/contextfactory.py` & `Scrapy-2.8.0/scrapy/core/downloader/contextfactory.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,27 @@
 import warnings
 
 from OpenSSL import SSL
 from twisted.internet._sslverify import _setAcceptableProtocols
-from twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust, AcceptableCiphers
+from twisted.internet.ssl import (
+    AcceptableCiphers,
+    CertificateOptions,
+    optionsForClientTLS,
+    platformTrust,
+)
 from twisted.web.client import BrowserLikePolicyForHTTPS
 from twisted.web.iweb import IPolicyForHTTPS
 from zope.interface.declarations import implementer
 from zope.interface.verify import verifyObject
 
-from scrapy.core.downloader.tls import DEFAULT_CIPHERS, openssl_methods, ScrapyClientTLSOptions
+from scrapy.core.downloader.tls import (
+    DEFAULT_CIPHERS,
+    ScrapyClientTLSOptions,
+    openssl_methods,
+)
 from scrapy.utils.misc import create_instance, load_object
 
 
 @implementer(IPolicyForHTTPS)
 class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):
     """
     Non-peer-certificate verifying HTTPS context factory
@@ -20,28 +29,41 @@
     Default OpenSSL method is TLS_METHOD (also called SSLv23_METHOD)
     which allows TLS protocol negotiation
 
     'A TLS/SSL connection established with [this method] may
      understand the TLSv1, TLSv1.1 and TLSv1.2 protocols.'
     """
 
-    def __init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, tls_ciphers=None, *args, **kwargs):
+    def __init__(
+        self,
+        method=SSL.SSLv23_METHOD,
+        tls_verbose_logging=False,
+        tls_ciphers=None,
+        *args,
+        **kwargs,
+    ):
         super().__init__(*args, **kwargs)
         self._ssl_method = method
         self.tls_verbose_logging = tls_verbose_logging
         if tls_ciphers:
             self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)
         else:
             self.tls_ciphers = DEFAULT_CIPHERS
 
     @classmethod
     def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):
-        tls_verbose_logging = settings.getbool('DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING')
-        tls_ciphers = settings['DOWNLOADER_CLIENT_TLS_CIPHERS']
-        return cls(method=method, tls_verbose_logging=tls_verbose_logging, tls_ciphers=tls_ciphers, *args, **kwargs)
+        tls_verbose_logging = settings.getbool("DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING")
+        tls_ciphers = settings["DOWNLOADER_CLIENT_TLS_CIPHERS"]
+        return cls(
+            method=method,
+            tls_verbose_logging=tls_verbose_logging,
+            tls_ciphers=tls_ciphers,
+            *args,
+            **kwargs,
+        )
 
     def getCertificateOptions(self):
         # setting verify=True will require you to provide CAs
         # to verify against; in other words: it's not that simple
 
         # backward-compatible SSL/TLS method:
         #
@@ -49,27 +71,32 @@
         #   `ScrapyClientContextFactory` subclass
         #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)
         #
         # * getattr() for `_ssl_method` attribute for context factories
         #   not calling super().__init__
         return CertificateOptions(
             verify=False,
-            method=getattr(self, 'method', getattr(self, '_ssl_method', None)),
+            method=getattr(self, "method", getattr(self, "_ssl_method", None)),
             fixBrokenPeers=True,
             acceptableCiphers=self.tls_ciphers,
         )
 
     # kept for old-style HTTP/1.0 downloader context twisted calls,
     # e.g. connectSSL()
     def getContext(self, hostname=None, port=None):
-        return self.getCertificateOptions().getContext()
+        ctx = self.getCertificateOptions().getContext()
+        ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT
+        return ctx
 
     def creatorForNetloc(self, hostname, port):
-        return ScrapyClientTLSOptions(hostname.decode("ascii"), self.getContext(),
-                                      verbose_logging=self.tls_verbose_logging)
+        return ScrapyClientTLSOptions(
+            hostname.decode("ascii"),
+            self.getContext(),
+            verbose_logging=self.tls_verbose_logging,
+        )
 
 
 @implementer(IPolicyForHTTPS)
 class BrowserLikeContextFactory(ScrapyClientContextFactory):
     """
     Twisted-recommended context factory for web clients.
 
@@ -91,15 +118,15 @@
         # trustRoot set to platformTrust() will use the platform's root CAs.
         #
         # This means that a website like https://www.cacert.org will be rejected
         # by default, since CAcert.org CA certificate is seldom shipped.
         return optionsForClientTLS(
             hostname=hostname.decode("ascii"),
             trustRoot=platformTrust(),
-            extraCertificateOptions={'method': self._ssl_method},
+            extraCertificateOptions={"method": self._ssl_method},
         )
 
 
 @implementer(IPolicyForHTTPS)
 class AcceptableProtocolsContextFactory:
     """Context factory to used to override the acceptable protocols
     to set up the [OpenSSL.SSL.Context] for doing NPN and/or ALPN
@@ -114,16 +141,16 @@
     def creatorForNetloc(self, hostname, port):
         options = self._wrapped_context_factory.creatorForNetloc(hostname, port)
         _setAcceptableProtocols(options._ctx, self._acceptable_protocols)
         return options
 
 
 def load_context_factory_from_settings(settings, crawler):
-    ssl_method = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]
-    context_factory_cls = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])
+    ssl_method = openssl_methods[settings.get("DOWNLOADER_CLIENT_TLS_METHOD")]
+    context_factory_cls = load_object(settings["DOWNLOADER_CLIENTCONTEXTFACTORY"])
     # try method-aware context factory
     try:
         context_factory = create_instance(
             objcls=context_factory_cls,
             settings=settings,
             crawler=crawler,
             method=ssl_method,
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/handlers/__init__.py` & `Scrapy-2.8.0/scrapy/core/downloader/handlers/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,27 +6,26 @@
 
 from scrapy import signals
 from scrapy.exceptions import NotConfigured, NotSupported
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.misc import create_instance, load_object
 from scrapy.utils.python import without_none_values
 
-
 logger = logging.getLogger(__name__)
 
 
 class DownloadHandlers:
-
     def __init__(self, crawler):
         self._crawler = crawler
         self._schemes = {}  # stores acceptable schemes on instancing
         self._handlers = {}  # stores instanced handlers for schemes
         self._notconfigured = {}  # remembers failed handlers
         handlers = without_none_values(
-            crawler.settings.getwithbase('DOWNLOAD_HANDLERS'))
+            crawler.settings.getwithbase("DOWNLOAD_HANDLERS")
+        )
         for scheme, clspath in handlers.items():
             self._schemes[scheme] = clspath
             self._load_handler(scheme, skip_lazy=True)
 
         crawler.signals.connect(self._close, signals.engine_stopped)
 
     def _get_handler(self, scheme):
@@ -34,48 +33,53 @@
         only on the first request for that scheme.
         """
         if scheme in self._handlers:
             return self._handlers[scheme]
         if scheme in self._notconfigured:
             return None
         if scheme not in self._schemes:
-            self._notconfigured[scheme] = 'no handler available for that scheme'
+            self._notconfigured[scheme] = "no handler available for that scheme"
             return None
 
         return self._load_handler(scheme)
 
     def _load_handler(self, scheme, skip_lazy=False):
         path = self._schemes[scheme]
         try:
             dhcls = load_object(path)
-            if skip_lazy and getattr(dhcls, 'lazy', True):
+            if skip_lazy and getattr(dhcls, "lazy", True):
                 return None
             dh = create_instance(
                 objcls=dhcls,
                 settings=self._crawler.settings,
                 crawler=self._crawler,
             )
         except NotConfigured as ex:
             self._notconfigured[scheme] = str(ex)
             return None
         except Exception as ex:
-            logger.error('Loading "%(clspath)s" for scheme "%(scheme)s"',
-                         {"clspath": path, "scheme": scheme},
-                         exc_info=True, extra={'crawler': self._crawler})
+            logger.error(
+                'Loading "%(clspath)s" for scheme "%(scheme)s"',
+                {"clspath": path, "scheme": scheme},
+                exc_info=True,
+                extra={"crawler": self._crawler},
+            )
             self._notconfigured[scheme] = str(ex)
             return None
         else:
             self._handlers[scheme] = dh
             return dh
 
     def download_request(self, request, spider):
         scheme = urlparse_cached(request).scheme
         handler = self._get_handler(scheme)
         if not handler:
-            raise NotSupported(f"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}")
+            raise NotSupported(
+                f"Unsupported URL scheme '{scheme}': {self._notconfigured[scheme]}"
+            )
         return handler.download_request(request, spider)
 
     @defer.inlineCallbacks
     def _close(self, *_a, **_kw):
         for dh in self._handlers.values():
-            if hasattr(dh, 'close'):
+            if hasattr(dh, "close"):
                 yield dh.close()
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/handlers/datauri.py` & `Scrapy-2.8.0/scrapy/core/downloader/handlers/datauri.py`

 * *Files 24% similar despite different names*

```diff
@@ -10,13 +10,12 @@
 
     @defers
     def download_request(self, request, spider):
         uri = parse_data_uri(request.url)
         respcls = responsetypes.from_mimetype(uri.media_type)
 
         resp_kwargs = {}
-        if (issubclass(respcls, TextResponse)
-                and uri.media_type.split('/')[0] == 'text'):
-            charset = uri.media_type_parameters.get('charset')
-            resp_kwargs['encoding'] = charset
+        if issubclass(respcls, TextResponse) and uri.media_type.split("/")[0] == "text":
+            charset = uri.media_type_parameters.get("charset")
+            resp_kwargs["encoding"] = charset
 
         return respcls(url=request.url, body=uri.data, **resp_kwargs)
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/handlers/ftp.py` & `Scrapy-2.8.0/scrapy/core/downloader/handlers/ftp.py`

 * *Files 2% similar despite different names*

```diff
@@ -67,30 +67,34 @@
 
     CODE_MAPPING = {
         "550": 404,
         "default": 503,
     }
 
     def __init__(self, settings):
-        self.default_user = settings['FTP_USER']
-        self.default_password = settings['FTP_PASSWORD']
-        self.passive_mode = settings['FTP_PASSIVE_MODE']
+        self.default_user = settings["FTP_USER"]
+        self.default_password = settings["FTP_PASSWORD"]
+        self.passive_mode = settings["FTP_PASSIVE_MODE"]
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler.settings)
 
     def download_request(self, request, spider):
         from twisted.internet import reactor
+
         parsed_url = urlparse_cached(request)
         user = request.meta.get("ftp_user", self.default_user)
         password = request.meta.get("ftp_password", self.default_password)
-        passive_mode = 1 if bool(request.meta.get("ftp_passive",
-                                                  self.passive_mode)) else 0
-        creator = ClientCreator(reactor, FTPClient, user, password, passive=passive_mode)
+        passive_mode = (
+            1 if bool(request.meta.get("ftp_passive", self.passive_mode)) else 0
+        )
+        creator = ClientCreator(
+            reactor, FTPClient, user, password, passive=passive_mode
+        )
         dfd = creator.connectTCP(parsed_url.hostname, parsed_url.port or 21)
         return dfd.addCallback(self.gotClient, request, unquote(parsed_url.path))
 
     def gotClient(self, client, request, filepath):
         self.client = client
         protocol = ReceivedDataProtocol(request.meta.get("ftp_local_filename"))
         return client.retrieveFile(filepath, protocol).addCallbacks(
@@ -99,21 +103,23 @@
             errback=self._failed,
             errbackArgs=(request,),
         )
 
     def _build_response(self, result, request, protocol):
         self.result = result
         protocol.close()
-        headers = {"local filename": protocol.filename or '', "size": protocol.size}
+        headers = {"local filename": protocol.filename or "", "size": protocol.size}
         body = to_bytes(protocol.filename or protocol.body.read())
         respcls = responsetypes.from_args(url=request.url, body=body)
         return respcls(url=request.url, status=200, body=body, headers=headers)
 
     def _failed(self, result, request):
         message = result.getErrorMessage()
         if result.type == CommandFailed:
             m = _CODE_RE.search(message)
             if m:
                 ftpcode = m.group()
                 httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING["default"])
-                return Response(url=request.url, status=httpcode, body=to_bytes(message))
+                return Response(
+                    url=request.url, status=httpcode, body=to_bytes(message)
+                )
         raise result.type(result.value)
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/handlers/http10.py` & `Scrapy-2.8.0/scrapy/core/downloader/handlers/http10.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,16 +4,18 @@
 from scrapy.utils.python import to_unicode
 
 
 class HTTP10DownloadHandler:
     lazy = False
 
     def __init__(self, settings, crawler=None):
-        self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])
-        self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])
+        self.HTTPClientFactory = load_object(settings["DOWNLOADER_HTTPCLIENTFACTORY"])
+        self.ClientContextFactory = load_object(
+            settings["DOWNLOADER_CLIENTCONTEXTFACTORY"]
+        )
         self._settings = settings
         self._crawler = crawler
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler.settings, crawler)
 
@@ -21,17 +23,17 @@
         """Return a deferred for the HTTP download"""
         factory = self.HTTPClientFactory(request)
         self._connect(factory)
         return factory.deferred
 
     def _connect(self, factory):
         from twisted.internet import reactor
+
         host, port = to_unicode(factory.host), factory.port
-        if factory.scheme == b'https':
+        if factory.scheme == b"https":
             client_context_factory = create_instance(
                 objcls=self.ClientContextFactory,
                 settings=self._settings,
                 crawler=self._crawler,
             )
             return reactor.connectSSL(host, port, factory, client_context_factory)
-        else:
-            return reactor.connectTCP(host, port, factory)
+        return reactor.connectTCP(host, port, factory)
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/handlers/http11.py` & `Scrapy-2.8.0/scrapy/core/downloader/handlers/http11.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,75 +1,83 @@
 """Download handlers for http and https schemes"""
 
 import ipaddress
 import logging
 import re
-import warnings
 from contextlib import suppress
 from io import BytesIO
 from time import time
 from urllib.parse import urldefrag, urlunparse
 
 from twisted.internet import defer, protocol, ssl
 from twisted.internet.endpoints import TCP4ClientEndpoint
 from twisted.internet.error import TimeoutError
 from twisted.python.failure import Failure
-from twisted.web.client import Agent, HTTPConnectionPool, ResponseDone, ResponseFailed, URI
-from twisted.web.http import _DataLoss, PotentialDataLoss
+from twisted.web.client import (
+    URI,
+    Agent,
+    HTTPConnectionPool,
+    ResponseDone,
+    ResponseFailed,
+)
+from twisted.web.http import PotentialDataLoss, _DataLoss
 from twisted.web.http_headers import Headers as TxHeaders
-from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH
+from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer
 from zope.interface import implementer
 
 from scrapy import signals
 from scrapy.core.downloader.contextfactory import load_context_factory_from_settings
 from scrapy.core.downloader.webclient import _parse
-from scrapy.exceptions import ScrapyDeprecationWarning, StopDownload
+from scrapy.exceptions import StopDownload
 from scrapy.http import Headers
 from scrapy.responsetypes import responsetypes
 from scrapy.utils.python import to_bytes, to_unicode
 
-
 logger = logging.getLogger(__name__)
 
 
 class HTTP11DownloadHandler:
     lazy = False
 
     def __init__(self, settings, crawler=None):
         self._crawler = crawler
 
         from twisted.internet import reactor
+
         self._pool = HTTPConnectionPool(reactor, persistent=True)
-        self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')
+        self._pool.maxPersistentPerHost = settings.getint(
+            "CONCURRENT_REQUESTS_PER_DOMAIN"
+        )
         self._pool._factory.noisy = False
 
         self._contextFactory = load_context_factory_from_settings(settings, crawler)
-        self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')
-        self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')
-        self._fail_on_dataloss = settings.getbool('DOWNLOAD_FAIL_ON_DATALOSS')
+        self._default_maxsize = settings.getint("DOWNLOAD_MAXSIZE")
+        self._default_warnsize = settings.getint("DOWNLOAD_WARNSIZE")
+        self._fail_on_dataloss = settings.getbool("DOWNLOAD_FAIL_ON_DATALOSS")
         self._disconnect_timeout = 1
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler.settings, crawler)
 
     def download_request(self, request, spider):
         """Return a deferred for the HTTP download"""
         agent = ScrapyAgent(
             contextFactory=self._contextFactory,
             pool=self._pool,
-            maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),
-            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),
+            maxsize=getattr(spider, "download_maxsize", self._default_maxsize),
+            warnsize=getattr(spider, "download_warnsize", self._default_warnsize),
             fail_on_dataloss=self._fail_on_dataloss,
             crawler=self._crawler,
         )
         return agent.download_request(request)
 
     def close(self):
         from twisted.internet import reactor
+
         d = self._pool.closeCachedConnections()
         # closeCachedConnections will hang on network or server issues, so
         # we'll manually timeout the deferred.
         #
         # Twisted issue addressing this problem can be found here:
         # https://twistedmatrix.com/trac/ticket/7738.
         #
@@ -94,30 +102,44 @@
     """An endpoint that tunnels through proxies to allow HTTPS downloads. To
     accomplish that, this endpoint sends an HTTP CONNECT to the proxy.
     The HTTP CONNECT is always sent when using this endpoint, I think this could
     be improved as the CONNECT will be redundant if the connection associated
     with this endpoint comes from the pool and a CONNECT has already been issued
     for it.
     """
+
     _truncatedLength = 1000
-    _responseAnswer = r'HTTP/1\.. (?P<status>\d{3})(?P<reason>.{,' + str(_truncatedLength) + r'})'
+    _responseAnswer = (
+        r"HTTP/1\.. (?P<status>\d{3})(?P<reason>.{," + str(_truncatedLength) + r"})"
+    )
     _responseMatcher = re.compile(_responseAnswer.encode())
 
-    def __init__(self, reactor, host, port, proxyConf, contextFactory, timeout=30, bindAddress=None):
+    def __init__(
+        self,
+        reactor,
+        host,
+        port,
+        proxyConf,
+        contextFactory,
+        timeout=30,
+        bindAddress=None,
+    ):
         proxyHost, proxyPort, self._proxyAuthHeader = proxyConf
         super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)
         self._tunnelReadyDeferred = defer.Deferred()
         self._tunneledHost = host
         self._tunneledPort = port
         self._contextFactory = contextFactory
         self._connectBuffer = bytearray()
 
     def requestTunnel(self, protocol):
         """Asks the proxy to open a tunnel."""
-        tunnelReq = tunnel_request_data(self._tunneledHost, self._tunneledPort, self._proxyAuthHeader)
+        tunnelReq = tunnel_request_data(
+            self._tunneledHost, self._tunneledPort, self._proxyAuthHeader
+        )
         protocol.transport.write(tunnelReq)
         self._protocolDataReceived = protocol.dataReceived
         protocol.dataReceived = self.processProxyResponse
         self._protocol = protocol
         return protocol
 
     def processProxyResponse(self, rcvd_bytes):
@@ -127,32 +149,38 @@
         """
         self._connectBuffer += rcvd_bytes
         # make sure that enough (all) bytes are consumed
         # and that we've got all HTTP headers (ending with a blank line)
         # from the proxy so that we don't send those bytes to the TLS layer
         #
         # see https://github.com/scrapy/scrapy/issues/2491
-        if b'\r\n\r\n' not in self._connectBuffer:
+        if b"\r\n\r\n" not in self._connectBuffer:
             return
         self._protocol.dataReceived = self._protocolDataReceived
         respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)
-        if respm and int(respm.group('status')) == 200:
+        if respm and int(respm.group("status")) == 200:
             # set proper Server Name Indication extension
-            sslOptions = self._contextFactory.creatorForNetloc(self._tunneledHost, self._tunneledPort)
+            sslOptions = self._contextFactory.creatorForNetloc(
+                self._tunneledHost, self._tunneledPort
+            )
             self._protocol.transport.startTLS(sslOptions, self._protocolFactory)
             self._tunnelReadyDeferred.callback(self._protocol)
         else:
             if respm:
-                extra = {'status': int(respm.group('status')),
-                         'reason': respm.group('reason').strip()}
+                extra = {
+                    "status": int(respm.group("status")),
+                    "reason": respm.group("reason").strip(),
+                }
             else:
-                extra = rcvd_bytes[:self._truncatedLength]
+                extra = rcvd_bytes[: self._truncatedLength]
             self._tunnelReadyDeferred.errback(
-                TunnelError('Could not open CONNECT tunnel with proxy '
-                            f'{self._host}:{self._port} [{extra!r}]')
+                TunnelError(
+                    "Could not open CONNECT tunnel with proxy "
+                    f"{self._host}:{self._port} [{extra!r}]"
+                )
             )
 
     def connectFailed(self, reason):
         """Propagates the errback to the appropriate deferred."""
         self._tunnelReadyDeferred.errback(reason)
 
     def connect(self, protocolFactory):
@@ -171,33 +199,40 @@
     >>> s(tunnel_request_data("example.com", 8080))
     'CONNECT example.com:8080 HTTP/1.1\r\nHost: example.com:8080\r\n\r\n'
     >>> s(tunnel_request_data("example.com", 8080, b"123"))
     'CONNECT example.com:8080 HTTP/1.1\r\nHost: example.com:8080\r\nProxy-Authorization: 123\r\n\r\n'
     >>> s(tunnel_request_data(b"example.com", "8090"))
     'CONNECT example.com:8090 HTTP/1.1\r\nHost: example.com:8090\r\n\r\n'
     """
-    host_value = to_bytes(host, encoding='ascii') + b':' + to_bytes(str(port))
-    tunnel_req = b'CONNECT ' + host_value + b' HTTP/1.1\r\n'
-    tunnel_req += b'Host: ' + host_value + b'\r\n'
+    host_value = to_bytes(host, encoding="ascii") + b":" + to_bytes(str(port))
+    tunnel_req = b"CONNECT " + host_value + b" HTTP/1.1\r\n"
+    tunnel_req += b"Host: " + host_value + b"\r\n"
     if proxy_auth_header:
-        tunnel_req += b'Proxy-Authorization: ' + proxy_auth_header + b'\r\n'
-    tunnel_req += b'\r\n'
+        tunnel_req += b"Proxy-Authorization: " + proxy_auth_header + b"\r\n"
+    tunnel_req += b"\r\n"
     return tunnel_req
 
 
 class TunnelingAgent(Agent):
     """An agent that uses a L{TunnelingTCP4ClientEndpoint} to make HTTPS
     downloads. It may look strange that we have chosen to subclass Agent and not
     ProxyAgent but consider that after the tunnel is opened the proxy is
     transparent to the client; thus the agent should behave like there is no
     proxy involved.
     """
 
-    def __init__(self, reactor, proxyConf, contextFactory=None,
-                 connectTimeout=None, bindAddress=None, pool=None):
+    def __init__(
+        self,
+        reactor,
+        proxyConf,
+        contextFactory=None,
+        connectTimeout=None,
+        bindAddress=None,
+        pool=None,
+    ):
         super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)
         self._proxyConf = proxyConf
         self._contextFactory = contextFactory
 
     def _getEndpoint(self, uri):
         return TunnelingTCP4ClientEndpoint(
             reactor=self._reactor,
@@ -205,15 +240,17 @@
             port=uri.port,
             proxyConf=self._proxyConf,
             contextFactory=self._contextFactory,
             timeout=self._endpointFactory._connectTimeout,
             bindAddress=self._endpointFactory._bindAddress,
         )
 
-    def _requestWithEndpoint(self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath):
+    def _requestWithEndpoint(
+        self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath
+    ):
         # proxy host and port are required for HTTP pool `key`
         # otherwise, same remote host connection request could reuse
         # a cached tunneled connection to a different proxy
         key += self._proxyConf
         return super()._requestWithEndpoint(
             key=key,
             endpoint=endpoint,
@@ -222,16 +259,17 @@
             headers=headers,
             bodyProducer=bodyProducer,
             requestPath=requestPath,
         )
 
 
 class ScrapyProxyAgent(Agent):
-
-    def __init__(self, reactor, proxyURI, connectTimeout=None, bindAddress=None, pool=None):
+    def __init__(
+        self, reactor, proxyURI, connectTimeout=None, bindAddress=None, pool=None
+    ):
         super().__init__(
             reactor=reactor,
             connectTimeout=connectTimeout,
             bindAddress=bindAddress,
             pool=pool,
         )
         self._proxyURI = URI.fromBytes(proxyURI)
@@ -255,93 +293,93 @@
 
 class ScrapyAgent:
 
     _Agent = Agent
     _ProxyAgent = ScrapyProxyAgent
     _TunnelingAgent = TunnelingAgent
 
-    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,
-                 maxsize=0, warnsize=0, fail_on_dataloss=True, crawler=None):
+    def __init__(
+        self,
+        contextFactory=None,
+        connectTimeout=10,
+        bindAddress=None,
+        pool=None,
+        maxsize=0,
+        warnsize=0,
+        fail_on_dataloss=True,
+        crawler=None,
+    ):
         self._contextFactory = contextFactory
         self._connectTimeout = connectTimeout
         self._bindAddress = bindAddress
         self._pool = pool
         self._maxsize = maxsize
         self._warnsize = warnsize
         self._fail_on_dataloss = fail_on_dataloss
         self._txresponse = None
         self._crawler = crawler
 
     def _get_agent(self, request, timeout):
         from twisted.internet import reactor
-        bindaddress = request.meta.get('bindaddress') or self._bindAddress
-        proxy = request.meta.get('proxy')
+
+        bindaddress = request.meta.get("bindaddress") or self._bindAddress
+        proxy = request.meta.get("proxy")
         if proxy:
             proxyScheme, proxyNetloc, proxyHost, proxyPort, proxyParams = _parse(proxy)
             scheme = _parse(request.url)[0]
             proxyHost = to_unicode(proxyHost)
-            omitConnectTunnel = b'noconnect' in proxyParams
-            if omitConnectTunnel:
-                warnings.warn(
-                    "Using HTTPS proxies in the noconnect mode is deprecated. "
-                    "If you use Zyte Smart Proxy Manager, it doesn't require "
-                    "this mode anymore, so you should update scrapy-crawlera "
-                    "to scrapy-zyte-smartproxy and remove '?noconnect' "
-                    "from the Zyte Smart Proxy Manager URL.",
-                    ScrapyDeprecationWarning,
-                )
-            if scheme == b'https' and not omitConnectTunnel:
-                proxyAuth = request.headers.get(b'Proxy-Authorization', None)
+            if scheme == b"https":
+                proxyAuth = request.headers.get(b"Proxy-Authorization", None)
                 proxyConf = (proxyHost, proxyPort, proxyAuth)
                 return self._TunnelingAgent(
                     reactor=reactor,
                     proxyConf=proxyConf,
                     contextFactory=self._contextFactory,
                     connectTimeout=timeout,
                     bindAddress=bindaddress,
                     pool=self._pool,
                 )
-            else:
-                proxyScheme = proxyScheme or b'http'
-                proxyHost = to_bytes(proxyHost, encoding='ascii')
-                proxyPort = to_bytes(str(proxyPort), encoding='ascii')
-                proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, '', '', ''))
-                return self._ProxyAgent(
-                    reactor=reactor,
-                    proxyURI=to_bytes(proxyURI, encoding='ascii'),
-                    connectTimeout=timeout,
-                    bindAddress=bindaddress,
-                    pool=self._pool,
-                )
+            proxyScheme = proxyScheme or b"http"
+            proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, "", "", ""))
+            return self._ProxyAgent(
+                reactor=reactor,
+                proxyURI=to_bytes(proxyURI, encoding="ascii"),
+                connectTimeout=timeout,
+                bindAddress=bindaddress,
+                pool=self._pool,
+            )
 
         return self._Agent(
             reactor=reactor,
             contextFactory=self._contextFactory,
             connectTimeout=timeout,
             bindAddress=bindaddress,
             pool=self._pool,
         )
 
     def download_request(self, request):
         from twisted.internet import reactor
-        timeout = request.meta.get('download_timeout') or self._connectTimeout
+
+        timeout = request.meta.get("download_timeout") or self._connectTimeout
         agent = self._get_agent(request, timeout)
 
         # request details
         url = urldefrag(request.url)[0]
         method = to_bytes(request.method)
         headers = TxHeaders(request.headers)
         if isinstance(agent, self._TunnelingAgent):
-            headers.removeHeader(b'Proxy-Authorization')
+            headers.removeHeader(b"Proxy-Authorization")
         if request.body:
             bodyproducer = _RequestBodyProducer(request.body)
         else:
             bodyproducer = None
         start_time = time()
-        d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)
+        d = agent.request(
+            method, to_bytes(url, encoding="ascii"), headers, bodyproducer
+        )
         # set download latency
         d.addCallback(self._cb_latency, request, start_time)
         # response body is ready to be consumed
         d.addCallback(self._cb_bodyready, request)
         d.addCallback(self._cb_bodydone, request, url)
         # check download timeout
         self._timeout_cl = reactor.callLater(timeout, d.cancel)
@@ -356,37 +394,39 @@
         # receive connectionLost()
         if self._txresponse:
             self._txresponse._transport.stopProducing()
 
         raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
 
     def _cb_latency(self, result, request, start_time):
-        request.meta['download_latency'] = time() - start_time
+        request.meta["download_latency"] = time() - start_time
         return result
 
     @staticmethod
     def _headers_from_twisted_response(response):
         headers = Headers()
         if response.length != UNKNOWN_LENGTH:
-            headers[b'Content-Length'] = str(response.length).encode()
+            headers[b"Content-Length"] = str(response.length).encode()
         headers.update(response.headers.getAllRawHeaders())
         return headers
 
     def _cb_bodyready(self, txresponse, request):
         headers_received_result = self._crawler.signals.send_catch_log(
             signal=signals.headers_received,
             headers=self._headers_from_twisted_response(txresponse),
             body_length=txresponse.length,
             request=request,
             spider=self._crawler.spider,
         )
         for handler, result in headers_received_result:
             if isinstance(result, Failure) and isinstance(result.value, StopDownload):
-                logger.debug("Download stopped for %(request)s from signal handler %(handler)s",
-                             {"request": request, "handler": handler.__qualname__})
+                logger.debug(
+                    "Download stopped for %(request)s from signal handler %(handler)s",
+                    {"request": request, "handler": handler.__qualname__},
+                )
                 txresponse._transport.stopProducing()
                 txresponse._transport.loseConnection()
                 return {
                     "txresponse": txresponse,
                     "body": b"",
                     "flags": ["download_stopped"],
                     "certificate": None,
@@ -400,33 +440,43 @@
                 "txresponse": txresponse,
                 "body": b"",
                 "flags": None,
                 "certificate": None,
                 "ip_address": None,
             }
 
-        maxsize = request.meta.get('download_maxsize', self._maxsize)
-        warnsize = request.meta.get('download_warnsize', self._warnsize)
+        maxsize = request.meta.get("download_maxsize", self._maxsize)
+        warnsize = request.meta.get("download_warnsize", self._warnsize)
         expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1
-        fail_on_dataloss = request.meta.get('download_fail_on_dataloss', self._fail_on_dataloss)
+        fail_on_dataloss = request.meta.get(
+            "download_fail_on_dataloss", self._fail_on_dataloss
+        )
 
         if maxsize and expected_size > maxsize:
-            warning_msg = ("Cancelling download of %(url)s: expected response "
-                           "size (%(size)s) larger than download max size (%(maxsize)s).")
-            warning_args = {'url': request.url, 'size': expected_size, 'maxsize': maxsize}
+            warning_msg = (
+                "Cancelling download of %(url)s: expected response "
+                "size (%(size)s) larger than download max size (%(maxsize)s)."
+            )
+            warning_args = {
+                "url": request.url,
+                "size": expected_size,
+                "maxsize": maxsize,
+            }
 
             logger.warning(warning_msg, warning_args)
 
             txresponse._transport.loseConnection()
             raise defer.CancelledError(warning_msg % warning_args)
 
         if warnsize and expected_size > warnsize:
-            logger.warning("Expected response size (%(size)s) larger than "
-                           "download warn size (%(warnsize)s) in request %(request)s.",
-                           {'size': expected_size, 'warnsize': warnsize, 'request': request})
+            logger.warning(
+                "Expected response size (%(size)s) larger than "
+                "download warn size (%(warnsize)s) in request %(request)s.",
+                {"size": expected_size, "warnsize": warnsize, "request": request},
+            )
 
         def _cancel(_):
             # Abort connection immediately.
             txresponse._transport._producer.abortConnection()
 
         d = defer.Deferred(_cancel)
         txresponse.deliverBody(
@@ -468,15 +518,14 @@
             result["failure"].value.response = response
             return result["failure"]
         return response
 
 
 @implementer(IBodyProducer)
 class _RequestBodyProducer:
-
     def __init__(self, body):
         self.body = body
         self.length = len(body)
 
     def startProducing(self, consumer):
         consumer.write(self.body)
         return defer.succeed(None)
@@ -485,16 +534,24 @@
         pass
 
     def stopProducing(self):
         pass
 
 
 class _ResponseReader(protocol.Protocol):
-
-    def __init__(self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss, crawler):
+    def __init__(
+        self,
+        finished,
+        txresponse,
+        request,
+        maxsize,
+        warnsize,
+        fail_on_dataloss,
+        crawler,
+    ):
         self._finished = finished
         self._txresponse = txresponse
         self._request = request
         self._bodybuf = BytesIO()
         self._maxsize = maxsize
         self._warnsize = warnsize
         self._fail_on_dataloss = fail_on_dataloss
@@ -502,30 +559,36 @@
         self._reached_warnsize = False
         self._bytes_received = 0
         self._certificate = None
         self._ip_address = None
         self._crawler = crawler
 
     def _finish_response(self, flags=None, failure=None):
-        self._finished.callback({
-            "txresponse": self._txresponse,
-            "body": self._bodybuf.getvalue(),
-            "flags": flags,
-            "certificate": self._certificate,
-            "ip_address": self._ip_address,
-            "failure": failure,
-        })
+        self._finished.callback(
+            {
+                "txresponse": self._txresponse,
+                "body": self._bodybuf.getvalue(),
+                "flags": flags,
+                "certificate": self._certificate,
+                "ip_address": self._ip_address,
+                "failure": failure,
+            }
+        )
 
     def connectionMade(self):
         if self._certificate is None:
             with suppress(AttributeError):
-                self._certificate = ssl.Certificate(self.transport._producer.getPeerCertificate())
+                self._certificate = ssl.Certificate(
+                    self.transport._producer.getPeerCertificate()
+                )
 
         if self._ip_address is None:
-            self._ip_address = ipaddress.ip_address(self.transport._producer.getPeer().host)
+            self._ip_address = ipaddress.ip_address(
+                self.transport._producer.getPeer().host
+            )
 
     def dataReceived(self, bodyBytes):
         # This maybe called several times after cancel was called with buffered data.
         if self._finished.called:
             return
 
         self._bodybuf.write(bodyBytes)
@@ -535,56 +598,71 @@
             signal=signals.bytes_received,
             data=bodyBytes,
             request=self._request,
             spider=self._crawler.spider,
         )
         for handler, result in bytes_received_result:
             if isinstance(result, Failure) and isinstance(result.value, StopDownload):
-                logger.debug("Download stopped for %(request)s from signal handler %(handler)s",
-                             {"request": self._request, "handler": handler.__qualname__})
+                logger.debug(
+                    "Download stopped for %(request)s from signal handler %(handler)s",
+                    {"request": self._request, "handler": handler.__qualname__},
+                )
                 self.transport.stopProducing()
                 self.transport.loseConnection()
                 failure = result if result.value.fail else None
                 self._finish_response(flags=["download_stopped"], failure=failure)
 
         if self._maxsize and self._bytes_received > self._maxsize:
-            logger.warning("Received (%(bytes)s) bytes larger than download "
-                           "max size (%(maxsize)s) in request %(request)s.",
-                           {'bytes': self._bytes_received,
-                            'maxsize': self._maxsize,
-                            'request': self._request})
+            logger.warning(
+                "Received (%(bytes)s) bytes larger than download "
+                "max size (%(maxsize)s) in request %(request)s.",
+                {
+                    "bytes": self._bytes_received,
+                    "maxsize": self._maxsize,
+                    "request": self._request,
+                },
+            )
             # Clear buffer earlier to avoid keeping data in memory for a long time.
             self._bodybuf.truncate(0)
             self._finished.cancel()
 
-        if self._warnsize and self._bytes_received > self._warnsize and not self._reached_warnsize:
+        if (
+            self._warnsize
+            and self._bytes_received > self._warnsize
+            and not self._reached_warnsize
+        ):
             self._reached_warnsize = True
-            logger.warning("Received more bytes than download "
-                           "warn size (%(warnsize)s) in request %(request)s.",
-                           {'warnsize': self._warnsize,
-                            'request': self._request})
+            logger.warning(
+                "Received more bytes than download "
+                "warn size (%(warnsize)s) in request %(request)s.",
+                {"warnsize": self._warnsize, "request": self._request},
+            )
 
     def connectionLost(self, reason):
         if self._finished.called:
             return
 
         if reason.check(ResponseDone):
             self._finish_response()
             return
 
         if reason.check(PotentialDataLoss):
             self._finish_response(flags=["partial"])
             return
 
-        if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):
+        if reason.check(ResponseFailed) and any(
+            r.check(_DataLoss) for r in reason.value.reasons
+        ):
             if not self._fail_on_dataloss:
                 self._finish_response(flags=["dataloss"])
                 return
 
-            elif not self._fail_on_dataloss_warned:
-                logger.warning("Got data loss in %s. If you want to process broken "
-                               "responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False"
-                               " -- This message won't be shown in further requests",
-                               self._txresponse.request.absoluteURI.decode())
+            if not self._fail_on_dataloss_warned:
+                logger.warning(
+                    "Got data loss in %s. If you want to process broken "
+                    "responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False"
+                    " -- This message won't be shown in further requests",
+                    self._txresponse.request.absoluteURI.decode(),
+                )
                 self._fail_on_dataloss_warned = True
 
         self._finished.errback(reason)
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/handlers/http2.py` & `Scrapy-2.8.0/scrapy/core/downloader/handlers/http2.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-import warnings
 from time import time
 from typing import Optional, Type, TypeVar
 from urllib.parse import urldefrag
 
 from twisted.internet.base import DelayedCall
 from twisted.internet.defer import Deferred
 from twisted.internet.error import TimeoutError
@@ -13,28 +12,32 @@
 from scrapy.core.http2.agent import H2Agent, H2ConnectionPool, ScrapyProxyH2Agent
 from scrapy.crawler import Crawler
 from scrapy.http import Request, Response
 from scrapy.settings import Settings
 from scrapy.spiders import Spider
 from scrapy.utils.python import to_bytes
 
-
-H2DownloadHandlerOrSubclass = TypeVar("H2DownloadHandlerOrSubclass", bound="H2DownloadHandler")
+H2DownloadHandlerOrSubclass = TypeVar(
+    "H2DownloadHandlerOrSubclass", bound="H2DownloadHandler"
+)
 
 
 class H2DownloadHandler:
     def __init__(self, settings: Settings, crawler: Optional[Crawler] = None):
         self._crawler = crawler
 
         from twisted.internet import reactor
+
         self._pool = H2ConnectionPool(reactor, settings)
         self._context_factory = load_context_factory_from_settings(settings, crawler)
 
     @classmethod
-    def from_crawler(cls: Type[H2DownloadHandlerOrSubclass], crawler: Crawler) -> H2DownloadHandlerOrSubclass:
+    def from_crawler(
+        cls: Type[H2DownloadHandlerOrSubclass], crawler: Crawler
+    ) -> H2DownloadHandlerOrSubclass:
         return cls(crawler.settings, crawler)
 
     def download_request(self, request: Request, spider: Spider) -> Deferred:
         agent = ScrapyH2Agent(
             context_factory=self._context_factory,
             pool=self._pool,
             crawler=self._crawler,
@@ -46,52 +49,45 @@
 
 
 class ScrapyH2Agent:
     _Agent = H2Agent
     _ProxyAgent = ScrapyProxyH2Agent
 
     def __init__(
-        self, context_factory,
+        self,
+        context_factory,
         pool: H2ConnectionPool,
         connect_timeout: int = 10,
         bind_address: Optional[bytes] = None,
         crawler: Optional[Crawler] = None,
     ) -> None:
         self._context_factory = context_factory
         self._connect_timeout = connect_timeout
         self._bind_address = bind_address
         self._pool = pool
         self._crawler = crawler
 
     def _get_agent(self, request: Request, timeout: Optional[float]) -> H2Agent:
         from twisted.internet import reactor
-        bind_address = request.meta.get('bindaddress') or self._bind_address
-        proxy = request.meta.get('proxy')
+
+        bind_address = request.meta.get("bindaddress") or self._bind_address
+        proxy = request.meta.get("proxy")
         if proxy:
             _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)
             scheme = _parse(request.url)[0]
-            proxy_host = proxy_host.decode()
-            omit_connect_tunnel = b'noconnect' in proxy_params
-            if omit_connect_tunnel:
-                warnings.warn(
-                    "Using HTTPS proxies in the noconnect mode is not "
-                    "supported by the downloader handler. If you use Zyte "
-                    "Smart Proxy Manager, it doesn't require this mode "
-                    "anymore, so you should update scrapy-crawlera to "
-                    "scrapy-zyte-smartproxy and remove '?noconnect' from the "
-                    "Zyte Smart Proxy Manager URL."
-                )
 
-            if scheme == b'https' and not omit_connect_tunnel:
+            if scheme == b"https":
                 # ToDo
-                raise NotImplementedError('Tunneling via CONNECT method using HTTP/2.0 is not yet supported')
+                raise NotImplementedError(
+                    "Tunneling via CONNECT method using HTTP/2.0 is not yet supported"
+                )
             return self._ProxyAgent(
                 reactor=reactor,
                 context_factory=self._context_factory,
-                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding='ascii')),
+                proxy_uri=URI.fromBytes(to_bytes(proxy, encoding="ascii")),
                 connect_timeout=timeout,
                 bind_address=bind_address,
                 pool=self._pool,
             )
 
         return self._Agent(
             reactor=reactor,
@@ -99,31 +95,36 @@
             connect_timeout=timeout,
             bind_address=bind_address,
             pool=self._pool,
         )
 
     def download_request(self, request: Request, spider: Spider) -> Deferred:
         from twisted.internet import reactor
-        timeout = request.meta.get('download_timeout') or self._connect_timeout
+
+        timeout = request.meta.get("download_timeout") or self._connect_timeout
         agent = self._get_agent(request, timeout)
 
         start_time = time()
         d = agent.request(request, spider)
         d.addCallback(self._cb_latency, request, start_time)
 
         timeout_cl = reactor.callLater(timeout, d.cancel)
         d.addBoth(self._cb_timeout, request, timeout, timeout_cl)
         return d
 
     @staticmethod
-    def _cb_latency(response: Response, request: Request, start_time: float) -> Response:
-        request.meta['download_latency'] = time() - start_time
+    def _cb_latency(
+        response: Response, request: Request, start_time: float
+    ) -> Response:
+        request.meta["download_latency"] = time() - start_time
         return response
 
     @staticmethod
-    def _cb_timeout(response: Response, request: Request, timeout: float, timeout_cl: DelayedCall) -> Response:
+    def _cb_timeout(
+        response: Response, request: Request, timeout: float, timeout_cl: DelayedCall
+    ) -> Response:
         if timeout_cl.active():
             timeout_cl.cancel()
             return response
 
         url = urldefrag(request.url)[0]
         raise TimeoutError(f"Getting {url} took longer than {timeout} seconds.")
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/middleware.py` & `Scrapy-2.8.0/scrapy/core/downloader/middleware.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,42 +8,45 @@
 from twisted.internet import defer
 from twisted.python.failure import Failure
 
 from scrapy import Spider
 from scrapy.exceptions import _InvalidOutput
 from scrapy.http import Request, Response
 from scrapy.middleware import MiddlewareManager
-from scrapy.utils.defer import mustbe_deferred, deferred_from_coro
 from scrapy.utils.conf import build_component_list
+from scrapy.utils.defer import deferred_from_coro, mustbe_deferred
 
 
 class DownloaderMiddlewareManager(MiddlewareManager):
 
-    component_name = 'downloader middleware'
+    component_name = "downloader middleware"
 
     @classmethod
     def _get_mwlist_from_settings(cls, settings):
-        return build_component_list(
-            settings.getwithbase('DOWNLOADER_MIDDLEWARES'))
+        return build_component_list(settings.getwithbase("DOWNLOADER_MIDDLEWARES"))
 
     def _add_middleware(self, mw):
-        if hasattr(mw, 'process_request'):
-            self.methods['process_request'].append(mw.process_request)
-        if hasattr(mw, 'process_response'):
-            self.methods['process_response'].appendleft(mw.process_response)
-        if hasattr(mw, 'process_exception'):
-            self.methods['process_exception'].appendleft(mw.process_exception)
+        if hasattr(mw, "process_request"):
+            self.methods["process_request"].append(mw.process_request)
+        if hasattr(mw, "process_response"):
+            self.methods["process_response"].appendleft(mw.process_response)
+        if hasattr(mw, "process_exception"):
+            self.methods["process_exception"].appendleft(mw.process_exception)
 
     def download(self, download_func: Callable, request: Request, spider: Spider):
         @defer.inlineCallbacks
         def process_request(request: Request):
-            for method in self.methods['process_request']:
+            for method in self.methods["process_request"]:
                 method = cast(Callable, method)
-                response = yield deferred_from_coro(method(request=request, spider=spider))
-                if response is not None and not isinstance(response, (Response, Request)):
+                response = yield deferred_from_coro(
+                    method(request=request, spider=spider)
+                )
+                if response is not None and not isinstance(
+                    response, (Response, Request)
+                ):
                     raise _InvalidOutput(
                         f"Middleware {method.__qualname__} must return None, Response or "
                         f"Request, got {response.__class__.__name__}"
                     )
                 if response:
                     return response
             return (yield download_func(request=request, spider=spider))
@@ -51,33 +54,39 @@
         @defer.inlineCallbacks
         def process_response(response: Union[Response, Request]):
             if response is None:
                 raise TypeError("Received None in process_response")
             elif isinstance(response, Request):
                 return response
 
-            for method in self.methods['process_response']:
+            for method in self.methods["process_response"]:
                 method = cast(Callable, method)
-                response = yield deferred_from_coro(method(request=request, response=response, spider=spider))
+                response = yield deferred_from_coro(
+                    method(request=request, response=response, spider=spider)
+                )
                 if not isinstance(response, (Response, Request)):
                     raise _InvalidOutput(
                         f"Middleware {method.__qualname__} must return Response or Request, "
                         f"got {type(response)}"
                     )
                 if isinstance(response, Request):
                     return response
             return response
 
         @defer.inlineCallbacks
         def process_exception(failure: Failure):
             exception = failure.value
-            for method in self.methods['process_exception']:
+            for method in self.methods["process_exception"]:
                 method = cast(Callable, method)
-                response = yield deferred_from_coro(method(request=request, exception=exception, spider=spider))
-                if response is not None and not isinstance(response, (Response, Request)):
+                response = yield deferred_from_coro(
+                    method(request=request, exception=exception, spider=spider)
+                )
+                if response is not None and not isinstance(
+                    response, (Response, Request)
+                ):
                     raise _InvalidOutput(
                         f"Middleware {method.__qualname__} must return None, Response or "
                         f"Request, got {type(response)}"
                     )
                 if response:
                     return response
             return failure
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/tls.py` & `Scrapy-2.8.0/scrapy/core/downloader/tls.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,31 +1,34 @@
 import logging
 
 from OpenSSL import SSL
 from service_identity.exceptions import CertificateError
-from twisted.internet._sslverify import ClientTLSOptions, verifyHostname, VerificationError
+from twisted.internet._sslverify import (
+    ClientTLSOptions,
+    VerificationError,
+    verifyHostname,
+)
 from twisted.internet.ssl import AcceptableCiphers
 
-from scrapy.utils.ssl import x509name_to_string, get_temp_key_info
-
+from scrapy.utils.ssl import get_temp_key_info, x509name_to_string
 
 logger = logging.getLogger(__name__)
 
 
-METHOD_TLS = 'TLS'
-METHOD_TLSv10 = 'TLSv1.0'
-METHOD_TLSv11 = 'TLSv1.1'
-METHOD_TLSv12 = 'TLSv1.2'
+METHOD_TLS = "TLS"
+METHOD_TLSv10 = "TLSv1.0"
+METHOD_TLSv11 = "TLSv1.1"
+METHOD_TLSv12 = "TLSv1.2"
 
 
 openssl_methods = {
-    METHOD_TLS: SSL.SSLv23_METHOD,                      # protocol negotiation (recommended)
-    METHOD_TLSv10: SSL.TLSv1_METHOD,                    # TLS 1.0 only
-    METHOD_TLSv11: getattr(SSL, 'TLSv1_1_METHOD', 5),   # TLS 1.1 only
-    METHOD_TLSv12: getattr(SSL, 'TLSv1_2_METHOD', 6),   # TLS 1.2 only
+    METHOD_TLS: SSL.SSLv23_METHOD,  # protocol negotiation (recommended)
+    METHOD_TLSv10: SSL.TLSv1_METHOD,  # TLS 1.0 only
+    METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only
+    METHOD_TLSv12: SSL.TLSv1_2_METHOD,  # TLS 1.2 only
 }
 
 
 class ScrapyClientTLSOptions(ClientTLSOptions):
     """
     SSL Client connection creator ignoring certificate verification errors
     (for genuinely invalid certificates or bugs in verification code).
@@ -41,36 +44,42 @@
         self.verbose_logging = verbose_logging
 
     def _identityVerifyingInfoCallback(self, connection, where, ret):
         if where & SSL.SSL_CB_HANDSHAKE_START:
             connection.set_tlsext_host_name(self._hostnameBytes)
         elif where & SSL.SSL_CB_HANDSHAKE_DONE:
             if self.verbose_logging:
-                logger.debug('SSL connection to %s using protocol %s, cipher %s',
-                             self._hostnameASCII,
-                             connection.get_protocol_version_name(),
-                             connection.get_cipher_name(),
-                             )
+                logger.debug(
+                    "SSL connection to %s using protocol %s, cipher %s",
+                    self._hostnameASCII,
+                    connection.get_protocol_version_name(),
+                    connection.get_cipher_name(),
+                )
                 server_cert = connection.get_peer_certificate()
-                logger.debug('SSL connection certificate: issuer "%s", subject "%s"',
-                             x509name_to_string(server_cert.get_issuer()),
-                             x509name_to_string(server_cert.get_subject()),
-                             )
+                logger.debug(
+                    'SSL connection certificate: issuer "%s", subject "%s"',
+                    x509name_to_string(server_cert.get_issuer()),
+                    x509name_to_string(server_cert.get_subject()),
+                )
                 key_info = get_temp_key_info(connection._ssl)
                 if key_info:
-                    logger.debug('SSL temp key: %s', key_info)
+                    logger.debug("SSL temp key: %s", key_info)
 
             try:
                 verifyHostname(connection, self._hostnameASCII)
             except (CertificateError, VerificationError) as e:
                 logger.warning(
                     'Remote certificate is not valid for hostname "%s"; %s',
-                    self._hostnameASCII, e)
+                    self._hostnameASCII,
+                    e,
+                )
 
             except ValueError as e:
                 logger.warning(
-                    'Ignoring error while verifying certificate '
+                    "Ignoring error while verifying certificate "
                     'from host "%s" (exception: %r)',
-                    self._hostnameASCII, e)
+                    self._hostnameASCII,
+                    e,
+                )
 
 
-DEFAULT_CIPHERS = AcceptableCiphers.fromOpenSSLCipherString('DEFAULT')
+DEFAULT_CIPHERS = AcceptableCiphers.fromOpenSSLCipherString("DEFAULT")
```

### Comparing `Scrapy-2.7.1/scrapy/core/downloader/webclient.py` & `Scrapy-2.8.0/scrapy/core/downloader/webclient.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,51 +1,51 @@
 import re
 from time import time
-from urllib.parse import urlparse, urlunparse, urldefrag
+from urllib.parse import urldefrag, urlparse, urlunparse
 
-from twisted.web.http import HTTPClient
 from twisted.internet import defer
 from twisted.internet.protocol import ClientFactory
+from twisted.web.http import HTTPClient
 
 from scrapy.http import Headers
+from scrapy.responsetypes import responsetypes
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.python import to_bytes, to_unicode
-from scrapy.responsetypes import responsetypes
 
 
 def _parsed_url_args(parsed):
     # Assume parsed is urlparse-d from Request.url,
     # which was passed via safe_url_string and is ascii-only.
-    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))
+    path = urlunparse(("", "", parsed.path or "/", parsed.params, parsed.query, ""))
     path = to_bytes(path, encoding="ascii")
     host = to_bytes(parsed.hostname, encoding="ascii")
     port = parsed.port
     scheme = to_bytes(parsed.scheme, encoding="ascii")
     netloc = to_bytes(parsed.netloc, encoding="ascii")
     if port is None:
-        port = 443 if scheme == b'https' else 80
+        port = 443 if scheme == b"https" else 80
     return scheme, netloc, host, port, path
 
 
 def _parse(url):
-    """ Return tuple of (scheme, netloc, host, port, path),
+    """Return tuple of (scheme, netloc, host, port, path),
     all in bytes except for port which is int.
     Assume url is from Request.url, which was passed via safe_url_string
     and is ascii-only.
     """
     url = url.strip()
-    if not re.match(r'^\w+://', url):
-        url = '//' + url
+    if not re.match(r"^\w+://", url):
+        url = "//" + url
     parsed = urlparse(url)
     return _parsed_url_args(parsed)
 
 
 class ScrapyHTTPPageGetter(HTTPClient):
 
-    delimiter = b'\n'
+    delimiter = b"\n"
 
     def connectionMade(self):
         self.headers = Headers()  # bucket for response headers
 
         # Method command
         self.sendCommand(self.factory.method, self.factory.path)
         # Headers
@@ -71,95 +71,106 @@
 
     def connectionLost(self, reason):
         self._connection_lost_reason = reason
         HTTPClient.connectionLost(self, reason)
         self.factory.noPage(reason)
 
     def handleResponse(self, response):
-        if self.factory.method.upper() == b'HEAD':
-            self.factory.page(b'')
+        if self.factory.method.upper() == b"HEAD":
+            self.factory.page(b"")
         elif self.length is not None and self.length > 0:
             self.factory.noPage(self._connection_lost_reason)
         else:
             self.factory.page(response)
         self.transport.loseConnection()
 
     def timeout(self):
         self.transport.loseConnection()
 
         # transport cleanup needed for HTTPS connections
-        if self.factory.url.startswith(b'https'):
+        if self.factory.url.startswith(b"https"):
             self.transport.stopProducing()
 
         self.factory.noPage(
-            defer.TimeoutError(f"Getting {self.factory.url} took longer "
-                               f"than {self.factory.timeout} seconds."))
+            defer.TimeoutError(
+                f"Getting {self.factory.url} took longer "
+                f"than {self.factory.timeout} seconds."
+            )
+        )
 
 
 # This class used to inherit from Twisteds
 # twisted.web.client.HTTPClientFactory. When that class was deprecated in
 # Twisted (https://github.com/twisted/twisted/pull/643), we merged its
-# non-overriden code into this class.
+# non-overridden code into this class.
 class ScrapyHTTPClientFactory(ClientFactory):
 
     protocol = ScrapyHTTPPageGetter
 
     waiting = 1
     noisy = False
     followRedirect = False
     afterFoundGet = False
 
     def _build_response(self, body, request):
-        request.meta['download_latency'] = self.headers_time - self.start_time
+        request.meta["download_latency"] = self.headers_time - self.start_time
         status = int(self.status)
         headers = Headers(self.response_headers)
         respcls = responsetypes.from_args(headers=headers, url=self._url, body=body)
-        return respcls(url=self._url, status=status, headers=headers, body=body, protocol=to_unicode(self.version))
+        return respcls(
+            url=self._url,
+            status=status,
+            headers=headers,
+            body=body,
+            protocol=to_unicode(self.version),
+        )
 
     def _set_connection_attributes(self, request):
         parsed = urlparse_cached(request)
-        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)
-        proxy = request.meta.get('proxy')
+        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(
+            parsed
+        )
+        proxy = request.meta.get("proxy")
         if proxy:
             self.scheme, _, self.host, self.port, _ = _parse(proxy)
             self.path = self.url
 
     def __init__(self, request, timeout=180):
         self._url = urldefrag(request.url)[0]
         # converting to bytes to comply to Twisted interface
-        self.url = to_bytes(self._url, encoding='ascii')
-        self.method = to_bytes(request.method, encoding='ascii')
+        self.url = to_bytes(self._url, encoding="ascii")
+        self.method = to_bytes(request.method, encoding="ascii")
         self.body = request.body or None
         self.headers = Headers(request.headers)
         self.response_headers = None
-        self.timeout = request.meta.get('download_timeout') or timeout
+        self.timeout = request.meta.get("download_timeout") or timeout
         self.start_time = time()
         self.deferred = defer.Deferred().addCallback(self._build_response, request)
 
         # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected
         # to have _disconnectedDeferred. See Twisted r32329.
         # As Scrapy implements it's own logic to handle redirects is not
         # needed to add the callback _waitForDisconnect.
         # Specifically this avoids the AttributeError exception when
         # clientConnectionFailed method is called.
         self._disconnectedDeferred = defer.Deferred()
 
         self._set_connection_attributes(request)
 
         # set Host header based on url
-        self.headers.setdefault('Host', self.netloc)
+        self.headers.setdefault("Host", self.netloc)
 
         # set Content-Length based len of body
         if self.body is not None:
-            self.headers['Content-Length'] = len(self.body)
+            self.headers["Content-Length"] = len(self.body)
             # just in case a broken http/1.1 decides to keep connection alive
             self.headers.setdefault("Connection", "close")
         # Content-Length must be specified in POST method even with no body
-        elif self.method == b'POST':
-            self.headers['Content-Length'] = 0
+        elif self.method == b"POST":
+            self.headers["Content-Length"] = 0
 
     def __repr__(self):
         return f"<{self.__class__.__name__}: {self.url}>"
 
     def _cancelTimeout(self, result, timeoutCall):
         if timeoutCall.active():
             timeoutCall.cancel()
@@ -167,29 +178,30 @@
 
     def buildProtocol(self, addr):
         p = ClientFactory.buildProtocol(self, addr)
         p.followRedirect = self.followRedirect
         p.afterFoundGet = self.afterFoundGet
         if self.timeout:
             from twisted.internet import reactor
+
             timeoutCall = reactor.callLater(self.timeout, p.timeout)
             self.deferred.addBoth(self._cancelTimeout, timeoutCall)
         return p
 
     def gotHeaders(self, headers):
         self.headers_time = time()
         self.response_headers = headers
 
     def gotStatus(self, version, status, message):
         """
         Set the status of the request on us.
         @param version: The HTTP version.
         @type version: L{bytes}
         @param status: The HTTP status code, an integer represented as a
-            bytestring.
+        bytestring.
         @type status: L{bytes}
         @param message: The HTTP status message.
         @type message: L{bytes}
         """
         self.version, self.status, self.message = version, status, message
 
     def page(self, page):
```

### Comparing `Scrapy-2.7.1/scrapy/core/engine.py` & `Scrapy-2.8.0/scrapy/core/engine.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,27 +11,22 @@
 
 from twisted.internet.defer import Deferred, inlineCallbacks, succeed
 from twisted.internet.task import LoopingCall
 from twisted.python.failure import Failure
 
 from scrapy import signals
 from scrapy.core.scraper import Scraper
-from scrapy.exceptions import (
-    CloseSpider,
-    DontCloseSpider,
-    ScrapyDeprecationWarning,
-)
-from scrapy.http import Response, Request
+from scrapy.exceptions import CloseSpider, DontCloseSpider, ScrapyDeprecationWarning
+from scrapy.http import Request, Response
 from scrapy.settings import BaseSettings
 from scrapy.spiders import Spider
-from scrapy.utils.log import logformatter_adapter, failure_to_exc_info
+from scrapy.utils.log import failure_to_exc_info, logformatter_adapter
 from scrapy.utils.misc import create_instance, load_object
 from scrapy.utils.reactor import CallLaterOnce
 
-
 logger = logging.getLogger(__name__)
 
 
 class Slot:
     def __init__(
         self,
         start_requests: Iterable,
@@ -75,21 +70,22 @@
         self.signals = crawler.signals
         self.logformatter = crawler.logformatter
         self.slot: Optional[Slot] = None
         self.spider: Optional[Spider] = None
         self.running = False
         self.paused = False
         self.scheduler_cls = self._get_scheduler_class(crawler.settings)
-        downloader_cls = load_object(self.settings['DOWNLOADER'])
+        downloader_cls = load_object(self.settings["DOWNLOADER"])
         self.downloader = downloader_cls(crawler)
         self.scraper = Scraper(crawler)
         self._spider_closed_callback = spider_closed_callback
 
     def _get_scheduler_class(self, settings: BaseSettings) -> type:
         from scrapy.core.scheduler import BaseScheduler
+
         scheduler_cls = load_object(settings["SCHEDULER"])
         if not issubclass(scheduler_cls, BaseScheduler):
             raise TypeError(
                 f"The provided scheduler class ({settings['SCHEDULER']})"
                 " does not fully implement the scheduler interface"
             )
         return scheduler_cls
@@ -102,35 +98,42 @@
         yield self.signals.send_catch_log_deferred(signal=signals.engine_started)
         self.running = True
         self._closewait = Deferred()
         yield self._closewait
 
     def stop(self) -> Deferred:
         """Gracefully stop the execution engine"""
+
         @inlineCallbacks
         def _finish_stopping_engine(_) -> Deferred:
             yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)
             self._closewait.callback(None)
 
         if not self.running:
             raise RuntimeError("Engine not running")
 
         self.running = False
-        dfd = self.close_spider(self.spider, reason="shutdown") if self.spider is not None else succeed(None)
+        dfd = (
+            self.close_spider(self.spider, reason="shutdown")
+            if self.spider is not None
+            else succeed(None)
+        )
         return dfd.addBoth(_finish_stopping_engine)
 
     def close(self) -> Deferred:
         """
         Gracefully close the execution engine.
         If it has already been started, stop it. In all cases, close the spider and the downloader.
         """
         if self.running:
             return self.stop()  # will also close spider and downloader
         if self.spider is not None:
-            return self.close_spider(self.spider, reason="shutdown")  # will also close downloader
+            return self.close_spider(
+                self.spider, reason="shutdown"
+            )  # will also close downloader
         return succeed(self.downloader.close())
 
     def pause(self) -> None:
         self.paused = True
 
     def unpause(self) -> None:
         self.paused = False
@@ -140,25 +143,32 @@
             return
 
         assert self.spider is not None  # typing
 
         if self.paused:
             return None
 
-        while not self._needs_backout() and self._next_request_from_scheduler() is not None:
+        while (
+            not self._needs_backout()
+            and self._next_request_from_scheduler() is not None
+        ):
             pass
 
         if self.slot.start_requests is not None and not self._needs_backout():
             try:
                 request = next(self.slot.start_requests)
             except StopIteration:
                 self.slot.start_requests = None
             except Exception:
                 self.slot.start_requests = None
-                logger.error('Error while obtaining start requests', exc_info=True, extra={'spider': self.spider})
+                logger.error(
+                    "Error while obtaining start requests",
+                    exc_info=True,
+                    extra={"spider": self.spider},
+                )
             else:
                 self.crawl(request)
 
         if self.spider_is_idle() and self.slot.close_if_idle:
             self._spider_idle()
 
     def _needs_backout(self) -> bool:
@@ -175,47 +185,61 @@
 
         request = self.slot.scheduler.next_request()
         if request is None:
             return None
 
         d = self._download(request, self.spider)
         d.addBoth(self._handle_downloader_output, request)
-        d.addErrback(lambda f: logger.info('Error while handling downloader output',
-                                           exc_info=failure_to_exc_info(f),
-                                           extra={'spider': self.spider}))
+        d.addErrback(
+            lambda f: logger.info(
+                "Error while handling downloader output",
+                exc_info=failure_to_exc_info(f),
+                extra={"spider": self.spider},
+            )
+        )
         d.addBoth(lambda _: self.slot.remove_request(request))
-        d.addErrback(lambda f: logger.info('Error while removing request from slot',
-                                           exc_info=failure_to_exc_info(f),
-                                           extra={'spider': self.spider}))
+        d.addErrback(
+            lambda f: logger.info(
+                "Error while removing request from slot",
+                exc_info=failure_to_exc_info(f),
+                extra={"spider": self.spider},
+            )
+        )
         slot = self.slot
         d.addBoth(lambda _: slot.nextcall.schedule())
-        d.addErrback(lambda f: logger.info('Error while scheduling new request',
-                                           exc_info=failure_to_exc_info(f),
-                                           extra={'spider': self.spider}))
+        d.addErrback(
+            lambda f: logger.info(
+                "Error while scheduling new request",
+                exc_info=failure_to_exc_info(f),
+                extra={"spider": self.spider},
+            )
+        )
         return d
 
     def _handle_downloader_output(
         self, result: Union[Request, Response, Failure], request: Request
     ) -> Optional[Deferred]:
         assert self.spider is not None  # typing
 
         if not isinstance(result, (Request, Response, Failure)):
-            raise TypeError(f"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}")
+            raise TypeError(
+                f"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}"
+            )
 
         # downloader middleware can return requests (for example, redirects)
         if isinstance(result, Request):
             self.crawl(result)
             return None
 
         d = self.scraper.enqueue_scrape(result, request, self.spider)
         d.addErrback(
             lambda f: logger.error(
                 "Error while enqueuing downloader output",
                 exc_info=failure_to_exc_info(f),
-                extra={'spider': self.spider},
+                extra={"spider": self.spider},
             )
         )
         return d
 
     def spider_is_idle(self, spider: Optional[Spider] = None) -> bool:
         if spider is not None:
             warnings.warn(
@@ -240,38 +264,48 @@
         if spider is not None:
             warnings.warn(
                 "Passing a 'spider' argument to ExecutionEngine.crawl is deprecated",
                 category=ScrapyDeprecationWarning,
                 stacklevel=2,
             )
             if spider is not self.spider:
-                raise RuntimeError(f"The spider {spider.name!r} does not match the open spider")
+                raise RuntimeError(
+                    f"The spider {spider.name!r} does not match the open spider"
+                )
         if self.spider is None:
             raise RuntimeError(f"No open spider to crawl: {request}")
         self._schedule_request(request, self.spider)
         self.slot.nextcall.schedule()  # type: ignore[union-attr]
 
     def _schedule_request(self, request: Request, spider: Spider) -> None:
-        self.signals.send_catch_log(signals.request_scheduled, request=request, spider=spider)
+        self.signals.send_catch_log(
+            signals.request_scheduled, request=request, spider=spider
+        )
         if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]
-            self.signals.send_catch_log(signals.request_dropped, request=request, spider=spider)
+            self.signals.send_catch_log(
+                signals.request_dropped, request=request, spider=spider
+            )
 
     def download(self, request: Request, spider: Optional[Spider] = None) -> Deferred:
         """Return a Deferred which fires with a Response as result, only downloader middlewares are applied"""
         if spider is not None:
             warnings.warn(
                 "Passing a 'spider' argument to ExecutionEngine.download is deprecated",
                 category=ScrapyDeprecationWarning,
                 stacklevel=2,
             )
             if spider is not self.spider:
-                logger.warning("The spider '%s' does not match the open spider", spider.name)
+                logger.warning(
+                    "The spider '%s' does not match the open spider", spider.name
+                )
         if self.spider is None:
             raise RuntimeError(f"No open spider to crawl: {request}")
-        return self._download(request, spider).addBoth(self._downloaded, request, spider)
+        return self._download(request, spider).addBoth(
+            self._downloaded, request, spider
+        )
 
     def _downloaded(
         self, result: Union[Response, Request], request: Request, spider: Spider
     ) -> Union[Deferred, Response]:
         assert self.slot is not None  # typing
         self.slot.remove_request(request)
         return self.download(result, spider) if isinstance(result, Request) else result
@@ -282,15 +316,17 @@
         self.slot.add_request(request)
 
         if spider is None:
             spider = self.spider
 
         def _on_success(result: Union[Response, Request]) -> Union[Response, Request]:
             if not isinstance(result, (Response, Request)):
-                raise TypeError(f"Incorrect type: expected Response or Request, got {type(result)}: {result!r}")
+                raise TypeError(
+                    f"Incorrect type: expected Response or Request, got {type(result)}: {result!r}"
+                )
             if isinstance(result, Response):
                 if result.request is None:
                     result.request = request
                 logkws = self.logformatter.crawled(result.request, result, spider)
                 if logkws is not None:
                     logger.log(*logformatter_adapter(logkws), extra={"spider": spider})
                 self.signals.send_catch_log(
@@ -307,21 +343,27 @@
 
         dwld = self.downloader.fetch(request, spider)
         dwld.addCallbacks(_on_success)
         dwld.addBoth(_on_complete)
         return dwld
 
     @inlineCallbacks
-    def open_spider(self, spider: Spider, start_requests: Iterable = (), close_if_idle: bool = True):
+    def open_spider(
+        self, spider: Spider, start_requests: Iterable = (), close_if_idle: bool = True
+    ):
         if self.slot is not None:
             raise RuntimeError(f"No free spider slot when opening {spider.name!r}")
-        logger.info("Spider opened", extra={'spider': spider})
+        logger.info("Spider opened", extra={"spider": spider})
         nextcall = CallLaterOnce(self._next_request)
-        scheduler = create_instance(self.scheduler_cls, settings=None, crawler=self.crawler)
-        start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)
+        scheduler = create_instance(
+            self.scheduler_cls, settings=None, crawler=self.crawler
+        )
+        start_requests = yield self.scraper.spidermw.process_start_requests(
+            start_requests, spider
+        )
         self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)
         self.spider = spider
         if hasattr(scheduler, "open"):
             yield scheduler.open(spider)
         yield self.scraper.open_spider(spider)
         self.crawler.stats.open_spider(spider)
         yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)
@@ -333,70 +375,87 @@
         Called when a spider gets idle, i.e. when there are no remaining requests to download or schedule.
         It can be called multiple times. If a handler for the spider_idle signal raises a DontCloseSpider
         exception, the spider is not closed until the next loop and this function is guaranteed to be called
         (at least) once again. A handler can raise CloseSpider to provide a custom closing reason.
         """
         assert self.spider is not None  # typing
         expected_ex = (DontCloseSpider, CloseSpider)
-        res = self.signals.send_catch_log(signals.spider_idle, spider=self.spider, dont_log=expected_ex)
+        res = self.signals.send_catch_log(
+            signals.spider_idle, spider=self.spider, dont_log=expected_ex
+        )
         detected_ex = {
             ex: x.value
             for _, x in res
             for ex in expected_ex
             if isinstance(x, Failure) and isinstance(x.value, ex)
         }
         if DontCloseSpider in detected_ex:
             return None
         if self.spider_is_idle():
-            ex = detected_ex.get(CloseSpider, CloseSpider(reason='finished'))
+            ex = detected_ex.get(CloseSpider, CloseSpider(reason="finished"))
             assert isinstance(ex, CloseSpider)  # typing
             self.close_spider(self.spider, reason=ex.reason)
 
     def close_spider(self, spider: Spider, reason: str = "cancelled") -> Deferred:
         """Close (cancel) spider and clear all its outstanding requests"""
         if self.slot is None:
             raise RuntimeError("Engine slot not assigned")
 
         if self.slot.closing is not None:
             return self.slot.closing
 
-        logger.info("Closing spider (%(reason)s)", {'reason': reason}, extra={'spider': spider})
+        logger.info(
+            "Closing spider (%(reason)s)", {"reason": reason}, extra={"spider": spider}
+        )
 
         dfd = self.slot.close()
 
         def log_failure(msg: str) -> Callable:
             def errback(failure: Failure) -> None:
-                logger.error(msg, exc_info=failure_to_exc_info(failure), extra={'spider': spider})
+                logger.error(
+                    msg, exc_info=failure_to_exc_info(failure), extra={"spider": spider}
+                )
+
             return errback
 
         dfd.addBoth(lambda _: self.downloader.close())
-        dfd.addErrback(log_failure('Downloader close failure'))
+        dfd.addErrback(log_failure("Downloader close failure"))
 
         dfd.addBoth(lambda _: self.scraper.close_spider(spider))
-        dfd.addErrback(log_failure('Scraper close failure'))
+        dfd.addErrback(log_failure("Scraper close failure"))
 
         if hasattr(self.slot.scheduler, "close"):
             dfd.addBoth(lambda _: self.slot.scheduler.close(reason))
             dfd.addErrback(log_failure("Scheduler close failure"))
 
-        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(
-            signal=signals.spider_closed, spider=spider, reason=reason,
-        ))
-        dfd.addErrback(log_failure('Error while sending spider_close signal'))
+        dfd.addBoth(
+            lambda _: self.signals.send_catch_log_deferred(
+                signal=signals.spider_closed,
+                spider=spider,
+                reason=reason,
+            )
+        )
+        dfd.addErrback(log_failure("Error while sending spider_close signal"))
 
         dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))
-        dfd.addErrback(log_failure('Stats close failure'))
+        dfd.addErrback(log_failure("Stats close failure"))
 
-        dfd.addBoth(lambda _: logger.info("Spider closed (%(reason)s)", {'reason': reason}, extra={'spider': spider}))
+        dfd.addBoth(
+            lambda _: logger.info(
+                "Spider closed (%(reason)s)",
+                {"reason": reason},
+                extra={"spider": spider},
+            )
+        )
 
-        dfd.addBoth(lambda _: setattr(self, 'slot', None))
-        dfd.addErrback(log_failure('Error while unassigning slot'))
+        dfd.addBoth(lambda _: setattr(self, "slot", None))
+        dfd.addErrback(log_failure("Error while unassigning slot"))
 
-        dfd.addBoth(lambda _: setattr(self, 'spider', None))
-        dfd.addErrback(log_failure('Error while unassigning spider'))
+        dfd.addBoth(lambda _: setattr(self, "spider", None))
+        dfd.addErrback(log_failure("Error while unassigning spider"))
 
         dfd.addBoth(lambda _: self._spider_closed_callback(spider))
 
         return dfd
 
     @property
     def open_spiders(self) -> list:
@@ -404,15 +463,19 @@
             "ExecutionEngine.open_spiders is deprecated, please use ExecutionEngine.spider instead",
             category=ScrapyDeprecationWarning,
             stacklevel=2,
         )
         return [self.spider] if self.spider is not None else []
 
     def has_capacity(self) -> bool:
-        warnings.warn("ExecutionEngine.has_capacity is deprecated", ScrapyDeprecationWarning, stacklevel=2)
+        warnings.warn(
+            "ExecutionEngine.has_capacity is deprecated",
+            ScrapyDeprecationWarning,
+            stacklevel=2,
+        )
         return not bool(self.slot)
 
     def schedule(self, request: Request, spider: Spider) -> None:
         warnings.warn(
             "ExecutionEngine.schedule is deprecated, please use "
             "ExecutionEngine.crawl or ExecutionEngine.download instead",
             category=ScrapyDeprecationWarning,
```

### Comparing `Scrapy-2.7.1/scrapy/core/http2/agent.py` & `Scrapy-2.8.0/scrapy/core/http2/agent.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 from twisted.internet.defer import Deferred
 from twisted.internet.endpoints import HostnameEndpoint
 from twisted.python.failure import Failure
 from twisted.web.client import URI, BrowserLikePolicyForHTTPS, _StandardEndpointFactory
 from twisted.web.error import SchemeNotSupported
 
 from scrapy.core.downloader.contextfactory import AcceptableProtocolsContextFactory
-from scrapy.core.http2.protocol import H2ClientProtocol, H2ClientFactory
+from scrapy.core.http2.protocol import H2ClientFactory, H2ClientProtocol
 from scrapy.http.request import Request
 from scrapy.settings import Settings
 from scrapy.spiders import Spider
 
 
 class H2ConnectionPool:
     def __init__(self, reactor: ReactorBase, settings: Settings) -> None:
@@ -24,15 +24,17 @@
         # Store a dictionary which is used to get the respective
         # H2ClientProtocolInstance using the  key as Tuple(scheme, hostname, port)
         self._connections: Dict[Tuple, H2ClientProtocol] = {}
 
         # Save all requests that arrive before the connection is established
         self._pending_requests: Dict[Tuple, Deque[Deferred]] = {}
 
-    def get_connection(self, key: Tuple, uri: URI, endpoint: HostnameEndpoint) -> Deferred:
+    def get_connection(
+        self, key: Tuple, uri: URI, endpoint: HostnameEndpoint
+    ) -> Deferred:
         if key in self._pending_requests:
             # Received a request while connecting to remote
             # Create a deferred which will fire with the H2ClientProtocol
             # instance
             d = Deferred()
             self._pending_requests[key].append(d)
             return d
@@ -42,15 +44,17 @@
         if conn:
             # Return this connection instance wrapped inside a deferred
             return defer.succeed(conn)
 
         # No connection is established for the given URI
         return self._new_connection(key, uri, endpoint)
 
-    def _new_connection(self, key: Tuple, uri: URI, endpoint: HostnameEndpoint) -> Deferred:
+    def _new_connection(
+        self, key: Tuple, uri: URI, endpoint: HostnameEndpoint
+    ) -> Deferred:
         self._pending_requests[key] = deque()
 
         conn_lost_deferred = Deferred()
         conn_lost_deferred.addCallback(self._remove_connection, key)
 
         factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)
         conn_d = endpoint.connect(factory)
@@ -98,15 +102,17 @@
         pool: H2ConnectionPool,
         context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),
         connect_timeout: Optional[float] = None,
         bind_address: Optional[bytes] = None,
     ) -> None:
         self._reactor = reactor
         self._pool = pool
-        self._context_factory = AcceptableProtocolsContextFactory(context_factory, acceptable_protocols=[b'h2'])
+        self._context_factory = AcceptableProtocolsContextFactory(
+            context_factory, acceptable_protocols=[b"h2"]
+        )
         self.endpoint_factory = _StandardEndpointFactory(
             self._reactor, self._context_factory, connect_timeout, bind_address
         )
 
     def get_endpoint(self, uri: URI):
         return self.endpoint_factory.endpointForURI(uri)
 
@@ -114,15 +120,15 @@
         """
         Arguments:
             uri - URI obtained directly from request URL
         """
         return uri.scheme, uri.host, uri.port
 
     def request(self, request: Request, spider: Spider) -> Deferred:
-        uri = URI.fromBytes(bytes(request.url, encoding='utf-8'))
+        uri = URI.fromBytes(bytes(request.url, encoding="utf-8"))
         try:
             endpoint = self.get_endpoint(uri)
         except SchemeNotSupported:
             return defer.fail(Failure())
 
         key = self.get_key(uri)
         d = self._pool.get_connection(key, uri, endpoint)
@@ -136,15 +142,15 @@
         reactor: ReactorBase,
         proxy_uri: URI,
         pool: H2ConnectionPool,
         context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),
         connect_timeout: Optional[float] = None,
         bind_address: Optional[bytes] = None,
     ) -> None:
-        super(ScrapyProxyH2Agent, self).__init__(
+        super().__init__(
             reactor=reactor,
             pool=pool,
             context_factory=context_factory,
             connect_timeout=connect_timeout,
             bind_address=bind_address,
         )
         self._proxy_uri = proxy_uri
```

### Comparing `Scrapy-2.7.1/scrapy/core/http2/protocol.py` & `Scrapy-2.8.0/scrapy/core/http2/protocol.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,88 +5,96 @@
 from ipaddress import IPv4Address, IPv6Address
 from typing import Dict, List, Optional, Union
 
 from h2.config import H2Configuration
 from h2.connection import H2Connection
 from h2.errors import ErrorCodes
 from h2.events import (
-    Event, ConnectionTerminated, DataReceived, ResponseReceived,
-    SettingsAcknowledged, StreamEnded, StreamReset, UnknownFrameReceived,
-    WindowUpdated
+    ConnectionTerminated,
+    DataReceived,
+    Event,
+    ResponseReceived,
+    SettingsAcknowledged,
+    StreamEnded,
+    StreamReset,
+    UnknownFrameReceived,
+    WindowUpdated,
 )
 from h2.exceptions import FrameTooLargeError, H2Error
 from twisted.internet.defer import Deferred
 from twisted.internet.error import TimeoutError
 from twisted.internet.interfaces import IHandshakeListener, IProtocolNegotiationFactory
-from twisted.internet.protocol import connectionDone, Factory, Protocol
+from twisted.internet.protocol import Factory, Protocol, connectionDone
 from twisted.internet.ssl import Certificate
 from twisted.protocols.policies import TimeoutMixin
 from twisted.python.failure import Failure
 from twisted.web.client import URI
 from zope.interface import implementer
 
 from scrapy.core.http2.stream import Stream, StreamCloseReason
 from scrapy.http import Request
 from scrapy.settings import Settings
 from scrapy.spiders import Spider
 
-
 logger = logging.getLogger(__name__)
 
 
 PROTOCOL_NAME = b"h2"
 
 
 class InvalidNegotiatedProtocol(H2Error):
-
     def __init__(self, negotiated_protocol: bytes) -> None:
         self.negotiated_protocol = negotiated_protocol
 
     def __str__(self) -> str:
-        return (f"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}")
+        return f"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}"
 
 
 class RemoteTerminatedConnection(H2Error):
     def __init__(
         self,
         remote_ip_address: Optional[Union[IPv4Address, IPv6Address]],
         event: ConnectionTerminated,
     ) -> None:
         self.remote_ip_address = remote_ip_address
         self.terminate_event = event
 
     def __str__(self) -> str:
-        return f'Received GOAWAY frame from {self.remote_ip_address!r}'
+        return f"Received GOAWAY frame from {self.remote_ip_address!r}"
 
 
 class MethodNotAllowed405(H2Error):
-    def __init__(self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]) -> None:
+    def __init__(
+        self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]
+    ) -> None:
         self.remote_ip_address = remote_ip_address
 
     def __str__(self) -> str:
         return f"Received 'HTTP/2.0 405 Method Not Allowed' from {self.remote_ip_address!r}"
 
 
 @implementer(IHandshakeListener)
 class H2ClientProtocol(Protocol, TimeoutMixin):
     IDLE_TIMEOUT = 240
 
-    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Deferred) -> None:
+    def __init__(
+        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred
+    ) -> None:
         """
         Arguments:
             uri -- URI of the base url to which HTTP/2 Connection will be made.
                 uri is used to verify that incoming client requests have correct
                 base URL.
             settings -- Scrapy project settings
             conn_lost_deferred -- Deferred fires with the reason: Failure to notify
                 that connection was lost
         """
         self._conn_lost_deferred = conn_lost_deferred
 
-        config = H2Configuration(client_side=True, header_encoding='utf-8')
+        config = H2Configuration(client_side=True, header_encoding="utf-8")
         self.conn = H2Connection(config=config)
 
         # ID of the next request stream
         # Following the convention - 'Streams initiated by a client MUST
         # use odd-numbered stream identifiers' (RFC 7540 - Section 5.1.1)
         self._stream_id_generator = itertools.count(start=1, step=2)
 
@@ -101,110 +109,108 @@
         # We pass these instances to the streams ResponseFailed() failure
         self._conn_lost_errors: List[BaseException] = []
 
         # Some meta data of this connection
         # initialized when connection is successfully made
         self.metadata: Dict = {
             # Peer certificate instance
-            'certificate': None,
-
+            "certificate": None,
             # Address of the server we are connected to which
             # is updated when HTTP/2 connection is  made successfully
-            'ip_address': None,
-
+            "ip_address": None,
             # URI of the peer HTTP/2 connection is made
-            'uri': uri,
-
+            "uri": uri,
             # Both ip_address and uri are used by the Stream before
             # initiating the request to verify that the base address
-
             # Variables taken from Project Settings
-            'default_download_maxsize': settings.getint('DOWNLOAD_MAXSIZE'),
-            'default_download_warnsize': settings.getint('DOWNLOAD_WARNSIZE'),
-
+            "default_download_maxsize": settings.getint("DOWNLOAD_MAXSIZE"),
+            "default_download_warnsize": settings.getint("DOWNLOAD_WARNSIZE"),
             # Counter to keep track of opened streams. This counter
             # is used to make sure that not more than MAX_CONCURRENT_STREAMS
             # streams are opened which leads to ProtocolError
             # We use simple FIFO policy to handle pending requests
-            'active_streams': 0,
-
+            "active_streams": 0,
             # Flag to keep track if settings were acknowledged by the remote
             # This ensures that we have established a HTTP/2 connection
-            'settings_acknowledged': False,
+            "settings_acknowledged": False,
         }
 
     @property
     def h2_connected(self) -> bool:
         """Boolean to keep track of the connection status.
         This is used while initiating pending streams to make sure
         that we initiate stream only during active HTTP/2 Connection
         """
-        return bool(self.transport.connected) and self.metadata['settings_acknowledged']
+        return bool(self.transport.connected) and self.metadata["settings_acknowledged"]
 
     @property
     def allowed_max_concurrent_streams(self) -> int:
         """We keep total two streams for client (sending data) and
         server side (receiving data) for a single request. To be safe
         we choose the minimum. Since this value can change in event
         RemoteSettingsChanged we make variable a property.
         """
         return min(
             self.conn.local_settings.max_concurrent_streams,
-            self.conn.remote_settings.max_concurrent_streams
+            self.conn.remote_settings.max_concurrent_streams,
         )
 
     def _send_pending_requests(self) -> None:
         """Initiate all pending requests from the deque following FIFO
         We make sure that at any time {allowed_max_concurrent_streams}
         streams are active.
         """
         while (
             self._pending_request_stream_pool
-            and self.metadata['active_streams'] < self.allowed_max_concurrent_streams
+            and self.metadata["active_streams"] < self.allowed_max_concurrent_streams
             and self.h2_connected
         ):
-            self.metadata['active_streams'] += 1
+            self.metadata["active_streams"] += 1
             stream = self._pending_request_stream_pool.popleft()
             stream.initiate_request()
             self._write_to_transport()
 
     def pop_stream(self, stream_id: int) -> Stream:
-        """Perform cleanup when a stream is closed
-        """
+        """Perform cleanup when a stream is closed"""
         stream = self.streams.pop(stream_id)
-        self.metadata['active_streams'] -= 1
+        self.metadata["active_streams"] -= 1
         self._send_pending_requests()
         return stream
 
     def _new_stream(self, request: Request, spider: Spider) -> Stream:
-        """Instantiates a new Stream object
-        """
+        """Instantiates a new Stream object"""
         stream = Stream(
             stream_id=next(self._stream_id_generator),
             request=request,
             protocol=self,
-            download_maxsize=getattr(spider, 'download_maxsize', self.metadata['default_download_maxsize']),
-            download_warnsize=getattr(spider, 'download_warnsize', self.metadata['default_download_warnsize']),
+            download_maxsize=getattr(
+                spider, "download_maxsize", self.metadata["default_download_maxsize"]
+            ),
+            download_warnsize=getattr(
+                spider, "download_warnsize", self.metadata["default_download_warnsize"]
+            ),
         )
         self.streams[stream.stream_id] = stream
         return stream
 
     def _write_to_transport(self) -> None:
-        """ Write data to the underlying transport connection
+        """Write data to the underlying transport connection
         from the HTTP2 connection instance if any
         """
         # Reset the idle timeout as connection is still actively sending data
         self.resetTimeout()
 
         data = self.conn.data_to_send()
         self.transport.write(data)
 
     def request(self, request: Request, spider: Spider) -> Deferred:
         if not isinstance(request, Request):
-            raise TypeError(f'Expected scrapy.http.Request, received {request.__class__.__qualname__}')
+            raise TypeError(
+                f"Expected scrapy.http.Request, received {request.__class__.__qualname__}"
+            )
 
         stream = self._new_stream(request, spider)
         d = stream.get_response()
 
         # Add the stream to the request pool
         self._pending_request_stream_pool.append(stream)
 
@@ -217,15 +223,15 @@
         """Called by Twisted when the connection is established. We can start
         sending some data now: we should open with the connection preamble.
         """
         # Initialize the timeout
         self.setTimeout(self.IDLE_TIMEOUT)
 
         destination = self.transport.getPeer()
-        self.metadata['ip_address'] = ipaddress.ip_address(destination.host)
+        self.metadata["ip_address"] = ipaddress.ip_address(destination.host)
 
         # Initiate H2 Connection
         self.conn.initiate_connection()
         self._write_to_transport()
 
     def _lose_connection_with_error(self, errors: List[BaseException]) -> None:
         """Helper function to lose the connection with the error sent as a
@@ -233,27 +239,32 @@
         self._conn_lost_errors += errors
         self.transport.loseConnection()
 
     def handshakeCompleted(self) -> None:
         """
         Close the connection if it's not made via the expected protocol
         """
-        if self.transport.negotiatedProtocol is not None and self.transport.negotiatedProtocol != PROTOCOL_NAME:
+        if (
+            self.transport.negotiatedProtocol is not None
+            and self.transport.negotiatedProtocol != PROTOCOL_NAME
+        ):
             # we have not initiated the connection yet, no need to send a GOAWAY frame to the remote peer
-            self._lose_connection_with_error([InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)])
+            self._lose_connection_with_error(
+                [InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)]
+            )
 
     def _check_received_data(self, data: bytes) -> None:
         """Checks for edge cases where the connection to remote fails
         without raising an appropriate H2Error
 
         Arguments:
             data -- Data received from the remote
         """
-        if data.startswith(b'HTTP/2.0 405 Method Not Allowed'):
-            raise MethodNotAllowed405(self.metadata['ip_address'])
+        if data.startswith(b"HTTP/2.0 405 Method Not Allowed"):
+            raise MethodNotAllowed405(self.metadata["ip_address"])
 
     def dataReceived(self, data: bytes) -> None:
         # Reset the idle timeout as connection is still actively receiving data
         self.resetTimeout()
 
         try:
             self._check_received_data(data)
@@ -280,25 +291,25 @@
 
         # Check whether there are open streams. If there are, we're going to
         # want to use the error code PROTOCOL_ERROR. If there aren't, use
         # NO_ERROR.
         if (
             self.conn.open_outbound_streams > 0
             or self.conn.open_inbound_streams > 0
-            or self.metadata['active_streams'] > 0
+            or self.metadata["active_streams"] > 0
         ):
             error_code = ErrorCodes.PROTOCOL_ERROR
         else:
             error_code = ErrorCodes.NO_ERROR
         self.conn.close_connection(error_code=error_code)
         self._write_to_transport()
 
-        self._lose_connection_with_error([
-            TimeoutError(f"Connection was IDLE for more than {self.IDLE_TIMEOUT}s")
-        ])
+        self._lose_connection_with_error(
+            [TimeoutError(f"Connection was IDLE for more than {self.IDLE_TIMEOUT}s")]
+        )
 
     def connectionLost(self, reason: Failure = connectionDone) -> None:
         """Called by Twisted when the transport connection is lost.
         No need to write anything to transport here.
         """
         # Cancel the timeout if not done yet
         self.setTimeout(None)
@@ -307,21 +318,21 @@
         # sent over current connection
         if not reason.check(connectionDone):
             self._conn_lost_errors.append(reason)
 
         self._conn_lost_deferred.callback(self._conn_lost_errors)
 
         for stream in self.streams.values():
-            if stream.metadata['request_sent']:
+            if stream.metadata["request_sent"]:
                 close_reason = StreamCloseReason.CONNECTION_LOST
             else:
                 close_reason = StreamCloseReason.INACTIVE
             stream.close(close_reason, self._conn_lost_errors, from_protocol=True)
 
-        self.metadata['active_streams'] -= len(self.streams)
+        self.metadata["active_streams"] -= len(self.streams)
         self.streams.clear()
         self._pending_request_stream_pool.clear()
         self.conn.close_connection()
 
     def _handle_events(self, events: List[Event]) -> None:
         """Private method which acts as a bridge between the events
         received from the HTTP/2 data and IH2EventsHandler
@@ -341,21 +352,21 @@
             elif isinstance(event, StreamReset):
                 self.stream_reset(event)
             elif isinstance(event, WindowUpdated):
                 self.window_updated(event)
             elif isinstance(event, SettingsAcknowledged):
                 self.settings_acknowledged(event)
             elif isinstance(event, UnknownFrameReceived):
-                logger.warning('Unknown frame received: %s', event.frame)
+                logger.warning("Unknown frame received: %s", event.frame)
 
     # Event handler functions starts here
     def connection_terminated(self, event: ConnectionTerminated) -> None:
-        self._lose_connection_with_error([
-            RemoteTerminatedConnection(self.metadata['ip_address'], event)
-        ])
+        self._lose_connection_with_error(
+            [RemoteTerminatedConnection(self.metadata["ip_address"], event)]
+        )
 
     def data_received(self, event: DataReceived) -> None:
         try:
             stream = self.streams[event.stream_id]
         except KeyError:
             pass  # We ignore server-initiated events
         else:
@@ -366,22 +377,22 @@
             stream = self.streams[event.stream_id]
         except KeyError:
             pass  # We ignore server-initiated events
         else:
             stream.receive_headers(event.headers)
 
     def settings_acknowledged(self, event: SettingsAcknowledged) -> None:
-        self.metadata['settings_acknowledged'] = True
+        self.metadata["settings_acknowledged"] = True
 
         # Send off all the pending requests as now we have
         # established a proper HTTP/2 connection
         self._send_pending_requests()
 
         # Update certificate when our HTTP/2 connection is established
-        self.metadata['certificate'] = Certificate(self.transport.getPeerCertificate())
+        self.metadata["certificate"] = Certificate(self.transport.getPeerCertificate())
 
     def stream_ended(self, event: StreamEnded) -> None:
         try:
             stream = self.pop_stream(event.stream_id)
         except KeyError:
             pass  # We ignore server-initiated events
         else:
@@ -402,15 +413,17 @@
             # Send leftover data for all the streams
             for stream in self.streams.values():
                 stream.receive_window_update()
 
 
 @implementer(IProtocolNegotiationFactory)
 class H2ClientFactory(Factory):
-    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Deferred) -> None:
+    def __init__(
+        self, uri: URI, settings: Settings, conn_lost_deferred: Deferred
+    ) -> None:
         self.uri = uri
         self.settings = settings
         self.conn_lost_deferred = conn_lost_deferred
 
     def buildProtocol(self, addr) -> H2ClientProtocol:
         return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)
```

### Comparing `Scrapy-2.7.1/scrapy/core/http2/stream.py` & `Scrapy-2.8.0/scrapy/core/http2/stream.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 import logging
 from enum import Enum
 from io import BytesIO
+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple
 from urllib.parse import urlparse
-from typing import Dict, List, Optional, Tuple, TYPE_CHECKING
 
 from h2.errors import ErrorCodes
 from h2.exceptions import H2Error, ProtocolError, StreamClosedError
 from hpack import HeaderTuple
-from twisted.internet.defer import Deferred, CancelledError
+from twisted.internet.defer import CancelledError, Deferred
 from twisted.internet.error import ConnectionClosed
 from twisted.python.failure import Failure
 from twisted.web.client import ResponseFailed
 
 from scrapy.http import Request
 from scrapy.http.headers import Headers
 from scrapy.responsetypes import responsetypes
@@ -28,26 +28,27 @@
     of the stream. This happens when a stream is waiting for other
     streams to close and connection is lost."""
 
     def __init__(self, request: Request) -> None:
         self.request = request
 
     def __str__(self) -> str:
-        return f'InactiveStreamClosed: Connection was closed without sending the request {self.request!r}'
+        return f"InactiveStreamClosed: Connection was closed without sending the request {self.request!r}"
 
 
 class InvalidHostname(H2Error):
-
-    def __init__(self, request: Request, expected_hostname: str, expected_netloc: str) -> None:
+    def __init__(
+        self, request: Request, expected_hostname: str, expected_netloc: str
+    ) -> None:
         self.request = request
         self.expected_hostname = expected_hostname
         self.expected_netloc = expected_netloc
 
     def __str__(self) -> str:
-        return f'InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}'
+        return f"InvalidHostname: Expected {self.expected_hostname} or {self.expected_netloc} in {self.request}"
 
 
 class StreamCloseReason(Enum):
     # Received a StreamEnded event from the remote
     ENDED = 1
 
     # Received a StreamReset event -- ended abruptly
@@ -96,315 +97,330 @@
             request -- The HTTP request associated to the stream
             protocol -- Parent H2ClientProtocol instance
         """
         self.stream_id: int = stream_id
         self._request: Request = request
         self._protocol: "H2ClientProtocol" = protocol
 
-        self._download_maxsize = self._request.meta.get('download_maxsize', download_maxsize)
-        self._download_warnsize = self._request.meta.get('download_warnsize', download_warnsize)
+        self._download_maxsize = self._request.meta.get(
+            "download_maxsize", download_maxsize
+        )
+        self._download_warnsize = self._request.meta.get(
+            "download_warnsize", download_warnsize
+        )
 
         # Metadata of an HTTP/2 connection stream
         # initialized when stream is instantiated
         self.metadata: Dict = {
-            'request_content_length': 0 if self._request.body is None else len(self._request.body),
-
+            "request_content_length": 0
+            if self._request.body is None
+            else len(self._request.body),
             # Flag to keep track whether the stream has initiated the request
-            'request_sent': False,
-
+            "request_sent": False,
             # Flag to track whether we have logged about exceeding download warnsize
-            'reached_warnsize': False,
-
+            "reached_warnsize": False,
             # Each time we send a data frame, we will decrease value by the amount send.
-            'remaining_content_length': 0 if self._request.body is None else len(self._request.body),
-
+            "remaining_content_length": 0
+            if self._request.body is None
+            else len(self._request.body),
             # Flag to keep track whether client (self) have closed this stream
-            'stream_closed_local': False,
-
+            "stream_closed_local": False,
             # Flag to keep track whether the server has closed the stream
-            'stream_closed_server': False,
+            "stream_closed_server": False,
         }
 
         # Private variable used to build the response
         # this response is then converted to appropriate Response class
         # passed to the response deferred callback
         self._response: Dict = {
             # Data received frame by frame from the server is appended
             # and passed to the response Deferred when completely received.
-            'body': BytesIO(),
-
+            "body": BytesIO(),
             # The amount of data received that counts against the
             # flow control window
-            'flow_controlled_size': 0,
-
+            "flow_controlled_size": 0,
             # Headers received after sending the request
-            'headers': Headers({}),
+            "headers": Headers({}),
         }
 
         def _cancel(_) -> None:
             # Close this stream as gracefully as possible
             # If the associated request is initiated we reset this stream
             # else we directly call close() method
-            if self.metadata['request_sent']:
+            if self.metadata["request_sent"]:
                 self.reset_stream(StreamCloseReason.CANCELLED)
             else:
                 self.close(StreamCloseReason.CANCELLED)
 
         self._deferred_response = Deferred(_cancel)
 
-    def __str__(self) -> str:
-        return f'Stream(id={self.stream_id!r})'
-
-    __repr__ = __str__
+    def __repr__(self) -> str:
+        return f"Stream(id={self.stream_id!r})"
 
     @property
     def _log_warnsize(self) -> bool:
         """Checks if we have received data which exceeds the download warnsize
         and whether we have not already logged about it.
 
         Returns:
             True if both the above conditions hold true
             False if any of the conditions is false
         """
-        content_length_header = int(self._response['headers'].get(b'Content-Length', -1))
+        content_length_header = int(
+            self._response["headers"].get(b"Content-Length", -1)
+        )
         return (
             self._download_warnsize
             and (
-                self._response['flow_controlled_size'] > self._download_warnsize
+                self._response["flow_controlled_size"] > self._download_warnsize
                 or content_length_header > self._download_warnsize
             )
-            and not self.metadata['reached_warnsize']
+            and not self.metadata["reached_warnsize"]
         )
 
     def get_response(self) -> Deferred:
         """Simply return a Deferred which fires when response
         from the asynchronous request is available
         """
         return self._deferred_response
 
     def check_request_url(self) -> bool:
         # Make sure that we are sending the request to the correct URL
         url = urlparse(self._request.url)
         return (
-            url.netloc == str(self._protocol.metadata['uri'].host, 'utf-8')
-            or url.netloc == str(self._protocol.metadata['uri'].netloc, 'utf-8')
-            or url.netloc == f'{self._protocol.metadata["ip_address"]}:{self._protocol.metadata["uri"].port}'
+            url.netloc == str(self._protocol.metadata["uri"].host, "utf-8")
+            or url.netloc == str(self._protocol.metadata["uri"].netloc, "utf-8")
+            or url.netloc
+            == f'{self._protocol.metadata["ip_address"]}:{self._protocol.metadata["uri"].port}'
         )
 
     def _get_request_headers(self) -> List[Tuple[str, str]]:
         url = urlparse(self._request.url)
 
         path = url.path
         if url.query:
-            path += '?' + url.query
+            path += "?" + url.query
 
         # This pseudo-header field MUST NOT be empty for "http" or "https"
         # URIs; "http" or "https" URIs that do not contain a path component
         # MUST include a value of '/'. The exception to this rule is an
         # OPTIONS request for an "http" or "https" URI that does not include
         # a path component; these MUST include a ":path" pseudo-header field
         # with a value of '*' (refer RFC 7540 - Section 8.1.2.3)
         if not path:
-            path = '*' if self._request.method == 'OPTIONS' else '/'
+            path = "*" if self._request.method == "OPTIONS" else "/"
 
         # Make sure pseudo-headers comes before all the other headers
         headers = [
-            (':method', self._request.method),
-            (':authority', url.netloc),
+            (":method", self._request.method),
+            (":authority", url.netloc),
         ]
 
         # The ":scheme" and ":path" pseudo-header fields MUST
         # be omitted for CONNECT method (refer RFC 7540 - Section 8.3)
-        if self._request.method != 'CONNECT':
+        if self._request.method != "CONNECT":
             headers += [
-                (':scheme', self._protocol.metadata['uri'].scheme),
-                (':path', path),
+                (":scheme", self._protocol.metadata["uri"].scheme),
+                (":path", path),
             ]
 
         content_length = str(len(self._request.body))
-        headers.append(('Content-Length', content_length))
+        headers.append(("Content-Length", content_length))
 
-        content_length_name = self._request.headers.normkey(b'Content-Length')
+        content_length_name = self._request.headers.normkey(b"Content-Length")
         for name, values in self._request.headers.items():
             for value in values:
-                value = str(value, 'utf-8')
+                value = str(value, "utf-8")
                 if name == content_length_name:
                     if value != content_length:
                         logger.warning(
-                            'Ignoring bad Content-Length header %r of request %r, '
-                            'sending %r instead',
+                            "Ignoring bad Content-Length header %r of request %r, "
+                            "sending %r instead",
                             value,
                             self._request,
                             content_length,
                         )
                     continue
-                headers.append((str(name, 'utf-8'), value))
+                headers.append((str(name, "utf-8"), value))
 
         return headers
 
     def initiate_request(self) -> None:
         if self.check_request_url():
             headers = self._get_request_headers()
             self._protocol.conn.send_headers(self.stream_id, headers, end_stream=False)
-            self.metadata['request_sent'] = True
+            self.metadata["request_sent"] = True
             self.send_data()
         else:
             # Close this stream calling the response errback
             # Note that we have not sent any headers
             self.close(StreamCloseReason.INVALID_HOSTNAME)
 
     def send_data(self) -> None:
         """Called immediately after the headers are sent. Here we send all the
-         data as part of the request.
+        data as part of the request.
 
-         If the content length is 0 initially then we end the stream immediately and
-         wait for response data.
+        If the content length is 0 initially then we end the stream immediately and
+        wait for response data.
 
-         Warning: Only call this method when stream not closed from client side
-            and has initiated request already by sending HEADER frame. If not then
-            stream will raise ProtocolError (raise by h2 state machine).
-         """
-        if self.metadata['stream_closed_local']:
+        Warning: Only call this method when stream not closed from client side
+           and has initiated request already by sending HEADER frame. If not then
+           stream will raise ProtocolError (raise by h2 state machine).
+        """
+        if self.metadata["stream_closed_local"]:
             raise StreamClosedError(self.stream_id)
 
         # Firstly, check what the flow control window is for current stream.
-        window_size = self._protocol.conn.local_flow_control_window(stream_id=self.stream_id)
+        window_size = self._protocol.conn.local_flow_control_window(
+            stream_id=self.stream_id
+        )
 
         # Next, check what the maximum frame size is.
         max_frame_size = self._protocol.conn.max_outbound_frame_size
 
         # We will send no more than the window size or the remaining file size
         # of data in this call, whichever is smaller.
-        bytes_to_send_size = min(window_size, self.metadata['remaining_content_length'])
+        bytes_to_send_size = min(window_size, self.metadata["remaining_content_length"])
 
         # We now need to send a number of data frames.
         while bytes_to_send_size > 0:
             chunk_size = min(bytes_to_send_size, max_frame_size)
 
-            data_chunk_start_id = self.metadata['request_content_length'] - self.metadata['remaining_content_length']
-            data_chunk = self._request.body[data_chunk_start_id:data_chunk_start_id + chunk_size]
+            data_chunk_start_id = (
+                self.metadata["request_content_length"]
+                - self.metadata["remaining_content_length"]
+            )
+            data_chunk = self._request.body[
+                data_chunk_start_id : data_chunk_start_id + chunk_size
+            ]
 
             self._protocol.conn.send_data(self.stream_id, data_chunk, end_stream=False)
 
             bytes_to_send_size -= chunk_size
-            self.metadata['remaining_content_length'] -= chunk_size
+            self.metadata["remaining_content_length"] -= chunk_size
 
-        self.metadata['remaining_content_length'] = max(0, self.metadata['remaining_content_length'])
+        self.metadata["remaining_content_length"] = max(
+            0, self.metadata["remaining_content_length"]
+        )
 
         # End the stream if no more data needs to be send
-        if self.metadata['remaining_content_length'] == 0:
+        if self.metadata["remaining_content_length"] == 0:
             self._protocol.conn.end_stream(self.stream_id)
 
         # Q. What about the rest of the data?
         # Ans: Remaining Data frames will be sent when we get a WindowUpdate frame
 
     def receive_window_update(self) -> None:
         """Flow control window size was changed.
         Send data that earlier could not be sent as we were
         blocked behind the flow control.
         """
         if (
-            self.metadata['remaining_content_length']
-            and not self.metadata['stream_closed_server']
-            and self.metadata['request_sent']
+            self.metadata["remaining_content_length"]
+            and not self.metadata["stream_closed_server"]
+            and self.metadata["request_sent"]
         ):
             self.send_data()
 
     def receive_data(self, data: bytes, flow_controlled_length: int) -> None:
-        self._response['body'].write(data)
-        self._response['flow_controlled_size'] += flow_controlled_length
+        self._response["body"].write(data)
+        self._response["flow_controlled_size"] += flow_controlled_length
 
         # We check maxsize here in case the Content-Length header was not received
-        if self._download_maxsize and self._response['flow_controlled_size'] > self._download_maxsize:
+        if (
+            self._download_maxsize
+            and self._response["flow_controlled_size"] > self._download_maxsize
+        ):
             self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)
             return
 
         if self._log_warnsize:
-            self.metadata['reached_warnsize'] = True
+            self.metadata["reached_warnsize"] = True
             warning_msg = (
                 f'Received more ({self._response["flow_controlled_size"]}) bytes than download '
-                f'warn size ({self._download_warnsize}) in request {self._request}'
+                f"warn size ({self._download_warnsize}) in request {self._request}"
             )
             logger.warning(warning_msg)
 
         # Acknowledge the data received
         self._protocol.conn.acknowledge_received_data(
-            self._response['flow_controlled_size'],
-            self.stream_id
+            self._response["flow_controlled_size"], self.stream_id
         )
 
     def receive_headers(self, headers: List[HeaderTuple]) -> None:
         for name, value in headers:
-            self._response['headers'][name] = value
+            self._response["headers"].appendlist(name, value)
 
         # Check if we exceed the allowed max data size which can be received
-        expected_size = int(self._response['headers'].get(b'Content-Length', -1))
+        expected_size = int(self._response["headers"].get(b"Content-Length", -1))
         if self._download_maxsize and expected_size > self._download_maxsize:
             self.reset_stream(StreamCloseReason.MAXSIZE_EXCEEDED)
             return
 
         if self._log_warnsize:
-            self.metadata['reached_warnsize'] = True
+            self.metadata["reached_warnsize"] = True
             warning_msg = (
-                f'Expected response size ({expected_size}) larger than '
-                f'download warn size ({self._download_warnsize}) in request {self._request}'
+                f"Expected response size ({expected_size}) larger than "
+                f"download warn size ({self._download_warnsize}) in request {self._request}"
             )
             logger.warning(warning_msg)
 
     def reset_stream(self, reason: StreamCloseReason = StreamCloseReason.RESET) -> None:
         """Close this stream by sending a RST_FRAME to the remote peer"""
-        if self.metadata['stream_closed_local']:
+        if self.metadata["stream_closed_local"]:
             raise StreamClosedError(self.stream_id)
 
         # Clear buffer earlier to avoid keeping data in memory for a long time
-        self._response['body'].truncate(0)
+        self._response["body"].truncate(0)
 
-        self.metadata['stream_closed_local'] = True
+        self.metadata["stream_closed_local"] = True
         self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)
         self.close(reason)
 
     def close(
         self,
         reason: StreamCloseReason,
         errors: Optional[List[BaseException]] = None,
         from_protocol: bool = False,
     ) -> None:
-        """Based on the reason sent we will handle each case.
-        """
-        if self.metadata['stream_closed_server']:
+        """Based on the reason sent we will handle each case."""
+        if self.metadata["stream_closed_server"]:
             raise StreamClosedError(self.stream_id)
 
         if not isinstance(reason, StreamCloseReason):
-            raise TypeError(f'Expected StreamCloseReason, received {reason.__class__.__qualname__}')
+            raise TypeError(
+                f"Expected StreamCloseReason, received {reason.__class__.__qualname__}"
+            )
 
         # Have default value of errors as an empty list as
         # some cases can add a list of exceptions
         errors = errors or []
 
         if not from_protocol:
             self._protocol.pop_stream(self.stream_id)
 
-        self.metadata['stream_closed_server'] = True
+        self.metadata["stream_closed_server"] = True
 
         # We do not check for Content-Length or Transfer-Encoding in response headers
         # and add `partial` flag as in HTTP/1.1 as 'A request or response that includes
         # a payload body can include a content-length header field' (RFC 7540 - Section 8.1.2.6)
 
         # NOTE: Order of handling the events is important here
         # As we immediately cancel the request when maxsize is exceeded while
         # receiving DATA_FRAME's when we have received the headers (not
         # having Content-Length)
         if reason is StreamCloseReason.MAXSIZE_EXCEEDED:
-            expected_size = int(self._response['headers'].get(
-                b'Content-Length',
-                self._response['flow_controlled_size'])
+            expected_size = int(
+                self._response["headers"].get(
+                    b"Content-Length", self._response["flow_controlled_size"]
+                )
             )
             error_msg = (
-                f'Cancelling download of {self._request.url}: received response '
-                f'size ({expected_size}) larger than download max size ({self._download_maxsize})'
+                f"Cancelling download of {self._request.url}: received response "
+                f"size ({expected_size}) larger than download max size ({self._download_maxsize})"
             )
             logger.error(error_msg)
             self._deferred_response.errback(CancelledError(error_msg))
 
         elif reason is StreamCloseReason.ENDED:
             self._fire_response_deferred()
 
@@ -414,57 +430,63 @@
             # received and fire the response deferred with no flags set
 
             # NOTE: The data is already flushed in Stream.reset_stream() called
             # immediately when the stream needs to be cancelled
 
             # There maybe no :status in headers, we make
             # HTTP Status Code: 499 - Client Closed Request
-            self._response['headers'][':status'] = '499'
+            self._response["headers"][":status"] = "499"
             self._fire_response_deferred()
 
         elif reason is StreamCloseReason.RESET:
-            self._deferred_response.errback(ResponseFailed([
-                Failure(
-                    f'Remote peer {self._protocol.metadata["ip_address"]} sent RST_STREAM',
-                    ProtocolError
+            self._deferred_response.errback(
+                ResponseFailed(
+                    [
+                        Failure(
+                            f'Remote peer {self._protocol.metadata["ip_address"]} sent RST_STREAM',
+                            ProtocolError,
+                        )
+                    ]
                 )
-            ]))
+            )
 
         elif reason is StreamCloseReason.CONNECTION_LOST:
             self._deferred_response.errback(ResponseFailed(errors))
 
         elif reason is StreamCloseReason.INACTIVE:
             errors.insert(0, InactiveStreamClosed(self._request))
             self._deferred_response.errback(ResponseFailed(errors))
 
         else:
             assert reason is StreamCloseReason.INVALID_HOSTNAME
-            self._deferred_response.errback(InvalidHostname(
-                self._request,
-                str(self._protocol.metadata['uri'].host, 'utf-8'),
-                f'{self._protocol.metadata["ip_address"]}:{self._protocol.metadata["uri"].port}'
-            ))
+            self._deferred_response.errback(
+                InvalidHostname(
+                    self._request,
+                    str(self._protocol.metadata["uri"].host, "utf-8"),
+                    f'{self._protocol.metadata["ip_address"]}:{self._protocol.metadata["uri"].port}',
+                )
+            )
 
     def _fire_response_deferred(self) -> None:
         """Builds response from the self._response dict
         and fires the response deferred callback with the
         generated response instance"""
 
-        body = self._response['body'].getvalue()
+        body = self._response["body"].getvalue()
         response_cls = responsetypes.from_args(
-            headers=self._response['headers'],
+            headers=self._response["headers"],
             url=self._request.url,
             body=body,
         )
 
         response = response_cls(
             url=self._request.url,
-            status=int(self._response['headers'][':status']),
-            headers=self._response['headers'],
+            status=int(self._response["headers"][":status"]),
+            headers=self._response["headers"],
             body=body,
             request=self._request,
-            certificate=self._protocol.metadata['certificate'],
-            ip_address=self._protocol.metadata['ip_address'],
-            protocol='h2',
+            certificate=self._protocol.metadata["certificate"],
+            ip_address=self._protocol.metadata["ip_address"],
+            protocol="h2",
         )
 
         self._deferred_response.callback(response)
```

### Comparing `Scrapy-2.7.1/scrapy/core/scheduler.py` & `Scrapy-2.8.0/scrapy/core/scheduler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,38 +1,40 @@
 import json
 import logging
-import os
 from abc import abstractmethod
-from os.path import exists, join
+from pathlib import Path
 from typing import Optional, Type, TypeVar
 
 from twisted.internet.defer import Deferred
 
 from scrapy.crawler import Crawler
 from scrapy.http.request import Request
 from scrapy.spiders import Spider
 from scrapy.utils.job import job_dir
 from scrapy.utils.misc import create_instance, load_object
 
-
 logger = logging.getLogger(__name__)
 
 
 class BaseSchedulerMeta(type):
     """
     Metaclass to check scheduler classes against the necessary interface
     """
+
     def __instancecheck__(cls, instance):
         return cls.__subclasscheck__(type(instance))
 
     def __subclasscheck__(cls, subclass):
         return (
-            hasattr(subclass, "has_pending_requests") and callable(subclass.has_pending_requests)
-            and hasattr(subclass, "enqueue_request") and callable(subclass.enqueue_request)
-            and hasattr(subclass, "next_request") and callable(subclass.next_request)
+            hasattr(subclass, "has_pending_requests")
+            and callable(subclass.has_pending_requests)
+            and hasattr(subclass, "enqueue_request")
+            and callable(subclass.enqueue_request)
+            and hasattr(subclass, "next_request")
+            and callable(subclass.next_request)
         )
 
 
 class BaseScheduler(metaclass=BaseSchedulerMeta):
     """
     The scheduler component is responsible for storing requests received from
     the engine, and feeding them back upon request (also to the engine).
@@ -159,14 +161,15 @@
     :param pqclass: A class to be used as priority queue for requests.
                     The value for the :setting:`SCHEDULER_PRIORITY_QUEUE` setting is used by default.
     :type pqclass: class
 
     :param crawler: The crawler object corresponding to the current crawl.
     :type crawler: :class:`scrapy.crawler.Crawler`
     """
+
     def __init__(
         self,
         dupefilter,
         jobdir: Optional[str] = None,
         dqclass=None,
         mqclass=None,
         logunser: bool = False,
@@ -184,23 +187,23 @@
         self.crawler = crawler
 
     @classmethod
     def from_crawler(cls: Type[SchedulerTV], crawler) -> SchedulerTV:
         """
         Factory method, initializes the scheduler with arguments taken from the crawl settings
         """
-        dupefilter_cls = load_object(crawler.settings['DUPEFILTER_CLASS'])
+        dupefilter_cls = load_object(crawler.settings["DUPEFILTER_CLASS"])
         return cls(
             dupefilter=create_instance(dupefilter_cls, crawler.settings, crawler),
             jobdir=job_dir(crawler.settings),
-            dqclass=load_object(crawler.settings['SCHEDULER_DISK_QUEUE']),
-            mqclass=load_object(crawler.settings['SCHEDULER_MEMORY_QUEUE']),
-            logunser=crawler.settings.getbool('SCHEDULER_DEBUG'),
+            dqclass=load_object(crawler.settings["SCHEDULER_DISK_QUEUE"]),
+            mqclass=load_object(crawler.settings["SCHEDULER_MEMORY_QUEUE"]),
+            logunser=crawler.settings.getbool("SCHEDULER_DEBUG"),
             stats=crawler.stats,
-            pqclass=load_object(crawler.settings['SCHEDULER_PRIORITY_QUEUE']),
+            pqclass=load_object(crawler.settings["SCHEDULER_PRIORITY_QUEUE"]),
             crawler=crawler,
         )
 
     def has_pending_requests(self) -> bool:
         return len(self) > 0
 
     def open(self, spider: Spider) -> Optional[Deferred]:
@@ -236,39 +239,39 @@
         Return ``True`` if the request was stored successfully, ``False`` otherwise.
         """
         if not request.dont_filter and self.df.request_seen(request):
             self.df.log(request, self.spider)
             return False
         dqok = self._dqpush(request)
         if dqok:
-            self.stats.inc_value('scheduler/enqueued/disk', spider=self.spider)
+            self.stats.inc_value("scheduler/enqueued/disk", spider=self.spider)
         else:
             self._mqpush(request)
-            self.stats.inc_value('scheduler/enqueued/memory', spider=self.spider)
-        self.stats.inc_value('scheduler/enqueued', spider=self.spider)
+            self.stats.inc_value("scheduler/enqueued/memory", spider=self.spider)
+        self.stats.inc_value("scheduler/enqueued", spider=self.spider)
         return True
 
     def next_request(self) -> Optional[Request]:
         """
         Return a :class:`~scrapy.http.Request` object from the memory queue,
         falling back to the disk queue if the memory queue is empty.
         Return ``None`` if there are no more enqueued requests.
 
         Increment the appropriate stats, such as: ``scheduler/dequeued``,
         ``scheduler/dequeued/disk``, ``scheduler/dequeued/memory``.
         """
         request = self.mqs.pop()
         if request is not None:
-            self.stats.inc_value('scheduler/dequeued/memory', spider=self.spider)
+            self.stats.inc_value("scheduler/dequeued/memory", spider=self.spider)
         else:
             request = self._dqpop()
             if request is not None:
-                self.stats.inc_value('scheduler/dequeued/disk', spider=self.spider)
+                self.stats.inc_value("scheduler/dequeued/disk", spider=self.spider)
         if request is not None:
-            self.stats.inc_value('scheduler/dequeued', spider=self.spider)
+            self.stats.inc_value("scheduler/dequeued", spider=self.spider)
         return request
 
     def __len__(self) -> int:
         """
         Return the total amount of enqueued requests
         """
         return len(self.dqs) + len(self.mqs) if self.dqs is not None else len(self.mqs)
@@ -276,67 +279,80 @@
     def _dqpush(self, request: Request) -> bool:
         if self.dqs is None:
             return False
         try:
             self.dqs.push(request)
         except ValueError as e:  # non serializable request
             if self.logunser:
-                msg = ("Unable to serialize request: %(request)s - reason:"
-                       " %(reason)s - no more unserializable requests will be"
-                       " logged (stats being collected)")
-                logger.warning(msg, {'request': request, 'reason': e},
-                               exc_info=True, extra={'spider': self.spider})
+                msg = (
+                    "Unable to serialize request: %(request)s - reason:"
+                    " %(reason)s - no more unserializable requests will be"
+                    " logged (stats being collected)"
+                )
+                logger.warning(
+                    msg,
+                    {"request": request, "reason": e},
+                    exc_info=True,
+                    extra={"spider": self.spider},
+                )
                 self.logunser = False
-            self.stats.inc_value('scheduler/unserializable', spider=self.spider)
+            self.stats.inc_value("scheduler/unserializable", spider=self.spider)
             return False
         else:
             return True
 
     def _mqpush(self, request: Request) -> None:
         self.mqs.push(request)
 
     def _dqpop(self) -> Optional[Request]:
         if self.dqs is not None:
             return self.dqs.pop()
         return None
 
     def _mq(self):
-        """ Create a new priority queue instance, with in-memory storage """
-        return create_instance(self.pqclass,
-                               settings=None,
-                               crawler=self.crawler,
-                               downstream_queue_cls=self.mqclass,
-                               key='')
+        """Create a new priority queue instance, with in-memory storage"""
+        return create_instance(
+            self.pqclass,
+            settings=None,
+            crawler=self.crawler,
+            downstream_queue_cls=self.mqclass,
+            key="",
+        )
 
     def _dq(self):
-        """ Create a new priority queue instance, with disk storage """
+        """Create a new priority queue instance, with disk storage"""
         state = self._read_dqs_state(self.dqdir)
-        q = create_instance(self.pqclass,
-                            settings=None,
-                            crawler=self.crawler,
-                            downstream_queue_cls=self.dqclass,
-                            key=self.dqdir,
-                            startprios=state)
+        q = create_instance(
+            self.pqclass,
+            settings=None,
+            crawler=self.crawler,
+            downstream_queue_cls=self.dqclass,
+            key=self.dqdir,
+            startprios=state,
+        )
         if q:
-            logger.info("Resuming crawl (%(queuesize)d requests scheduled)",
-                        {'queuesize': len(q)}, extra={'spider': self.spider})
+            logger.info(
+                "Resuming crawl (%(queuesize)d requests scheduled)",
+                {"queuesize": len(q)},
+                extra={"spider": self.spider},
+            )
         return q
 
     def _dqdir(self, jobdir: Optional[str]) -> Optional[str]:
-        """ Return a folder name to keep disk queue state at """
+        """Return a folder name to keep disk queue state at"""
         if jobdir is not None:
-            dqdir = join(jobdir, 'requests.queue')
-            if not exists(dqdir):
-                os.makedirs(dqdir)
-            return dqdir
+            dqdir = Path(jobdir, "requests.queue")
+            if not dqdir.exists():
+                dqdir.mkdir(parents=True)
+            return str(dqdir)
         return None
 
     def _read_dqs_state(self, dqdir: str) -> list:
-        path = join(dqdir, 'active.json')
-        if not exists(path):
+        path = Path(dqdir, "active.json")
+        if not path.exists():
             return []
-        with open(path) as f:
+        with path.open(encoding="utf-8") as f:
             return json.load(f)
 
     def _write_dqs_state(self, dqdir: str, state: list) -> None:
-        with open(join(dqdir, 'active.json'), 'w') as f:
+        with Path(dqdir, "active.json").open("w", encoding="utf-8") as f:
             json.dump(state, f)
```

### Comparing `Scrapy-2.7.1/scrapy/core/scraper.py` & `Scrapy-2.8.0/scrapy/core/scraper.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,34 +1,50 @@
 """This module implements the Scraper component which parses responses and
 extracts information from them"""
+from __future__ import annotations
+
 import logging
 from collections import deque
-from typing import Any, AsyncGenerator, AsyncIterable, Deque, Generator, Iterable, Optional, Set, Tuple, Union
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    AsyncGenerator,
+    AsyncIterable,
+    Deque,
+    Generator,
+    Iterable,
+    Optional,
+    Set,
+    Tuple,
+    Union,
+)
 
 from itemadapter import is_item
 from twisted.internet.defer import Deferred, inlineCallbacks
 from twisted.python.failure import Failure
 
-from scrapy import signals, Spider
+from scrapy import Spider, signals
 from scrapy.core.spidermw import SpiderMiddlewareManager
 from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest
 from scrapy.http import Request, Response
 from scrapy.utils.defer import (
     aiter_errback,
     defer_fail,
     defer_succeed,
     iter_errback,
     parallel,
     parallel_async,
 )
-
 from scrapy.utils.log import failure_to_exc_info, logformatter_adapter
 from scrapy.utils.misc import load_object, warn_on_generator_with_return_value
 from scrapy.utils.spider import iterate_spider_output
 
+if TYPE_CHECKING:
+    from scrapy.crawler import Crawler
+
 
 QueueTuple = Tuple[Union[Response, Failure], Request, Deferred]
 
 
 logger = logging.getLogger(__name__)
 
 
@@ -41,58 +57,61 @@
         self.max_active_size = max_active_size
         self.queue: Deque[QueueTuple] = deque()
         self.active: Set[Request] = set()
         self.active_size: int = 0
         self.itemproc_size: int = 0
         self.closing: Optional[Deferred] = None
 
-    def add_response_request(self, result: Union[Response, Failure], request: Request) -> Deferred:
+    def add_response_request(
+        self, result: Union[Response, Failure], request: Request
+    ) -> Deferred:
         deferred = Deferred()
         self.queue.append((result, request, deferred))
         if isinstance(result, Response):
             self.active_size += max(len(result.body), self.MIN_RESPONSE_SIZE)
         else:
             self.active_size += self.MIN_RESPONSE_SIZE
         return deferred
 
     def next_response_request_deferred(self) -> QueueTuple:
         response, request, deferred = self.queue.popleft()
         self.active.add(request)
         return response, request, deferred
 
-    def finish_response(self, result: Union[Response, Failure], request: Request) -> None:
+    def finish_response(
+        self, result: Union[Response, Failure], request: Request
+    ) -> None:
         self.active.remove(request)
         if isinstance(result, Response):
             self.active_size -= max(len(result.body), self.MIN_RESPONSE_SIZE)
         else:
             self.active_size -= self.MIN_RESPONSE_SIZE
 
     def is_idle(self) -> bool:
         return not (self.queue or self.active)
 
     def needs_backout(self) -> bool:
         return self.active_size > self.max_active_size
 
 
 class Scraper:
-
-    def __init__(self, crawler):
+    def __init__(self, crawler: Crawler) -> None:
         self.slot: Optional[Slot] = None
         self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)
-        itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])
+        itemproc_cls = load_object(crawler.settings["ITEM_PROCESSOR"])
         self.itemproc = itemproc_cls.from_crawler(crawler)
-        self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')
+        self.concurrent_items = crawler.settings.getint("CONCURRENT_ITEMS")
         self.crawler = crawler
         self.signals = crawler.signals
         self.logformatter = crawler.logformatter
 
     @inlineCallbacks
     def open_spider(self, spider: Spider):
         """Open the given spider for scraping and allocate resources for it"""
-        self.slot = Slot(self.crawler.settings.getint('SCRAPER_SLOT_MAX_ACTIVE_SIZE'))
+        self.slot = Slot(self.crawler.settings.getint("SCRAPER_SLOT_MAX_ACTIVE_SIZE"))
         yield self.itemproc.open_spider(spider)
 
     def close_spider(self, spider: Spider) -> Deferred:
         """Close a spider being scraped and release its resources"""
         if self.slot is None:
             raise RuntimeError("Scraper slot not assigned")
         self.slot.closing = Deferred()
@@ -105,188 +124,251 @@
         return not self.slot
 
     def _check_if_closing(self, spider: Spider) -> None:
         assert self.slot is not None  # typing
         if self.slot.closing and self.slot.is_idle():
             self.slot.closing.callback(spider)
 
-    def enqueue_scrape(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:
+    def enqueue_scrape(
+        self, result: Union[Response, Failure], request: Request, spider: Spider
+    ) -> Deferred:
         if self.slot is None:
             raise RuntimeError("Scraper slot not assigned")
         dfd = self.slot.add_response_request(result, request)
 
         def finish_scraping(_):
             self.slot.finish_response(result, request)
             self._check_if_closing(spider)
             self._scrape_next(spider)
             return _
 
         dfd.addBoth(finish_scraping)
         dfd.addErrback(
-            lambda f: logger.error('Scraper bug processing %(request)s',
-                                   {'request': request},
-                                   exc_info=failure_to_exc_info(f),
-                                   extra={'spider': spider}))
+            lambda f: logger.error(
+                "Scraper bug processing %(request)s",
+                {"request": request},
+                exc_info=failure_to_exc_info(f),
+                extra={"spider": spider},
+            )
+        )
         self._scrape_next(spider)
         return dfd
 
     def _scrape_next(self, spider: Spider) -> None:
         assert self.slot is not None  # typing
         while self.slot.queue:
             response, request, deferred = self.slot.next_response_request_deferred()
             self._scrape(response, request, spider).chainDeferred(deferred)
 
-    def _scrape(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:
+    def _scrape(
+        self, result: Union[Response, Failure], request: Request, spider: Spider
+    ) -> Deferred:
         """
         Handle the downloaded response or failure through the spider callback/errback
         """
         if not isinstance(result, (Response, Failure)):
-            raise TypeError(f"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}")
-        dfd = self._scrape2(result, request, spider)  # returns spider's processed output
+            raise TypeError(
+                f"Incorrect type: expected Response or Failure, got {type(result)}: {result!r}"
+            )
+        dfd = self._scrape2(
+            result, request, spider
+        )  # returns spider's processed output
         dfd.addErrback(self.handle_spider_error, request, result, spider)
         dfd.addCallback(self.handle_spider_output, request, result, spider)
         return dfd
 
-    def _scrape2(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:
+    def _scrape2(
+        self, result: Union[Response, Failure], request: Request, spider: Spider
+    ) -> Deferred:
         """
         Handle the different cases of request's result been a Response or a Failure
         """
         if isinstance(result, Response):
-            return self.spidermw.scrape_response(self.call_spider, result, request, spider)
-        else:  # result is a Failure
-            dfd = self.call_spider(result, request, spider)
-            return dfd.addErrback(self._log_download_errors, result, request, spider)
-
-    def call_spider(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:
+            return self.spidermw.scrape_response(
+                self.call_spider, result, request, spider
+            )
+        # else result is a Failure
+        dfd = self.call_spider(result, request, spider)
+        return dfd.addErrback(self._log_download_errors, result, request, spider)
+
+    def call_spider(
+        self, result: Union[Response, Failure], request: Request, spider: Spider
+    ) -> Deferred:
         if isinstance(result, Response):
             if getattr(result, "request", None) is None:
                 result.request = request
             callback = result.request.callback or spider._parse
             warn_on_generator_with_return_value(spider, callback)
             dfd = defer_succeed(result)
-            dfd.addCallbacks(callback=callback, callbackKeywords=result.request.cb_kwargs)
+            dfd.addCallbacks(
+                callback=callback, callbackKeywords=result.request.cb_kwargs
+            )
         else:  # result is a Failure
             result.request = request
             warn_on_generator_with_return_value(spider, request.errback)
             dfd = defer_fail(result)
             dfd.addErrback(request.errback)
         return dfd.addCallback(iterate_spider_output)
 
-    def handle_spider_error(self, _failure: Failure, request: Request, response: Response, spider: Spider) -> None:
+    def handle_spider_error(
+        self, _failure: Failure, request: Request, response: Response, spider: Spider
+    ) -> None:
         exc = _failure.value
         if isinstance(exc, CloseSpider):
-            self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')
+            assert self.crawler.engine is not None  # typing
+            self.crawler.engine.close_spider(spider, exc.reason or "cancelled")
             return
         logkws = self.logformatter.spider_error(_failure, request, response, spider)
         logger.log(
             *logformatter_adapter(logkws),
             exc_info=failure_to_exc_info(_failure),
-            extra={'spider': spider}
+            extra={"spider": spider},
         )
         self.signals.send_catch_log(
             signal=signals.spider_error,
-            failure=_failure, response=response,
-            spider=spider
+            failure=_failure,
+            response=response,
+            spider=spider,
         )
         self.crawler.stats.inc_value(
-            f"spider_exceptions/{_failure.value.__class__.__name__}",
-            spider=spider
+            f"spider_exceptions/{_failure.value.__class__.__name__}", spider=spider
         )
 
-    def handle_spider_output(self, result: Union[Iterable, AsyncIterable], request: Request,
-                             response: Response, spider: Spider) -> Deferred:
+    def handle_spider_output(
+        self,
+        result: Union[Iterable, AsyncIterable],
+        request: Request,
+        response: Response,
+        spider: Spider,
+    ) -> Deferred:
         if not result:
             return defer_succeed(None)
         it: Union[Generator, AsyncGenerator]
         if isinstance(result, AsyncIterable):
-            it = aiter_errback(result, self.handle_spider_error, request, response, spider)
-            dfd = parallel_async(it, self.concurrent_items, self._process_spidermw_output,
-                                 request, response, spider)
+            it = aiter_errback(
+                result, self.handle_spider_error, request, response, spider
+            )
+            dfd = parallel_async(
+                it,
+                self.concurrent_items,
+                self._process_spidermw_output,
+                request,
+                response,
+                spider,
+            )
         else:
-            it = iter_errback(result, self.handle_spider_error, request, response, spider)
-            dfd = parallel(it, self.concurrent_items, self._process_spidermw_output,
-                           request, response, spider)
+            it = iter_errback(
+                result, self.handle_spider_error, request, response, spider
+            )
+            dfd = parallel(
+                it,
+                self.concurrent_items,
+                self._process_spidermw_output,
+                request,
+                response,
+                spider,
+            )
         return dfd
 
-    def _process_spidermw_output(self, output: Any, request: Request, response: Response,
-                                 spider: Spider) -> Optional[Deferred]:
+    def _process_spidermw_output(
+        self, output: Any, request: Request, response: Response, spider: Spider
+    ) -> Optional[Deferred]:
         """Process each Request/Item (given in the output parameter) returned
         from the given spider
         """
         assert self.slot is not None  # typing
         if isinstance(output, Request):
+            assert self.crawler.engine is not None  # typing
             self.crawler.engine.crawl(request=output)
         elif is_item(output):
             self.slot.itemproc_size += 1
             dfd = self.itemproc.process_item(output, spider)
             dfd.addBoth(self._itemproc_finished, output, response, spider)
             return dfd
         elif output is None:
             pass
         else:
             typename = type(output).__name__
             logger.error(
-                'Spider must return request, item, or None, got %(typename)r in %(request)s',
-                {'request': request, 'typename': typename},
-                extra={'spider': spider},
+                "Spider must return request, item, or None, got %(typename)r in %(request)s",
+                {"request": request, "typename": typename},
+                extra={"spider": spider},
             )
         return None
 
-    def _log_download_errors(self, spider_failure: Failure, download_failure: Failure, request: Request,
-                             spider: Spider) -> Union[Failure, None]:
+    def _log_download_errors(
+        self,
+        spider_failure: Failure,
+        download_failure: Failure,
+        request: Request,
+        spider: Spider,
+    ) -> Union[Failure, None]:
         """Log and silence errors that come from the engine (typically download
         errors that got propagated thru here).
 
         spider_failure: the value passed into the errback of self.call_spider()
         download_failure: the value passed into _scrape2() from
         ExecutionEngine._handle_downloader_output() as "result"
         """
         if not download_failure.check(IgnoreRequest):
             if download_failure.frames:
-                logkws = self.logformatter.download_error(download_failure, request, spider)
+                logkws = self.logformatter.download_error(
+                    download_failure, request, spider
+                )
                 logger.log(
                     *logformatter_adapter(logkws),
-                    extra={'spider': spider},
+                    extra={"spider": spider},
                     exc_info=failure_to_exc_info(download_failure),
                 )
             else:
                 errmsg = download_failure.getErrorMessage()
                 if errmsg:
                     logkws = self.logformatter.download_error(
-                        download_failure, request, spider, errmsg)
+                        download_failure, request, spider, errmsg
+                    )
                     logger.log(
                         *logformatter_adapter(logkws),
-                        extra={'spider': spider},
+                        extra={"spider": spider},
                     )
 
         if spider_failure is not download_failure:
             return spider_failure
         return None
 
-    def _itemproc_finished(self, output: Any, item: Any, response: Response, spider: Spider) -> None:
-        """ItemProcessor finished for the given ``item`` and returned ``output``
-        """
+    def _itemproc_finished(
+        self, output: Any, item: Any, response: Response, spider: Spider
+    ) -> None:
+        """ItemProcessor finished for the given ``item`` and returned ``output``"""
         assert self.slot is not None  # typing
         self.slot.itemproc_size -= 1
         if isinstance(output, Failure):
             ex = output.value
             if isinstance(ex, DropItem):
                 logkws = self.logformatter.dropped(item, ex, response, spider)
                 if logkws is not None:
-                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
+                    logger.log(*logformatter_adapter(logkws), extra={"spider": spider})
                 return self.signals.send_catch_log_deferred(
-                    signal=signals.item_dropped, item=item, response=response,
-                    spider=spider, exception=output.value)
-            else:
-                logkws = self.logformatter.item_error(item, ex, response, spider)
-                logger.log(*logformatter_adapter(logkws), extra={'spider': spider},
-                           exc_info=failure_to_exc_info(output))
-                return self.signals.send_catch_log_deferred(
-                    signal=signals.item_error, item=item, response=response,
-                    spider=spider, failure=output)
-        else:
-            logkws = self.logformatter.scraped(output, response, spider)
-            if logkws is not None:
-                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
+                    signal=signals.item_dropped,
+                    item=item,
+                    response=response,
+                    spider=spider,
+                    exception=output.value,
+                )
+            logkws = self.logformatter.item_error(item, ex, response, spider)
+            logger.log(
+                *logformatter_adapter(logkws),
+                extra={"spider": spider},
+                exc_info=failure_to_exc_info(output),
+            )
             return self.signals.send_catch_log_deferred(
-                signal=signals.item_scraped, item=output, response=response,
-                spider=spider)
+                signal=signals.item_error,
+                item=item,
+                response=response,
+                spider=spider,
+                failure=output,
+            )
+        logkws = self.logformatter.scraped(output, response, spider)
+        if logkws is not None:
+            logger.log(*logformatter_adapter(logkws), extra={"spider": spider})
+        return self.signals.send_catch_log_deferred(
+            signal=signals.item_scraped, item=output, response=response, spider=spider
+        )
```

### Comparing `Scrapy-2.7.1/scrapy/core/spidermw.py` & `Scrapy-2.8.0/scrapy/core/spidermw.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,149 +2,190 @@
 Spider Middleware manager
 
 See documentation in docs/topics/spider-middleware.rst
 """
 import logging
 from inspect import isasyncgenfunction, iscoroutine
 from itertools import islice
-from typing import Any, AsyncGenerator, AsyncIterable, Callable, Generator, Iterable, Tuple, Union, cast
+from typing import (
+    Any,
+    AsyncGenerator,
+    AsyncIterable,
+    Callable,
+    Generator,
+    Iterable,
+    Tuple,
+    Union,
+    cast,
+)
 
 from twisted.internet.defer import Deferred, inlineCallbacks
 from twisted.python.failure import Failure
 
 from scrapy import Request, Spider
 from scrapy.exceptions import _InvalidOutput
 from scrapy.http import Response
 from scrapy.middleware import MiddlewareManager
 from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen
 from scrapy.utils.conf import build_component_list
-from scrapy.utils.defer import mustbe_deferred, deferred_from_coro, deferred_f_from_coro_f, maybe_deferred_to_future
+from scrapy.utils.defer import (
+    deferred_f_from_coro_f,
+    deferred_from_coro,
+    maybe_deferred_to_future,
+    mustbe_deferred,
+)
 from scrapy.utils.python import MutableAsyncChain, MutableChain
 
-
 logger = logging.getLogger(__name__)
 
 
 ScrapeFunc = Callable[[Union[Response, Failure], Request, Spider], Any]
 
 
 def _isiterable(o) -> bool:
     return isinstance(o, (Iterable, AsyncIterable))
 
 
 class SpiderMiddlewareManager(MiddlewareManager):
 
-    component_name = 'spider middleware'
+    component_name = "spider middleware"
 
     def __init__(self, *middlewares):
         super().__init__(*middlewares)
         self.downgrade_warning_done = False
 
     @classmethod
     def _get_mwlist_from_settings(cls, settings):
-        return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))
+        return build_component_list(settings.getwithbase("SPIDER_MIDDLEWARES"))
 
     def _add_middleware(self, mw):
         super()._add_middleware(mw)
-        if hasattr(mw, 'process_spider_input'):
-            self.methods['process_spider_input'].append(mw.process_spider_input)
-        if hasattr(mw, 'process_start_requests'):
-            self.methods['process_start_requests'].appendleft(mw.process_start_requests)
-        process_spider_output = self._get_async_method_pair(mw, 'process_spider_output')
-        self.methods['process_spider_output'].appendleft(process_spider_output)
-        process_spider_exception = getattr(mw, 'process_spider_exception', None)
-        self.methods['process_spider_exception'].appendleft(process_spider_exception)
-
-    def _process_spider_input(self, scrape_func: ScrapeFunc, response: Response, request: Request,
-                              spider: Spider) -> Any:
-        for method in self.methods['process_spider_input']:
+        if hasattr(mw, "process_spider_input"):
+            self.methods["process_spider_input"].append(mw.process_spider_input)
+        if hasattr(mw, "process_start_requests"):
+            self.methods["process_start_requests"].appendleft(mw.process_start_requests)
+        process_spider_output = self._get_async_method_pair(mw, "process_spider_output")
+        self.methods["process_spider_output"].appendleft(process_spider_output)
+        process_spider_exception = getattr(mw, "process_spider_exception", None)
+        self.methods["process_spider_exception"].appendleft(process_spider_exception)
+
+    def _process_spider_input(
+        self,
+        scrape_func: ScrapeFunc,
+        response: Response,
+        request: Request,
+        spider: Spider,
+    ) -> Any:
+        for method in self.methods["process_spider_input"]:
             method = cast(Callable, method)
             try:
                 result = method(response=response, spider=spider)
                 if result is not None:
-                    msg = (f"{method.__qualname__} must return None "
-                           f"or raise an exception, got {type(result)}")
+                    msg = (
+                        f"{method.__qualname__} must return None "
+                        f"or raise an exception, got {type(result)}"
+                    )
                     raise _InvalidOutput(msg)
             except _InvalidOutput:
                 raise
             except Exception:
                 return scrape_func(Failure(), request, spider)
         return scrape_func(response, request, spider)
 
-    def _evaluate_iterable(self, response: Response, spider: Spider, iterable: Union[Iterable, AsyncIterable],
-                           exception_processor_index: int, recover_to: Union[MutableChain, MutableAsyncChain]
-                           ) -> Union[Generator, AsyncGenerator]:
-
+    def _evaluate_iterable(
+        self,
+        response: Response,
+        spider: Spider,
+        iterable: Union[Iterable, AsyncIterable],
+        exception_processor_index: int,
+        recover_to: Union[MutableChain, MutableAsyncChain],
+    ) -> Union[Generator, AsyncGenerator]:
         def process_sync(iterable: Iterable):
             try:
                 for r in iterable:
                     yield r
             except Exception as ex:
-                exception_result = self._process_spider_exception(response, spider, Failure(ex),
-                                                                  exception_processor_index)
+                exception_result = self._process_spider_exception(
+                    response, spider, Failure(ex), exception_processor_index
+                )
                 if isinstance(exception_result, Failure):
                     raise
                 recover_to.extend(exception_result)
 
         async def process_async(iterable: AsyncIterable):
             try:
                 async for r in iterable:
                     yield r
             except Exception as ex:
-                exception_result = self._process_spider_exception(response, spider, Failure(ex),
-                                                                  exception_processor_index)
+                exception_result = self._process_spider_exception(
+                    response, spider, Failure(ex), exception_processor_index
+                )
                 if isinstance(exception_result, Failure):
                     raise
                 recover_to.extend(exception_result)
 
         if isinstance(iterable, AsyncIterable):
             return process_async(iterable)
         return process_sync(iterable)
 
-    def _process_spider_exception(self, response: Response, spider: Spider, _failure: Failure,
-                                  start_index: int = 0) -> Union[Failure, MutableChain]:
+    def _process_spider_exception(
+        self,
+        response: Response,
+        spider: Spider,
+        _failure: Failure,
+        start_index: int = 0,
+    ) -> Union[Failure, MutableChain]:
         exception = _failure.value
         # don't handle _InvalidOutput exception
         if isinstance(exception, _InvalidOutput):
             return _failure
-        method_list = islice(self.methods['process_spider_exception'], start_index, None)
+        method_list = islice(
+            self.methods["process_spider_exception"], start_index, None
+        )
         for method_index, method in enumerate(method_list, start=start_index):
             if method is None:
                 continue
             method = cast(Callable, method)
             result = method(response=response, exception=exception, spider=spider)
             if _isiterable(result):
                 # stop exception handling by handing control over to the
                 # process_spider_output chain if an iterable has been returned
-                dfd: Deferred = self._process_spider_output(response, spider, result, method_index + 1)
+                dfd: Deferred = self._process_spider_output(
+                    response, spider, result, method_index + 1
+                )
                 # _process_spider_output() returns a Deferred only because of downgrading so this can be
                 # simplified when downgrading is removed.
                 if dfd.called:
                     # the result is available immediately if _process_spider_output didn't do downgrading
                     return dfd.result
-                else:
-                    # we forbid waiting here because otherwise we would need to return a deferred from
-                    # _process_spider_exception too, which complicates the architecture
-                    msg = f"Async iterable returned from {method.__qualname__} cannot be downgraded"
-                    raise _InvalidOutput(msg)
+                # we forbid waiting here because otherwise we would need to return a deferred from
+                # _process_spider_exception too, which complicates the architecture
+                msg = f"Async iterable returned from {method.__qualname__} cannot be downgraded"
+                raise _InvalidOutput(msg)
             elif result is None:
                 continue
             else:
-                msg = (f"{method.__qualname__} must return None "
-                       f"or an iterable, got {type(result)}")
+                msg = (
+                    f"{method.__qualname__} must return None "
+                    f"or an iterable, got {type(result)}"
+                )
                 raise _InvalidOutput(msg)
         return _failure
 
     # This method cannot be made async def, as _process_spider_exception relies on the Deferred result
     # being available immediately which doesn't work when it's a wrapped coroutine.
     # It also needs @inlineCallbacks only because of downgrading so it can be removed when downgrading is removed.
     @inlineCallbacks
-    def _process_spider_output(self, response: Response, spider: Spider,
-                               result: Union[Iterable, AsyncIterable], start_index: int = 0
-                               ) -> Deferred:
+    def _process_spider_output(
+        self,
+        response: Response,
+        spider: Spider,
+        result: Union[Iterable, AsyncIterable],
+        start_index: int = 0,
+    ) -> Deferred:
         # items in this iterable do not need to go through the process_spider_output
         # chain, they went through it already from the process_spider_exception method
         recovered: Union[MutableChain, MutableAsyncChain]
         last_result_is_async = isinstance(result, AsyncIterable)
         if last_result_is_async:
             recovered = MutableAsyncChain()
         else:
@@ -153,15 +194,15 @@
         # There are three cases for the middleware: def foo, async def foo, def foo + async def foo_async.
         # 1. def foo. Sync iterables are passed as is, async ones are downgraded.
         # 2. async def foo. Sync iterables are upgraded, async ones are passed as is.
         # 3. def foo + async def foo_async. Iterables are passed to the respective method.
         # Storing methods and method tuples in the same list is weird but we should be able to roll this back
         # when we drop this compatibility feature.
 
-        method_list = islice(self.methods['process_spider_output'], start_index, None)
+        method_list = islice(self.methods["process_spider_output"], start_index, None)
         for method_index, method_pair in enumerate(method_list, start=start_index):
             if method_pair is None:
                 continue
             need_upgrade = need_downgrade = False
             if isinstance(method_pair, tuple):
                 # This tuple handling is only needed until _async compatibility methods are removed.
                 method_sync, method_async = method_pair
@@ -174,32 +215,40 @@
                     need_downgrade = True
             try:
                 if need_upgrade:
                     # Iterable -> AsyncIterable
                     result = as_async_generator(result)
                 elif need_downgrade:
                     if not self.downgrade_warning_done:
-                        logger.warning(f"Async iterable passed to {method.__qualname__} "
-                                       f"was downgraded to a non-async one")
+                        logger.warning(
+                            f"Async iterable passed to {method.__qualname__} "
+                            f"was downgraded to a non-async one"
+                        )
                         self.downgrade_warning_done = True
                     assert isinstance(result, AsyncIterable)
                     # AsyncIterable -> Iterable
                     result = yield deferred_from_coro(collect_asyncgen(result))
                     if isinstance(recovered, AsyncIterable):
-                        recovered_collected = yield deferred_from_coro(collect_asyncgen(recovered))
+                        recovered_collected = yield deferred_from_coro(
+                            collect_asyncgen(recovered)
+                        )
                         recovered = MutableChain(recovered_collected)
                 # might fail directly if the output value is not a generator
                 result = method(response=response, result=result, spider=spider)
             except Exception as ex:
-                exception_result = self._process_spider_exception(response, spider, Failure(ex), method_index + 1)
+                exception_result = self._process_spider_exception(
+                    response, spider, Failure(ex), method_index + 1
+                )
                 if isinstance(exception_result, Failure):
                     raise
                 return exception_result
             if _isiterable(result):
-                result = self._evaluate_iterable(response, spider, result, method_index + 1, recovered)
+                result = self._evaluate_iterable(
+                    response, spider, result, method_index + 1, recovered
+                )
             else:
                 if iscoroutine(result):
                     result.close()  # Silence warning about not awaiting
                     msg = (
                         f"{method.__qualname__} must be an asynchronous "
                         f"generator (i.e. use yield)"
                     )
@@ -209,65 +258,85 @@
                         f"{type(result)}"
                     )
                 raise _InvalidOutput(msg)
             last_result_is_async = isinstance(result, AsyncIterable)
 
         if last_result_is_async:
             return MutableAsyncChain(result, recovered)
-        else:
-            return MutableChain(result, recovered)  # type: ignore[arg-type]
+        return MutableChain(result, recovered)  # type: ignore[arg-type]
 
-    async def _process_callback_output(self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]
-                                       ) -> Union[MutableChain, MutableAsyncChain]:
+    async def _process_callback_output(
+        self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]
+    ) -> Union[MutableChain, MutableAsyncChain]:
         recovered: Union[MutableChain, MutableAsyncChain]
         if isinstance(result, AsyncIterable):
             recovered = MutableAsyncChain()
         else:
             recovered = MutableChain()
         result = self._evaluate_iterable(response, spider, result, 0, recovered)
-        result = await maybe_deferred_to_future(self._process_spider_output(response, spider, result))
+        result = await maybe_deferred_to_future(
+            self._process_spider_output(response, spider, result)
+        )
         if isinstance(result, AsyncIterable):
             return MutableAsyncChain(result, recovered)
-        else:
-            if isinstance(recovered, AsyncIterable):
-                recovered_collected = await collect_asyncgen(recovered)
-                recovered = MutableChain(recovered_collected)
-            return MutableChain(result, recovered)  # type: ignore[arg-type]
-
-    def scrape_response(self, scrape_func: ScrapeFunc, response: Response, request: Request,
-                        spider: Spider) -> Deferred:
-        async def process_callback_output(result: Union[Iterable, AsyncIterable]
-                                          ) -> Union[MutableChain, MutableAsyncChain]:
+        if isinstance(recovered, AsyncIterable):
+            recovered_collected = await collect_asyncgen(recovered)
+            recovered = MutableChain(recovered_collected)
+        return MutableChain(result, recovered)  # type: ignore[arg-type]
+
+    def scrape_response(
+        self,
+        scrape_func: ScrapeFunc,
+        response: Response,
+        request: Request,
+        spider: Spider,
+    ) -> Deferred:
+        async def process_callback_output(
+            result: Union[Iterable, AsyncIterable]
+        ) -> Union[MutableChain, MutableAsyncChain]:
             return await self._process_callback_output(response, spider, result)
 
         def process_spider_exception(_failure: Failure) -> Union[Failure, MutableChain]:
             return self._process_spider_exception(response, spider, _failure)
 
-        dfd = mustbe_deferred(self._process_spider_input, scrape_func, response, request, spider)
-        dfd.addCallbacks(callback=deferred_f_from_coro_f(process_callback_output), errback=process_spider_exception)
+        dfd = mustbe_deferred(
+            self._process_spider_input, scrape_func, response, request, spider
+        )
+        dfd.addCallbacks(
+            callback=deferred_f_from_coro_f(process_callback_output),
+            errback=process_spider_exception,
+        )
         return dfd
 
     def process_start_requests(self, start_requests, spider: Spider) -> Deferred:
-        return self._process_chain('process_start_requests', start_requests, spider)
+        return self._process_chain("process_start_requests", start_requests, spider)
 
     # This method is only needed until _async compatibility methods are removed.
     @staticmethod
-    def _get_async_method_pair(mw: Any, methodname: str) -> Union[None, Callable, Tuple[Callable, Callable]]:
+    def _get_async_method_pair(
+        mw: Any, methodname: str
+    ) -> Union[None, Callable, Tuple[Callable, Callable]]:
         normal_method = getattr(mw, methodname, None)
         methodname_async = methodname + "_async"
         async_method = getattr(mw, methodname_async, None)
         if not async_method:
             return normal_method
         if not normal_method:
-            logger.error(f"Middleware {mw.__qualname__} has {methodname_async} "
-                         f"without {methodname}, skipping this method.")
+            logger.error(
+                f"Middleware {mw.__qualname__} has {methodname_async} "
+                f"without {methodname}, skipping this method."
+            )
             return None
         if not isasyncgenfunction(async_method):
-            logger.error(f"{async_method.__qualname__} is not "
-                         f"an async generator function, skipping this method.")
+            logger.error(
+                f"{async_method.__qualname__} is not "
+                f"an async generator function, skipping this method."
+            )
             return normal_method
         if isasyncgenfunction(normal_method):
-            logger.error(f"{normal_method.__qualname__} is an async "
-                         f"generator function while {methodname_async} exists, "
-                         f"skipping both methods.")
+            logger.error(
+                f"{normal_method.__qualname__} is an async "
+                f"generator function while {methodname_async} exists, "
+                f"skipping both methods."
+            )
             return None
         return normal_method, async_method
```

### Comparing `Scrapy-2.7.1/scrapy/crawler.py` & `Scrapy-2.8.0/scrapy/crawler.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,88 +1,94 @@
+from __future__ import annotations
+
 import logging
 import pprint
 import signal
 import warnings
+from typing import TYPE_CHECKING, Optional
 
 from twisted.internet import defer
 from zope.interface.exceptions import DoesNotImplement
 
 try:
     # zope >= 5.0 only supports MultipleInvalid
     from zope.interface.exceptions import MultipleInvalid
 except ImportError:
     MultipleInvalid = None
 
 from zope.interface.verify import verifyClass
 
-from scrapy import signals, Spider
+from scrapy import Spider, signals
 from scrapy.core.engine import ExecutionEngine
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.extension import ExtensionManager
 from scrapy.interfaces import ISpiderLoader
-from scrapy.settings import overridden_settings, Settings
+from scrapy.settings import Settings, overridden_settings
 from scrapy.signalmanager import SignalManager
 from scrapy.utils.log import (
+    LogCounterHandler,
     configure_logging,
     get_scrapy_root_handler,
     install_scrapy_root_handler,
     log_reactor_info,
     log_scrapy_info,
-    LogCounterHandler,
 )
 from scrapy.utils.misc import create_instance, load_object
 from scrapy.utils.ossignal import install_shutdown_handlers, signal_names
 from scrapy.utils.reactor import (
     install_reactor,
     is_asyncio_reactor_installed,
     verify_installed_asyncio_event_loop,
     verify_installed_reactor,
 )
 
+if TYPE_CHECKING:
+    from scrapy.utils.request import RequestFingerprinter
+
 
 logger = logging.getLogger(__name__)
 
 
 class Crawler:
-
     def __init__(self, spidercls, settings=None, init_reactor: bool = False):
         if isinstance(spidercls, Spider):
-            raise ValueError('The spidercls argument must be a class, not an object')
+            raise ValueError("The spidercls argument must be a class, not an object")
 
         if isinstance(settings, dict) or settings is None:
             settings = Settings(settings)
 
         self.spidercls = spidercls
         self.settings = settings.copy()
         self.spidercls.update_settings(self.settings)
 
         self.signals = SignalManager(self)
 
-        self.stats = load_object(self.settings['STATS_CLASS'])(self)
+        self.stats = load_object(self.settings["STATS_CLASS"])(self)
 
-        handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))
+        handler = LogCounterHandler(self, level=self.settings.get("LOG_LEVEL"))
         logging.root.addHandler(handler)
 
         d = dict(overridden_settings(self.settings))
-        logger.info("Overridden settings:\n%(settings)s",
-                    {'settings': pprint.pformat(d)})
+        logger.info(
+            "Overridden settings:\n%(settings)s", {"settings": pprint.pformat(d)}
+        )
 
         if get_scrapy_root_handler() is not None:
             # scrapy root handler already installed: update it with new settings
             install_scrapy_root_handler(self.settings)
         # lambda is assigned to Crawler attribute because this way it is not
         # garbage collected after leaving __init__ scope
         self.__remove_handler = lambda: logging.root.removeHandler(handler)
         self.signals.connect(self.__remove_handler, signals.engine_stopped)
 
-        lf_cls = load_object(self.settings['LOG_FORMATTER'])
+        lf_cls = load_object(self.settings["LOG_FORMATTER"])
         self.logformatter = lf_cls.from_crawler(self)
 
-        self.request_fingerprinter = create_instance(
-            load_object(self.settings['REQUEST_FINGERPRINTER_CLASS']),
+        self.request_fingerprinter: RequestFingerprinter = create_instance(
+            load_object(self.settings["REQUEST_FINGERPRINTER_CLASS"]),
             settings=self.settings,
             crawler=self,
         )
 
         reactor_class = self.settings["TWISTED_REACTOR"]
         event_loop = self.settings["ASYNCIO_EVENT_LOOP"]
         if init_reactor:
@@ -99,15 +105,15 @@
                 verify_installed_asyncio_event_loop(event_loop)
 
         self.extensions = ExtensionManager.from_crawler(self)
 
         self.settings.freeze()
         self.crawling = False
         self.spider = None
-        self.engine = None
+        self.engine: Optional[ExecutionEngine] = None
 
     @defer.inlineCallbacks
     def crawl(self, *args, **kwargs):
         if self.crawling:
             raise RuntimeError("Crawling already taking place")
         self.crawling = True
 
@@ -150,48 +156,54 @@
     accordingly) unless writing scripts that manually handle the crawling
     process. See :ref:`run-from-script` for an example.
     """
 
     crawlers = property(
         lambda self: self._crawlers,
         doc="Set of :class:`crawlers <scrapy.crawler.Crawler>` started by "
-            ":meth:`crawl` and managed by this class."
+        ":meth:`crawl` and managed by this class.",
     )
 
     @staticmethod
     def _get_spider_loader(settings):
-        """ Get SpiderLoader instance from settings """
-        cls_path = settings.get('SPIDER_LOADER_CLASS')
+        """Get SpiderLoader instance from settings"""
+        cls_path = settings.get("SPIDER_LOADER_CLASS")
         loader_cls = load_object(cls_path)
-        excs = (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement
+        excs = (
+            (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement
+        )
         try:
             verifyClass(ISpiderLoader, loader_cls)
         except excs:
             warnings.warn(
-                'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '
-                'not fully implement scrapy.interfaces.ISpiderLoader interface. '
-                'Please add all missing methods to avoid unexpected runtime errors.',
-                category=ScrapyDeprecationWarning, stacklevel=2
+                "SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does "
+                "not fully implement scrapy.interfaces.ISpiderLoader interface. "
+                "Please add all missing methods to avoid unexpected runtime errors.",
+                category=ScrapyDeprecationWarning,
+                stacklevel=2,
             )
         return loader_cls.from_settings(settings.frozencopy())
 
     def __init__(self, settings=None):
         if isinstance(settings, dict) or settings is None:
             settings = Settings(settings)
         self.settings = settings
         self.spider_loader = self._get_spider_loader(settings)
         self._crawlers = set()
         self._active = set()
         self.bootstrap_failed = False
 
     @property
     def spiders(self):
-        warnings.warn("CrawlerRunner.spiders attribute is renamed to "
-                      "CrawlerRunner.spider_loader.",
-                      category=ScrapyDeprecationWarning, stacklevel=2)
+        warnings.warn(
+            "CrawlerRunner.spiders attribute is renamed to "
+            "CrawlerRunner.spider_loader.",
+            category=ScrapyDeprecationWarning,
+            stacklevel=2,
+        )
         return self.spider_loader
 
     def crawl(self, crawler_or_spidercls, *args, **kwargs):
         """
         Run a crawler with the provided arguments.
 
         It will call the given Crawler's :meth:`~Crawler.crawl` method, while
@@ -210,28 +222,29 @@
 
         :param args: arguments to initialize the spider
 
         :param kwargs: keyword arguments to initialize the spider
         """
         if isinstance(crawler_or_spidercls, Spider):
             raise ValueError(
-                'The crawler_or_spidercls argument cannot be a spider object, '
-                'it must be a spider class (or a Crawler object)')
+                "The crawler_or_spidercls argument cannot be a spider object, "
+                "it must be a spider class (or a Crawler object)"
+            )
         crawler = self.create_crawler(crawler_or_spidercls)
         return self._crawl(crawler, *args, **kwargs)
 
     def _crawl(self, crawler, *args, **kwargs):
         self.crawlers.add(crawler)
         d = crawler.crawl(*args, **kwargs)
         self._active.add(d)
 
         def _done(result):
             self.crawlers.discard(crawler)
             self._active.discard(d)
-            self.bootstrap_failed |= not getattr(crawler, 'spider', None)
+            self.bootstrap_failed |= not getattr(crawler, "spider", None)
             return result
 
         return d.addBoth(_done)
 
     def create_crawler(self, crawler_or_spidercls):
         """
         Return a :class:`~scrapy.crawler.Crawler` object.
@@ -241,16 +254,17 @@
           is constructed for it.
         * If ``crawler_or_spidercls`` is a string, this function finds
           a spider with this name in a Scrapy project (using spider loader),
           then creates a Crawler instance for it.
         """
         if isinstance(crawler_or_spidercls, Spider):
             raise ValueError(
-                'The crawler_or_spidercls argument cannot be a spider object, '
-                'it must be a spider class (or a Crawler object)')
+                "The crawler_or_spidercls argument cannot be a spider object, "
+                "it must be a spider class (or a Crawler object)"
+            )
         if isinstance(crawler_or_spidercls, Crawler):
             return crawler_or_spidercls
         return self._create_crawler(crawler_or_spidercls)
 
     def _create_crawler(self, spidercls):
         if isinstance(spidercls, str):
             spidercls = self.spider_loader.load(spidercls)
@@ -304,26 +318,31 @@
         super().__init__(settings)
         configure_logging(self.settings, install_root_handler)
         log_scrapy_info(self.settings)
         self._initialized_reactor = False
 
     def _signal_shutdown(self, signum, _):
         from twisted.internet import reactor
+
         install_shutdown_handlers(self._signal_kill)
         signame = signal_names[signum]
-        logger.info("Received %(signame)s, shutting down gracefully. Send again to force ",
-                    {'signame': signame})
+        logger.info(
+            "Received %(signame)s, shutting down gracefully. Send again to force ",
+            {"signame": signame},
+        )
         reactor.callFromThread(self._graceful_stop_reactor)
 
     def _signal_kill(self, signum, _):
         from twisted.internet import reactor
+
         install_shutdown_handlers(signal.SIG_IGN)
         signame = signal_names[signum]
-        logger.info('Received %(signame)s twice, forcing unclean shutdown',
-                    {'signame': signame})
+        logger.info(
+            "Received %(signame)s twice, forcing unclean shutdown", {"signame": signame}
+        )
         reactor.callFromThread(self._stop_reactor)
 
     def _create_crawler(self, spidercls):
         if isinstance(spidercls, str):
             spidercls = self.spider_loader.load(spidercls)
         init_reactor = not self._initialized_reactor
         self._initialized_reactor = True
@@ -341,35 +360,37 @@
         :param bool stop_after_crawl: stop or not the reactor when all
             crawlers have finished
 
         :param bool install_signal_handlers: whether to install the shutdown
             handlers (default: True)
         """
         from twisted.internet import reactor
+
         if stop_after_crawl:
             d = self.join()
             # Don't start the reactor if the deferreds are already fired
             if d.called:
                 return
             d.addBoth(self._stop_reactor)
 
         if install_signal_handlers:
             install_shutdown_handlers(self._signal_shutdown)
         resolver_class = load_object(self.settings["DNS_RESOLVER"])
         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)
         resolver.install_on_reactor()
         tp = reactor.getThreadPool()
-        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))
-        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)
+        tp.adjustPoolsize(maxthreads=self.settings.getint("REACTOR_THREADPOOL_MAXSIZE"))
+        reactor.addSystemEventTrigger("before", "shutdown", self.stop)
         reactor.run(installSignalHandlers=False)  # blocking call
 
     def _graceful_stop_reactor(self):
         d = self.stop()
         d.addBoth(self._stop_reactor)
         return d
 
     def _stop_reactor(self, _=None):
         from twisted.internet import reactor
+
         try:
             reactor.stop()
         except RuntimeError:  # raised if already stopped or in shutdown stage
             pass
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/ajaxcrawl.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/ajaxcrawl.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,74 +1,77 @@
-import re
 import logging
+import re
 
 from w3lib import html
 
 from scrapy.exceptions import NotConfigured
 from scrapy.http import HtmlResponse
 
-
 logger = logging.getLogger(__name__)
 
 
 class AjaxCrawlMiddleware:
     """
     Handle 'AJAX crawlable' pages marked as crawlable via meta tag.
     For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.
     """
 
     def __init__(self, settings):
-        if not settings.getbool('AJAXCRAWL_ENABLED'):
+        if not settings.getbool("AJAXCRAWL_ENABLED"):
             raise NotConfigured
 
         # XXX: Google parses at least first 100k bytes; scrapy's redirect
         # middleware parses first 4k. 4k turns out to be insufficient
         # for this middleware, and parsing 100k could be slow.
         # We use something in between (32K) by default.
-        self.lookup_bytes = settings.getint('AJAXCRAWL_MAXSIZE', 32768)
+        self.lookup_bytes = settings.getint("AJAXCRAWL_MAXSIZE", 32768)
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler.settings)
 
     def process_response(self, request, response, spider):
 
         if not isinstance(response, HtmlResponse) or response.status != 200:
             return response
 
-        if request.method != 'GET':
+        if request.method != "GET":
             # other HTTP methods are either not safe or don't have a body
             return response
 
-        if 'ajax_crawlable' in request.meta:  # prevent loops
+        if "ajax_crawlable" in request.meta:  # prevent loops
             return response
 
         if not self._has_ajax_crawlable_variant(response):
             return response
 
         # scrapy already handles #! links properly
-        ajax_crawl_request = request.replace(url=request.url + '#!')
-        logger.debug("Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s",
-                     {'ajax_crawl_request': ajax_crawl_request, 'request': request},
-                     extra={'spider': spider})
+        ajax_crawl_request = request.replace(url=request.url + "#!")
+        logger.debug(
+            "Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s",
+            {"ajax_crawl_request": ajax_crawl_request, "request": request},
+            extra={"spider": spider},
+        )
 
-        ajax_crawl_request.meta['ajax_crawlable'] = True
+        ajax_crawl_request.meta["ajax_crawlable"] = True
         return ajax_crawl_request
 
     def _has_ajax_crawlable_variant(self, response):
         """
         Return True if a page without hash fragment could be "AJAX crawlable"
         according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.
         """
-        body = response.text[:self.lookup_bytes]
+        body = response.text[: self.lookup_bytes]
         return _has_ajaxcrawlable_meta(body)
 
 
 # XXX: move it to w3lib?
-_ajax_crawlable_re = re.compile(r'<meta\s+name=["\']fragment["\']\s+content=["\']!["\']/?>')
+_ajax_crawlable_re = re.compile(
+    r'<meta\s+name=["\']fragment["\']\s+content=["\']!["\']/?>'
+)
 
 
 def _has_ajaxcrawlable_meta(text):
     """
     >>> _has_ajaxcrawlable_meta('<html><head><meta name="fragment"  content="!"/></head><body></body></html>')
     True
     >>> _has_ajaxcrawlable_meta("<html><head><meta name='fragment' content='!'></head></html>")
@@ -78,16 +81,16 @@
     >>> _has_ajaxcrawlable_meta('<html></html>')
     False
     """
 
     # Stripping scripts and comments is slow (about 20x slower than
     # just checking if a string is in text); this is a quick fail-fast
     # path that should work for most pages.
-    if 'fragment' not in text:
+    if "fragment" not in text:
         return False
-    if 'content' not in text:
+    if "content" not in text:
         return False
 
-    text = html.remove_tags_with_content(text, ('script', 'noscript'))
+    text = html.remove_tags_with_content(text, ("script", "noscript"))
     text = html.replace_entities(text)
     text = html.remove_comments(text)
     return _ajax_crawlable_re.search(text) is not None
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/cookies.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/cookies.py`

 * *Files 8% similar despite different names*

```diff
@@ -5,15 +5,14 @@
 
 from scrapy.exceptions import NotConfigured
 from scrapy.http import Response
 from scrapy.http.cookies import CookieJar
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.python import to_unicode
 
-
 logger = logging.getLogger(__name__)
 
 
 _split_domain = TLDExtract(include_psl_private_domains=True)
 
 
 def _is_public_domain(domain):
@@ -26,78 +25,82 @@
 
     def __init__(self, debug=False):
         self.jars = defaultdict(CookieJar)
         self.debug = debug
 
     @classmethod
     def from_crawler(cls, crawler):
-        if not crawler.settings.getbool('COOKIES_ENABLED'):
+        if not crawler.settings.getbool("COOKIES_ENABLED"):
             raise NotConfigured
-        return cls(crawler.settings.getbool('COOKIES_DEBUG'))
+        return cls(crawler.settings.getbool("COOKIES_DEBUG"))
 
     def _process_cookies(self, cookies, *, jar, request):
         for cookie in cookies:
             cookie_domain = cookie.domain
-            if cookie_domain.startswith('.'):
+            if cookie_domain.startswith("."):
                 cookie_domain = cookie_domain[1:]
 
             request_domain = urlparse_cached(request).hostname.lower()
 
             if cookie_domain and _is_public_domain(cookie_domain):
                 if cookie_domain != request_domain:
                     continue
                 cookie.domain = request_domain
 
             jar.set_cookie_if_ok(cookie, request)
 
     def process_request(self, request, spider):
-        if request.meta.get('dont_merge_cookies', False):
+        if request.meta.get("dont_merge_cookies", False):
             return
 
         cookiejarkey = request.meta.get("cookiejar")
         jar = self.jars[cookiejarkey]
         cookies = self._get_request_cookies(jar, request)
         self._process_cookies(cookies, jar=jar, request=request)
 
         # set Cookie header
-        request.headers.pop('Cookie', None)
+        request.headers.pop("Cookie", None)
         jar.add_cookie_header(request)
         self._debug_cookie(request, spider)
 
     def process_response(self, request, response, spider):
-        if request.meta.get('dont_merge_cookies', False):
+        if request.meta.get("dont_merge_cookies", False):
             return response
 
         # extract cookies from Set-Cookie and drop invalid/expired cookies
         cookiejarkey = request.meta.get("cookiejar")
         jar = self.jars[cookiejarkey]
         cookies = jar.make_cookies(response, request)
         self._process_cookies(cookies, jar=jar, request=request)
 
         self._debug_set_cookie(response, spider)
 
         return response
 
     def _debug_cookie(self, request, spider):
         if self.debug:
-            cl = [to_unicode(c, errors='replace')
-                  for c in request.headers.getlist('Cookie')]
+            cl = [
+                to_unicode(c, errors="replace")
+                for c in request.headers.getlist("Cookie")
+            ]
             if cl:
                 cookies = "\n".join(f"Cookie: {c}\n" for c in cl)
                 msg = f"Sending cookies to: {request}\n{cookies}"
-                logger.debug(msg, extra={'spider': spider})
+                logger.debug(msg, extra={"spider": spider})
 
     def _debug_set_cookie(self, response, spider):
         if self.debug:
-            cl = [to_unicode(c, errors='replace')
-                  for c in response.headers.getlist('Set-Cookie')]
+            cl = [
+                to_unicode(c, errors="replace")
+                for c in response.headers.getlist("Set-Cookie")
+            ]
             if cl:
                 cookies = "\n".join(f"Set-Cookie: {c}\n" for c in cl)
                 msg = f"Received cookies from: {response}\n{cookies}"
-                logger.debug(msg, extra={'spider': spider})
+                logger.debug(msg, extra={"spider": spider})
 
     def _format_cookie(self, cookie, request):
         """
         Given a dict consisting of cookie components, return its string representation.
         Decode from bytes if necessary.
         """
         decoded = {}
@@ -110,29 +113,32 @@
                 continue
             if isinstance(cookie[key], (bool, float, int, str)):
                 decoded[key] = str(cookie[key])
             else:
                 try:
                     decoded[key] = cookie[key].decode("utf8")
                 except UnicodeDecodeError:
-                    logger.warning("Non UTF-8 encoded cookie found in request %s: %s",
-                                   request, cookie)
+                    logger.warning(
+                        "Non UTF-8 encoded cookie found in request %s: %s",
+                        request,
+                        cookie,
+                    )
                     decoded[key] = cookie[key].decode("latin1", errors="replace")
 
         cookie_str = f"{decoded.pop('name')}={decoded.pop('value')}"
         for key, value in decoded.items():  # path, domain
             cookie_str += f"; {key.capitalize()}={value}"
         return cookie_str
 
     def _get_request_cookies(self, jar, request):
         """
         Extract cookies from the Request.cookies attribute
         """
         if not request.cookies:
             return []
-        elif isinstance(request.cookies, dict):
+        if isinstance(request.cookies, dict):
             cookies = ({"name": k, "value": v} for k, v in request.cookies.items())
         else:
             cookies = request.cookies
         formatted = filter(None, (self._format_cookie(c, request) for c in cookies))
         response = Response(request.url, headers={"Set-Cookie": formatted})
         return jar.make_cookies(response, request)
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/decompression.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/decompression.py`

 * *Files 9% similar despite different names*

```diff
@@ -10,35 +10,34 @@
 from io import BytesIO
 from tempfile import mktemp
 from warnings import warn
 
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.responsetypes import responsetypes
 
-
 warn(
-    'scrapy.downloadermiddlewares.decompression is deprecated',
+    "scrapy.downloadermiddlewares.decompression is deprecated",
     ScrapyDeprecationWarning,
     stacklevel=2,
 )
 
 
 logger = logging.getLogger(__name__)
 
 
 class DecompressionMiddleware:
-    """ This middleware tries to recognise and extract the possibly compressed
-    responses that may arrive. """
+    """This middleware tries to recognise and extract the possibly compressed
+    responses that may arrive."""
 
     def __init__(self):
         self._formats = {
-            'tar': self._is_tar,
-            'zip': self._is_zip,
-            'gz': self._is_gzip,
-            'bz2': self._is_bzip2
+            "tar": self._is_tar,
+            "zip": self._is_zip,
+            "gz": self._is_gzip,
+            "bz2": self._is_bzip2,
         }
 
     def _is_tar(self, response):
         archive = BytesIO(response.body)
         try:
             tar_file = tarfile.open(name=mktemp(), fileobj=archive)
         except tarfile.ReadError:
@@ -82,11 +81,14 @@
     def process_response(self, request, response, spider):
         if not response.body:
             return response
 
         for fmt, func in self._formats.items():
             new_response = func(response)
             if new_response:
-                logger.debug('Decompressed response with format: %(responsefmt)s',
-                             {'responsefmt': fmt}, extra={'spider': spider})
+                logger.debug(
+                    "Decompressed response with format: %(responsefmt)s",
+                    {"responsefmt": fmt},
+                    extra={"spider": spider},
+                )
                 return new_response
         return response
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/defaultheaders.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/defaultheaders.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,19 +4,18 @@
 See documentation in docs/topics/downloader-middleware.rst
 """
 
 from scrapy.utils.python import without_none_values
 
 
 class DefaultHeadersMiddleware:
-
     def __init__(self, headers):
         self._headers = headers
 
     @classmethod
     def from_crawler(cls, crawler):
-        headers = without_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])
+        headers = without_none_values(crawler.settings["DEFAULT_REQUEST_HEADERS"])
         return cls(headers.items())
 
     def process_request(self, request, spider):
         for k, v in self._headers:
             request.headers.setdefault(k, v)
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/downloadtimeout.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/downloadtimeout.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,23 +4,22 @@
 See documentation in docs/topics/downloader-middleware.rst
 """
 
 from scrapy import signals
 
 
 class DownloadTimeoutMiddleware:
-
     def __init__(self, timeout=180):
         self._timeout = timeout
 
     @classmethod
     def from_crawler(cls, crawler):
-        o = cls(crawler.settings.getfloat('DOWNLOAD_TIMEOUT'))
+        o = cls(crawler.settings.getfloat("DOWNLOAD_TIMEOUT"))
         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
         return o
 
     def spider_opened(self, spider):
-        self._timeout = getattr(spider, 'download_timeout', self._timeout)
+        self._timeout = getattr(spider, "download_timeout", self._timeout)
 
     def process_request(self, request, spider):
         if self._timeout:
-            request.meta.setdefault('download_timeout', self._timeout)
+            request.meta.setdefault("download_timeout", self._timeout)
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/httpauth.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/httpauth.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,31 +20,33 @@
     @classmethod
     def from_crawler(cls, crawler):
         o = cls()
         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
         return o
 
     def spider_opened(self, spider):
-        usr = getattr(spider, 'http_user', '')
-        pwd = getattr(spider, 'http_pass', '')
+        usr = getattr(spider, "http_user", "")
+        pwd = getattr(spider, "http_pass", "")
         if usr or pwd:
             self.auth = basic_auth_header(usr, pwd)
-            if not hasattr(spider, 'http_auth_domain'):
-                warnings.warn('Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security '
-                              'problems if the spider makes requests to several different domains. http_auth_domain '
-                              'will be set to the domain of the first request, please set it to the correct value '
-                              'explicitly.',
-                              category=ScrapyDeprecationWarning)
+            if not hasattr(spider, "http_auth_domain"):
+                warnings.warn(
+                    "Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security "
+                    "problems if the spider makes requests to several different domains. http_auth_domain "
+                    "will be set to the domain of the first request, please set it to the correct value "
+                    "explicitly.",
+                    category=ScrapyDeprecationWarning,
+                )
                 self.domain_unset = True
             else:
                 self.domain = spider.http_auth_domain
                 self.domain_unset = False
 
     def process_request(self, request, spider):
-        auth = getattr(self, 'auth', None)
-        if auth and b'Authorization' not in request.headers:
+        auth = getattr(self, "auth", None)
+        if auth and b"Authorization" not in request.headers:
             domain = urlparse_cached(request).hostname
             if self.domain_unset:
                 self.domain = domain
                 self.domain_unset = False
             if not self.domain or url_is_from_any_domain(request.url, [self.domain]):
-                request.headers[b'Authorization'] = auth
+                request.headers[b"Authorization"] = auth
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/httpcache.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/httpcache.py`

 * *Files 8% similar despite different names*

```diff
@@ -19,115 +19,132 @@
 from scrapy.http.request import Request
 from scrapy.http.response import Response
 from scrapy.settings import Settings
 from scrapy.spiders import Spider
 from scrapy.statscollectors import StatsCollector
 from scrapy.utils.misc import load_object
 
-
 HttpCacheMiddlewareTV = TypeVar("HttpCacheMiddlewareTV", bound="HttpCacheMiddleware")
 
 
 class HttpCacheMiddleware:
 
-    DOWNLOAD_EXCEPTIONS = (defer.TimeoutError, TimeoutError, DNSLookupError,
-                           ConnectionRefusedError, ConnectionDone, ConnectError,
-                           ConnectionLost, TCPTimedOutError, ResponseFailed,
-                           IOError)
+    DOWNLOAD_EXCEPTIONS = (
+        defer.TimeoutError,
+        TimeoutError,
+        DNSLookupError,
+        ConnectionRefusedError,
+        ConnectionDone,
+        ConnectError,
+        ConnectionLost,
+        TCPTimedOutError,
+        ResponseFailed,
+        IOError,
+    )
 
     def __init__(self, settings: Settings, stats: StatsCollector) -> None:
-        if not settings.getbool('HTTPCACHE_ENABLED'):
+        if not settings.getbool("HTTPCACHE_ENABLED"):
             raise NotConfigured
-        self.policy = load_object(settings['HTTPCACHE_POLICY'])(settings)
-        self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)
-        self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')
+        self.policy = load_object(settings["HTTPCACHE_POLICY"])(settings)
+        self.storage = load_object(settings["HTTPCACHE_STORAGE"])(settings)
+        self.ignore_missing = settings.getbool("HTTPCACHE_IGNORE_MISSING")
         self.stats = stats
 
     @classmethod
-    def from_crawler(cls: Type[HttpCacheMiddlewareTV], crawler: Crawler) -> HttpCacheMiddlewareTV:
+    def from_crawler(
+        cls: Type[HttpCacheMiddlewareTV], crawler: Crawler
+    ) -> HttpCacheMiddlewareTV:
         o = cls(crawler.settings, crawler.stats)
         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)
         return o
 
     def spider_opened(self, spider: Spider) -> None:
         self.storage.open_spider(spider)
 
     def spider_closed(self, spider: Spider) -> None:
         self.storage.close_spider(spider)
 
     def process_request(self, request: Request, spider: Spider) -> Optional[Response]:
-        if request.meta.get('dont_cache', False):
+        if request.meta.get("dont_cache", False):
             return None
 
         # Skip uncacheable requests
         if not self.policy.should_cache_request(request):
-            request.meta['_dont_cache'] = True  # flag as uncacheable
+            request.meta["_dont_cache"] = True  # flag as uncacheable
             return None
 
         # Look for cached response and check if expired
         cachedresponse = self.storage.retrieve_response(spider, request)
         if cachedresponse is None:
-            self.stats.inc_value('httpcache/miss', spider=spider)
+            self.stats.inc_value("httpcache/miss", spider=spider)
             if self.ignore_missing:
-                self.stats.inc_value('httpcache/ignore', spider=spider)
+                self.stats.inc_value("httpcache/ignore", spider=spider)
                 raise IgnoreRequest(f"Ignored request not in cache: {request}")
             return None  # first time request
 
         # Return cached response only if not expired
-        cachedresponse.flags.append('cached')
+        cachedresponse.flags.append("cached")
         if self.policy.is_cached_response_fresh(cachedresponse, request):
-            self.stats.inc_value('httpcache/hit', spider=spider)
+            self.stats.inc_value("httpcache/hit", spider=spider)
             return cachedresponse
 
         # Keep a reference to cached response to avoid a second cache lookup on
         # process_response hook
-        request.meta['cached_response'] = cachedresponse
+        request.meta["cached_response"] = cachedresponse
 
         return None
 
-    def process_response(self, request: Request, response: Response, spider: Spider) -> Response:
-        if request.meta.get('dont_cache', False):
+    def process_response(
+        self, request: Request, response: Response, spider: Spider
+    ) -> Response:
+        if request.meta.get("dont_cache", False):
             return response
 
         # Skip cached responses and uncacheable requests
-        if 'cached' in response.flags or '_dont_cache' in request.meta:
-            request.meta.pop('_dont_cache', None)
+        if "cached" in response.flags or "_dont_cache" in request.meta:
+            request.meta.pop("_dont_cache", None)
             return response
 
         # RFC2616 requires origin server to set Date header,
         # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18
-        if 'Date' not in response.headers:
-            response.headers['Date'] = formatdate(usegmt=True)
+        if "Date" not in response.headers:
+            response.headers["Date"] = formatdate(usegmt=True)
 
         # Do not validate first-hand responses
-        cachedresponse = request.meta.pop('cached_response', None)
+        cachedresponse = request.meta.pop("cached_response", None)
         if cachedresponse is None:
-            self.stats.inc_value('httpcache/firsthand', spider=spider)
+            self.stats.inc_value("httpcache/firsthand", spider=spider)
             self._cache_response(spider, response, request, cachedresponse)
             return response
 
         if self.policy.is_cached_response_valid(cachedresponse, response, request):
-            self.stats.inc_value('httpcache/revalidate', spider=spider)
+            self.stats.inc_value("httpcache/revalidate", spider=spider)
             return cachedresponse
 
-        self.stats.inc_value('httpcache/invalidate', spider=spider)
+        self.stats.inc_value("httpcache/invalidate", spider=spider)
         self._cache_response(spider, response, request, cachedresponse)
         return response
 
     def process_exception(
         self, request: Request, exception: Exception, spider: Spider
     ) -> Optional[Response]:
-        cachedresponse = request.meta.pop('cached_response', None)
-        if cachedresponse is not None and isinstance(exception, self.DOWNLOAD_EXCEPTIONS):
-            self.stats.inc_value('httpcache/errorrecovery', spider=spider)
+        cachedresponse = request.meta.pop("cached_response", None)
+        if cachedresponse is not None and isinstance(
+            exception, self.DOWNLOAD_EXCEPTIONS
+        ):
+            self.stats.inc_value("httpcache/errorrecovery", spider=spider)
             return cachedresponse
         return None
 
     def _cache_response(
-        self, spider: Spider, response: Response, request: Request, cachedresponse: Optional[Response]
+        self,
+        spider: Spider,
+        response: Response,
+        request: Request,
+        cachedresponse: Optional[Response],
     ) -> None:
         if self.policy.should_cache_response(response, request):
-            self.stats.inc_value('httpcache/store', spider=spider)
+            self.stats.inc_value("httpcache/store", spider=spider)
             self.storage.store_response(spider, request, response)
         else:
-            self.stats.inc_value('httpcache/uncacheable', spider=spider)
+            self.stats.inc_value("httpcache/uncacheable", spider=spider)
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/httpcompression.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/httpcompression.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,39 +4,41 @@
 
 from scrapy.exceptions import NotConfigured
 from scrapy.http import Response, TextResponse
 from scrapy.responsetypes import responsetypes
 from scrapy.utils.deprecate import ScrapyDeprecationWarning
 from scrapy.utils.gz import gunzip
 
-
-ACCEPTED_ENCODINGS = [b'gzip', b'deflate']
+ACCEPTED_ENCODINGS = [b"gzip", b"deflate"]
 
 try:
     import brotli
-    ACCEPTED_ENCODINGS.append(b'br')
+
+    ACCEPTED_ENCODINGS.append(b"br")
 except ImportError:
     pass
 
 try:
     import zstandard
-    ACCEPTED_ENCODINGS.append(b'zstd')
+
+    ACCEPTED_ENCODINGS.append(b"zstd")
 except ImportError:
     pass
 
 
 class HttpCompressionMiddleware:
     """This middleware allows compressed (gzip, deflate) traffic to be
     sent/received from web sites"""
+
     def __init__(self, stats=None):
         self.stats = stats
 
     @classmethod
     def from_crawler(cls, crawler):
-        if not crawler.settings.getbool('COMPRESSION_ENABLED'):
+        if not crawler.settings.getbool("COMPRESSION_ENABLED"):
             raise NotConfigured
         try:
             return cls(stats=crawler.stats)
         except TypeError:
             warnings.warn(
                 "HttpCompressionMiddleware subclasses must either modify "
                 "their '__init__' method to support a 'stats' parameter or "
@@ -44,58 +46,63 @@
                 ScrapyDeprecationWarning,
             )
             result = cls()
             result.stats = crawler.stats
             return result
 
     def process_request(self, request, spider):
-        request.headers.setdefault('Accept-Encoding',
-                                   b", ".join(ACCEPTED_ENCODINGS))
+        request.headers.setdefault("Accept-Encoding", b", ".join(ACCEPTED_ENCODINGS))
 
     def process_response(self, request, response, spider):
 
-        if request.method == 'HEAD':
+        if request.method == "HEAD":
             return response
         if isinstance(response, Response):
-            content_encoding = response.headers.getlist('Content-Encoding')
+            content_encoding = response.headers.getlist("Content-Encoding")
             if content_encoding:
                 encoding = content_encoding.pop()
                 decoded_body = self._decode(response.body, encoding.lower())
                 if self.stats:
-                    self.stats.inc_value('httpcompression/response_bytes', len(decoded_body), spider=spider)
-                    self.stats.inc_value('httpcompression/response_count', spider=spider)
+                    self.stats.inc_value(
+                        "httpcompression/response_bytes",
+                        len(decoded_body),
+                        spider=spider,
+                    )
+                    self.stats.inc_value(
+                        "httpcompression/response_count", spider=spider
+                    )
                 respcls = responsetypes.from_args(
                     headers=response.headers, url=response.url, body=decoded_body
                 )
                 kwargs = dict(cls=respcls, body=decoded_body)
                 if issubclass(respcls, TextResponse):
                     # force recalculating the encoding until we make sure the
                     # responsetypes guessing is reliable
-                    kwargs['encoding'] = None
+                    kwargs["encoding"] = None
                 response = response.replace(**kwargs)
                 if not content_encoding:
-                    del response.headers['Content-Encoding']
+                    del response.headers["Content-Encoding"]
 
         return response
 
     def _decode(self, body, encoding):
-        if encoding == b'gzip' or encoding == b'x-gzip':
+        if encoding == b"gzip" or encoding == b"x-gzip":
             body = gunzip(body)
 
-        if encoding == b'deflate':
+        if encoding == b"deflate":
             try:
                 body = zlib.decompress(body)
             except zlib.error:
                 # ugly hack to work with raw deflate content that may
                 # be sent by microsoft servers. For more information, see:
                 # http://carsten.codimi.de/gzip.yaws/
                 # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx
                 # http://www.gzip.org/zlib/zlib_faq.html#faq38
                 body = zlib.decompress(body, -15)
-        if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:
+        if encoding == b"br" and b"br" in ACCEPTED_ENCODINGS:
             body = brotli.decompress(body)
-        if encoding == b'zstd' and b'zstd' in ACCEPTED_ENCODINGS:
+        if encoding == b"zstd" and b"zstd" in ACCEPTED_ENCODINGS:
             # Using its streaming API since its simple API could handle only cases
             # where there is content size data embedded in the frame
             reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))
             body = reader.read()
         return body
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/httpproxy.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/httpproxy.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,84 +1,80 @@
 import base64
 from urllib.parse import unquote, urlunparse
-from urllib.request import getproxies, proxy_bypass, _parse_proxy
+from urllib.request import _parse_proxy, getproxies, proxy_bypass
 
 from scrapy.exceptions import NotConfigured
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.python import to_bytes
 
 
 class HttpProxyMiddleware:
-
-    def __init__(self, auth_encoding='latin-1'):
+    def __init__(self, auth_encoding="latin-1"):
         self.auth_encoding = auth_encoding
         self.proxies = {}
         for type_, url in getproxies().items():
             try:
                 self.proxies[type_] = self._get_proxy(url, type_)
             # some values such as '/var/run/docker.sock' can't be parsed
             # by _parse_proxy and as such should be skipped
             except ValueError:
                 continue
 
     @classmethod
     def from_crawler(cls, crawler):
-        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):
+        if not crawler.settings.getbool("HTTPPROXY_ENABLED"):
             raise NotConfigured
-        auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')
+        auth_encoding = crawler.settings.get("HTTPPROXY_AUTH_ENCODING")
         return cls(auth_encoding)
 
     def _basic_auth_header(self, username, password):
         user_pass = to_bytes(
-            f'{unquote(username)}:{unquote(password)}',
-            encoding=self.auth_encoding)
+            f"{unquote(username)}:{unquote(password)}", encoding=self.auth_encoding
+        )
         return base64.b64encode(user_pass)
 
     def _get_proxy(self, url, orig_type):
         proxy_type, user, password, hostport = _parse_proxy(url)
-        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))
+        proxy_url = urlunparse((proxy_type or orig_type, hostport, "", "", "", ""))
 
         if user:
             creds = self._basic_auth_header(user, password)
         else:
             creds = None
 
         return creds, proxy_url
 
     def process_request(self, request, spider):
         creds, proxy_url = None, None
-        if 'proxy' in request.meta:
-            if request.meta['proxy'] is not None:
-                creds, proxy_url = self._get_proxy(request.meta['proxy'], '')
+        if "proxy" in request.meta:
+            if request.meta["proxy"] is not None:
+                creds, proxy_url = self._get_proxy(request.meta["proxy"], "")
         elif self.proxies:
             parsed = urlparse_cached(request)
             scheme = parsed.scheme
             if (
-                (
-                    # 'no_proxy' is only supported by http schemes
-                    scheme not in ('http', 'https')
-                    or not proxy_bypass(parsed.hostname)
-                )
-                and scheme in self.proxies
-            ):
+                # 'no_proxy' is only supported by http schemes
+                scheme not in ("http", "https")
+                or not proxy_bypass(parsed.hostname)
+            ) and scheme in self.proxies:
                 creds, proxy_url = self.proxies[scheme]
 
         self._set_proxy_and_creds(request, proxy_url, creds)
 
     def _set_proxy_and_creds(self, request, proxy_url, creds):
         if proxy_url:
-            request.meta['proxy'] = proxy_url
-        elif request.meta.get('proxy') is not None:
-            request.meta['proxy'] = None
+            request.meta["proxy"] = proxy_url
+        elif request.meta.get("proxy") is not None:
+            request.meta["proxy"] = None
         if creds:
-            request.headers[b'Proxy-Authorization'] = b'Basic ' + creds
-            request.meta['_auth_proxy'] = proxy_url
-        elif '_auth_proxy' in request.meta:
-            if proxy_url != request.meta['_auth_proxy']:
-                if b'Proxy-Authorization' in request.headers:
-                    del request.headers[b'Proxy-Authorization']
-                del request.meta['_auth_proxy']
-        elif b'Proxy-Authorization' in request.headers:
+            request.headers[b"Proxy-Authorization"] = b"Basic " + creds
+            request.meta["_auth_proxy"] = proxy_url
+        elif "_auth_proxy" in request.meta:
+            if proxy_url != request.meta["_auth_proxy"]:
+                if b"Proxy-Authorization" in request.headers:
+                    del request.headers[b"Proxy-Authorization"]
+                del request.meta["_auth_proxy"]
+        elif b"Proxy-Authorization" in request.headers:
             if proxy_url:
-                request.meta['_auth_proxy'] = proxy_url
+                request.meta["_auth_proxy"] = proxy_url
             else:
-                del request.headers[b'Proxy-Authorization']
+                del request.headers[b"Proxy-Authorization"]
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/redirect.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/redirect.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,133 +1,139 @@
 import logging
 from urllib.parse import urljoin, urlparse
 
 from w3lib.url import safe_url_string
 
+from scrapy.exceptions import IgnoreRequest, NotConfigured
 from scrapy.http import HtmlResponse
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.response import get_meta_refresh
-from scrapy.exceptions import IgnoreRequest, NotConfigured
-
 
 logger = logging.getLogger(__name__)
 
 
 def _build_redirect_request(source_request, *, url, **kwargs):
     redirect_request = source_request.replace(
         url=url,
         **kwargs,
         cookies=None,
     )
-    if 'Cookie' in redirect_request.headers:
+    if "Cookie" in redirect_request.headers:
         source_request_netloc = urlparse_cached(source_request).netloc
         redirect_request_netloc = urlparse_cached(redirect_request).netloc
         if source_request_netloc != redirect_request_netloc:
-            del redirect_request.headers['Cookie']
+            del redirect_request.headers["Cookie"]
     return redirect_request
 
 
 class BaseRedirectMiddleware:
 
-    enabled_setting = 'REDIRECT_ENABLED'
+    enabled_setting = "REDIRECT_ENABLED"
 
     def __init__(self, settings):
         if not settings.getbool(self.enabled_setting):
             raise NotConfigured
 
-        self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')
-        self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')
+        self.max_redirect_times = settings.getint("REDIRECT_MAX_TIMES")
+        self.priority_adjust = settings.getint("REDIRECT_PRIORITY_ADJUST")
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler.settings)
 
     def _redirect(self, redirected, request, spider, reason):
-        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)
-        redirects = request.meta.get('redirect_times', 0) + 1
+        ttl = request.meta.setdefault("redirect_ttl", self.max_redirect_times)
+        redirects = request.meta.get("redirect_times", 0) + 1
 
         if ttl and redirects <= self.max_redirect_times:
-            redirected.meta['redirect_times'] = redirects
-            redirected.meta['redirect_ttl'] = ttl - 1
-            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + [request.url]
-            redirected.meta['redirect_reasons'] = request.meta.get('redirect_reasons', []) + [reason]
+            redirected.meta["redirect_times"] = redirects
+            redirected.meta["redirect_ttl"] = ttl - 1
+            redirected.meta["redirect_urls"] = request.meta.get("redirect_urls", []) + [
+                request.url
+            ]
+            redirected.meta["redirect_reasons"] = request.meta.get(
+                "redirect_reasons", []
+            ) + [reason]
             redirected.dont_filter = request.dont_filter
             redirected.priority = request.priority + self.priority_adjust
-            logger.debug("Redirecting (%(reason)s) to %(redirected)s from %(request)s",
-                         {'reason': reason, 'redirected': redirected, 'request': request},
-                         extra={'spider': spider})
+            logger.debug(
+                "Redirecting (%(reason)s) to %(redirected)s from %(request)s",
+                {"reason": reason, "redirected": redirected, "request": request},
+                extra={"spider": spider},
+            )
             return redirected
-        else:
-            logger.debug("Discarding %(request)s: max redirections reached",
-                         {'request': request}, extra={'spider': spider})
-            raise IgnoreRequest("max redirections reached")
+        logger.debug(
+            "Discarding %(request)s: max redirections reached",
+            {"request": request},
+            extra={"spider": spider},
+        )
+        raise IgnoreRequest("max redirections reached")
 
     def _redirect_request_using_get(self, request, redirect_url):
         redirect_request = _build_redirect_request(
             request,
             url=redirect_url,
-            method='GET',
-            body='',
+            method="GET",
+            body="",
         )
-        redirect_request.headers.pop('Content-Type', None)
-        redirect_request.headers.pop('Content-Length', None)
+        redirect_request.headers.pop("Content-Type", None)
+        redirect_request.headers.pop("Content-Length", None)
         return redirect_request
 
 
 class RedirectMiddleware(BaseRedirectMiddleware):
     """
     Handle redirection of requests based on response status
     and meta-refresh html tag.
     """
 
     def process_response(self, request, response, spider):
         if (
-            request.meta.get('dont_redirect', False)
-            or response.status in getattr(spider, 'handle_httpstatus_list', [])
-            or response.status in request.meta.get('handle_httpstatus_list', [])
-            or request.meta.get('handle_httpstatus_all', False)
+            request.meta.get("dont_redirect", False)
+            or response.status in getattr(spider, "handle_httpstatus_list", [])
+            or response.status in request.meta.get("handle_httpstatus_list", [])
+            or request.meta.get("handle_httpstatus_all", False)
         ):
             return response
 
         allowed_status = (301, 302, 303, 307, 308)
-        if 'Location' not in response.headers or response.status not in allowed_status:
+        if "Location" not in response.headers or response.status not in allowed_status:
             return response
 
-        location = safe_url_string(response.headers['Location'])
-        if response.headers['Location'].startswith(b'//'):
+        location = safe_url_string(response.headers["Location"])
+        if response.headers["Location"].startswith(b"//"):
             request_scheme = urlparse(request.url).scheme
-            location = request_scheme + '://' + location.lstrip('/')
+            location = request_scheme + "://" + location.lstrip("/")
 
         redirected_url = urljoin(request.url, location)
 
-        if response.status in (301, 307, 308) or request.method == 'HEAD':
+        if response.status in (301, 307, 308) or request.method == "HEAD":
             redirected = _build_redirect_request(request, url=redirected_url)
             return self._redirect(redirected, request, spider, response.status)
 
         redirected = self._redirect_request_using_get(request, redirected_url)
         return self._redirect(redirected, request, spider, response.status)
 
 
 class MetaRefreshMiddleware(BaseRedirectMiddleware):
 
-    enabled_setting = 'METAREFRESH_ENABLED'
+    enabled_setting = "METAREFRESH_ENABLED"
 
     def __init__(self, settings):
         super().__init__(settings)
-        self._ignore_tags = settings.getlist('METAREFRESH_IGNORE_TAGS')
-        self._maxdelay = settings.getint('METAREFRESH_MAXDELAY')
+        self._ignore_tags = settings.getlist("METAREFRESH_IGNORE_TAGS")
+        self._maxdelay = settings.getint("METAREFRESH_MAXDELAY")
 
     def process_response(self, request, response, spider):
         if (
-            request.meta.get('dont_redirect', False)
-            or request.method == 'HEAD'
+            request.meta.get("dont_redirect", False)
+            or request.method == "HEAD"
             or not isinstance(response, HtmlResponse)
         ):
             return response
 
-        interval, url = get_meta_refresh(response,
-                                         ignore_tags=self._ignore_tags)
+        interval, url = get_meta_refresh(response, ignore_tags=self._ignore_tags)
         if url and interval < self._maxdelay:
             redirected = self._redirect_request_using_get(request, url)
-            return self._redirect(redirected, request, spider, 'meta refresh')
+            return self._redirect(redirected, request, spider, "meta refresh")
 
         return response
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/retry.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/retry.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 You can change the behaviour of this middleware by modifying the scraping settings:
 RETRY_TIMES - how many times to retry a failed page
 RETRY_HTTP_CODES - which HTTP response codes to retry
 
 Failed pages are collected on the scraping process and rescheduled at the end,
 once the spider has finished crawling all regular (non failed) pages.
 """
-from logging import getLogger, Logger
+from logging import Logger, getLogger
 from typing import Optional, Union
 
 from twisted.internet import defer
 from twisted.internet.error import (
     ConnectError,
     ConnectionDone,
     ConnectionLost,
@@ -27,27 +27,26 @@
 from scrapy.core.downloader.handlers.http11 import TunnelError
 from scrapy.exceptions import NotConfigured
 from scrapy.http.request import Request
 from scrapy.spiders import Spider
 from scrapy.utils.python import global_object_name
 from scrapy.utils.response import response_status_message
 
-
 retry_logger = getLogger(__name__)
 
 
 def get_retry_request(
     request: Request,
     *,
     spider: Spider,
-    reason: Union[str, Exception] = 'unspecified',
+    reason: Union[str, Exception] = "unspecified",
     max_retry_times: Optional[int] = None,
     priority_adjust: Optional[int] = None,
     logger: Logger = retry_logger,
-    stats_base_key: str = 'retry',
+    stats_base_key: str = "retry",
 ):
     """
     Returns a new :class:`~scrapy.Request` object to retry the specified
     request, or ``None`` if retries of the specified request have been
     exhausted.
 
     For example, in a :class:`~scrapy.Spider` callback, you could use it as
@@ -83,89 +82,97 @@
     *logger* is the logging.Logger object to be used when logging messages
 
     *stats_base_key* is a string to be used as the base key for the
     retry-related job stats
     """
     settings = spider.crawler.settings
     stats = spider.crawler.stats
-    retry_times = request.meta.get('retry_times', 0) + 1
+    retry_times = request.meta.get("retry_times", 0) + 1
     if max_retry_times is None:
-        max_retry_times = request.meta.get('max_retry_times')
+        max_retry_times = request.meta.get("max_retry_times")
         if max_retry_times is None:
-            max_retry_times = settings.getint('RETRY_TIMES')
+            max_retry_times = settings.getint("RETRY_TIMES")
     if retry_times <= max_retry_times:
         logger.debug(
             "Retrying %(request)s (failed %(retry_times)d times): %(reason)s",
-            {'request': request, 'retry_times': retry_times, 'reason': reason},
-            extra={'spider': spider}
+            {"request": request, "retry_times": retry_times, "reason": reason},
+            extra={"spider": spider},
         )
         new_request: Request = request.copy()
-        new_request.meta['retry_times'] = retry_times
+        new_request.meta["retry_times"] = retry_times
         new_request.dont_filter = True
         if priority_adjust is None:
-            priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')
+            priority_adjust = settings.getint("RETRY_PRIORITY_ADJUST")
         new_request.priority = request.priority + priority_adjust
 
         if callable(reason):
             reason = reason()
         if isinstance(reason, Exception):
             reason = global_object_name(reason.__class__)
 
-        stats.inc_value(f'{stats_base_key}/count')
-        stats.inc_value(f'{stats_base_key}/reason_count/{reason}')
+        stats.inc_value(f"{stats_base_key}/count")
+        stats.inc_value(f"{stats_base_key}/reason_count/{reason}")
         return new_request
-    else:
-        stats.inc_value(f'{stats_base_key}/max_reached')
-        logger.error(
-            "Gave up retrying %(request)s (failed %(retry_times)d times): "
-            "%(reason)s",
-            {'request': request, 'retry_times': retry_times, 'reason': reason},
-            extra={'spider': spider},
-        )
-        return None
+    stats.inc_value(f"{stats_base_key}/max_reached")
+    logger.error(
+        "Gave up retrying %(request)s (failed %(retry_times)d times): " "%(reason)s",
+        {"request": request, "retry_times": retry_times, "reason": reason},
+        extra={"spider": spider},
+    )
+    return None
 
 
 class RetryMiddleware:
 
     # IOError is raised by the HttpCompression middleware when trying to
     # decompress an empty response
-    EXCEPTIONS_TO_RETRY = (defer.TimeoutError, TimeoutError, DNSLookupError,
-                           ConnectionRefusedError, ConnectionDone, ConnectError,
-                           ConnectionLost, TCPTimedOutError, ResponseFailed,
-                           IOError, TunnelError)
+    EXCEPTIONS_TO_RETRY = (
+        defer.TimeoutError,
+        TimeoutError,
+        DNSLookupError,
+        ConnectionRefusedError,
+        ConnectionDone,
+        ConnectError,
+        ConnectionLost,
+        TCPTimedOutError,
+        ResponseFailed,
+        IOError,
+        TunnelError,
+    )
 
     def __init__(self, settings):
-        if not settings.getbool('RETRY_ENABLED'):
+        if not settings.getbool("RETRY_ENABLED"):
             raise NotConfigured
-        self.max_retry_times = settings.getint('RETRY_TIMES')
-        self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))
-        self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')
+        self.max_retry_times = settings.getint("RETRY_TIMES")
+        self.retry_http_codes = set(
+            int(x) for x in settings.getlist("RETRY_HTTP_CODES")
+        )
+        self.priority_adjust = settings.getint("RETRY_PRIORITY_ADJUST")
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler.settings)
 
     def process_response(self, request, response, spider):
-        if request.meta.get('dont_retry', False):
+        if request.meta.get("dont_retry", False):
             return response
         if response.status in self.retry_http_codes:
             reason = response_status_message(response.status)
             return self._retry(request, reason, spider) or response
         return response
 
     def process_exception(self, request, exception, spider):
-        if (
-            isinstance(exception, self.EXCEPTIONS_TO_RETRY)
-            and not request.meta.get('dont_retry', False)
+        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) and not request.meta.get(
+            "dont_retry", False
         ):
             return self._retry(request, exception, spider)
 
     def _retry(self, request, reason, spider):
-        max_retry_times = request.meta.get('max_retry_times', self.max_retry_times)
-        priority_adjust = request.meta.get('priority_adjust', self.priority_adjust)
+        max_retry_times = request.meta.get("max_retry_times", self.max_retry_times)
+        priority_adjust = request.meta.get("priority_adjust", self.priority_adjust)
         return get_retry_request(
             request,
             reason=reason,
             spider=spider,
             max_retry_times=max_retry_times,
             priority_adjust=priority_adjust,
         )
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/robotstxt.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/robotstxt.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,107 +3,119 @@
 enable this middleware and enable the ROBOTSTXT_OBEY setting.
 
 """
 
 import logging
 
 from twisted.internet.defer import Deferred, maybeDeferred
-from scrapy.exceptions import NotConfigured, IgnoreRequest
+
+from scrapy.exceptions import IgnoreRequest, NotConfigured
 from scrapy.http import Request
+from scrapy.http.request import NO_CALLBACK
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.log import failure_to_exc_info
 from scrapy.utils.misc import load_object
 
 logger = logging.getLogger(__name__)
 
 
 class RobotsTxtMiddleware:
     DOWNLOAD_PRIORITY = 1000
 
     def __init__(self, crawler):
-        if not crawler.settings.getbool('ROBOTSTXT_OBEY'):
+        if not crawler.settings.getbool("ROBOTSTXT_OBEY"):
             raise NotConfigured
-        self._default_useragent = crawler.settings.get('USER_AGENT', 'Scrapy')
-        self._robotstxt_useragent = crawler.settings.get('ROBOTSTXT_USER_AGENT', None)
+        self._default_useragent = crawler.settings.get("USER_AGENT", "Scrapy")
+        self._robotstxt_useragent = crawler.settings.get("ROBOTSTXT_USER_AGENT", None)
         self.crawler = crawler
         self._parsers = {}
-        self._parserimpl = load_object(crawler.settings.get('ROBOTSTXT_PARSER'))
+        self._parserimpl = load_object(crawler.settings.get("ROBOTSTXT_PARSER"))
 
         # check if parser dependencies are met, this should throw an error otherwise.
-        self._parserimpl.from_crawler(self.crawler, b'')
+        self._parserimpl.from_crawler(self.crawler, b"")
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler)
 
     def process_request(self, request, spider):
-        if request.meta.get('dont_obey_robotstxt'):
+        if request.meta.get("dont_obey_robotstxt"):
+            return
+        if request.url.startswith("data:") or request.url.startswith("file:"):
             return
         d = maybeDeferred(self.robot_parser, request, spider)
         d.addCallback(self.process_request_2, request, spider)
         return d
 
     def process_request_2(self, rp, request, spider):
         if rp is None:
             return
 
         useragent = self._robotstxt_useragent
         if not useragent:
-            useragent = request.headers.get(b'User-Agent', self._default_useragent)
+            useragent = request.headers.get(b"User-Agent", self._default_useragent)
         if not rp.allowed(request.url, useragent):
-            logger.debug("Forbidden by robots.txt: %(request)s",
-                         {'request': request}, extra={'spider': spider})
-            self.crawler.stats.inc_value('robotstxt/forbidden')
+            logger.debug(
+                "Forbidden by robots.txt: %(request)s",
+                {"request": request},
+                extra={"spider": spider},
+            )
+            self.crawler.stats.inc_value("robotstxt/forbidden")
             raise IgnoreRequest("Forbidden by robots.txt")
 
     def robot_parser(self, request, spider):
         url = urlparse_cached(request)
         netloc = url.netloc
 
         if netloc not in self._parsers:
             self._parsers[netloc] = Deferred()
             robotsurl = f"{url.scheme}://{url.netloc}/robots.txt"
             robotsreq = Request(
                 robotsurl,
                 priority=self.DOWNLOAD_PRIORITY,
-                meta={'dont_obey_robotstxt': True}
+                meta={"dont_obey_robotstxt": True},
+                callback=NO_CALLBACK,
             )
             dfd = self.crawler.engine.download(robotsreq)
             dfd.addCallback(self._parse_robots, netloc, spider)
             dfd.addErrback(self._logerror, robotsreq, spider)
             dfd.addErrback(self._robots_error, netloc)
-            self.crawler.stats.inc_value('robotstxt/request_count')
+            self.crawler.stats.inc_value("robotstxt/request_count")
 
         if isinstance(self._parsers[netloc], Deferred):
             d = Deferred()
 
             def cb(result):
                 d.callback(result)
                 return result
+
             self._parsers[netloc].addCallback(cb)
             return d
-        else:
-            return self._parsers[netloc]
+        return self._parsers[netloc]
 
     def _logerror(self, failure, request, spider):
         if failure.type is not IgnoreRequest:
-            logger.error("Error downloading %(request)s: %(f_exception)s",
-                         {'request': request, 'f_exception': failure.value},
-                         exc_info=failure_to_exc_info(failure),
-                         extra={'spider': spider})
+            logger.error(
+                "Error downloading %(request)s: %(f_exception)s",
+                {"request": request, "f_exception": failure.value},
+                exc_info=failure_to_exc_info(failure),
+                extra={"spider": spider},
+            )
         return failure
 
     def _parse_robots(self, response, netloc, spider):
-        self.crawler.stats.inc_value('robotstxt/response_count')
-        self.crawler.stats.inc_value(f'robotstxt/response_status_count/{response.status}')
+        self.crawler.stats.inc_value("robotstxt/response_count")
+        self.crawler.stats.inc_value(
+            f"robotstxt/response_status_count/{response.status}"
+        )
         rp = self._parserimpl.from_crawler(self.crawler, response.body)
         rp_dfd = self._parsers[netloc]
         self._parsers[netloc] = rp
         rp_dfd.callback(rp)
 
     def _robots_error(self, failure, netloc):
         if failure.type is not IgnoreRequest:
-            key = f'robotstxt/exception_count/{failure.type}'
+            key = f"robotstxt/exception_count/{failure.type}"
             self.crawler.stats.inc_value(key)
         rp_dfd = self._parsers[netloc]
         self._parsers[netloc] = None
         rp_dfd.callback(None)
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/stats.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/stats.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,50 +1,60 @@
+from twisted.web import http
+
 from scrapy.exceptions import NotConfigured
 from scrapy.utils.python import global_object_name, to_bytes
 from scrapy.utils.request import request_httprepr
 
-from twisted.web import http
-
 
 def get_header_size(headers):
     size = 0
     for key, value in headers.items():
         if isinstance(value, (list, tuple)):
             for v in value:
                 size += len(b": ") + len(key) + len(v)
-    return size + len(b'\r\n') * (len(headers.keys()) - 1)
+    return size + len(b"\r\n") * (len(headers.keys()) - 1)
 
 
 def get_status_size(response_status):
-    return len(to_bytes(http.RESPONSES.get(response_status, b''))) + 15
+    return len(to_bytes(http.RESPONSES.get(response_status, b""))) + 15
     # resp.status + b"\r\n" + b"HTTP/1.1 <100-599> "
 
 
 class DownloaderStats:
-
     def __init__(self, stats):
         self.stats = stats
 
     @classmethod
     def from_crawler(cls, crawler):
-        if not crawler.settings.getbool('DOWNLOADER_STATS'):
+        if not crawler.settings.getbool("DOWNLOADER_STATS"):
             raise NotConfigured
         return cls(crawler.stats)
 
     def process_request(self, request, spider):
-        self.stats.inc_value('downloader/request_count', spider=spider)
-        self.stats.inc_value(f'downloader/request_method_count/{request.method}', spider=spider)
+        self.stats.inc_value("downloader/request_count", spider=spider)
+        self.stats.inc_value(
+            f"downloader/request_method_count/{request.method}", spider=spider
+        )
         reqlen = len(request_httprepr(request))
-        self.stats.inc_value('downloader/request_bytes', reqlen, spider=spider)
+        self.stats.inc_value("downloader/request_bytes", reqlen, spider=spider)
 
     def process_response(self, request, response, spider):
-        self.stats.inc_value('downloader/response_count', spider=spider)
-        self.stats.inc_value(f'downloader/response_status_count/{response.status}', spider=spider)
-        reslen = len(response.body) + get_header_size(response.headers) + get_status_size(response.status) + 4
+        self.stats.inc_value("downloader/response_count", spider=spider)
+        self.stats.inc_value(
+            f"downloader/response_status_count/{response.status}", spider=spider
+        )
+        reslen = (
+            len(response.body)
+            + get_header_size(response.headers)
+            + get_status_size(response.status)
+            + 4
+        )
         # response.body + b"\r\n"+ response.header + b"\r\n" + response.status
-        self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)
+        self.stats.inc_value("downloader/response_bytes", reslen, spider=spider)
         return response
 
     def process_exception(self, request, exception, spider):
         ex_class = global_object_name(exception.__class__)
-        self.stats.inc_value('downloader/exception_count', spider=spider)
-        self.stats.inc_value(f'downloader/exception_type_count/{ex_class}', spider=spider)
+        self.stats.inc_value("downloader/exception_count", spider=spider)
+        self.stats.inc_value(
+            f"downloader/exception_type_count/{ex_class}", spider=spider
+        )
```

### Comparing `Scrapy-2.7.1/scrapy/downloadermiddlewares/useragent.py` & `Scrapy-2.8.0/scrapy/downloadermiddlewares/useragent.py`

 * *Files 17% similar despite different names*

```diff
@@ -2,22 +2,22 @@
 
 from scrapy import signals
 
 
 class UserAgentMiddleware:
     """This middleware allows spiders to override the user_agent"""
 
-    def __init__(self, user_agent='Scrapy'):
+    def __init__(self, user_agent="Scrapy"):
         self.user_agent = user_agent
 
     @classmethod
     def from_crawler(cls, crawler):
-        o = cls(crawler.settings['USER_AGENT'])
+        o = cls(crawler.settings["USER_AGENT"])
         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
         return o
 
     def spider_opened(self, spider):
-        self.user_agent = getattr(spider, 'user_agent', self.user_agent)
+        self.user_agent = getattr(spider, "user_agent", self.user_agent)
 
     def process_request(self, request, spider):
         if self.user_agent:
-            request.headers.setdefault(b'User-Agent', self.user_agent)
+            request.headers.setdefault(b"User-Agent", self.user_agent)
```

### Comparing `Scrapy-2.7.1/scrapy/dupefilters.py` & `Scrapy-2.8.0/scrapy/dupefilters.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 import logging
-import os
+from pathlib import Path
 from typing import Optional, Set, Type, TypeVar
 from warnings import warn
 
 from twisted.internet.defer import Deferred
 
 from scrapy.http.request import Request
 from scrapy.settings import BaseSettings
 from scrapy.spiders import Spider
 from scrapy.utils.deprecate import ScrapyDeprecationWarning
 from scrapy.utils.job import job_dir
-from scrapy.utils.request import referer_str, RequestFingerprinter
-
+from scrapy.utils.request import RequestFingerprinter, referer_str
 
 BaseDupeFilterTV = TypeVar("BaseDupeFilterTV", bound="BaseDupeFilter")
 
 
 class BaseDupeFilter:
     @classmethod
-    def from_settings(cls: Type[BaseDupeFilterTV], settings: BaseSettings) -> BaseDupeFilterTV:
+    def from_settings(
+        cls: Type[BaseDupeFilterTV], settings: BaseSettings
+    ) -> BaseDupeFilterTV:
         return cls()
 
     def request_seen(self, request: Request) -> bool:
         return False
 
     def open(self) -> Optional[Deferred]:
         pass
@@ -51,21 +52,23 @@
         self.file = None
         self.fingerprinter = fingerprinter or RequestFingerprinter()
         self.fingerprints: Set[str] = set()
         self.logdupes = True
         self.debug = debug
         self.logger = logging.getLogger(__name__)
         if path:
-            self.file = open(os.path.join(path, 'requests.seen'), 'a+')
+            self.file = Path(path, "requests.seen").open("a+", encoding="utf-8")
             self.file.seek(0)
             self.fingerprints.update(x.rstrip() for x in self.file)
 
     @classmethod
-    def from_settings(cls: Type[RFPDupeFilterTV], settings: BaseSettings, *, fingerprinter=None) -> RFPDupeFilterTV:
-        debug = settings.getbool('DUPEFILTER_DEBUG')
+    def from_settings(
+        cls: Type[RFPDupeFilterTV], settings: BaseSettings, *, fingerprinter=None
+    ) -> RFPDupeFilterTV:
+        debug = settings.getbool("DUPEFILTER_DEBUG")
         try:
             return cls(job_dir(settings), debug, fingerprinter=fingerprinter)
         except TypeError:
             warn(
                 "RFPDupeFilter subclasses must either modify their '__init__' "
                 "method to support a 'fingerprinter' parameter or reimplement "
                 "the 'from_settings' class method.",
@@ -96,30 +99,32 @@
 
     def request_seen(self, request: Request) -> bool:
         fp = self.request_fingerprint(request)
         if fp in self.fingerprints:
             return True
         self.fingerprints.add(fp)
         if self.file:
-            self.file.write(fp + '\n')
+            self.file.write(fp + "\n")
         return False
 
     def request_fingerprint(self, request: Request) -> str:
         return self.fingerprinter.fingerprint(request).hex()
 
     def close(self, reason: str) -> None:
         if self.file:
             self.file.close()
 
     def log(self, request: Request, spider: Spider) -> None:
         if self.debug:
             msg = "Filtered duplicate request: %(request)s (referer: %(referer)s)"
-            args = {'request': request, 'referer': referer_str(request)}
-            self.logger.debug(msg, args, extra={'spider': spider})
+            args = {"request": request, "referer": referer_str(request)}
+            self.logger.debug(msg, args, extra={"spider": spider})
         elif self.logdupes:
-            msg = ("Filtered duplicate request: %(request)s"
-                   " - no more duplicates will be shown"
-                   " (see DUPEFILTER_DEBUG to show all duplicates)")
-            self.logger.debug(msg, {'request': request}, extra={'spider': spider})
+            msg = (
+                "Filtered duplicate request: %(request)s"
+                " - no more duplicates will be shown"
+                " (see DUPEFILTER_DEBUG to show all duplicates)"
+            )
+            self.logger.debug(msg, {"request": request}, extra={"spider": spider})
             self.logdupes = False
 
-        spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider)
+        spider.crawler.stats.inc_value("dupefilter/filtered", spider=spider)
```

### Comparing `Scrapy-2.7.1/scrapy/exceptions.py` & `Scrapy-2.8.0/scrapy/exceptions.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,41 +6,44 @@
 """
 
 # Internal
 
 
 class NotConfigured(Exception):
     """Indicates a missing configuration situation"""
+
     pass
 
 
 class _InvalidOutput(TypeError):
     """
     Indicates an invalid value has been returned by a middleware's processing method.
     Internal and undocumented, it should not be raised or caught by user code.
     """
+
     pass
 
 
 # HTTP and crawling
 
 
 class IgnoreRequest(Exception):
     """Indicates a decision was made not to process a request"""
 
 
 class DontCloseSpider(Exception):
     """Request the spider not to be closed yet"""
+
     pass
 
 
 class CloseSpider(Exception):
     """Raise this from callbacks to request the spider to be closed"""
 
-    def __init__(self, reason='cancelled'):
+    def __init__(self, reason="cancelled"):
         super().__init__()
         self.reason = reason
 
 
 class StopDownload(Exception):
     """
     Stop the download of the body for a given response.
@@ -54,36 +57,40 @@
 
 
 # Items
 
 
 class DropItem(Exception):
     """Drop item from the item pipeline"""
+
     pass
 
 
 class NotSupported(Exception):
     """Indicates a feature or method is not supported"""
+
     pass
 
 
 # Commands
 
 
 class UsageError(Exception):
     """To indicate a command-line usage error"""
 
     def __init__(self, *a, **kw):
-        self.print_help = kw.pop('print_help', True)
+        self.print_help = kw.pop("print_help", True)
         super().__init__(*a, **kw)
 
 
 class ScrapyDeprecationWarning(Warning):
     """Warning category for deprecated features, since the default
     DeprecationWarning is silenced on Python 2.7+
     """
+
     pass
 
 
 class ContractFail(AssertionError):
     """Error raised in case of a failing contract"""
+
     pass
```

### Comparing `Scrapy-2.7.1/scrapy/exporters.py` & `Scrapy-2.8.0/scrapy/exporters.py`

 * *Files 5% similar despite different names*

```diff
@@ -7,50 +7,55 @@
 import marshal
 import pickle
 import pprint
 import warnings
 from collections.abc import Mapping
 from xml.sax.saxutils import XMLGenerator
 
-from itemadapter import is_item, ItemAdapter
+from itemadapter import ItemAdapter, is_item
 
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.item import Item
 from scrapy.utils.python import is_listlike, to_bytes, to_unicode
 from scrapy.utils.serialize import ScrapyJSONEncoder
 
-
-__all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',
-           'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter',
-           'JsonItemExporter', 'MarshalItemExporter']
+__all__ = [
+    "BaseItemExporter",
+    "PprintItemExporter",
+    "PickleItemExporter",
+    "CsvItemExporter",
+    "XmlItemExporter",
+    "JsonLinesItemExporter",
+    "JsonItemExporter",
+    "MarshalItemExporter",
+]
 
 
 class BaseItemExporter:
-
     def __init__(self, *, dont_fail=False, **kwargs):
         self._kwargs = kwargs
         self._configure(kwargs, dont_fail=dont_fail)
 
     def _configure(self, options, dont_fail=False):
         """Configure the exporter by popping options from the ``options`` dict.
         If dont_fail is set, it won't raise an exception on unexpected options
         (useful for using with keyword arguments in subclasses ``__init__`` methods)
         """
-        self.encoding = options.pop('encoding', None)
-        self.fields_to_export = options.pop('fields_to_export', None)
-        self.export_empty_fields = options.pop('export_empty_fields', False)
-        self.indent = options.pop('indent', None)
+        self.encoding = options.pop("encoding", None)
+        self.fields_to_export = options.pop("fields_to_export", None)
+        self.export_empty_fields = options.pop("export_empty_fields", False)
+        self.indent = options.pop("indent", None)
         if not dont_fail and options:
             raise TypeError(f"Unexpected options: {', '.join(options.keys())}")
 
     def export_item(self, item):
         raise NotImplementedError
 
     def serialize_field(self, field, name, value):
-        serializer = field.get('serializer', lambda x: x)
+        serializer = field.get("serializer", lambda x: x)
         return serializer(value)
 
     def start_exporting(self):
         pass
 
     def finish_exporting(self):
         pass
@@ -70,16 +75,15 @@
             else:
                 field_iter = item.keys()
         elif isinstance(self.fields_to_export, Mapping):
             if include_empty:
                 field_iter = self.fields_to_export.items()
             else:
                 field_iter = (
-                    (x, y) for x, y in self.fields_to_export.items()
-                    if x in item
+                    (x, y) for x, y in self.fields_to_export.items() if x in item
                 )
         else:
             if include_empty:
                 field_iter = self.fields_to_export
             else:
                 field_iter = (x for x in self.fields_to_export if x in item)
 
@@ -94,143 +98,148 @@
             else:
                 value = default_value
 
             yield output_field, value
 
 
 class JsonLinesItemExporter(BaseItemExporter):
-
     def __init__(self, file, **kwargs):
         super().__init__(dont_fail=True, **kwargs)
         self.file = file
-        self._kwargs.setdefault('ensure_ascii', not self.encoding)
+        self._kwargs.setdefault("ensure_ascii", not self.encoding)
         self.encoder = ScrapyJSONEncoder(**self._kwargs)
 
     def export_item(self, item):
         itemdict = dict(self._get_serialized_fields(item))
-        data = self.encoder.encode(itemdict) + '\n'
+        data = self.encoder.encode(itemdict) + "\n"
         self.file.write(to_bytes(data, self.encoding))
 
 
 class JsonItemExporter(BaseItemExporter):
-
     def __init__(self, file, **kwargs):
         super().__init__(dont_fail=True, **kwargs)
         self.file = file
         # there is a small difference between the behaviour or JsonItemExporter.indent
         # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent
         # the addition of newlines everywhere
-        json_indent = self.indent if self.indent is not None and self.indent > 0 else None
-        self._kwargs.setdefault('indent', json_indent)
-        self._kwargs.setdefault('ensure_ascii', not self.encoding)
+        json_indent = (
+            self.indent if self.indent is not None and self.indent > 0 else None
+        )
+        self._kwargs.setdefault("indent", json_indent)
+        self._kwargs.setdefault("ensure_ascii", not self.encoding)
         self.encoder = ScrapyJSONEncoder(**self._kwargs)
         self.first_item = True
 
     def _beautify_newline(self):
         if self.indent is not None:
-            self.file.write(b'\n')
+            self.file.write(b"\n")
 
     def start_exporting(self):
         self.file.write(b"[")
         self._beautify_newline()
 
     def finish_exporting(self):
         self._beautify_newline()
         self.file.write(b"]")
 
     def export_item(self, item):
         if self.first_item:
             self.first_item = False
         else:
-            self.file.write(b',')
+            self.file.write(b",")
             self._beautify_newline()
         itemdict = dict(self._get_serialized_fields(item))
         data = self.encoder.encode(itemdict)
         self.file.write(to_bytes(data, self.encoding))
 
 
 class XmlItemExporter(BaseItemExporter):
-
     def __init__(self, file, **kwargs):
-        self.item_element = kwargs.pop('item_element', 'item')
-        self.root_element = kwargs.pop('root_element', 'items')
+        self.item_element = kwargs.pop("item_element", "item")
+        self.root_element = kwargs.pop("root_element", "items")
         super().__init__(**kwargs)
         if not self.encoding:
-            self.encoding = 'utf-8'
+            self.encoding = "utf-8"
         self.xg = XMLGenerator(file, encoding=self.encoding)
 
     def _beautify_newline(self, new_item=False):
         if self.indent is not None and (self.indent > 0 or new_item):
-            self.xg.characters('\n')
+            self.xg.characters("\n")
 
     def _beautify_indent(self, depth=1):
         if self.indent:
-            self.xg.characters(' ' * self.indent * depth)
+            self.xg.characters(" " * self.indent * depth)
 
     def start_exporting(self):
         self.xg.startDocument()
         self.xg.startElement(self.root_element, {})
         self._beautify_newline(new_item=True)
 
     def export_item(self, item):
         self._beautify_indent(depth=1)
         self.xg.startElement(self.item_element, {})
         self._beautify_newline()
-        for name, value in self._get_serialized_fields(item, default_value=''):
+        for name, value in self._get_serialized_fields(item, default_value=""):
             self._export_xml_field(name, value, depth=2)
         self._beautify_indent(depth=1)
         self.xg.endElement(self.item_element)
         self._beautify_newline(new_item=True)
 
     def finish_exporting(self):
         self.xg.endElement(self.root_element)
         self.xg.endDocument()
 
     def _export_xml_field(self, name, serialized_value, depth):
         self._beautify_indent(depth=depth)
         self.xg.startElement(name, {})
-        if hasattr(serialized_value, 'items'):
+        if hasattr(serialized_value, "items"):
             self._beautify_newline()
             for subname, value in serialized_value.items():
                 self._export_xml_field(subname, value, depth=depth + 1)
             self._beautify_indent(depth=depth)
         elif is_listlike(serialized_value):
             self._beautify_newline()
             for value in serialized_value:
-                self._export_xml_field('value', value, depth=depth + 1)
+                self._export_xml_field("value", value, depth=depth + 1)
             self._beautify_indent(depth=depth)
         elif isinstance(serialized_value, str):
             self.xg.characters(serialized_value)
         else:
             self.xg.characters(str(serialized_value))
         self.xg.endElement(name)
         self._beautify_newline()
 
 
 class CsvItemExporter(BaseItemExporter):
-
-    def __init__(self, file, include_headers_line=True, join_multivalued=',', errors=None, **kwargs):
+    def __init__(
+        self,
+        file,
+        include_headers_line=True,
+        join_multivalued=",",
+        errors=None,
+        **kwargs,
+    ):
         super().__init__(dont_fail=True, **kwargs)
         if not self.encoding:
-            self.encoding = 'utf-8'
+            self.encoding = "utf-8"
         self.include_headers_line = include_headers_line
         self.stream = io.TextIOWrapper(
             file,
             line_buffering=False,
             write_through=True,
             encoding=self.encoding,
-            newline='',  # Windows needs this https://github.com/scrapy/scrapy/issues/3034
+            newline="",  # Windows needs this https://github.com/scrapy/scrapy/issues/3034
             errors=errors,
         )
         self.csv_writer = csv.writer(self.stream, **self._kwargs)
         self._headers_not_written = True
         self._join_multivalued = join_multivalued
 
     def serialize_field(self, field, name, value):
-        serializer = field.get('serializer', self._join_if_needed)
+        serializer = field.get("serializer", self._join_if_needed)
         return serializer(value)
 
     def _join_if_needed(self, value):
         if isinstance(value, (list, tuple)):
             try:
                 return self._join_multivalued.join(value)
             except TypeError:  # list in value may not contain strings
@@ -238,16 +247,15 @@
         return value
 
     def export_item(self, item):
         if self._headers_not_written:
             self._headers_not_written = False
             self._write_headers_and_set_fields_to_export(item)
 
-        fields = self._get_serialized_fields(item, default_value='',
-                                             include_empty=True)
+        fields = self._get_serialized_fields(item, default_value="", include_empty=True)
         values = list(self._build_row(x for _, x in fields))
         self.csv_writer.writerow(values)
 
     def _build_row(self, values):
         for s in values:
             try:
                 yield to_unicode(s, self.encoding)
@@ -264,15 +272,14 @@
             else:
                 fields = self.fields_to_export
             row = list(self._build_row(fields))
             self.csv_writer.writerow(row)
 
 
 class PickleItemExporter(BaseItemExporter):
-
     def __init__(self, file, protocol=4, **kwargs):
         super().__init__(**kwargs)
         self.file = file
         self.protocol = protocol
 
     def export_item(self, item):
         d = dict(self._get_serialized_fields(item))
@@ -293,54 +300,54 @@
         self.file = file
 
     def export_item(self, item):
         marshal.dump(dict(self._get_serialized_fields(item)), self.file)
 
 
 class PprintItemExporter(BaseItemExporter):
-
     def __init__(self, file, **kwargs):
         super().__init__(**kwargs)
         self.file = file
 
     def export_item(self, item):
         itemdict = dict(self._get_serialized_fields(item))
-        self.file.write(to_bytes(pprint.pformat(itemdict) + '\n'))
+        self.file.write(to_bytes(pprint.pformat(itemdict) + "\n"))
 
 
 class PythonItemExporter(BaseItemExporter):
     """This is a base class for item exporters that extends
     :class:`BaseItemExporter` with support for nested items.
 
     It serializes items to built-in Python types, so that any serialization
     library (e.g. :mod:`json` or msgpack_) can be used on top of it.
 
     .. _msgpack: https://pypi.org/project/msgpack/
     """
 
     def _configure(self, options, dont_fail=False):
-        self.binary = options.pop('binary', True)
+        self.binary = options.pop("binary", True)
         super()._configure(options, dont_fail)
         if self.binary:
             warnings.warn(
                 "PythonItemExporter will drop support for binary export in the future",
-                ScrapyDeprecationWarning)
+                ScrapyDeprecationWarning,
+            )
         if not self.encoding:
-            self.encoding = 'utf-8'
+            self.encoding = "utf-8"
 
     def serialize_field(self, field, name, value):
-        serializer = field.get('serializer', self._serialize_value)
+        serializer = field.get("serializer", self._serialize_value)
         return serializer(value)
 
     def _serialize_value(self, value):
         if isinstance(value, Item):
             return self.export_item(value)
-        elif is_item(value):
+        if is_item(value):
             return dict(self._serialize_item(value))
-        elif is_listlike(value):
+        if is_listlike(value):
             return [self._serialize_value(v) for v in value]
         encode_func = to_bytes if self.binary else to_unicode
         if isinstance(value, (str, bytes)):
             return encode_func(value, encoding=self.encoding)
         return value
 
     def _serialize_item(self, item):
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/closespider.py` & `Scrapy-2.8.0/scrapy/extensions/closespider.py`

 * *Files 18% similar despite different names*

```diff
@@ -7,62 +7,65 @@
 from collections import defaultdict
 
 from scrapy import signals
 from scrapy.exceptions import NotConfigured
 
 
 class CloseSpider:
-
     def __init__(self, crawler):
         self.crawler = crawler
 
         self.close_on = {
-            'timeout': crawler.settings.getfloat('CLOSESPIDER_TIMEOUT'),
-            'itemcount': crawler.settings.getint('CLOSESPIDER_ITEMCOUNT'),
-            'pagecount': crawler.settings.getint('CLOSESPIDER_PAGECOUNT'),
-            'errorcount': crawler.settings.getint('CLOSESPIDER_ERRORCOUNT'),
+            "timeout": crawler.settings.getfloat("CLOSESPIDER_TIMEOUT"),
+            "itemcount": crawler.settings.getint("CLOSESPIDER_ITEMCOUNT"),
+            "pagecount": crawler.settings.getint("CLOSESPIDER_PAGECOUNT"),
+            "errorcount": crawler.settings.getint("CLOSESPIDER_ERRORCOUNT"),
         }
 
         if not any(self.close_on.values()):
             raise NotConfigured
 
         self.counter = defaultdict(int)
 
-        if self.close_on.get('errorcount'):
+        if self.close_on.get("errorcount"):
             crawler.signals.connect(self.error_count, signal=signals.spider_error)
-        if self.close_on.get('pagecount'):
+        if self.close_on.get("pagecount"):
             crawler.signals.connect(self.page_count, signal=signals.response_received)
-        if self.close_on.get('timeout'):
+        if self.close_on.get("timeout"):
             crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)
-        if self.close_on.get('itemcount'):
+        if self.close_on.get("itemcount"):
             crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)
         crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler)
 
     def error_count(self, failure, response, spider):
-        self.counter['errorcount'] += 1
-        if self.counter['errorcount'] == self.close_on['errorcount']:
-            self.crawler.engine.close_spider(spider, 'closespider_errorcount')
+        self.counter["errorcount"] += 1
+        if self.counter["errorcount"] == self.close_on["errorcount"]:
+            self.crawler.engine.close_spider(spider, "closespider_errorcount")
 
     def page_count(self, response, request, spider):
-        self.counter['pagecount'] += 1
-        if self.counter['pagecount'] == self.close_on['pagecount']:
-            self.crawler.engine.close_spider(spider, 'closespider_pagecount')
+        self.counter["pagecount"] += 1
+        if self.counter["pagecount"] == self.close_on["pagecount"]:
+            self.crawler.engine.close_spider(spider, "closespider_pagecount")
 
     def spider_opened(self, spider):
         from twisted.internet import reactor
-        self.task = reactor.callLater(self.close_on['timeout'],
-                                      self.crawler.engine.close_spider, spider,
-                                      reason='closespider_timeout')
+
+        self.task = reactor.callLater(
+            self.close_on["timeout"],
+            self.crawler.engine.close_spider,
+            spider,
+            reason="closespider_timeout",
+        )
 
     def item_scraped(self, item, spider):
-        self.counter['itemcount'] += 1
-        if self.counter['itemcount'] == self.close_on['itemcount']:
-            self.crawler.engine.close_spider(spider, 'closespider_itemcount')
+        self.counter["itemcount"] += 1
+        if self.counter["itemcount"] == self.close_on["itemcount"]:
+            self.crawler.engine.close_spider(spider, "closespider_itemcount")
 
     def spider_closed(self, spider):
-        task = getattr(self, 'task', False)
+        task = getattr(self, "task", False)
         if task and task.active():
             task.cancel()
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/corestats.py` & `Scrapy-2.8.0/scrapy/extensions/corestats.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 """
 from datetime import datetime
 
 from scrapy import signals
 
 
 class CoreStats:
-
     def __init__(self, stats):
         self.stats = stats
         self.start_time = None
 
     @classmethod
     def from_crawler(cls, crawler):
         o = cls(crawler.stats)
@@ -20,27 +19,29 @@
         crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)
         crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)
         crawler.signals.connect(o.response_received, signal=signals.response_received)
         return o
 
     def spider_opened(self, spider):
         self.start_time = datetime.utcnow()
-        self.stats.set_value('start_time', self.start_time, spider=spider)
+        self.stats.set_value("start_time", self.start_time, spider=spider)
 
     def spider_closed(self, spider, reason):
         finish_time = datetime.utcnow()
         elapsed_time = finish_time - self.start_time
         elapsed_time_seconds = elapsed_time.total_seconds()
-        self.stats.set_value('elapsed_time_seconds', elapsed_time_seconds, spider=spider)
-        self.stats.set_value('finish_time', finish_time, spider=spider)
-        self.stats.set_value('finish_reason', reason, spider=spider)
+        self.stats.set_value(
+            "elapsed_time_seconds", elapsed_time_seconds, spider=spider
+        )
+        self.stats.set_value("finish_time", finish_time, spider=spider)
+        self.stats.set_value("finish_reason", reason, spider=spider)
 
     def item_scraped(self, item, spider):
-        self.stats.inc_value('item_scraped_count', spider=spider)
+        self.stats.inc_value("item_scraped_count", spider=spider)
 
     def response_received(self, spider):
-        self.stats.inc_value('response_received_count', spider=spider)
+        self.stats.inc_value("response_received_count", spider=spider)
 
     def item_dropped(self, item, spider, exception):
         reason = exception.__class__.__name__
-        self.stats.inc_value('item_dropped_count', spider=spider)
-        self.stats.inc_value(f'item_dropped_reasons_count/{reason}', spider=spider)
+        self.stats.inc_value("item_dropped_count", spider=spider)
+        self.stats.inc_value(f"item_dropped_reasons_count/{reason}", spider=spider)
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/debug.py` & `Scrapy-2.8.0/scrapy/extensions/debug.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,28 +1,27 @@
 """
 Extensions for debugging Scrapy
 
 See documentation in docs/topics/extensions.rst
 """
 
-import sys
-import signal
 import logging
-import traceback
+import signal
+import sys
 import threading
+import traceback
 from pdb import Pdb
 
 from scrapy.utils.engine import format_engine_status
 from scrapy.utils.trackref import format_live_refs
 
 logger = logging.getLogger(__name__)
 
 
 class StackTraceDump:
-
     def __init__(self, crawler=None):
         self.crawler = crawler
         try:
             signal.signal(signal.SIGUSR2, self.dump_stacktrace)
             signal.signal(signal.SIGQUIT, self.dump_stacktrace)
         except AttributeError:
             # win32 platforms don't support SIGUSR signals
@@ -30,28 +29,31 @@
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler)
 
     def dump_stacktrace(self, signum, frame):
         log_args = {
-            'stackdumps': self._thread_stacks(),
-            'enginestatus': format_engine_status(self.crawler.engine),
-            'liverefs': format_live_refs(),
+            "stackdumps": self._thread_stacks(),
+            "enginestatus": format_engine_status(self.crawler.engine),
+            "liverefs": format_live_refs(),
         }
-        logger.info("Dumping stack trace and engine status\n"
-                    "%(enginestatus)s\n%(liverefs)s\n%(stackdumps)s",
-                    log_args, extra={'crawler': self.crawler})
+        logger.info(
+            "Dumping stack trace and engine status\n"
+            "%(enginestatus)s\n%(liverefs)s\n%(stackdumps)s",
+            log_args,
+            extra={"crawler": self.crawler},
+        )
 
     def _thread_stacks(self):
         id2name = dict((th.ident, th.name) for th in threading.enumerate())
-        dumps = ''
+        dumps = ""
         for id_, frame in sys._current_frames().items():
-            name = id2name.get(id_, '')
-            dump = ''.join(traceback.format_stack(frame))
+            name = id2name.get(id_, "")
+            dump = "".join(traceback.format_stack(frame))
             dumps += f"# Thread: {name}({id_})\n{dump}\n"
         return dumps
 
 
 class Debugger:
     def __init__(self):
         try:
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/feedexport.py` & `Scrapy-2.8.0/scrapy/extensions/feedexport.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,72 +1,73 @@
 """
 Feed Exports extension
 
 See documentation in docs/topics/feed-exports.rst
 """
 
 import logging
-import os
 import re
 import sys
 import warnings
 from datetime import datetime
+from pathlib import Path
 from tempfile import NamedTemporaryFile
-from typing import Any, Callable, Optional, Tuple, Union
+from typing import IO, Any, Callable, Optional, Tuple, Union
 from urllib.parse import unquote, urlparse
 
 from twisted.internet import defer, threads
 from w3lib.url import file_uri_to_path
-from zope.interface import implementer, Interface
+from zope.interface import Interface, implementer
 
-from scrapy import signals, Spider
+from scrapy import Spider, signals
 from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning
 from scrapy.extensions.postprocessing import PostProcessingManager
 from scrapy.utils.boto import is_botocore_available
 from scrapy.utils.conf import feed_complete_default_values_from_settings
 from scrapy.utils.ftp import ftp_store_file
 from scrapy.utils.log import failure_to_exc_info
 from scrapy.utils.misc import create_instance, load_object
 from scrapy.utils.python import get_func_args, without_none_values
 
-
 logger = logging.getLogger(__name__)
 
 
 def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):
     argument_names = get_func_args(builder)
-    if 'feed_options' in argument_names:
-        kwargs['feed_options'] = feed_options
+    if "feed_options" in argument_names:
+        kwargs["feed_options"] = feed_options
     else:
         warnings.warn(
             f"{builder.__qualname__} does not support the 'feed_options' keyword argument. Add a "
             "'feed_options' parameter to its signature to remove this "
             "warning. This parameter will become mandatory in a future "
             "version of Scrapy.",
-            category=ScrapyDeprecationWarning
+            category=ScrapyDeprecationWarning,
         )
     return builder(*preargs, uri, *args, **kwargs)
 
 
 class ItemFilter:
     """
     This will be used by FeedExporter to decide if an item should be allowed
     to be exported to a particular feed.
 
     :param feed_options: feed specific options passed from FeedExporter
     :type feed_options: dict
     """
+
     feed_options: Optional[dict]
     item_classes: Tuple
 
     def __init__(self, feed_options: Optional[dict]) -> None:
         self.feed_options = feed_options
         if feed_options is not None:
             self.item_classes = tuple(
-                load_object(item_class) for item_class in feed_options.get("item_classes") or ()
+                load_object(item_class)
+                for item_class in feed_options.get("item_classes") or ()
             )
         else:
             self.item_classes = tuple()
 
     def accepts(self, item: Any) -> bool:
         """
         Return ``True`` if `item` should be exported or ``False`` otherwise.
@@ -94,173 +95,199 @@
 
     def store(file):
         """Store the given file stream"""
 
 
 @implementer(IFeedStorage)
 class BlockingFeedStorage:
-
     def open(self, spider):
-        path = spider.crawler.settings['FEED_TEMPDIR']
-        if path and not os.path.isdir(path):
-            raise OSError('Not a Directory: ' + str(path))
+        path = spider.crawler.settings["FEED_TEMPDIR"]
+        if path and not Path(path).is_dir():
+            raise OSError("Not a Directory: " + str(path))
 
-        return NamedTemporaryFile(prefix='feed-', dir=path)
+        return NamedTemporaryFile(prefix="feed-", dir=path)
 
     def store(self, file):
         return threads.deferToThread(self._store_in_thread, file)
 
     def _store_in_thread(self, file):
         raise NotImplementedError
 
 
 @implementer(IFeedStorage)
 class StdoutFeedStorage:
-
     def __init__(self, uri, _stdout=None, *, feed_options=None):
         if not _stdout:
             _stdout = sys.stdout.buffer
         self._stdout = _stdout
-        if feed_options and feed_options.get('overwrite', False) is True:
-            logger.warning('Standard output (stdout) storage does not support '
-                           'overwriting. To suppress this warning, remove the '
-                           'overwrite option from your FEEDS setting, or set '
-                           'it to False.')
+        if feed_options and feed_options.get("overwrite", False) is True:
+            logger.warning(
+                "Standard output (stdout) storage does not support "
+                "overwriting. To suppress this warning, remove the "
+                "overwrite option from your FEEDS setting, or set "
+                "it to False."
+            )
 
     def open(self, spider):
         return self._stdout
 
     def store(self, file):
         pass
 
 
 @implementer(IFeedStorage)
 class FileFeedStorage:
-
     def __init__(self, uri, *, feed_options=None):
         self.path = file_uri_to_path(uri)
         feed_options = feed_options or {}
-        self.write_mode = 'wb' if feed_options.get('overwrite', False) else 'ab'
+        self.write_mode = "wb" if feed_options.get("overwrite", False) else "ab"
 
-    def open(self, spider):
-        dirname = os.path.dirname(self.path)
-        if dirname and not os.path.exists(dirname):
-            os.makedirs(dirname)
-        return open(self.path, self.write_mode)
+    def open(self, spider) -> IO[Any]:
+        dirname = Path(self.path).parent
+        if dirname and not dirname.exists():
+            dirname.mkdir(parents=True)
+        return Path(self.path).open(self.write_mode)
 
     def store(self, file):
         file.close()
 
 
 class S3FeedStorage(BlockingFeedStorage):
-
-    def __init__(self, uri, access_key=None, secret_key=None, acl=None, endpoint_url=None, *,
-                 feed_options=None, session_token=None):
+    def __init__(
+        self,
+        uri,
+        access_key=None,
+        secret_key=None,
+        acl=None,
+        endpoint_url=None,
+        *,
+        feed_options=None,
+        session_token=None,
+    ):
         if not is_botocore_available():
-            raise NotConfigured('missing botocore library')
+            raise NotConfigured("missing botocore library")
         u = urlparse(uri)
         self.bucketname = u.hostname
         self.access_key = u.username or access_key
         self.secret_key = u.password or secret_key
         self.session_token = session_token
         self.keyname = u.path[1:]  # remove first "/"
         self.acl = acl
         self.endpoint_url = endpoint_url
         import botocore.session
+
         session = botocore.session.get_session()
         self.s3_client = session.create_client(
-            's3', aws_access_key_id=self.access_key,
+            "s3",
+            aws_access_key_id=self.access_key,
             aws_secret_access_key=self.secret_key,
             aws_session_token=self.session_token,
-            endpoint_url=self.endpoint_url)
-        if feed_options and feed_options.get('overwrite', True) is False:
-            logger.warning('S3 does not support appending to files. To '
-                           'suppress this warning, remove the overwrite '
-                           'option from your FEEDS setting or set it to True.')
+            endpoint_url=self.endpoint_url,
+        )
+        if feed_options and feed_options.get("overwrite", True) is False:
+            logger.warning(
+                "S3 does not support appending to files. To "
+                "suppress this warning, remove the overwrite "
+                "option from your FEEDS setting or set it to True."
+            )
 
     @classmethod
     def from_crawler(cls, crawler, uri, *, feed_options=None):
         return build_storage(
             cls,
             uri,
-            access_key=crawler.settings['AWS_ACCESS_KEY_ID'],
-            secret_key=crawler.settings['AWS_SECRET_ACCESS_KEY'],
-            session_token=crawler.settings['AWS_SESSION_TOKEN'],
-            acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None,
-            endpoint_url=crawler.settings['AWS_ENDPOINT_URL'] or None,
+            access_key=crawler.settings["AWS_ACCESS_KEY_ID"],
+            secret_key=crawler.settings["AWS_SECRET_ACCESS_KEY"],
+            session_token=crawler.settings["AWS_SESSION_TOKEN"],
+            acl=crawler.settings["FEED_STORAGE_S3_ACL"] or None,
+            endpoint_url=crawler.settings["AWS_ENDPOINT_URL"] or None,
             feed_options=feed_options,
         )
 
     def _store_in_thread(self, file):
         file.seek(0)
-        kwargs = {'ACL': self.acl} if self.acl else {}
+        kwargs = {"ACL": self.acl} if self.acl else {}
         self.s3_client.put_object(
-            Bucket=self.bucketname, Key=self.keyname, Body=file,
-            **kwargs)
+            Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs
+        )
         file.close()
 
 
 class GCSFeedStorage(BlockingFeedStorage):
-
     def __init__(self, uri, project_id, acl):
         self.project_id = project_id
         self.acl = acl
         u = urlparse(uri)
         self.bucket_name = u.hostname
         self.blob_name = u.path[1:]  # remove first "/"
 
     @classmethod
     def from_crawler(cls, crawler, uri):
         return cls(
             uri,
-            crawler.settings['GCS_PROJECT_ID'],
-            crawler.settings['FEED_STORAGE_GCS_ACL'] or None
+            crawler.settings["GCS_PROJECT_ID"],
+            crawler.settings["FEED_STORAGE_GCS_ACL"] or None,
         )
 
     def _store_in_thread(self, file):
         file.seek(0)
         from google.cloud.storage import Client
+
         client = Client(project=self.project_id)
         bucket = client.get_bucket(self.bucket_name)
         blob = bucket.blob(self.blob_name)
         blob.upload_from_file(file, predefined_acl=self.acl)
 
 
 class FTPFeedStorage(BlockingFeedStorage):
-
     def __init__(self, uri, use_active_mode=False, *, feed_options=None):
         u = urlparse(uri)
         self.host = u.hostname
-        self.port = int(u.port or '21')
+        self.port = int(u.port or "21")
         self.username = u.username
-        self.password = unquote(u.password or '')
+        self.password = unquote(u.password or "")
         self.path = u.path
         self.use_active_mode = use_active_mode
-        self.overwrite = not feed_options or feed_options.get('overwrite', True)
+        self.overwrite = not feed_options or feed_options.get("overwrite", True)
 
     @classmethod
     def from_crawler(cls, crawler, uri, *, feed_options=None):
         return build_storage(
             cls,
             uri,
-            crawler.settings.getbool('FEED_STORAGE_FTP_ACTIVE'),
+            crawler.settings.getbool("FEED_STORAGE_FTP_ACTIVE"),
             feed_options=feed_options,
         )
 
     def _store_in_thread(self, file):
         ftp_store_file(
-            path=self.path, file=file, host=self.host,
-            port=self.port, username=self.username,
-            password=self.password, use_active_mode=self.use_active_mode,
+            path=self.path,
+            file=file,
+            host=self.host,
+            port=self.port,
+            username=self.username,
+            password=self.password,
+            use_active_mode=self.use_active_mode,
             overwrite=self.overwrite,
         )
 
 
 class _FeedSlot:
-    def __init__(self, file, exporter, storage, uri, format, store_empty, batch_id, uri_template, filter):
+    def __init__(
+        self,
+        file,
+        exporter,
+        storage,
+        uri,
+        format,
+        store_empty,
+        batch_id,
+        uri_template,
+        filter,
+    ):
         self.file = file
         self.exporter = exporter
         self.storage = storage
         # feed params
         self.batch_id = batch_id
         self.format = format
         self.store_empty = store_empty
@@ -279,15 +306,14 @@
     def finish_exporting(self):
         if self._exporting:
             self.exporter.finish_exporting()
             self._exporting = False
 
 
 class FeedExporter:
-
     @classmethod
     def from_crawler(cls, crawler):
         exporter = cls(crawler)
         crawler.signals.connect(exporter.open_spider, signals.spider_opened)
         crawler.signals.connect(exporter.close_spider, signals.spider_closed)
         crawler.signals.connect(exporter.item_scraped, signals.item_scraped)
         return exporter
@@ -295,93 +321,99 @@
     def __init__(self, crawler):
         self.crawler = crawler
         self.settings = crawler.settings
         self.feeds = {}
         self.slots = []
         self.filters = {}
 
-        if not self.settings['FEEDS'] and not self.settings['FEED_URI']:
+        if not self.settings["FEEDS"] and not self.settings["FEED_URI"]:
             raise NotConfigured
 
         # Begin: Backward compatibility for FEED_URI and FEED_FORMAT settings
-        if self.settings['FEED_URI']:
+        if self.settings["FEED_URI"]:
             warnings.warn(
-                'The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of '
-                'the `FEEDS` setting. Please see the `FEEDS` setting docs for more details',
-                category=ScrapyDeprecationWarning, stacklevel=2,
+                "The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of "
+                "the `FEEDS` setting. Please see the `FEEDS` setting docs for more details",
+                category=ScrapyDeprecationWarning,
+                stacklevel=2,
+            )
+            uri = str(self.settings["FEED_URI"])  # handle pathlib.Path objects
+            feed_options = {"format": self.settings.get("FEED_FORMAT", "jsonlines")}
+            self.feeds[uri] = feed_complete_default_values_from_settings(
+                feed_options, self.settings
             )
-            uri = str(self.settings['FEED_URI'])  # handle pathlib.Path objects
-            feed_options = {'format': self.settings.get('FEED_FORMAT', 'jsonlines')}
-            self.feeds[uri] = feed_complete_default_values_from_settings(feed_options, self.settings)
             self.filters[uri] = self._load_filter(feed_options)
         # End: Backward compatibility for FEED_URI and FEED_FORMAT settings
 
         # 'FEEDS' setting takes precedence over 'FEED_URI'
-        for uri, feed_options in self.settings.getdict('FEEDS').items():
+        for uri, feed_options in self.settings.getdict("FEEDS").items():
             uri = str(uri)  # handle pathlib.Path objects
-            self.feeds[uri] = feed_complete_default_values_from_settings(feed_options, self.settings)
+            self.feeds[uri] = feed_complete_default_values_from_settings(
+                feed_options, self.settings
+            )
             self.filters[uri] = self._load_filter(feed_options)
 
-        self.storages = self._load_components('FEED_STORAGES')
-        self.exporters = self._load_components('FEED_EXPORTERS')
+        self.storages = self._load_components("FEED_STORAGES")
+        self.exporters = self._load_components("FEED_EXPORTERS")
         for uri, feed_options in self.feeds.items():
             if not self._storage_supported(uri, feed_options):
                 raise NotConfigured
             if not self._settings_are_valid():
                 raise NotConfigured
-            if not self._exporter_supported(feed_options['format']):
+            if not self._exporter_supported(feed_options["format"]):
                 raise NotConfigured
 
     def open_spider(self, spider):
         for uri, feed_options in self.feeds.items():
-            uri_params = self._get_uri_params(spider, feed_options['uri_params'])
-            self.slots.append(self._start_new_batch(
-                batch_id=1,
-                uri=uri % uri_params,
-                feed_options=feed_options,
-                spider=spider,
-                uri_template=uri,
-            ))
+            uri_params = self._get_uri_params(spider, feed_options["uri_params"])
+            self.slots.append(
+                self._start_new_batch(
+                    batch_id=1,
+                    uri=uri % uri_params,
+                    feed_options=feed_options,
+                    spider=spider,
+                    uri_template=uri,
+                )
+            )
 
     def close_spider(self, spider):
         deferred_list = []
         for slot in self.slots:
             d = self._close_slot(slot, spider)
             deferred_list.append(d)
         return defer.DeferredList(deferred_list) if deferred_list else None
 
     def _close_slot(self, slot, spider):
+        slot.finish_exporting()
         if not slot.itemcount and not slot.store_empty:
             # We need to call slot.storage.store nonetheless to get the file
             # properly closed.
             return defer.maybeDeferred(slot.storage.store, slot.file)
-        slot.finish_exporting()
         logmsg = f"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}"
         d = defer.maybeDeferred(slot.storage.store, slot.file)
 
         d.addCallback(
             self._handle_store_success, logmsg, spider, type(slot.storage).__name__
         )
         d.addErrback(
             self._handle_store_error, logmsg, spider, type(slot.storage).__name__
         )
         return d
 
     def _handle_store_error(self, f, logmsg, spider, slot_type):
         logger.error(
-            "Error storing %s", logmsg,
-            exc_info=failure_to_exc_info(f), extra={'spider': spider}
+            "Error storing %s",
+            logmsg,
+            exc_info=failure_to_exc_info(f),
+            extra={"spider": spider},
         )
         self.crawler.stats.inc_value(f"feedexport/failed_count/{slot_type}")
 
     def _handle_store_success(self, f, logmsg, spider, slot_type):
-        logger.info(
-            "Stored %s", logmsg,
-            extra={'spider': spider}
-        )
+        logger.info("Stored %s", logmsg, extra={"spider": spider})
         self.crawler.stats.inc_value(f"feedexport/success_count/{slot_type}")
 
     def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):
         """
         Redirect the output data stream to a new file.
         Execute multiple times if FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified
         :param batch_id: sequence number of current batch
@@ -389,63 +421,71 @@
         :param feed_options: dict with parameters of feed
         :param spider: user spider
         :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri
         """
         storage = self._get_storage(uri, feed_options)
         file = storage.open(spider)
         if "postprocessing" in feed_options:
-            file = PostProcessingManager(feed_options["postprocessing"], file, feed_options)
+            file = PostProcessingManager(
+                feed_options["postprocessing"], file, feed_options
+            )
 
         exporter = self._get_exporter(
             file=file,
-            format=feed_options['format'],
-            fields_to_export=feed_options['fields'],
-            encoding=feed_options['encoding'],
-            indent=feed_options['indent'],
-            **feed_options['item_export_kwargs'],
+            format=feed_options["format"],
+            fields_to_export=feed_options["fields"],
+            encoding=feed_options["encoding"],
+            indent=feed_options["indent"],
+            **feed_options["item_export_kwargs"],
         )
         slot = _FeedSlot(
             file=file,
             exporter=exporter,
             storage=storage,
             uri=uri,
-            format=feed_options['format'],
-            store_empty=feed_options['store_empty'],
+            format=feed_options["format"],
+            store_empty=feed_options["store_empty"],
             batch_id=batch_id,
             uri_template=uri_template,
-            filter=self.filters[uri_template]
+            filter=self.filters[uri_template],
         )
         if slot.store_empty:
             slot.start_exporting()
         return slot
 
     def item_scraped(self, item, spider):
         slots = []
         for slot in self.slots:
             if not slot.filter.accepts(item):
-                slots.append(slot)    # if slot doesn't accept item, continue with next slot
+                slots.append(
+                    slot
+                )  # if slot doesn't accept item, continue with next slot
                 continue
 
             slot.start_exporting()
             slot.exporter.export_item(item)
             slot.itemcount += 1
             # create new slot for each slot with itemcount == FEED_EXPORT_BATCH_ITEM_COUNT and close the old one
             if (
-                self.feeds[slot.uri_template]['batch_item_count']
-                and slot.itemcount >= self.feeds[slot.uri_template]['batch_item_count']
+                self.feeds[slot.uri_template]["batch_item_count"]
+                and slot.itemcount >= self.feeds[slot.uri_template]["batch_item_count"]
             ):
-                uri_params = self._get_uri_params(spider, self.feeds[slot.uri_template]['uri_params'], slot)
+                uri_params = self._get_uri_params(
+                    spider, self.feeds[slot.uri_template]["uri_params"], slot
+                )
                 self._close_slot(slot, spider)
-                slots.append(self._start_new_batch(
-                    batch_id=slot.batch_id + 1,
-                    uri=slot.uri_template % uri_params,
-                    feed_options=self.feeds[slot.uri_template],
-                    spider=spider,
-                    uri_template=slot.uri_template,
-                ))
+                slots.append(
+                    self._start_new_batch(
+                        batch_id=slot.batch_id + 1,
+                        uri=slot.uri_template % uri_params,
+                        feed_options=self.feeds[slot.uri_template],
+                        spider=spider,
+                        uri_template=slot.uri_template,
+                    )
+                )
             else:
                 slots.append(slot)
         self.slots = slots
 
     def _load_components(self, setting_prefix):
         conf = without_none_values(self.settings.getwithbase(setting_prefix))
         d = {}
@@ -455,102 +495,110 @@
             except NotConfigured:
                 pass
         return d
 
     def _exporter_supported(self, format):
         if format in self.exporters:
             return True
-        logger.error("Unknown feed format: %(format)s", {'format': format})
+        logger.error("Unknown feed format: %(format)s", {"format": format})
 
     def _settings_are_valid(self):
         """
         If FEED_EXPORT_BATCH_ITEM_COUNT setting or FEEDS.batch_item_count is specified uri has to contain
         %(batch_time)s or %(batch_id)d to distinguish different files of partial output
         """
         for uri_template, values in self.feeds.items():
-            if values['batch_item_count'] and not re.search(r'%\(batch_time\)s|%\(batch_id\)', uri_template):
+            if values["batch_item_count"] and not re.search(
+                r"%\(batch_time\)s|%\(batch_id\)", uri_template
+            ):
                 logger.error(
-                    '%%(batch_time)s or %%(batch_id)d must be in the feed URI (%s) if FEED_EXPORT_BATCH_ITEM_COUNT '
-                    'setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: '
-                    'https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count',
-                    uri_template
+                    "%%(batch_time)s or %%(batch_id)d must be in the feed URI (%s) if FEED_EXPORT_BATCH_ITEM_COUNT "
+                    "setting or FEEDS.batch_item_count is specified and greater than 0. For more info see: "
+                    "https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-export-batch-item-count",
+                    uri_template,
                 )
                 return False
         return True
 
     def _storage_supported(self, uri, feed_options):
         scheme = urlparse(uri).scheme
         if scheme in self.storages:
             try:
                 self._get_storage(uri, feed_options)
                 return True
             except NotConfigured as e:
-                logger.error("Disabled feed storage scheme: %(scheme)s. "
-                             "Reason: %(reason)s",
-                             {'scheme': scheme, 'reason': str(e)})
+                logger.error(
+                    "Disabled feed storage scheme: %(scheme)s. " "Reason: %(reason)s",
+                    {"scheme": scheme, "reason": str(e)},
+                )
         else:
-            logger.error("Unknown feed storage scheme: %(scheme)s",
-                         {'scheme': scheme})
+            logger.error("Unknown feed storage scheme: %(scheme)s", {"scheme": scheme})
 
     def _get_instance(self, objcls, *args, **kwargs):
         return create_instance(
-            objcls, self.settings, getattr(self, 'crawler', None),
-            *args, **kwargs)
+            objcls, self.settings, getattr(self, "crawler", None), *args, **kwargs
+        )
 
     def _get_exporter(self, file, format, *args, **kwargs):
         return self._get_instance(self.exporters[format], file, *args, **kwargs)
 
     def _get_storage(self, uri, feed_options):
         """Fork of create_instance specific to feed storage classes
 
         It supports not passing the *feed_options* parameters to classes that
         do not support it, and issuing a deprecation warning instead.
         """
         feedcls = self.storages[urlparse(uri).scheme]
-        crawler = getattr(self, 'crawler', None)
+        crawler = getattr(self, "crawler", None)
 
         def build_instance(builder, *preargs):
-            return build_storage(builder, uri, feed_options=feed_options, preargs=preargs)
+            return build_storage(
+                builder, uri, feed_options=feed_options, preargs=preargs
+            )
 
-        if crawler and hasattr(feedcls, 'from_crawler'):
+        if crawler and hasattr(feedcls, "from_crawler"):
             instance = build_instance(feedcls.from_crawler, crawler)
-            method_name = 'from_crawler'
-        elif hasattr(feedcls, 'from_settings'):
+            method_name = "from_crawler"
+        elif hasattr(feedcls, "from_settings"):
             instance = build_instance(feedcls.from_settings, self.settings)
-            method_name = 'from_settings'
+            method_name = "from_settings"
         else:
             instance = build_instance(feedcls)
-            method_name = '__new__'
+            method_name = "__new__"
         if instance is None:
             raise TypeError(f"{feedcls.__qualname__}.{method_name} returned None")
         return instance
 
     def _get_uri_params(
         self,
         spider: Spider,
         uri_params_function: Optional[Union[str, Callable[[dict, Spider], dict]]],
         slot: Optional[_FeedSlot] = None,
     ) -> dict:
         params = {}
         for k in dir(spider):
             params[k] = getattr(spider, k)
         utc_now = datetime.utcnow()
-        params['time'] = utc_now.replace(microsecond=0).isoformat().replace(':', '-')
-        params['batch_time'] = utc_now.isoformat().replace(':', '-')
-        params['batch_id'] = slot.batch_id + 1 if slot is not None else 1
+        params["time"] = utc_now.replace(microsecond=0).isoformat().replace(":", "-")
+        params["batch_time"] = utc_now.isoformat().replace(":", "-")
+        params["batch_id"] = slot.batch_id + 1 if slot is not None else 1
         original_params = params.copy()
-        uripar_function = load_object(uri_params_function) if uri_params_function else lambda params, _: params
+        uripar_function = (
+            load_object(uri_params_function)
+            if uri_params_function
+            else lambda params, _: params
+        )
         new_params = uripar_function(params, spider)
         if new_params is None or original_params != params:
             warnings.warn(
-                'Modifying the params dictionary in-place in the function defined in '
-                'the FEED_URI_PARAMS setting or in the uri_params key of the FEEDS '
-                'setting is deprecated. The function must return a new dictionary '
-                'instead.',
-                category=ScrapyDeprecationWarning
+                "Modifying the params dictionary in-place in the function defined in "
+                "the FEED_URI_PARAMS setting or in the uri_params key of the FEEDS "
+                "setting is deprecated. The function must return a new dictionary "
+                "instead.",
+                category=ScrapyDeprecationWarning,
             )
         return new_params if new_params is not None else params
 
     def _load_filter(self, feed_options):
         # load the item filter if declared else load the default filter class
         item_filter_class = load_object(feed_options.get("item_filter", ItemFilter))
         return item_filter_class(feed_options)
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/httpcache.py` & `Scrapy-2.8.0/scrapy/extensions/httpcache.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,33 +1,35 @@
 import gzip
 import logging
-import os
 import pickle
 from email.utils import mktime_tz, parsedate_tz
 from importlib import import_module
+from pathlib import Path
 from time import time
 from weakref import WeakKeyDictionary
 
-from w3lib.http import headers_raw_to_dict, headers_dict_to_raw
+from w3lib.http import headers_dict_to_raw, headers_raw_to_dict
 
 from scrapy.http import Headers, Response
+from scrapy.http.request import Request
 from scrapy.responsetypes import responsetypes
+from scrapy.spiders import Spider
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.project import data_path
 from scrapy.utils.python import to_bytes, to_unicode
 
-
 logger = logging.getLogger(__name__)
 
 
 class DummyPolicy:
-
     def __init__(self, settings):
-        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')
-        self.ignore_http_codes = [int(x) for x in settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES')]
+        self.ignore_schemes = settings.getlist("HTTPCACHE_IGNORE_SCHEMES")
+        self.ignore_http_codes = [
+            int(x) for x in settings.getlist("HTTPCACHE_IGNORE_HTTP_CODES")
+        ]
 
     def should_cache_request(self, request):
         return urlparse_cached(request).scheme not in self.ignore_schemes
 
     def should_cache_response(self, response, request):
         return response.status not in self.ignore_http_codes
 
@@ -39,96 +41,98 @@
 
 
 class RFC2616Policy:
 
     MAXAGE = 3600 * 24 * 365  # one year
 
     def __init__(self, settings):
-        self.always_store = settings.getbool('HTTPCACHE_ALWAYS_STORE')
-        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')
+        self.always_store = settings.getbool("HTTPCACHE_ALWAYS_STORE")
+        self.ignore_schemes = settings.getlist("HTTPCACHE_IGNORE_SCHEMES")
         self._cc_parsed = WeakKeyDictionary()
         self.ignore_response_cache_controls = [
-            to_bytes(cc) for cc in settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')
+            to_bytes(cc)
+            for cc in settings.getlist("HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS")
         ]
 
     def _parse_cachecontrol(self, r):
         if r not in self._cc_parsed:
-            cch = r.headers.get(b'Cache-Control', b'')
+            cch = r.headers.get(b"Cache-Control", b"")
             parsed = parse_cachecontrol(cch)
             if isinstance(r, Response):
                 for key in self.ignore_response_cache_controls:
                     parsed.pop(key, None)
             self._cc_parsed[r] = parsed
         return self._cc_parsed[r]
 
     def should_cache_request(self, request):
         if urlparse_cached(request).scheme in self.ignore_schemes:
             return False
         cc = self._parse_cachecontrol(request)
         # obey user-agent directive "Cache-Control: no-store"
-        if b'no-store' in cc:
+        if b"no-store" in cc:
             return False
         # Any other is eligible for caching
         return True
 
     def should_cache_response(self, response, request):
         # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1
         # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4
         # Status code 206 is not included because cache can not deal with partial contents
         cc = self._parse_cachecontrol(response)
         # obey directive "Cache-Control: no-store"
-        if b'no-store' in cc:
+        if b"no-store" in cc:
             return False
         # Never cache 304 (Not Modified) responses
-        elif response.status == 304:
+        if response.status == 304:
             return False
         # Cache unconditionally if configured to do so
-        elif self.always_store:
+        if self.always_store:
             return True
         # Any hint on response expiration is good
-        elif b'max-age' in cc or b'Expires' in response.headers:
+        if b"max-age" in cc or b"Expires" in response.headers:
             return True
         # Firefox fallbacks this statuses to one year expiration if none is set
-        elif response.status in (300, 301, 308):
+        if response.status in (300, 301, 308):
             return True
         # Other statuses without expiration requires at least one validator
-        elif response.status in (200, 203, 401):
-            return b'Last-Modified' in response.headers or b'ETag' in response.headers
+        if response.status in (200, 203, 401):
+            return b"Last-Modified" in response.headers or b"ETag" in response.headers
         # Any other is probably not eligible for caching
         # Makes no sense to cache responses that does not contain expiration
         # info and can not be revalidated
-        else:
-            return False
+        return False
 
     def is_cached_response_fresh(self, cachedresponse, request):
         cc = self._parse_cachecontrol(cachedresponse)
         ccreq = self._parse_cachecontrol(request)
-        if b'no-cache' in cc or b'no-cache' in ccreq:
+        if b"no-cache" in cc or b"no-cache" in ccreq:
             return False
 
         now = time()
-        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)
+        freshnesslifetime = self._compute_freshness_lifetime(
+            cachedresponse, request, now
+        )
         currentage = self._compute_current_age(cachedresponse, request, now)
 
         reqmaxage = self._get_max_age(ccreq)
         if reqmaxage is not None:
             freshnesslifetime = min(freshnesslifetime, reqmaxage)
 
         if currentage < freshnesslifetime:
             return True
 
-        if b'max-stale' in ccreq and b'must-revalidate' not in cc:
+        if b"max-stale" in ccreq and b"must-revalidate" not in cc:
             # From RFC2616: "Indicates that the client is willing to
             # accept a response that has exceeded its expiration time.
             # If max-stale is assigned a value, then the client is
             # willing to accept a response that has exceeded its
             # expiration time by no more than the specified number of
             # seconds. If no value is assigned to max-stale, then the
             # client is willing to accept a stale response of any age."
-            staleage = ccreq[b'max-stale']
+            staleage = ccreq[b"max-stale"]
             if staleage is None:
                 return True
 
             try:
                 if currentage < freshnesslifetime + max(0, int(staleage)):
                     return True
             except ValueError:
@@ -139,212 +143,219 @@
         return False
 
     def is_cached_response_valid(self, cachedresponse, response, request):
         # Use the cached response if the new response is a server error,
         # as long as the old response didn't specify must-revalidate.
         if response.status >= 500:
             cc = self._parse_cachecontrol(cachedresponse)
-            if b'must-revalidate' not in cc:
+            if b"must-revalidate" not in cc:
                 return True
 
         # Use the cached response if the server says it hasn't changed.
         return response.status == 304
 
     def _set_conditional_validators(self, request, cachedresponse):
-        if b'Last-Modified' in cachedresponse.headers:
-            request.headers[b'If-Modified-Since'] = cachedresponse.headers[b'Last-Modified']
+        if b"Last-Modified" in cachedresponse.headers:
+            request.headers[b"If-Modified-Since"] = cachedresponse.headers[
+                b"Last-Modified"
+            ]
 
-        if b'ETag' in cachedresponse.headers:
-            request.headers[b'If-None-Match'] = cachedresponse.headers[b'ETag']
+        if b"ETag" in cachedresponse.headers:
+            request.headers[b"If-None-Match"] = cachedresponse.headers[b"ETag"]
 
     def _get_max_age(self, cc):
         try:
-            return max(0, int(cc[b'max-age']))
+            return max(0, int(cc[b"max-age"]))
         except (KeyError, ValueError):
             return None
 
     def _compute_freshness_lifetime(self, response, request, now):
         # Reference nsHttpResponseHead::ComputeFreshnessLifetime
         # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706
         cc = self._parse_cachecontrol(response)
         maxage = self._get_max_age(cc)
         if maxage is not None:
             return maxage
 
         # Parse date header or synthesize it if none exists
-        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now
+        date = rfc1123_to_epoch(response.headers.get(b"Date")) or now
 
         # Try HTTP/1.0 Expires header
-        if b'Expires' in response.headers:
-            expires = rfc1123_to_epoch(response.headers[b'Expires'])
+        if b"Expires" in response.headers:
+            expires = rfc1123_to_epoch(response.headers[b"Expires"])
             # When parsing Expires header fails RFC 2616 section 14.21 says we
             # should treat this as an expiration time in the past.
             return max(0, expires - date) if expires else 0
 
         # Fallback to heuristic using last-modified header
         # This is not in RFC but on Firefox caching implementation
-        lastmodified = rfc1123_to_epoch(response.headers.get(b'Last-Modified'))
+        lastmodified = rfc1123_to_epoch(response.headers.get(b"Last-Modified"))
         if lastmodified and lastmodified <= date:
             return (date - lastmodified) / 10
 
         # This request can be cached indefinitely
         if response.status in (300, 301, 308):
             return self.MAXAGE
 
-        # Insufficient information to compute fresshness lifetime
+        # Insufficient information to compute freshness lifetime
         return 0
 
     def _compute_current_age(self, response, request, now):
         # Reference nsHttpResponseHead::ComputeCurrentAge
         # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658
         currentage = 0
         # If Date header is not set we assume it is a fast connection, and
         # clock is in sync with the server
-        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now
+        date = rfc1123_to_epoch(response.headers.get(b"Date")) or now
         if now > date:
             currentage = now - date
 
-        if b'Age' in response.headers:
+        if b"Age" in response.headers:
             try:
-                age = int(response.headers[b'Age'])
+                age = int(response.headers[b"Age"])
                 currentage = max(currentage, age)
             except ValueError:
                 pass
 
         return currentage
 
 
 class DbmCacheStorage:
-
     def __init__(self, settings):
-        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)
-        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')
-        self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])
+        self.cachedir = data_path(settings["HTTPCACHE_DIR"], createdir=True)
+        self.expiration_secs = settings.getint("HTTPCACHE_EXPIRATION_SECS")
+        self.dbmodule = import_module(settings["HTTPCACHE_DBM_MODULE"])
         self.db = None
 
-    def open_spider(self, spider):
-        dbpath = os.path.join(self.cachedir, f'{spider.name}.db')
-        self.db = self.dbmodule.open(dbpath, 'c')
-
-        logger.debug("Using DBM cache storage in %(cachepath)s", {'cachepath': dbpath}, extra={'spider': spider})
+    def open_spider(self, spider: Spider):
+        dbpath = Path(self.cachedir, f"{spider.name}.db")
+        self.db = self.dbmodule.open(str(dbpath), "c")
+
+        logger.debug(
+            "Using DBM cache storage in %(cachepath)s",
+            {"cachepath": dbpath},
+            extra={"spider": spider},
+        )
 
         self._fingerprinter = spider.crawler.request_fingerprinter
 
     def close_spider(self, spider):
         self.db.close()
 
     def retrieve_response(self, spider, request):
         data = self._read_data(spider, request)
         if data is None:
             return  # not cached
-        url = data['url']
-        status = data['status']
-        headers = Headers(data['headers'])
-        body = data['body']
+        url = data["url"]
+        status = data["status"]
+        headers = Headers(data["headers"])
+        body = data["body"]
         respcls = responsetypes.from_args(headers=headers, url=url, body=body)
         response = respcls(url=url, headers=headers, status=status, body=body)
         return response
 
     def store_response(self, spider, request, response):
         key = self._fingerprinter.fingerprint(request).hex()
         data = {
-            'status': response.status,
-            'url': response.url,
-            'headers': dict(response.headers),
-            'body': response.body,
+            "status": response.status,
+            "url": response.url,
+            "headers": dict(response.headers),
+            "body": response.body,
         }
-        self.db[f'{key}_data'] = pickle.dumps(data, protocol=4)
-        self.db[f'{key}_time'] = str(time())
+        self.db[f"{key}_data"] = pickle.dumps(data, protocol=4)
+        self.db[f"{key}_time"] = str(time())
 
     def _read_data(self, spider, request):
         key = self._fingerprinter.fingerprint(request).hex()
         db = self.db
-        tkey = f'{key}_time'
+        tkey = f"{key}_time"
         if tkey not in db:
             return  # not found
 
         ts = db[tkey]
         if 0 < self.expiration_secs < time() - float(ts):
             return  # expired
 
-        return pickle.loads(db[f'{key}_data'])
+        return pickle.loads(db[f"{key}_data"])
 
 
 class FilesystemCacheStorage:
-
     def __init__(self, settings):
-        self.cachedir = data_path(settings['HTTPCACHE_DIR'])
-        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')
-        self.use_gzip = settings.getbool('HTTPCACHE_GZIP')
+        self.cachedir = data_path(settings["HTTPCACHE_DIR"])
+        self.expiration_secs = settings.getint("HTTPCACHE_EXPIRATION_SECS")
+        self.use_gzip = settings.getbool("HTTPCACHE_GZIP")
         self._open = gzip.open if self.use_gzip else open
 
-    def open_spider(self, spider):
-        logger.debug("Using filesystem cache storage in %(cachedir)s", {'cachedir': self.cachedir},
-                     extra={'spider': spider})
+    def open_spider(self, spider: Spider):
+        logger.debug(
+            "Using filesystem cache storage in %(cachedir)s",
+            {"cachedir": self.cachedir},
+            extra={"spider": spider},
+        )
 
         self._fingerprinter = spider.crawler.request_fingerprinter
 
     def close_spider(self, spider):
         pass
 
-    def retrieve_response(self, spider, request):
+    def retrieve_response(self, spider: Spider, request: Request):
         """Return response if present in cache, or None otherwise."""
         metadata = self._read_meta(spider, request)
         if metadata is None:
             return  # not cached
-        rpath = self._get_request_path(spider, request)
-        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:
+        rpath = Path(self._get_request_path(spider, request))
+        with self._open(rpath / "response_body", "rb") as f:
             body = f.read()
-        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:
+        with self._open(rpath / "response_headers", "rb") as f:
             rawheaders = f.read()
-        url = metadata.get('response_url')
-        status = metadata['status']
+        url = metadata.get("response_url")
+        status = metadata["status"]
         headers = Headers(headers_raw_to_dict(rawheaders))
         respcls = responsetypes.from_args(headers=headers, url=url, body=body)
         response = respcls(url=url, headers=headers, status=status, body=body)
         return response
 
-    def store_response(self, spider, request, response):
+    def store_response(self, spider: Spider, request: Request, response):
         """Store the given response in the cache."""
-        rpath = self._get_request_path(spider, request)
-        if not os.path.exists(rpath):
-            os.makedirs(rpath)
+        rpath = Path(self._get_request_path(spider, request))
+        if not rpath.exists():
+            rpath.mkdir(parents=True)
         metadata = {
-            'url': request.url,
-            'method': request.method,
-            'status': response.status,
-            'response_url': response.url,
-            'timestamp': time(),
+            "url": request.url,
+            "method": request.method,
+            "status": response.status,
+            "response_url": response.url,
+            "timestamp": time(),
         }
-        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:
+        with self._open(rpath / "meta", "wb") as f:
             f.write(to_bytes(repr(metadata)))
-        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:
+        with self._open(rpath / "pickled_meta", "wb") as f:
             pickle.dump(metadata, f, protocol=4)
-        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:
+        with self._open(rpath / "response_headers", "wb") as f:
             f.write(headers_dict_to_raw(response.headers))
-        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:
+        with self._open(rpath / "response_body", "wb") as f:
             f.write(response.body)
-        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:
+        with self._open(rpath / "request_headers", "wb") as f:
             f.write(headers_dict_to_raw(request.headers))
-        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:
+        with self._open(rpath / "request_body", "wb") as f:
             f.write(request.body)
 
-    def _get_request_path(self, spider, request):
+    def _get_request_path(self, spider: Spider, request: Request) -> str:
         key = self._fingerprinter.fingerprint(request).hex()
-        return os.path.join(self.cachedir, spider.name, key[0:2], key)
+        return str(Path(self.cachedir, spider.name, key[0:2], key))
 
-    def _read_meta(self, spider, request):
-        rpath = self._get_request_path(spider, request)
-        metapath = os.path.join(rpath, 'pickled_meta')
-        if not os.path.exists(metapath):
+    def _read_meta(self, spider: Spider, request: Request):
+        rpath = Path(self._get_request_path(spider, request))
+        metapath = rpath / "pickled_meta"
+        if not metapath.exists():
             return  # not found
-        mtime = os.stat(metapath).st_mtime
+        mtime = metapath.stat().st_mtime
         if 0 < self.expiration_secs < time() - mtime:
             return  # expired
-        with self._open(metapath, 'rb') as f:
+        with self._open(metapath, "rb") as f:
             return pickle.load(f)
 
 
 def parse_cachecontrol(header):
     """Parse Cache-Control header
 
     https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9
@@ -353,20 +364,20 @@
     ...                                                 b'max-age': b'3600'}
     True
     >>> parse_cachecontrol(b'') == {}
     True
 
     """
     directives = {}
-    for directive in header.split(b','):
-        key, sep, val = directive.strip().partition(b'=')
+    for directive in header.split(b","):
+        key, sep, val = directive.strip().partition(b"=")
         if key:
             directives[key.lower()] = val if sep else None
     return directives
 
 
 def rfc1123_to_epoch(date_str):
     try:
-        date_str = to_unicode(date_str, encoding='ascii')
+        date_str = to_unicode(date_str, encoding="ascii")
         return mktime_tz(parsedate_tz(date_str))
     except Exception:
         return None
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/logstats.py` & `Scrapy-2.8.0/scrapy/extensions/logstats.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import logging
 
 from twisted.internet import task
 
-from scrapy.exceptions import NotConfigured
 from scrapy import signals
+from scrapy.exceptions import NotConfigured
 
 logger = logging.getLogger(__name__)
 
 
 class LogStats:
     """Log basic scraping stats periodically"""
 
@@ -15,15 +15,15 @@
         self.stats = stats
         self.interval = interval
         self.multiplier = 60.0 / self.interval
         self.task = None
 
     @classmethod
     def from_crawler(cls, crawler):
-        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')
+        interval = crawler.settings.getfloat("LOGSTATS_INTERVAL")
         if not interval:
             raise NotConfigured
         o = cls(crawler.stats, interval)
         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)
         return o
 
@@ -31,22 +31,28 @@
         self.pagesprev = 0
         self.itemsprev = 0
 
         self.task = task.LoopingCall(self.log, spider)
         self.task.start(self.interval)
 
     def log(self, spider):
-        items = self.stats.get_value('item_scraped_count', 0)
-        pages = self.stats.get_value('response_received_count', 0)
+        items = self.stats.get_value("item_scraped_count", 0)
+        pages = self.stats.get_value("response_received_count", 0)
         irate = (items - self.itemsprev) * self.multiplier
         prate = (pages - self.pagesprev) * self.multiplier
         self.pagesprev, self.itemsprev = pages, items
 
-        msg = ("Crawled %(pages)d pages (at %(pagerate)d pages/min), "
-               "scraped %(items)d items (at %(itemrate)d items/min)")
-        log_args = {'pages': pages, 'pagerate': prate,
-                    'items': items, 'itemrate': irate}
-        logger.info(msg, log_args, extra={'spider': spider})
+        msg = (
+            "Crawled %(pages)d pages (at %(pagerate)d pages/min), "
+            "scraped %(items)d items (at %(itemrate)d items/min)"
+        )
+        log_args = {
+            "pages": pages,
+            "pagerate": prate,
+            "items": items,
+            "itemrate": irate,
+        }
+        logger.info(msg, log_args, extra={"spider": spider})
 
     def spider_closed(self, spider, reason):
         if self.task and self.task.running:
             self.task.stop()
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/memdebug.py` & `Scrapy-2.8.0/scrapy/extensions/memdebug.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,26 +8,29 @@
 
 from scrapy import signals
 from scrapy.exceptions import NotConfigured
 from scrapy.utils.trackref import live_refs
 
 
 class MemoryDebugger:
-
     def __init__(self, stats):
         self.stats = stats
 
     @classmethod
     def from_crawler(cls, crawler):
-        if not crawler.settings.getbool('MEMDEBUG_ENABLED'):
+        if not crawler.settings.getbool("MEMDEBUG_ENABLED"):
             raise NotConfigured
         o = cls(crawler.stats)
         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)
         return o
 
     def spider_closed(self, spider, reason):
         gc.collect()
-        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)
+        self.stats.set_value(
+            "memdebug/gc_garbage_count", len(gc.garbage), spider=spider
+        )
         for cls, wdict in live_refs.items():
             if not wdict:
                 continue
-            self.stats.set_value(f'memdebug/live_refs/{cls.__name__}', len(wdict), spider=spider)
+            self.stats.set_value(
+                f"memdebug/live_refs/{cls.__name__}", len(wdict), spider=spider
+            )
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/memusage.py` & `Scrapy-2.8.0/scrapy/extensions/memusage.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,62 +1,63 @@
 """
 MemoryUsage extension
 
 See documentation in docs/topics/extensions.rst
 """
-import sys
-import socket
 import logging
-from pprint import pformat
+import socket
+import sys
 from importlib import import_module
+from pprint import pformat
 
 from twisted.internet import task
 
 from scrapy import signals
 from scrapy.exceptions import NotConfigured
 from scrapy.mail import MailSender
 from scrapy.utils.engine import get_engine_status
 
 logger = logging.getLogger(__name__)
 
 
 class MemoryUsage:
-
     def __init__(self, crawler):
-        if not crawler.settings.getbool('MEMUSAGE_ENABLED'):
+        if not crawler.settings.getbool("MEMUSAGE_ENABLED"):
             raise NotConfigured
         try:
             # stdlib's resource module is only available on unix platforms.
-            self.resource = import_module('resource')
+            self.resource = import_module("resource")
         except ImportError:
             raise NotConfigured
 
         self.crawler = crawler
         self.warned = False
-        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY_MAIL')
-        self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB') * 1024 * 1024
-        self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB') * 1024 * 1024
-        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS')
+        self.notify_mails = crawler.settings.getlist("MEMUSAGE_NOTIFY_MAIL")
+        self.limit = crawler.settings.getint("MEMUSAGE_LIMIT_MB") * 1024 * 1024
+        self.warning = crawler.settings.getint("MEMUSAGE_WARNING_MB") * 1024 * 1024
+        self.check_interval = crawler.settings.getfloat(
+            "MEMUSAGE_CHECK_INTERVAL_SECONDS"
+        )
         self.mail = MailSender.from_settings(crawler.settings)
         crawler.signals.connect(self.engine_started, signal=signals.engine_started)
         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler)
 
     def get_virtual_size(self):
         size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss
-        if sys.platform != 'darwin':
+        if sys.platform != "darwin":
             # on macOS ru_maxrss is in bytes, on Linux it is in KB
             size *= 1024
         return size
 
     def engine_started(self):
-        self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())
+        self.crawler.stats.set_value("memusage/startup", self.get_virtual_size())
         self.tasks = []
         tsk = task.LoopingCall(self.update)
         self.tasks.append(tsk)
         tsk.start(self.check_interval, now=True)
         if self.limit:
             tsk = task.LoopingCall(self._check_limit)
             self.tasks.append(tsk)
@@ -68,57 +69,73 @@
 
     def engine_stopped(self):
         for tsk in self.tasks:
             if tsk.running:
                 tsk.stop()
 
     def update(self):
-        self.crawler.stats.max_value('memusage/max', self.get_virtual_size())
+        self.crawler.stats.max_value("memusage/max", self.get_virtual_size())
 
     def _check_limit(self):
-        if self.get_virtual_size() > self.limit:
-            self.crawler.stats.set_value('memusage/limit_reached', 1)
+        peak_mem_usage = self.get_virtual_size()
+        if peak_mem_usage > self.limit:
+            self.crawler.stats.set_value("memusage/limit_reached", 1)
             mem = self.limit / 1024 / 1024
-            logger.error("Memory usage exceeded %(memusage)dM. Shutting down Scrapy...",
-                         {'memusage': mem}, extra={'crawler': self.crawler})
+            logger.error(
+                "Memory usage exceeded %(memusage)dMiB. Shutting down Scrapy...",
+                {"memusage": mem},
+                extra={"crawler": self.crawler},
+            )
             if self.notify_mails:
                 subj = (
                     f"{self.crawler.settings['BOT_NAME']} terminated: "
-                    f"memory usage exceeded {mem}M at {socket.gethostname()}"
+                    f"memory usage exceeded {mem}MiB at {socket.gethostname()}"
                 )
                 self._send_report(self.notify_mails, subj)
-                self.crawler.stats.set_value('memusage/limit_notified', 1)
+                self.crawler.stats.set_value("memusage/limit_notified", 1)
 
             if self.crawler.engine.spider is not None:
-                self.crawler.engine.close_spider(self.crawler.engine.spider, 'memusage_exceeded')
+                self.crawler.engine.close_spider(
+                    self.crawler.engine.spider, "memusage_exceeded"
+                )
             else:
                 self.crawler.stop()
+        else:
+            logger.info(
+                "Peak memory usage is %(virtualsize)dMiB",
+                {"virtualsize": peak_mem_usage / 1024 / 1024},
+            )
 
     def _check_warning(self):
         if self.warned:  # warn only once
             return
         if self.get_virtual_size() > self.warning:
-            self.crawler.stats.set_value('memusage/warning_reached', 1)
+            self.crawler.stats.set_value("memusage/warning_reached", 1)
             mem = self.warning / 1024 / 1024
-            logger.warning("Memory usage reached %(memusage)dM",
-                           {'memusage': mem}, extra={'crawler': self.crawler})
+            logger.warning(
+                "Memory usage reached %(memusage)dMiB",
+                {"memusage": mem},
+                extra={"crawler": self.crawler},
+            )
             if self.notify_mails:
                 subj = (
                     f"{self.crawler.settings['BOT_NAME']} warning: "
-                    f"memory usage reached {mem}M at {socket.gethostname()}"
+                    f"memory usage reached {mem}MiB at {socket.gethostname()}"
                 )
                 self._send_report(self.notify_mails, subj)
-                self.crawler.stats.set_value('memusage/warning_notified', 1)
+                self.crawler.stats.set_value("memusage/warning_notified", 1)
             self.warned = True
 
     def _send_report(self, rcpts, subject):
         """send notification mail with some additional useful info"""
         stats = self.crawler.stats
         s = f"Memory usage at engine startup : {stats.get_value('memusage/startup')/1024/1024}M\r\n"
         s += f"Maximum memory usage          : {stats.get_value('memusage/max')/1024/1024}M\r\n"
         s += f"Current memory usage          : {self.get_virtual_size()/1024/1024}M\r\n"
 
-        s += "ENGINE STATUS ------------------------------------------------------- \r\n"
+        s += (
+            "ENGINE STATUS ------------------------------------------------------- \r\n"
+        )
         s += "\r\n"
         s += pformat(get_engine_status(self.crawler.engine))
         s += "\r\n"
         self.mail.send(rcpts, subject, s)
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/postprocessing.py` & `Scrapy-2.8.0/scrapy/extensions/postprocessing.py`

 * *Files 3% similar despite different names*

```diff
@@ -25,16 +25,21 @@
 
     def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:
         self.file = file
         self.feed_options = feed_options
         compress_level = self.feed_options.get("gzip_compresslevel", 9)
         mtime = self.feed_options.get("gzip_mtime")
         filename = self.feed_options.get("gzip_filename")
-        self.gzipfile = GzipFile(fileobj=self.file, mode="wb", compresslevel=compress_level,
-                                 mtime=mtime, filename=filename)
+        self.gzipfile = GzipFile(
+            fileobj=self.file,
+            mode="wb",
+            compresslevel=compress_level,
+            mtime=mtime,
+            filename=filename,
+        )
 
     def write(self, data: bytes) -> int:
         return self.gzipfile.write(data)
 
     def close(self) -> None:
         self.gzipfile.close()
         self.file.close()
@@ -51,15 +56,17 @@
     See :py:class:`bz2.BZ2File` for more info about parameters.
     """
 
     def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:
         self.file = file
         self.feed_options = feed_options
         compress_level = self.feed_options.get("bz2_compresslevel", 9)
-        self.bz2file = BZ2File(filename=self.file, mode="wb", compresslevel=compress_level)
+        self.bz2file = BZ2File(
+            filename=self.file, mode="wb", compresslevel=compress_level
+        )
 
     def write(self, data: bytes) -> int:
         return self.bz2file.write(data)
 
     def close(self) -> None:
         self.bz2file.close()
         self.file.close()
@@ -86,16 +93,22 @@
         self.file = file
         self.feed_options = feed_options
 
         format = self.feed_options.get("lzma_format")
         check = self.feed_options.get("lzma_check", -1)
         preset = self.feed_options.get("lzma_preset")
         filters = self.feed_options.get("lzma_filters")
-        self.lzmafile = LZMAFile(filename=self.file, mode="wb", format=format,
-                                 check=check, preset=preset, filters=filters)
+        self.lzmafile = LZMAFile(
+            filename=self.file,
+            mode="wb",
+            format=format,
+            check=check,
+            preset=preset,
+            filters=filters,
+        )
 
     def write(self, data: bytes) -> int:
         return self.lzmafile.write(data)
 
     def close(self) -> None:
         self.lzmafile.close()
         self.file.close()
@@ -110,15 +123,17 @@
     pipeline-ish way.
     :param plugins: all the declared plugins for the feed
     :type plugins: list
     :param file: final target file where the processed data will be written
     :type file: file like object
     """
 
-    def __init__(self, plugins: List[Any], file: BinaryIO, feed_options: Dict[str, Any]) -> None:
+    def __init__(
+        self, plugins: List[Any], file: BinaryIO, feed_options: Dict[str, Any]
+    ) -> None:
         self.plugins = self._load_plugins(plugins)
         self.file = file
         self.feed_options = feed_options
         self.head_plugin = self._get_head_plugin()
 
     def write(self, data: bytes) -> int:
         """
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/spiderstate.py` & `Scrapy-2.8.0/scrapy/extensions/spiderstate.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-import os
 import pickle
+from pathlib import Path
 
 from scrapy import signals
 from scrapy.exceptions import NotConfigured
 from scrapy.utils.job import job_dir
 
 
 class SpiderState:
@@ -21,20 +21,20 @@
         obj = cls(jobdir)
         crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)
         crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)
         return obj
 
     def spider_closed(self, spider):
         if self.jobdir:
-            with open(self.statefn, 'wb') as f:
+            with Path(self.statefn).open("wb") as f:
                 pickle.dump(spider.state, f, protocol=4)
 
     def spider_opened(self, spider):
-        if self.jobdir and os.path.exists(self.statefn):
-            with open(self.statefn, 'rb') as f:
+        if self.jobdir and Path(self.statefn).exists():
+            with Path(self.statefn).open("rb") as f:
                 spider.state = pickle.load(f)
         else:
             spider.state = {}
 
     @property
-    def statefn(self):
-        return os.path.join(self.jobdir, 'spider.state')
+    def statefn(self) -> str:
+        return str(Path(self.jobdir, "spider.state"))
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/statsmailer.py` & `Scrapy-2.8.0/scrapy/extensions/statsmailer.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,20 +1,19 @@
 """
 StatsMailer extension sends an email when a spider finishes scraping.
 
 Use STATSMAILER_RCPTS setting to enable and give the recipient mail address
 """
 
 from scrapy import signals
-from scrapy.mail import MailSender
 from scrapy.exceptions import NotConfigured
+from scrapy.mail import MailSender
 
 
 class StatsMailer:
-
     def __init__(self, stats, recipients, mail):
         self.stats = stats
         self.recipients = recipients
         self.mail = mail
 
     @classmethod
     def from_crawler(cls, crawler):
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/telnet.py` & `Scrapy-2.8.0/scrapy/extensions/telnet.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,114 +1,115 @@
 """
 Scrapy Telnet Console extension
 
 See documentation in docs/topics/telnetconsole.rst
 """
 
-import pprint
-import logging
-import traceback
 import binascii
+import logging
 import os
+import pprint
+import traceback
 
 from twisted.internet import protocol
+
 try:
     from twisted.conch import manhole, telnet
     from twisted.conch.insults import insults
+
     TWISTED_CONCH_AVAILABLE = True
 except (ImportError, SyntaxError):
     _TWISTED_CONCH_TRACEBACK = traceback.format_exc()
     TWISTED_CONCH_AVAILABLE = False
 
-from scrapy.exceptions import NotConfigured
 from scrapy import signals
-from scrapy.utils.trackref import print_live_refs
+from scrapy.exceptions import NotConfigured
+from scrapy.utils.decorators import defers
 from scrapy.utils.engine import print_engine_status
 from scrapy.utils.reactor import listen_tcp
-from scrapy.utils.decorators import defers
-
+from scrapy.utils.trackref import print_live_refs
 
 logger = logging.getLogger(__name__)
 
 # signal to update telnet variables
 # args: telnet_vars
 update_telnet_vars = object()
 
 
 class TelnetConsole(protocol.ServerFactory):
-
     def __init__(self, crawler):
-        if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):
+        if not crawler.settings.getbool("TELNETCONSOLE_ENABLED"):
             raise NotConfigured
         if not TWISTED_CONCH_AVAILABLE:
             raise NotConfigured(
-                'TELNETCONSOLE_ENABLED setting is True but required twisted '
-                'modules failed to import:\n' + _TWISTED_CONCH_TRACEBACK)
+                "TELNETCONSOLE_ENABLED setting is True but required twisted "
+                "modules failed to import:\n" + _TWISTED_CONCH_TRACEBACK
+            )
         self.crawler = crawler
         self.noisy = False
-        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]
-        self.host = crawler.settings['TELNETCONSOLE_HOST']
-        self.username = crawler.settings['TELNETCONSOLE_USERNAME']
-        self.password = crawler.settings['TELNETCONSOLE_PASSWORD']
+        self.portrange = [
+            int(x) for x in crawler.settings.getlist("TELNETCONSOLE_PORT")
+        ]
+        self.host = crawler.settings["TELNETCONSOLE_HOST"]
+        self.username = crawler.settings["TELNETCONSOLE_USERNAME"]
+        self.password = crawler.settings["TELNETCONSOLE_PASSWORD"]
 
         if not self.password:
-            self.password = binascii.hexlify(os.urandom(8)).decode('utf8')
-            logger.info('Telnet Password: %s', self.password)
+            self.password = binascii.hexlify(os.urandom(8)).decode("utf8")
+            logger.info("Telnet Password: %s", self.password)
 
         self.crawler.signals.connect(self.start_listening, signals.engine_started)
         self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler)
 
     def start_listening(self):
         self.port = listen_tcp(self.portrange, self.host, self)
         h = self.port.getHost()
-        logger.info("Telnet console listening on %(host)s:%(port)d",
-                    {'host': h.host, 'port': h.port},
-                    extra={'crawler': self.crawler})
+        logger.info(
+            "Telnet console listening on %(host)s:%(port)d",
+            {"host": h.host, "port": h.port},
+            extra={"crawler": self.crawler},
+        )
 
     def stop_listening(self):
         self.port.stopListening()
 
     def protocol(self):
         class Portal:
             """An implementation of IPortal"""
+
             @defers
             def login(self_, credentials, mind, *interfaces):
                 if not (
-                    credentials.username == self.username.encode('utf8')
-                    and credentials.checkPassword(self.password.encode('utf8'))
+                    credentials.username == self.username.encode("utf8")
+                    and credentials.checkPassword(self.password.encode("utf8"))
                 ):
                     raise ValueError("Invalid credentials")
 
                 protocol = telnet.TelnetBootstrapProtocol(
-                    insults.ServerProtocol,
-                    manhole.Manhole,
-                    self._get_telnet_vars()
+                    insults.ServerProtocol, manhole.Manhole, self._get_telnet_vars()
                 )
                 return (interfaces[0], protocol, lambda: None)
 
-        return telnet.TelnetTransport(
-            telnet.AuthenticatingTelnetProtocol,
-            Portal()
-        )
+        return telnet.TelnetTransport(telnet.AuthenticatingTelnetProtocol, Portal())
 
     def _get_telnet_vars(self):
         # Note: if you add entries here also update topics/telnetconsole.rst
         telnet_vars = {
-            'engine': self.crawler.engine,
-            'spider': self.crawler.engine.spider,
-            'slot': self.crawler.engine.slot,
-            'crawler': self.crawler,
-            'extensions': self.crawler.extensions,
-            'stats': self.crawler.stats,
-            'settings': self.crawler.settings,
-            'est': lambda: print_engine_status(self.crawler.engine),
-            'p': pprint.pprint,
-            'prefs': print_live_refs,
-            'help': "This is Scrapy telnet console. For more info see: "
-                    "https://docs.scrapy.org/en/latest/topics/telnetconsole.html",
+            "engine": self.crawler.engine,
+            "spider": self.crawler.engine.spider,
+            "slot": self.crawler.engine.slot,
+            "crawler": self.crawler,
+            "extensions": self.crawler.extensions,
+            "stats": self.crawler.stats,
+            "settings": self.crawler.settings,
+            "est": lambda: print_engine_status(self.crawler.engine),
+            "p": pprint.pprint,
+            "prefs": print_live_refs,
+            "help": "This is Scrapy telnet console. For more info see: "
+            "https://docs.scrapy.org/en/latest/topics/telnetconsole.html",
         }
         self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)
         return telnet_vars
```

### Comparing `Scrapy-2.7.1/scrapy/extensions/throttle.py` & `Scrapy-2.8.0/scrapy/extensions/throttle.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,72 +1,80 @@
 import logging
 
-from scrapy.exceptions import NotConfigured
 from scrapy import signals
+from scrapy.exceptions import NotConfigured
 
 logger = logging.getLogger(__name__)
 
 
 class AutoThrottle:
-
     def __init__(self, crawler):
         self.crawler = crawler
-        if not crawler.settings.getbool('AUTOTHROTTLE_ENABLED'):
+        if not crawler.settings.getbool("AUTOTHROTTLE_ENABLED"):
             raise NotConfigured
 
         self.debug = crawler.settings.getbool("AUTOTHROTTLE_DEBUG")
-        self.target_concurrency = crawler.settings.getfloat("AUTOTHROTTLE_TARGET_CONCURRENCY")
+        self.target_concurrency = crawler.settings.getfloat(
+            "AUTOTHROTTLE_TARGET_CONCURRENCY"
+        )
         crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)
-        crawler.signals.connect(self._response_downloaded, signal=signals.response_downloaded)
+        crawler.signals.connect(
+            self._response_downloaded, signal=signals.response_downloaded
+        )
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler)
 
     def _spider_opened(self, spider):
         self.mindelay = self._min_delay(spider)
         self.maxdelay = self._max_delay(spider)
         spider.download_delay = self._start_delay(spider)
 
     def _min_delay(self, spider):
         s = self.crawler.settings
-        return getattr(spider, 'download_delay', s.getfloat('DOWNLOAD_DELAY'))
+        return getattr(spider, "download_delay", s.getfloat("DOWNLOAD_DELAY"))
 
     def _max_delay(self, spider):
-        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')
+        return self.crawler.settings.getfloat("AUTOTHROTTLE_MAX_DELAY")
 
     def _start_delay(self, spider):
-        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY'))
+        return max(
+            self.mindelay, self.crawler.settings.getfloat("AUTOTHROTTLE_START_DELAY")
+        )
 
     def _response_downloaded(self, response, request, spider):
         key, slot = self._get_slot(request, spider)
-        latency = request.meta.get('download_latency')
+        latency = request.meta.get("download_latency")
         if latency is None or slot is None:
             return
 
         olddelay = slot.delay
         self._adjust_delay(slot, latency, response)
         if self.debug:
             diff = slot.delay - olddelay
             size = len(response.body)
             conc = len(slot.transferring)
             logger.info(
                 "slot: %(slot)s | conc:%(concurrency)2d | "
                 "delay:%(delay)5d ms (%(delaydiff)+d) | "
                 "latency:%(latency)5d ms | size:%(size)6d bytes",
                 {
-                    'slot': key, 'concurrency': conc,
-                    'delay': slot.delay * 1000, 'delaydiff': diff * 1000,
-                    'latency': latency * 1000, 'size': size
+                    "slot": key,
+                    "concurrency": conc,
+                    "delay": slot.delay * 1000,
+                    "delaydiff": diff * 1000,
+                    "latency": latency * 1000,
+                    "size": size,
                 },
-                extra={'spider': spider}
+                extra={"spider": spider},
             )
 
     def _get_slot(self, request, spider):
-        key = request.meta.get('download_slot')
+        key = request.meta.get("download_slot")
         return key, self.crawler.engine.downloader.slots.get(key)
 
     def _adjust_delay(self, slot, latency, response):
         """Define delay adjustment policy"""
 
         # If a server needs `latency` seconds to respond then
         # we should send a request each `latency/N` seconds
```

### Comparing `Scrapy-2.7.1/scrapy/http/__init__.py` & `Scrapy-2.8.0/scrapy/http/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -2,17 +2,15 @@
 Module containing all HTTP related classes
 
 Use this module (instead of the more specific ones) when importing Headers,
 Request and Response outside this module.
 """
 
 from scrapy.http.headers import Headers
-
 from scrapy.http.request import Request
 from scrapy.http.request.form import FormRequest
-from scrapy.http.request.rpc import XmlRpcRequest
 from scrapy.http.request.json_request import JsonRequest
-
+from scrapy.http.request.rpc import XmlRpcRequest
 from scrapy.http.response import Response
 from scrapy.http.response.html import HtmlResponse
-from scrapy.http.response.xml import XmlResponse
 from scrapy.http.response.text import TextResponse
+from scrapy.http.response.xml import XmlResponse
```

### Comparing `Scrapy-2.7.1/scrapy/http/cookies.py` & `Scrapy-2.8.0/scrapy/http/cookies.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import re
 import time
-from http.cookiejar import CookieJar as _CookieJar, DefaultCookiePolicy
+from http.cookiejar import CookieJar as _CookieJar
+from http.cookiejar import DefaultCookiePolicy
 
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.python import to_unicode
 
-
 # Defined in the http.cookiejar module, but undocumented:
 # https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527
 IPV4_RE = re.compile(r"\.\d+$", re.ASCII)
 
 
 class CookieJar:
     def __init__(self, policy=None, check_expired_frequency=10000):
@@ -32,15 +32,15 @@
         # instead we restrict to potential matches on the domain
         req_host = urlparse_cached(request).hostname
         if not req_host:
             return
 
         if not IPV4_RE.search(req_host):
             hosts = potential_domain_matches(req_host)
-            if '.' not in req_host:
+            if "." not in req_host:
                 hosts += [req_host + ".local"]
         else:
             hosts = [req_host]
 
         cookies = []
         for host in hosts:
             if host in self.jar._cookies:
@@ -92,22 +92,22 @@
 
     >>> potential_domain_matches('www.example.com')
     ['www.example.com', 'example.com', '.www.example.com', '.example.com']
 
     """
     matches = [domain]
     try:
-        start = domain.index('.') + 1
-        end = domain.rindex('.')
+        start = domain.index(".") + 1
+        end = domain.rindex(".")
         while start < end:
             matches.append(domain[start:])
-            start = domain.index('.', start) + 1
+            start = domain.index(".", start) + 1
     except ValueError:
         pass
-    return matches + ['.' + d for d in matches]
+    return matches + ["." + d for d in matches]
 
 
 class _DummyLock:
     def acquire(self):
         pass
 
     def release(self):
@@ -136,15 +136,15 @@
         """Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.
 
         It defaults to False. An unverifiable request is one whose URL the user did not have the
         option to approve. For example, if the request is for an image in an
         HTML document, and the user had no option to approve the automatic
         fetching of the image, this should be true.
         """
-        return self.request.meta.get('is_unverifiable', False)
+        return self.request.meta.get("is_unverifiable", False)
 
     @property
     def full_url(self):
         return self.get_full_url()
 
     @property
     def host(self):
@@ -162,32 +162,33 @@
     def origin_req_host(self):
         return urlparse_cached(self.request).hostname
 
     def has_header(self, name):
         return name in self.request.headers
 
     def get_header(self, name, default=None):
-        return to_unicode(self.request.headers.get(name, default),
-                          errors='replace')
+        return to_unicode(self.request.headers.get(name, default), errors="replace")
 
     def header_items(self):
         return [
-            (to_unicode(k, errors='replace'),
-             [to_unicode(x, errors='replace') for x in v])
+            (
+                to_unicode(k, errors="replace"),
+                [to_unicode(x, errors="replace") for x in v],
+            )
             for k, v in self.request.headers.items()
         ]
 
     def add_unredirected_header(self, name, value):
         self.request.headers.appendlist(name, value)
 
 
 class WrappedResponse:
-
     def __init__(self, response):
         self.response = response
 
     def info(self):
         return self
 
     def get_all(self, name, default=None):
-        return [to_unicode(v, errors='replace')
-                for v in self.response.headers.getlist(name)]
+        return [
+            to_unicode(v, errors="replace") for v in self.response.headers.getlist(name)
+        ]
```

### Comparing `Scrapy-2.7.1/scrapy/http/headers.py` & `Scrapy-2.8.0/scrapy/http/headers.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 from collections.abc import Mapping
 
 from w3lib.http import headers_dict_to_raw
+
 from scrapy.utils.datatypes import CaselessDict
 from scrapy.utils.python import to_unicode
 
 
 class Headers(CaselessDict):
     """Case insensitive http headers dictionary"""
 
-    def __init__(self, seq=None, encoding='utf-8'):
+    def __init__(self, seq=None, encoding="utf-8"):
         self.encoding = encoding
         super().__init__(seq)
 
     def update(self, seq):
         seq = seq.items() if isinstance(seq, Mapping) else seq
         iseq = {}
         for k, v in seq:
@@ -25,28 +26,27 @@
 
     def normvalue(self, value):
         """Normalize values to bytes"""
         if value is None:
             value = []
         elif isinstance(value, (str, bytes)):
             value = [value]
-        elif not hasattr(value, '__iter__'):
+        elif not hasattr(value, "__iter__"):
             value = [value]
 
         return [self._tobytes(x) for x in value]
 
     def _tobytes(self, x):
         if isinstance(x, bytes):
             return x
-        elif isinstance(x, str):
+        if isinstance(x, str):
             return x.encode(self.encoding)
-        elif isinstance(x, int):
+        if isinstance(x, int):
             return str(x).encode(self.encoding)
-        else:
-            raise TypeError(f'Unsupported value type: {type(x)}')
+        raise TypeError(f"Unsupported value type: {type(x)}")
 
     def __getitem__(self, key):
         try:
             return super().__getitem__(key)[-1]
         except IndexError:
             return None
 
@@ -81,19 +81,22 @@
     def values(self):
         return [self[k] for k in self.keys()]
 
     def to_string(self):
         return headers_dict_to_raw(self)
 
     def to_unicode_dict(self):
-        """ Return headers as a CaselessDict with unicode keys
+        """Return headers as a CaselessDict with unicode keys
         and unicode values. Multiple values are joined with ','.
         """
         return CaselessDict(
-            (to_unicode(key, encoding=self.encoding),
-             to_unicode(b','.join(value), encoding=self.encoding))
-            for key, value in self.items())
+            (
+                to_unicode(key, encoding=self.encoding),
+                to_unicode(b",".join(value), encoding=self.encoding),
+            )
+            for key, value in self.items()
+        )
 
     def __copy__(self):
         return self.__class__(self)
 
     copy = __copy__
```

### Comparing `Scrapy-2.7.1/scrapy/http/request/__init__.py` & `Scrapy-2.8.0/scrapy/http/request/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,27 +13,60 @@
 from scrapy.http.common import obsolete_setter
 from scrapy.http.headers import Headers
 from scrapy.utils.curl import curl_to_request_kwargs
 from scrapy.utils.python import to_bytes
 from scrapy.utils.trackref import object_ref
 from scrapy.utils.url import escape_ajax
 
-
 RequestTypeVar = TypeVar("RequestTypeVar", bound="Request")
 
 
+def NO_CALLBACK(*args, **kwargs):
+    """When assigned to the ``callback`` parameter of
+    :class:`~scrapy.http.Request`, it indicates that the request is not meant
+    to have a spider callback at all.
+
+    For example:
+
+    .. code-block:: python
+
+       Request("https://example.com", callback=NO_CALLBACK)
+
+    This value should be used by :ref:`components <topics-components>` that
+    create and handle their own requests, e.g. through
+    :meth:`scrapy.core.engine.ExecutionEngine.download`, so that downloader
+    middlewares handling such requests can treat them differently from requests
+    intended for the :meth:`~scrapy.Spider.parse` callback.
+    """
+    raise RuntimeError(
+        "The NO_CALLBACK callback has been called. This is a special callback "
+        "value intended for requests whose callback is never meant to be "
+        "called."
+    )
+
+
 class Request(object_ref):
     """Represents an HTTP request, which is usually generated in a Spider and
     executed by the Downloader, thus generating a :class:`Response`.
     """
 
     attributes: Tuple[str, ...] = (
-        "url", "callback", "method", "headers", "body",
-        "cookies", "meta", "encoding", "priority",
-        "dont_filter", "errback", "flags", "cb_kwargs",
+        "url",
+        "callback",
+        "method",
+        "headers",
+        "body",
+        "cookies",
+        "meta",
+        "encoding",
+        "priority",
+        "dont_filter",
+        "errback",
+        "flags",
+        "cb_kwargs",
     )
     """A tuple of :class:`str` objects containing the name of all public
     attributes of the class that are also keyword parameters of the
     ``__init__`` method.
 
     Currently used by :meth:`Request.replace`, :meth:`Request.to_dict` and
     :func:`~scrapy.utils.request.request_from_dict`.
@@ -59,18 +92,20 @@
         self.method = str(method).upper()
         self._set_url(url)
         self._set_body(body)
         if not isinstance(priority, int):
             raise TypeError(f"Request priority not an integer: {priority!r}")
         self.priority = priority
 
-        if callback is not None and not callable(callback):
-            raise TypeError(f'callback must be a callable, got {type(callback).__name__}')
-        if errback is not None and not callable(errback):
-            raise TypeError(f'errback must be a callable, got {type(errback).__name__}')
+        if not (callable(callback) or callback is None):
+            raise TypeError(
+                f"callback must be a callable, got {type(callback).__name__}"
+            )
+        if not (callable(errback) or errback is None):
+            raise TypeError(f"errback must be a callable, got {type(errback).__name__}")
         self.callback = callback
         self.errback = errback
 
         self.cookies = cookies or {}
         self.headers = Headers(headers or {}, encoding=encoding)
         self.dont_filter = dont_filter
 
@@ -97,52 +132,53 @@
         if not isinstance(url, str):
             raise TypeError(f"Request url must be str, got {type(url).__name__}")
 
         s = safe_url_string(url, self.encoding)
         self._url = escape_ajax(s)
 
         if (
-            '://' not in self._url
-            and not self._url.startswith('about:')
-            and not self._url.startswith('data:')
+            "://" not in self._url
+            and not self._url.startswith("about:")
+            and not self._url.startswith("data:")
         ):
-            raise ValueError(f'Missing scheme in request url: {self._url}')
+            raise ValueError(f"Missing scheme in request url: {self._url}")
 
-    url = property(_get_url, obsolete_setter(_set_url, 'url'))
+    url = property(_get_url, obsolete_setter(_set_url, "url"))
 
     def _get_body(self) -> bytes:
         return self._body
 
     def _set_body(self, body: Optional[Union[str, bytes]]) -> None:
         self._body = b"" if body is None else to_bytes(body, self.encoding)
 
-    body = property(_get_body, obsolete_setter(_set_body, 'body'))
+    body = property(_get_body, obsolete_setter(_set_body, "body"))
 
     @property
     def encoding(self) -> str:
         return self._encoding
 
-    def __str__(self) -> str:
+    def __repr__(self) -> str:
         return f"<{self.method} {self.url}>"
 
-    __repr__ = __str__
-
     def copy(self) -> "Request":
         return self.replace()
 
     def replace(self, *args, **kwargs) -> "Request":
         """Create a new Request with the same attributes except for those given new values"""
         for x in self.attributes:
             kwargs.setdefault(x, getattr(self, x))
-        cls = kwargs.pop('cls', self.__class__)
+        cls = kwargs.pop("cls", self.__class__)
         return cls(*args, **kwargs)
 
     @classmethod
     def from_curl(
-        cls: Type[RequestTypeVar], curl_command: str, ignore_unknown_options: bool = True, **kwargs
+        cls: Type[RequestTypeVar],
+        curl_command: str,
+        ignore_unknown_options: bool = True,
+        **kwargs,
     ) -> RequestTypeVar:
         """Create a Request object from a string containing a `cURL
         <https://curl.haxx.se/>`_ command. It populates the HTTP method, the
         URL, the headers, the cookies and the body. It accepts the same
         arguments as the :class:`Request` class, taking preference and
         overriding the values of the same arguments contained in the cURL
         command.
@@ -177,29 +213,33 @@
         Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.
 
         If a spider is given, this method will try to find out the name of the spider methods used as callback
         and errback and include them in the output dict, raising an exception if they cannot be found.
         """
         d = {
             "url": self.url,  # urls are safe (safe_string_url)
-            "callback": _find_method(spider, self.callback) if callable(self.callback) else self.callback,
-            "errback": _find_method(spider, self.errback) if callable(self.errback) else self.errback,
+            "callback": _find_method(spider, self.callback)
+            if callable(self.callback)
+            else self.callback,
+            "errback": _find_method(spider, self.errback)
+            if callable(self.errback)
+            else self.errback,
             "headers": dict(self.headers),
         }
         for attr in self.attributes:
             d.setdefault(attr, getattr(self, attr))
-        if type(self) is not Request:
-            d["_class"] = self.__module__ + '.' + self.__class__.__name__
+        if type(self) is not Request:  # pylint: disable=unidiomatic-typecheck
+            d["_class"] = self.__module__ + "." + self.__class__.__name__
         return d
 
 
 def _find_method(obj, func):
     """Helper function for Request.to_dict"""
     # Only instance methods contain ``__func__``
-    if obj and hasattr(func, '__func__'):
+    if obj and hasattr(func, "__func__"):
         members = inspect.getmembers(obj, predicate=inspect.ismethod)
         for name, obj_func in members:
             # We need to use __func__ to access the original function object because instance
             # method objects are generated each time attribute is retrieved from instance.
             #
             # Reference: The standard type hierarchy
             # https://docs.python.org/3/reference/datamodel.html
```

### Comparing `Scrapy-2.7.1/scrapy/http/request/form.py` & `Scrapy-2.8.0/scrapy/http/request/form.py`

 * *Files 7% similar despite different names*

```diff
@@ -2,48 +2,51 @@
 This module implements the FormRequest class which is a more convenient class
 (than Request) to generate Requests based on form data.
 
 See documentation in docs/topics/request-response.rst
 """
 
 from typing import Iterable, List, Optional, Tuple, Type, TypeVar, Union
-from urllib.parse import urljoin, urlencode, urlsplit, urlunsplit
+from urllib.parse import urlencode, urljoin, urlsplit, urlunsplit
 
 from lxml.html import FormElement, HtmlElement, HTMLParser, SelectElement
 from parsel.selector import create_root_node
 from w3lib.html import strip_html5_whitespace
 
 from scrapy.http.request import Request
 from scrapy.http.response.text import TextResponse
-from scrapy.utils.python import to_bytes, is_listlike
+from scrapy.utils.python import is_listlike, to_bytes
 from scrapy.utils.response import get_base_url
 
-
 FormRequestTypeVar = TypeVar("FormRequestTypeVar", bound="FormRequest")
 
 FormdataType = Optional[Union[dict, List[Tuple[str, str]]]]
 
 
 class FormRequest(Request):
-    valid_form_methods = ['GET', 'POST']
+    valid_form_methods = ["GET", "POST"]
 
     def __init__(self, *args, formdata: FormdataType = None, **kwargs) -> None:
-        if formdata and kwargs.get('method') is None:
-            kwargs['method'] = 'POST'
+        if formdata and kwargs.get("method") is None:
+            kwargs["method"] = "POST"
 
         super().__init__(*args, **kwargs)
 
         if formdata:
             items = formdata.items() if isinstance(formdata, dict) else formdata
             form_query_str = _urlencode(items, self.encoding)
-            if self.method == 'POST':
-                self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')
+            if self.method == "POST":
+                self.headers.setdefault(
+                    b"Content-Type", b"application/x-www-form-urlencoded"
+                )
                 self._set_body(form_query_str)
             else:
-                self._set_url(urlunsplit(urlsplit(self.url)._replace(query=form_query_str)))
+                self._set_url(
+                    urlunsplit(urlsplit(self.url)._replace(query=form_query_str))
+                )
 
     @classmethod
     def from_response(
         cls: Type[FormRequestTypeVar],
         response: TextResponse,
         formname: Optional[str] = None,
         formid: Optional[str] = None,
@@ -51,59 +54,62 @@
         formdata: FormdataType = None,
         clickdata: Optional[dict] = None,
         dont_click: bool = False,
         formxpath: Optional[str] = None,
         formcss: Optional[str] = None,
         **kwargs,
     ) -> FormRequestTypeVar:
-        kwargs.setdefault('encoding', response.encoding)
+        kwargs.setdefault("encoding", response.encoding)
 
         if formcss is not None:
             from parsel.csstranslator import HTMLTranslator
+
             formxpath = HTMLTranslator().css_to_xpath(formcss)
 
         form = _get_form(response, formname, formid, formnumber, formxpath)
         formdata = _get_inputs(form, formdata, dont_click, clickdata)
-        url = _get_form_url(form, kwargs.pop('url', None))
+        url = _get_form_url(form, kwargs.pop("url", None))
 
-        method = kwargs.pop('method', form.method)
+        method = kwargs.pop("method", form.method)
         if method is not None:
             method = method.upper()
             if method not in cls.valid_form_methods:
-                method = 'GET'
+                method = "GET"
 
         return cls(url=url, method=method, formdata=formdata, **kwargs)
 
 
 def _get_form_url(form: FormElement, url: Optional[str]) -> str:
     if url is None:
-        action = form.get('action')
+        action = form.get("action")
         if action is None:
             return form.base_url
         return urljoin(form.base_url, strip_html5_whitespace(action))
     return urljoin(form.base_url, url)
 
 
 def _urlencode(seq: Iterable, enc: str) -> str:
-    values = [(to_bytes(k, enc), to_bytes(v, enc))
-              for k, vs in seq
-              for v in (vs if is_listlike(vs) else [vs])]
+    values = [
+        (to_bytes(k, enc), to_bytes(v, enc))
+        for k, vs in seq
+        for v in (vs if is_listlike(vs) else [vs])
+    ]
     return urlencode(values, doseq=True)
 
 
 def _get_form(
     response: TextResponse,
     formname: Optional[str],
     formid: Optional[str],
     formnumber: Optional[int],
     formxpath: Optional[str],
 ) -> FormElement:
     """Find the wanted form element within the given response."""
     root = create_root_node(response.text, HTMLParser, base_url=get_base_url(response))
-    forms = root.xpath('//form')
+    forms = root.xpath("//form")
     if not forms:
         raise ValueError(f"No <form> element found in {response}")
 
     if formname is not None:
         f = root.xpath(f'//form[@name="{formname}"]')
         if f:
             return f[0]
@@ -115,20 +121,20 @@
 
     # Get form element from xpath, if not found, go up
     if formxpath is not None:
         nodes = root.xpath(formxpath)
         if nodes:
             el = nodes[0]
             while True:
-                if el.tag == 'form':
+                if el.tag == "form":
                     return el
                 el = el.getparent()
                 if el is None:
                     break
-        raise ValueError(f'No <form> element found with {formxpath}')
+        raise ValueError(f"No <form> element found with {formxpath}")
 
     # If we get here, it means that either formname was None or invalid
     if formnumber is not None:
         try:
             form = forms[formnumber]
         except IndexError:
             raise IndexError(f"Form number {formnumber} not found in {response}")
@@ -142,27 +148,29 @@
     dont_click: bool,
     clickdata: Optional[dict],
 ) -> List[Tuple[str, str]]:
     """Return a list of key-value pairs for the inputs found in the given form."""
     try:
         formdata_keys = dict(formdata or ()).keys()
     except (ValueError, TypeError):
-        raise ValueError('formdata should be a dict or iterable of tuples')
+        raise ValueError("formdata should be a dict or iterable of tuples")
 
     if not formdata:
         formdata = []
-    inputs = form.xpath('descendant::textarea'
-                        '|descendant::select'
-                        '|descendant::input[not(@type) or @type['
-                        ' not(re:test(., "^(?:submit|image|reset)$", "i"))'
-                        ' and (../@checked or'
-                        '  not(re:test(., "^(?:checkbox|radio)$", "i")))]]',
-                        namespaces={"re": "http://exslt.org/regular-expressions"})
+    inputs = form.xpath(
+        "descendant::textarea"
+        "|descendant::select"
+        "|descendant::input[not(@type) or @type["
+        ' not(re:test(., "^(?:submit|image|reset)$", "i"))'
+        " and (../@checked or"
+        '  not(re:test(., "^(?:checkbox|radio)$", "i")))]]',
+        namespaces={"re": "http://exslt.org/regular-expressions"},
+    )
     values = [
-        (k, '' if v is None else v)
+        (k, "" if v is None else v)
         for k, v in (_value(e) for e in inputs)
         if k and k not in formdata_keys
     ]
 
     if not dont_click:
         clickable = _get_clickable(clickdata, form)
         if clickable and clickable[0] not in formdata and not clickable[0] is None:
@@ -174,70 +182,76 @@
     values.extend((k, v) for k, v in formdata if v is not None)
     return values
 
 
 def _value(ele: HtmlElement):
     n = ele.name
     v = ele.value
-    if ele.tag == 'select':
+    if ele.tag == "select":
         return _select_value(ele, n, v)
     return n, v
 
 
 def _select_value(ele: SelectElement, n: str, v: str):
     multiple = ele.multiple
     if v is None and not multiple:
         # Match browser behaviour on simple select tag without options selected
         # And for select tags without options
         o = ele.value_options
         return (n, o[0]) if o else (None, None)
-    elif v is not None and multiple:
-        # This is a workround to bug in lxml fixed 2.3.1
+    if v is not None and multiple:
+        # This is a workaround to bug in lxml fixed 2.3.1
         # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139
-        selected_options = ele.xpath('.//option[@selected]')
-        values = [(o.get('value') or o.text or '').strip() for o in selected_options]
+        selected_options = ele.xpath(".//option[@selected]")
+        values = [(o.get("value") or o.text or "").strip() for o in selected_options]
         return n, values
     return n, v
 
 
-def _get_clickable(clickdata: Optional[dict], form: FormElement) -> Optional[Tuple[str, str]]:
+def _get_clickable(
+    clickdata: Optional[dict], form: FormElement
+) -> Optional[Tuple[str, str]]:
     """
     Returns the clickable element specified in clickdata,
     if the latter is given. If not, it returns the first
     clickable element found
     """
-    clickables = list(form.xpath(
-        'descendant::input[re:test(@type, "^(submit|image)$", "i")]'
-        '|descendant::button[not(@type) or re:test(@type, "^submit$", "i")]',
-        namespaces={"re": "http://exslt.org/regular-expressions"}
-    ))
+    clickables = list(
+        form.xpath(
+            'descendant::input[re:test(@type, "^(submit|image)$", "i")]'
+            '|descendant::button[not(@type) or re:test(@type, "^submit$", "i")]',
+            namespaces={"re": "http://exslt.org/regular-expressions"},
+        )
+    )
     if not clickables:
         return None
 
     # If we don't have clickdata, we just use the first clickable element
     if clickdata is None:
         el = clickables[0]
-        return (el.get('name'), el.get('value') or '')
+        return (el.get("name"), el.get("value") or "")
 
     # If clickdata is given, we compare it to the clickable elements to find a
     # match. We first look to see if the number is specified in clickdata,
     # because that uniquely identifies the element
-    nr = clickdata.get('nr', None)
+    nr = clickdata.get("nr", None)
     if nr is not None:
         try:
             el = list(form.inputs)[nr]
         except IndexError:
             pass
         else:
-            return (el.get('name'), el.get('value') or '')
+            return (el.get("name"), el.get("value") or "")
 
     # We didn't find it, so now we build an XPath expression out of the other
     # arguments, because they can be used as such
-    xpath = './/*' + ''.join(f'[@{k}="{v}"]' for k, v in clickdata.items())
+    xpath = ".//*" + "".join(f'[@{k}="{v}"]' for k, v in clickdata.items())
     el = form.xpath(xpath)
     if len(el) == 1:
-        return (el[0].get('name'), el[0].get('value') or '')
-    elif len(el) > 1:
-        raise ValueError(f"Multiple elements found ({el!r}) matching the "
-                         f"criteria in clickdata: {clickdata!r}")
+        return (el[0].get("name"), el[0].get("value") or "")
+    if len(el) > 1:
+        raise ValueError(
+            f"Multiple elements found ({el!r}) matching the "
+            f"criteria in clickdata: {clickdata!r}"
+        )
     else:
-        raise ValueError(f'No clickable element matching clickdata: {clickdata!r}')
+        raise ValueError(f"No clickable element matching clickdata: {clickdata!r}")
```

### Comparing `Scrapy-2.7.1/scrapy/http/request/json_request.py` & `Scrapy-2.8.0/scrapy/http/request/json_request.py`

 * *Files 8% similar despite different names*

```diff
@@ -16,47 +16,49 @@
 
 class JsonRequest(Request):
 
     attributes: Tuple[str, ...] = Request.attributes + ("dumps_kwargs",)
 
     def __init__(self, *args, dumps_kwargs: Optional[dict] = None, **kwargs) -> None:
         dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}
-        dumps_kwargs.setdefault('sort_keys', True)
+        dumps_kwargs.setdefault("sort_keys", True)
         self._dumps_kwargs = dumps_kwargs
 
-        body_passed = kwargs.get('body', None) is not None
-        data = kwargs.pop('data', None)
+        body_passed = kwargs.get("body", None) is not None
+        data = kwargs.pop("data", None)
         data_passed = data is not None
 
         if body_passed and data_passed:
-            warnings.warn('Both body and data passed. data will be ignored')
+            warnings.warn("Both body and data passed. data will be ignored")
         elif not body_passed and data_passed:
-            kwargs['body'] = self._dumps(data)
-            if 'method' not in kwargs:
-                kwargs['method'] = 'POST'
+            kwargs["body"] = self._dumps(data)
+            if "method" not in kwargs:
+                kwargs["method"] = "POST"
 
         super().__init__(*args, **kwargs)
-        self.headers.setdefault('Content-Type', 'application/json')
-        self.headers.setdefault('Accept', 'application/json, text/javascript, */*; q=0.01')
+        self.headers.setdefault("Content-Type", "application/json")
+        self.headers.setdefault(
+            "Accept", "application/json, text/javascript, */*; q=0.01"
+        )
 
     @property
     def dumps_kwargs(self) -> dict:
         return self._dumps_kwargs
 
     def replace(self, *args, **kwargs) -> Request:
-        body_passed = kwargs.get('body', None) is not None
-        data = kwargs.pop('data', None)
+        body_passed = kwargs.get("body", None) is not None
+        data = kwargs.pop("data", None)
         data_passed = data is not None
 
         if body_passed and data_passed:
-            warnings.warn('Both body and data passed. data will be ignored')
+            warnings.warn("Both body and data passed. data will be ignored")
         elif not body_passed and data_passed:
-            kwargs['body'] = self._dumps(data)
+            kwargs["body"] = self._dumps(data)
 
         return super().replace(*args, **kwargs)
 
     def _dumps(self, data: dict) -> str:
-        """Convert to JSON """
+        """Convert to JSON"""
         return json.dumps(data, **self._dumps_kwargs)
 
 
 JSONRequest = create_deprecated_class("JSONRequest", JsonRequest)
```

### Comparing `Scrapy-2.7.1/scrapy/http/request/rpc.py` & `Scrapy-2.8.0/scrapy/http/request/rpc.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,30 +6,28 @@
 """
 import xmlrpc.client as xmlrpclib
 from typing import Optional
 
 from scrapy.http.request import Request
 from scrapy.utils.python import get_func_args
 
-
 DUMPS_ARGS = get_func_args(xmlrpclib.dumps)
 
 
 class XmlRpcRequest(Request):
-
     def __init__(self, *args, encoding: Optional[str] = None, **kwargs):
-        if 'body' not in kwargs and 'params' in kwargs:
+        if "body" not in kwargs and "params" in kwargs:
             kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)
-            kwargs['body'] = xmlrpclib.dumps(**kw)
+            kwargs["body"] = xmlrpclib.dumps(**kw)
 
         # spec defines that requests must use POST method
-        kwargs.setdefault('method', 'POST')
+        kwargs.setdefault("method", "POST")
 
         # xmlrpc query multiples times over the same url
-        kwargs.setdefault('dont_filter', True)
+        kwargs.setdefault("dont_filter", True)
 
         # restore encoding
         if encoding is not None:
-            kwargs['encoding'] = encoding
+            kwargs["encoding"] = encoding
 
         super().__init__(*args, **kwargs)
-        self.headers.setdefault('Content-Type', 'text/xml')
+        self.headers.setdefault("Content-Type", "text/xml")
```

### Comparing `Scrapy-2.7.1/scrapy/http/response/__init__.py` & `Scrapy-2.8.0/scrapy/http/response/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,26 +17,34 @@
 
 class Response(object_ref):
     """An object that represents an HTTP response, which is usually
     downloaded (by the Downloader) and fed to the Spiders for processing.
     """
 
     attributes: Tuple[str, ...] = (
-        "url", "status", "headers", "body", "flags", "request", "certificate", "ip_address", "protocol",
+        "url",
+        "status",
+        "headers",
+        "body",
+        "flags",
+        "request",
+        "certificate",
+        "ip_address",
+        "protocol",
     )
     """A tuple of :class:`str` objects containing the name of all public
     attributes of the class that are also keyword parameters of the
     ``__init__`` method.
 
     Currently used by :meth:`Response.replace`.
     """
 
     def __init__(
         self,
-        url,
+        url: str,
         status=200,
         headers=None,
         body=b"",
         flags=None,
         request=None,
         certificate=None,
         ip_address=None,
@@ -71,53 +79,53 @@
                 "Response.meta not available, this response "
                 "is not tied to any request"
             )
 
     def _get_url(self):
         return self._url
 
-    def _set_url(self, url):
+    def _set_url(self, url: str):
         if isinstance(url, str):
             self._url = url
         else:
-            raise TypeError(f'{type(self).__name__} url must be str, '
-                            f'got {type(url).__name__}')
+            raise TypeError(
+                f"{type(self).__name__} url must be str, " f"got {type(url).__name__}"
+            )
 
-    url = property(_get_url, obsolete_setter(_set_url, 'url'))
+    url = property(_get_url, obsolete_setter(_set_url, "url"))
 
     def _get_body(self):
         return self._body
 
     def _set_body(self, body):
         if body is None:
-            self._body = b''
+            self._body = b""
         elif not isinstance(body, bytes):
             raise TypeError(
                 "Response body must be bytes. "
                 "If you want to pass unicode body use TextResponse "
-                "or HtmlResponse.")
+                "or HtmlResponse."
+            )
         else:
             self._body = body
 
-    body = property(_get_body, obsolete_setter(_set_body, 'body'))
+    body = property(_get_body, obsolete_setter(_set_body, "body"))
 
-    def __str__(self):
+    def __repr__(self):
         return f"<{self.status} {self.url}>"
 
-    __repr__ = __str__
-
     def copy(self):
         """Return a copy of this Response"""
         return self.replace()
 
     def replace(self, *args, **kwargs):
         """Create a new Response with the same attributes except for those given new values"""
         for x in self.attributes:
             kwargs.setdefault(x, getattr(self, x))
-        cls = kwargs.pop('cls', self.__class__)
+        cls = kwargs.pop("cls", self.__class__)
         return cls(*args, **kwargs)
 
     def urljoin(self, url):
         """Join this Response's url with a possible relative url to form an
         absolute interpretation of the latter."""
         return urljoin(self.url, url)
 
@@ -136,18 +144,30 @@
 
     def xpath(self, *a, **kw):
         """Shortcut method implemented only by responses whose content
         is text (subclasses of TextResponse).
         """
         raise NotSupported("Response content isn't text")
 
-    def follow(self, url, callback=None, method='GET', headers=None, body=None,
-               cookies=None, meta=None, encoding='utf-8', priority=0,
-               dont_filter=False, errback=None, cb_kwargs=None, flags=None):
-        # type: (...) -> Request
+    def follow(
+        self,
+        url,
+        callback=None,
+        method="GET",
+        headers=None,
+        body=None,
+        cookies=None,
+        meta=None,
+        encoding="utf-8",
+        priority=0,
+        dont_filter=False,
+        errback=None,
+        cb_kwargs=None,
+        flags=None,
+    ) -> Request:
         """
         Return a :class:`~.Request` instance to follow a link ``url``.
         It accepts the same arguments as ``Request.__init__`` method,
         but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,
         not only an absolute URL.
 
         :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow`
@@ -175,31 +195,43 @@
             priority=priority,
             dont_filter=dont_filter,
             errback=errback,
             cb_kwargs=cb_kwargs,
             flags=flags,
         )
 
-    def follow_all(self, urls, callback=None, method='GET', headers=None, body=None,
-                   cookies=None, meta=None, encoding='utf-8', priority=0,
-                   dont_filter=False, errback=None, cb_kwargs=None, flags=None):
-        # type: (...) -> Generator[Request, None, None]
+    def follow_all(
+        self,
+        urls,
+        callback=None,
+        method="GET",
+        headers=None,
+        body=None,
+        cookies=None,
+        meta=None,
+        encoding="utf-8",
+        priority=0,
+        dont_filter=False,
+        errback=None,
+        cb_kwargs=None,
+        flags=None,
+    ) -> Generator[Request, None, None]:
         """
         .. versionadded:: 2.0
 
         Return an iterable of :class:`~.Request` instances to follow all links
         in ``urls``. It accepts the same arguments as ``Request.__init__`` method,
         but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,
         not only absolute URLs.
 
         :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all`
         method which supports selectors in addition to absolute/relative URLs
         and Link objects.
         """
-        if not hasattr(urls, '__iter__'):
+        if not hasattr(urls, "__iter__"):
             raise TypeError("'urls' argument must be an iterable")
         return (
             self.follow(
                 url=url,
                 callback=callback,
                 method=method,
                 headers=headers,
```

### Comparing `Scrapy-2.7.1/scrapy/http/response/text.py` & `Scrapy-2.8.0/scrapy/http/response/text.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,53 +11,55 @@
 from urllib.parse import urljoin
 
 import parsel
 from w3lib.encoding import (
     html_body_declared_encoding,
     html_to_unicode,
     http_content_type_encoding,
-    resolve_encoding,
     read_bom,
+    resolve_encoding,
 )
 from w3lib.html import strip_html5_whitespace
 
 from scrapy.http import Request
 from scrapy.http.response import Response
 from scrapy.utils.python import memoizemethod_noargs, to_unicode
 from scrapy.utils.response import get_base_url
 
 _NONE = object()
 
 
 class TextResponse(Response):
 
-    _DEFAULT_ENCODING = 'ascii'
+    _DEFAULT_ENCODING = "ascii"
     _cached_decoded_json = _NONE
 
     attributes: Tuple[str, ...] = Response.attributes + ("encoding",)
 
     def __init__(self, *args, **kwargs):
-        self._encoding = kwargs.pop('encoding', None)
+        self._encoding = kwargs.pop("encoding", None)
         self._cached_benc = None
         self._cached_ubody = None
         self._cached_selector = None
         super().__init__(*args, **kwargs)
 
     def _set_url(self, url):
         if isinstance(url, str):
             self._url = to_unicode(url, self.encoding)
         else:
             super()._set_url(url)
 
     def _set_body(self, body):
-        self._body = b''  # used by encoding detection
+        self._body = b""  # used by encoding detection
         if isinstance(body, str):
             if self._encoding is None:
-                raise TypeError('Cannot convert unicode body - '
-                                f'{type(self).__name__} has no encoding')
+                raise TypeError(
+                    "Cannot convert unicode body - "
+                    f"{type(self).__name__} has no encoding"
+                )
             self._body = body.encode(self._encoding)
         else:
             super()._set_body(body)
 
     @property
     def encoding(self):
         return self._declared_encoding() or self._body_inferred_encoding()
@@ -78,45 +80,48 @@
         """
         if self._cached_decoded_json is _NONE:
             self._cached_decoded_json = json.loads(self.text)
         return self._cached_decoded_json
 
     @property
     def text(self):
-        """ Body as unicode """
+        """Body as unicode"""
         # access self.encoding before _cached_ubody to make sure
         # _body_inferred_encoding is called
         benc = self.encoding
         if self._cached_ubody is None:
-            charset = f'charset={benc}'
+            charset = f"charset={benc}"
             self._cached_ubody = html_to_unicode(charset, self.body)[1]
         return self._cached_ubody
 
     def urljoin(self, url):
         """Join this Response's url with a possible relative url to form an
         absolute interpretation of the latter."""
         return urljoin(get_base_url(self), url)
 
     @memoizemethod_noargs
     def _headers_encoding(self):
-        content_type = self.headers.get(b'Content-Type', b'')
+        content_type = self.headers.get(b"Content-Type", b"")
         return http_content_type_encoding(to_unicode(content_type))
 
     def _body_inferred_encoding(self):
         if self._cached_benc is None:
-            content_type = to_unicode(self.headers.get(b'Content-Type', b''))
-            benc, ubody = html_to_unicode(content_type, self.body,
-                                          auto_detect_fun=self._auto_detect_fun,
-                                          default_encoding=self._DEFAULT_ENCODING)
+            content_type = to_unicode(self.headers.get(b"Content-Type", b""))
+            benc, ubody = html_to_unicode(
+                content_type,
+                self.body,
+                auto_detect_fun=self._auto_detect_fun,
+                default_encoding=self._DEFAULT_ENCODING,
+            )
             self._cached_benc = benc
             self._cached_ubody = ubody
         return self._cached_benc
 
     def _auto_detect_fun(self, text):
-        for enc in (self._DEFAULT_ENCODING, 'utf-8', 'cp1252'):
+        for enc in (self._DEFAULT_ENCODING, "utf-8", "cp1252"):
             try:
                 text.decode(enc)
             except UnicodeError:
                 continue
             return resolve_encoding(enc)
 
     @memoizemethod_noargs
@@ -126,28 +131,41 @@
     @memoizemethod_noargs
     def _bom_encoding(self):
         return read_bom(self.body)[0]
 
     @property
     def selector(self):
         from scrapy.selector import Selector
+
         if self._cached_selector is None:
             self._cached_selector = Selector(self)
         return self._cached_selector
 
     def xpath(self, query, **kwargs):
         return self.selector.xpath(query, **kwargs)
 
     def css(self, query):
         return self.selector.css(query)
 
-    def follow(self, url, callback=None, method='GET', headers=None, body=None,
-               cookies=None, meta=None, encoding=None, priority=0,
-               dont_filter=False, errback=None, cb_kwargs=None, flags=None):
-        # type: (...) -> Request
+    def follow(
+        self,
+        url,
+        callback=None,
+        method="GET",
+        headers=None,
+        body=None,
+        cookies=None,
+        meta=None,
+        encoding=None,
+        priority=0,
+        dont_filter=False,
+        errback=None,
+        cb_kwargs=None,
+        flags=None,
+    ) -> Request:
         """
         Return a :class:`~.Request` instance to follow a link ``url``.
         It accepts the same arguments as ``Request.__init__`` method,
         but ``url`` can be not only an absolute URL, but also
 
         * a relative URL
         * a :class:`~scrapy.link.Link` object, e.g. the result of
@@ -177,19 +195,32 @@
             priority=priority,
             dont_filter=dont_filter,
             errback=errback,
             cb_kwargs=cb_kwargs,
             flags=flags,
         )
 
-    def follow_all(self, urls=None, callback=None, method='GET', headers=None, body=None,
-                   cookies=None, meta=None, encoding=None, priority=0,
-                   dont_filter=False, errback=None, cb_kwargs=None, flags=None,
-                   css=None, xpath=None):
-        # type: (...) -> Generator[Request, None, None]
+    def follow_all(
+        self,
+        urls=None,
+        callback=None,
+        method="GET",
+        headers=None,
+        body=None,
+        cookies=None,
+        meta=None,
+        encoding=None,
+        priority=0,
+        dont_filter=False,
+        errback=None,
+        cb_kwargs=None,
+        flags=None,
+        css=None,
+        xpath=None,
+    ) -> Generator[Request, None, None]:
         """
         A generator that produces :class:`~.Request` instances to follow all
         links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s
         ``__init__`` method, except that each ``urls`` element does not need to be
         an absolute URL, it can be any of the following:
 
         * a relative URL
@@ -249,16 +280,17 @@
 
 
 def _url_from_selector(sel):
     # type: (parsel.Selector) -> str
     if isinstance(sel.root, str):
         # e.g. ::attr(href) result
         return strip_html5_whitespace(sel.root)
-    if not hasattr(sel.root, 'tag'):
+    if not hasattr(sel.root, "tag"):
         raise _InvalidSelector(f"Unsupported selector: {sel}")
-    if sel.root.tag not in ('a', 'link'):
-        raise _InvalidSelector("Only <a> and <link> elements are supported; "
-                               f"got <{sel.root.tag}>")
-    href = sel.root.get('href')
+    if sel.root.tag not in ("a", "link"):
+        raise _InvalidSelector(
+            "Only <a> and <link> elements are supported; " f"got <{sel.root.tag}>"
+        )
+    href = sel.root.get("href")
     if href is None:
         raise _InvalidSelector(f"<{sel.root.tag}> element has no href attribute: {sel}")
     return strip_html5_whitespace(href)
```

### Comparing `Scrapy-2.7.1/scrapy/interfaces.py` & `Scrapy-2.8.0/scrapy/interfaces.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 from zope.interface import Interface
 
 
 class ISpiderLoader(Interface):
-
     def from_settings(settings):
         """Return an instance of the class for the given settings"""
 
     def load(spider_name):
         """Return the Spider class for the given spider name. If the spider
         name is not found, it must raise a KeyError."""
```

### Comparing `Scrapy-2.7.1/scrapy/item.py` & `Scrapy-2.8.0/scrapy/item.py`

 * *Files 14% similar despite different names*

```diff
@@ -20,31 +20,31 @@
 class ItemMeta(ABCMeta):
     """Metaclass_ of :class:`Item` that handles field definitions.
 
     .. _metaclass: https://realpython.com/python-metaclasses
     """
 
     def __new__(mcs, class_name, bases, attrs):
-        classcell = attrs.pop('__classcell__', None)
-        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))
-        _class = super().__new__(mcs, 'x_' + class_name, new_bases, attrs)
+        classcell = attrs.pop("__classcell__", None)
+        new_bases = tuple(base._class for base in bases if hasattr(base, "_class"))
+        _class = super().__new__(mcs, "x_" + class_name, new_bases, attrs)
 
-        fields = getattr(_class, 'fields', {})
+        fields = getattr(_class, "fields", {})
         new_attrs = {}
         for n in dir(_class):
             v = getattr(_class, n)
             if isinstance(v, Field):
                 fields[n] = v
             elif n in attrs:
                 new_attrs[n] = attrs[n]
 
-        new_attrs['fields'] = fields
-        new_attrs['_class'] = _class
+        new_attrs["fields"] = fields
+        new_attrs["_class"] = _class
         if classcell is not None:
-            new_attrs['__classcell__'] = classcell
+            new_attrs["__classcell__"] = classcell
         return super().__new__(mcs, class_name, bases, new_attrs)
 
 
 class Item(MutableMapping, object_ref, metaclass=ItemMeta):
     """
     Base class for scraped items.
 
@@ -89,15 +89,15 @@
 
     def __getattr__(self, name):
         if name in self.fields:
             raise AttributeError(f"Use item[{name!r}] to get field value")
         raise AttributeError(name)
 
     def __setattr__(self, name, value):
-        if not name.startswith('_'):
+        if not name.startswith("_"):
             raise AttributeError(f"Use item[{name!r}] = {value!r} to set field value")
         super().__setattr__(name, value)
 
     def __len__(self):
         return len(self._values)
 
     def __iter__(self):
@@ -111,10 +111,9 @@
     def __repr__(self):
         return pformat(dict(self))
 
     def copy(self):
         return self.__class__(self)
 
     def deepcopy(self):
-        """Return a :func:`~copy.deepcopy` of this item.
-        """
+        """Return a :func:`~copy.deepcopy` of this item."""
         return deepcopy(self)
```

### Comparing `Scrapy-2.7.1/scrapy/link.py` & `Scrapy-2.8.0/scrapy/link.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,17 +20,17 @@
 
     :param fragment: the part of the url after the hash symbol. From the sample, this is ``foo``.
 
     :param nofollow: an indication of the presence or absence of a nofollow value in the ``rel`` attribute
                     of the anchor tag.
     """
 
-    __slots__ = ['url', 'text', 'fragment', 'nofollow']
+    __slots__ = ["url", "text", "fragment", "nofollow"]
 
-    def __init__(self, url, text='', fragment='', nofollow=False):
+    def __init__(self, url, text="", fragment="", nofollow=False):
         if not isinstance(url, str):
             got = url.__class__.__name__
             raise TypeError(f"Link urls must be str objects, got {got}")
         self.url = url
         self.text = text
         self.fragment = fragment
         self.nofollow = nofollow
@@ -40,14 +40,16 @@
             self.url == other.url
             and self.text == other.text
             and self.fragment == other.fragment
             and self.nofollow == other.nofollow
         )
 
     def __hash__(self):
-        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)
+        return (
+            hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)
+        )
 
     def __repr__(self):
         return (
-            f'Link(url={self.url!r}, text={self.text!r}, '
-            f'fragment={self.fragment!r}, nofollow={self.nofollow!r})'
+            f"Link(url={self.url!r}, text={self.text!r}, "
+            f"fragment={self.fragment!r}, nofollow={self.nofollow!r})"
         )
```

### Comparing `Scrapy-2.7.1/scrapy/loader/__init__.py` & `Scrapy-2.8.0/scrapy/loader/__init__.py`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/scrapy/loader/common.py` & `Scrapy-2.8.0/scrapy/loader/common.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,11 +11,11 @@
     """Wrap functions that receive loader_context to contain the context
     "pre-loaded" and expose a interface that receives only one argument
     """
     warnings.warn(
         "scrapy.loader.common.wrap_loader_context has moved to a new library."
         "Please update your reference to itemloaders.common.wrap_loader_context",
         ScrapyDeprecationWarning,
-        stacklevel=2
+        stacklevel=2,
     )
 
     return common.wrap_loader_context(function, context)
```

### Comparing `Scrapy-2.7.1/scrapy/loader/processors.py` & `Scrapy-2.8.0/scrapy/loader/processors.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,19 +3,18 @@
 
 See documentation in docs/topics/loaders.rst
 """
 from itemloaders import processors
 
 from scrapy.utils.deprecate import create_deprecated_class
 
+MapCompose = create_deprecated_class("MapCompose", processors.MapCompose)
 
-MapCompose = create_deprecated_class('MapCompose', processors.MapCompose)
+Compose = create_deprecated_class("Compose", processors.Compose)
 
-Compose = create_deprecated_class('Compose', processors.Compose)
+TakeFirst = create_deprecated_class("TakeFirst", processors.TakeFirst)
 
-TakeFirst = create_deprecated_class('TakeFirst', processors.TakeFirst)
+Identity = create_deprecated_class("Identity", processors.Identity)
 
-Identity = create_deprecated_class('Identity', processors.Identity)
+SelectJmes = create_deprecated_class("SelectJmes", processors.SelectJmes)
 
-SelectJmes = create_deprecated_class('SelectJmes', processors.SelectJmes)
-
-Join = create_deprecated_class('Join', processors.Join)
+Join = create_deprecated_class("Join", processors.Join)
```

### Comparing `Scrapy-2.7.1/scrapy/logformatter.py` & `Scrapy-2.8.0/scrapy/logformatter.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-import os
 import logging
+import os
 
 from twisted.python.failure import Failure
 
 from scrapy.utils.request import referer_str
 
 SCRAPEDMSG = "Scraped from %(src)s" + os.linesep + "%(item)s"
 DROPPEDMSG = "Dropped: %(exception)s" + os.linesep + "%(item)s"
@@ -50,98 +50,98 @@
                             'item': item,
                         }
                     }
     """
 
     def crawled(self, request, response, spider):
         """Logs a message when the crawler finds a webpage."""
-        request_flags = f' {str(request.flags)}' if request.flags else ''
-        response_flags = f' {str(response.flags)}' if response.flags else ''
+        request_flags = f" {str(request.flags)}" if request.flags else ""
+        response_flags = f" {str(response.flags)}" if response.flags else ""
         return {
-            'level': logging.DEBUG,
-            'msg': CRAWLEDMSG,
-            'args': {
-                'status': response.status,
-                'request': request,
-                'request_flags': request_flags,
-                'referer': referer_str(request),
-                'response_flags': response_flags,
+            "level": logging.DEBUG,
+            "msg": CRAWLEDMSG,
+            "args": {
+                "status": response.status,
+                "request": request,
+                "request_flags": request_flags,
+                "referer": referer_str(request),
+                "response_flags": response_flags,
                 # backward compatibility with Scrapy logformatter below 1.4 version
-                'flags': response_flags
-            }
+                "flags": response_flags,
+            },
         }
 
     def scraped(self, item, response, spider):
         """Logs a message when an item is scraped by a spider."""
         if isinstance(response, Failure):
             src = response.getErrorMessage()
         else:
             src = response
         return {
-            'level': logging.DEBUG,
-            'msg': SCRAPEDMSG,
-            'args': {
-                'src': src,
-                'item': item,
-            }
+            "level": logging.DEBUG,
+            "msg": SCRAPEDMSG,
+            "args": {
+                "src": src,
+                "item": item,
+            },
         }
 
     def dropped(self, item, exception, response, spider):
         """Logs a message when an item is dropped while it is passing through the item pipeline."""
         return {
-            'level': logging.WARNING,
-            'msg': DROPPEDMSG,
-            'args': {
-                'exception': exception,
-                'item': item,
-            }
+            "level": logging.WARNING,
+            "msg": DROPPEDMSG,
+            "args": {
+                "exception": exception,
+                "item": item,
+            },
         }
 
     def item_error(self, item, exception, response, spider):
         """Logs a message when an item causes an error while it is passing
         through the item pipeline.
 
         .. versionadded:: 2.0
         """
         return {
-            'level': logging.ERROR,
-            'msg': ITEMERRORMSG,
-            'args': {
-                'item': item,
-            }
+            "level": logging.ERROR,
+            "msg": ITEMERRORMSG,
+            "args": {
+                "item": item,
+            },
         }
 
     def spider_error(self, failure, request, response, spider):
         """Logs an error message from a spider.
 
         .. versionadded:: 2.0
         """
         return {
-            'level': logging.ERROR,
-            'msg': SPIDERERRORMSG,
-            'args': {
-                'request': request,
-                'referer': referer_str(request),
-            }
+            "level": logging.ERROR,
+            "msg": SPIDERERRORMSG,
+            "args": {
+                "request": request,
+                "referer": referer_str(request),
+            },
         }
 
     def download_error(self, failure, request, spider, errmsg=None):
         """Logs a download error message from a spider (typically coming from
         the engine).
 
         .. versionadded:: 2.0
         """
-        args = {'request': request}
+        args = {"request": request}
         if errmsg:
             msg = DOWNLOADERRORMSG_LONG
-            args['errmsg'] = errmsg
+            args["errmsg"] = errmsg
         else:
             msg = DOWNLOADERRORMSG_SHORT
         return {
-            'level': logging.ERROR,
-            'msg': msg,
-            'args': args,
+            "level": logging.ERROR,
+            "msg": msg,
+            "args": args,
         }
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls()
```

### Comparing `Scrapy-2.7.1/scrapy/mail.py` & `Scrapy-2.8.0/scrapy/mail.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,20 +8,21 @@
 from email.mime.base import MIMEBase
 from email.mime.multipart import MIMEMultipart
 from email.mime.nonmultipart import MIMENonMultipart
 from email.mime.text import MIMEText
 from email.utils import formatdate
 from io import BytesIO
 
+from twisted import version as twisted_version
 from twisted.internet import defer, ssl
+from twisted.python.versions import Version
 
 from scrapy.utils.misc import arg_to_iter
 from scrapy.utils.python import to_bytes
 
-
 logger = logging.getLogger(__name__)
 
 
 # Defined in the email.utils module, but undocumented:
 # https://github.com/python/cpython/blob/v3.9.0/Lib/email/utils.py#L42
 COMMASPACE = ", "
 
@@ -30,116 +31,178 @@
     if text is None:
         return None
     return to_bytes(text)
 
 
 class MailSender:
     def __init__(
-        self, smtphost='localhost', mailfrom='scrapy@localhost', smtpuser=None,
-        smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False
+        self,
+        smtphost="localhost",
+        mailfrom="scrapy@localhost",
+        smtpuser=None,
+        smtppass=None,
+        smtpport=25,
+        smtptls=False,
+        smtpssl=False,
+        debug=False,
     ):
         self.smtphost = smtphost
         self.smtpport = smtpport
         self.smtpuser = _to_bytes_or_none(smtpuser)
         self.smtppass = _to_bytes_or_none(smtppass)
         self.smtptls = smtptls
         self.smtpssl = smtpssl
         self.mailfrom = mailfrom
         self.debug = debug
 
     @classmethod
     def from_settings(cls, settings):
         return cls(
-            smtphost=settings['MAIL_HOST'],
-            mailfrom=settings['MAIL_FROM'],
-            smtpuser=settings['MAIL_USER'],
-            smtppass=settings['MAIL_PASS'],
-            smtpport=settings.getint('MAIL_PORT'),
-            smtptls=settings.getbool('MAIL_TLS'),
-            smtpssl=settings.getbool('MAIL_SSL'),
+            smtphost=settings["MAIL_HOST"],
+            mailfrom=settings["MAIL_FROM"],
+            smtpuser=settings["MAIL_USER"],
+            smtppass=settings["MAIL_PASS"],
+            smtpport=settings.getint("MAIL_PORT"),
+            smtptls=settings.getbool("MAIL_TLS"),
+            smtpssl=settings.getbool("MAIL_SSL"),
         )
 
-    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):
+    def send(
+        self,
+        to,
+        subject,
+        body,
+        cc=None,
+        attachs=(),
+        mimetype="text/plain",
+        charset=None,
+        _callback=None,
+    ):
         from twisted.internet import reactor
+
         if attachs:
             msg = MIMEMultipart()
         else:
-            msg = MIMENonMultipart(*mimetype.split('/', 1))
+            msg = MIMENonMultipart(*mimetype.split("/", 1))
 
         to = list(arg_to_iter(to))
         cc = list(arg_to_iter(cc))
 
-        msg['From'] = self.mailfrom
-        msg['To'] = COMMASPACE.join(to)
-        msg['Date'] = formatdate(localtime=True)
-        msg['Subject'] = subject
+        msg["From"] = self.mailfrom
+        msg["To"] = COMMASPACE.join(to)
+        msg["Date"] = formatdate(localtime=True)
+        msg["Subject"] = subject
         rcpts = to[:]
         if cc:
             rcpts.extend(cc)
-            msg['Cc'] = COMMASPACE.join(cc)
+            msg["Cc"] = COMMASPACE.join(cc)
 
         if charset:
             msg.set_charset(charset)
 
         if attachs:
-            msg.attach(MIMEText(body, 'plain', charset or 'us-ascii'))
+            msg.attach(MIMEText(body, "plain", charset or "us-ascii"))
             for attach_name, mimetype, f in attachs:
-                part = MIMEBase(*mimetype.split('/'))
+                part = MIMEBase(*mimetype.split("/"))
                 part.set_payload(f.read())
                 Encoders.encode_base64(part)
-                part.add_header('Content-Disposition', 'attachment', filename=attach_name)
+                part.add_header(
+                    "Content-Disposition", "attachment", filename=attach_name
+                )
                 msg.attach(part)
         else:
             msg.set_payload(body)
 
         if _callback:
             _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)
 
         if self.debug:
-            logger.debug('Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s '
-                         'Subject="%(mailsubject)s" Attachs=%(mailattachs)d',
-                         {'mailto': to, 'mailcc': cc, 'mailsubject': subject,
-                          'mailattachs': len(attachs)})
+            logger.debug(
+                "Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s "
+                'Subject="%(mailsubject)s" Attachs=%(mailattachs)d',
+                {
+                    "mailto": to,
+                    "mailcc": cc,
+                    "mailsubject": subject,
+                    "mailattachs": len(attachs),
+                },
+            )
             return
 
-        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or 'utf-8'))
+        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or "utf-8"))
         dfd.addCallbacks(
             callback=self._sent_ok,
             errback=self._sent_failed,
             callbackArgs=[to, cc, subject, len(attachs)],
             errbackArgs=[to, cc, subject, len(attachs)],
         )
-        reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)
+        reactor.addSystemEventTrigger("before", "shutdown", lambda: dfd)
         return dfd
 
     def _sent_ok(self, result, to, cc, subject, nattachs):
-        logger.info('Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '
-                    'Subject="%(mailsubject)s" Attachs=%(mailattachs)d',
-                    {'mailto': to, 'mailcc': cc, 'mailsubject': subject,
-                     'mailattachs': nattachs})
+        logger.info(
+            "Mail sent OK: To=%(mailto)s Cc=%(mailcc)s "
+            'Subject="%(mailsubject)s" Attachs=%(mailattachs)d',
+            {
+                "mailto": to,
+                "mailcc": cc,
+                "mailsubject": subject,
+                "mailattachs": nattachs,
+            },
+        )
 
     def _sent_failed(self, failure, to, cc, subject, nattachs):
         errstr = str(failure.value)
-        logger.error('Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '
-                     'Subject="%(mailsubject)s" Attachs=%(mailattachs)d'
-                     '- %(mailerr)s',
-                     {'mailto': to, 'mailcc': cc, 'mailsubject': subject,
-                      'mailattachs': nattachs, 'mailerr': errstr})
+        logger.error(
+            "Unable to send mail: To=%(mailto)s Cc=%(mailcc)s "
+            'Subject="%(mailsubject)s" Attachs=%(mailattachs)d'
+            "- %(mailerr)s",
+            {
+                "mailto": to,
+                "mailcc": cc,
+                "mailsubject": subject,
+                "mailattachs": nattachs,
+                "mailerr": errstr,
+            },
+        )
 
     def _sendmail(self, to_addrs, msg):
-        # Import twisted.mail here because it is not available in python3
         from twisted.internet import reactor
-        from twisted.mail.smtp import ESMTPSenderFactory
+
         msg = BytesIO(msg)
         d = defer.Deferred()
-        factory = ESMTPSenderFactory(
-            self.smtpuser, self.smtppass, self.mailfrom, to_addrs, msg, d,
-            heloFallback=True, requireAuthentication=False, requireTransportSecurity=self.smtptls,
-        )
-        factory.noisy = False
+
+        factory = self._create_sender_factory(to_addrs, msg, d)
 
         if self.smtpssl:
-            reactor.connectSSL(self.smtphost, self.smtpport, factory, ssl.ClientContextFactory())
+            reactor.connectSSL(
+                self.smtphost, self.smtpport, factory, ssl.ClientContextFactory()
+            )
         else:
             reactor.connectTCP(self.smtphost, self.smtpport, factory)
 
         return d
+
+    def _create_sender_factory(self, to_addrs, msg, d):
+        from twisted.mail.smtp import ESMTPSenderFactory
+
+        factory_keywords = {
+            "heloFallback": True,
+            "requireAuthentication": False,
+            "requireTransportSecurity": self.smtptls,
+        }
+
+        # Newer versions of twisted require the hostname to use STARTTLS
+        if twisted_version >= Version("twisted", 21, 2, 0):
+            factory_keywords["hostname"] = self.smtphost
+
+        factory = ESMTPSenderFactory(
+            self.smtpuser,
+            self.smtppass,
+            self.mailfrom,
+            to_addrs,
+            msg,
+            d,
+            **factory_keywords
+        )
+        factory.noisy = False
+        return factory
```

### Comparing `Scrapy-2.7.1/scrapy/middleware.py` & `Scrapy-2.8.0/scrapy/middleware.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,33 +1,35 @@
 import logging
 import pprint
 from collections import defaultdict, deque
-from typing import Callable, Deque, Dict, Iterable, Tuple, Union, cast
+from typing import Any, Callable, Deque, Dict, Iterable, Tuple, Union, cast
 
 from twisted.internet.defer import Deferred
 
 from scrapy import Spider
 from scrapy.exceptions import NotConfigured
 from scrapy.settings import Settings
+from scrapy.utils.defer import process_chain, process_parallel
 from scrapy.utils.misc import create_instance, load_object
-from scrapy.utils.defer import process_parallel, process_chain
 
 logger = logging.getLogger(__name__)
 
 
 class MiddlewareManager:
     """Base class for implementing middleware managers"""
 
-    component_name = 'foo middleware'
+    component_name = "foo middleware"
 
-    def __init__(self, *middlewares):
+    def __init__(self, *middlewares: Any) -> None:
         self.middlewares = middlewares
         # Only process_spider_output and process_spider_exception can be None.
         # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.
-        self.methods: Dict[str, Deque[Union[None, Callable, Tuple[Callable, Callable]]]] = defaultdict(deque)
+        self.methods: Dict[
+            str, Deque[Union[None, Callable, Tuple[Callable, Callable]]]
+        ] = defaultdict(deque)
         for mw in middlewares:
             self._add_middleware(mw)
 
     @classmethod
     def _get_mwlist_from_settings(cls, settings: Settings) -> list:
         raise NotImplementedError
 
@@ -40,41 +42,47 @@
             try:
                 mwcls = load_object(clspath)
                 mw = create_instance(mwcls, settings, crawler)
                 middlewares.append(mw)
                 enabled.append(clspath)
             except NotConfigured as e:
                 if e.args:
-                    clsname = clspath.split('.')[-1]
-                    logger.warning("Disabled %(clsname)s: %(eargs)s",
-                                   {'clsname': clsname, 'eargs': e.args[0]},
-                                   extra={'crawler': crawler})
-
-        logger.info("Enabled %(componentname)ss:\n%(enabledlist)s",
-                    {'componentname': cls.component_name,
-                     'enabledlist': pprint.pformat(enabled)},
-                    extra={'crawler': crawler})
+                    clsname = clspath.split(".")[-1]
+                    logger.warning(
+                        "Disabled %(clsname)s: %(eargs)s",
+                        {"clsname": clsname, "eargs": e.args[0]},
+                        extra={"crawler": crawler},
+                    )
+
+        logger.info(
+            "Enabled %(componentname)ss:\n%(enabledlist)s",
+            {
+                "componentname": cls.component_name,
+                "enabledlist": pprint.pformat(enabled),
+            },
+            extra={"crawler": crawler},
+        )
         return cls(*middlewares)
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls.from_settings(crawler.settings, crawler)
 
     def _add_middleware(self, mw) -> None:
-        if hasattr(mw, 'open_spider'):
-            self.methods['open_spider'].append(mw.open_spider)
-        if hasattr(mw, 'close_spider'):
-            self.methods['close_spider'].appendleft(mw.close_spider)
+        if hasattr(mw, "open_spider"):
+            self.methods["open_spider"].append(mw.open_spider)
+        if hasattr(mw, "close_spider"):
+            self.methods["close_spider"].appendleft(mw.close_spider)
 
     def _process_parallel(self, methodname: str, obj, *args) -> Deferred:
         methods = cast(Iterable[Callable], self.methods[methodname])
         return process_parallel(methods, obj, *args)
 
     def _process_chain(self, methodname: str, obj, *args) -> Deferred:
         methods = cast(Iterable[Callable], self.methods[methodname])
         return process_chain(methods, obj, *args)
 
     def open_spider(self, spider: Spider) -> Deferred:
-        return self._process_parallel('open_spider', spider)
+        return self._process_parallel("open_spider", spider)
 
     def close_spider(self, spider: Spider) -> Deferred:
-        return self._process_parallel('close_spider', spider)
+        return self._process_parallel("close_spider", spider)
```

### Comparing `Scrapy-2.7.1/scrapy/mime.types` & `Scrapy-2.8.0/scrapy/mime.types`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/scrapy/pipelines/__init__.py` & `Scrapy-2.8.0/scrapy/pipelines/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -7,20 +7,22 @@
 from scrapy.middleware import MiddlewareManager
 from scrapy.utils.conf import build_component_list
 from scrapy.utils.defer import deferred_f_from_coro_f
 
 
 class ItemPipelineManager(MiddlewareManager):
 
-    component_name = 'item pipeline'
+    component_name = "item pipeline"
 
     @classmethod
     def _get_mwlist_from_settings(cls, settings):
-        return build_component_list(settings.getwithbase('ITEM_PIPELINES'))
+        return build_component_list(settings.getwithbase("ITEM_PIPELINES"))
 
     def _add_middleware(self, pipe):
-        super(ItemPipelineManager, self)._add_middleware(pipe)
-        if hasattr(pipe, 'process_item'):
-            self.methods['process_item'].append(deferred_f_from_coro_f(pipe.process_item))
+        super()._add_middleware(pipe)
+        if hasattr(pipe, "process_item"):
+            self.methods["process_item"].append(
+                deferred_f_from_coro_f(pipe.process_item)
+            )
 
     def process_item(self, item, spider):
-        return self._process_chain('process_item', item, spider)
+        return self._process_chain("process_item", item, spider)
```

### Comparing `Scrapy-2.7.1/scrapy/pipelines/files.py` & `Scrapy-2.8.0/scrapy/pipelines/files.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,173 +9,176 @@
 import mimetypes
 import os
 import time
 from collections import defaultdict
 from contextlib import suppress
 from ftplib import FTP
 from io import BytesIO
+from pathlib import Path
+from typing import DefaultDict, Optional, Set
 from urllib.parse import urlparse
 
 from itemadapter import ItemAdapter
 from twisted.internet import defer, threads
 
 from scrapy.exceptions import IgnoreRequest, NotConfigured
 from scrapy.http import Request
+from scrapy.http.request import NO_CALLBACK
 from scrapy.pipelines.media import MediaPipeline
 from scrapy.settings import Settings
 from scrapy.utils.boto import is_botocore_available
 from scrapy.utils.datatypes import CaselessDict
 from scrapy.utils.ftp import ftp_store_file
 from scrapy.utils.log import failure_to_exc_info
 from scrapy.utils.misc import md5sum
 from scrapy.utils.python import to_bytes
 from scrapy.utils.request import referer_str
 
-
 logger = logging.getLogger(__name__)
 
 
 class FileException(Exception):
     """General media error exception"""
 
 
 class FSFilesStore:
-    def __init__(self, basedir):
-        if '://' in basedir:
-            basedir = basedir.split('://', 1)[1]
+    def __init__(self, basedir: str):
+        if "://" in basedir:
+            basedir = basedir.split("://", 1)[1]
         self.basedir = basedir
-        self._mkdir(self.basedir)
-        self.created_directories = defaultdict(set)
+        self._mkdir(Path(self.basedir))
+        self.created_directories: DefaultDict[str, Set[str]] = defaultdict(set)
 
-    def persist_file(self, path, buf, info, meta=None, headers=None):
+    def persist_file(self, path: str, buf, info, meta=None, headers=None):
         absolute_path = self._get_filesystem_path(path)
-        self._mkdir(os.path.dirname(absolute_path), info)
-        with open(absolute_path, 'wb') as f:
-            f.write(buf.getvalue())
+        self._mkdir(absolute_path.parent, info)
+        absolute_path.write_bytes(buf.getvalue())
 
-    def stat_file(self, path, info):
+    def stat_file(self, path: str, info):
         absolute_path = self._get_filesystem_path(path)
         try:
-            last_modified = os.path.getmtime(absolute_path)
+            last_modified = absolute_path.stat().st_mtime
         except os.error:
             return {}
 
-        with open(absolute_path, 'rb') as f:
+        with absolute_path.open("rb") as f:
             checksum = md5sum(f)
 
-        return {'last_modified': last_modified, 'checksum': checksum}
+        return {"last_modified": last_modified, "checksum": checksum}
 
-    def _get_filesystem_path(self, path):
-        path_comps = path.split('/')
-        return os.path.join(self.basedir, *path_comps)
+    def _get_filesystem_path(self, path: str) -> Path:
+        path_comps = path.split("/")
+        return Path(self.basedir, *path_comps)
 
-    def _mkdir(self, dirname, domain=None):
+    def _mkdir(self, dirname: Path, domain: Optional[str] = None):
         seen = self.created_directories[domain] if domain else set()
-        if dirname not in seen:
-            if not os.path.exists(dirname):
-                os.makedirs(dirname)
-            seen.add(dirname)
+        if str(dirname) not in seen:
+            if not dirname.exists():
+                dirname.mkdir(parents=True)
+            seen.add(str(dirname))
 
 
 class S3FilesStore:
     AWS_ACCESS_KEY_ID = None
     AWS_SECRET_ACCESS_KEY = None
     AWS_SESSION_TOKEN = None
     AWS_ENDPOINT_URL = None
     AWS_REGION_NAME = None
     AWS_USE_SSL = None
     AWS_VERIFY = None
 
-    POLICY = 'private'  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings
+    POLICY = "private"  # Overridden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings
     HEADERS = {
-        'Cache-Control': 'max-age=172800',
+        "Cache-Control": "max-age=172800",
     }
 
     def __init__(self, uri):
         if not is_botocore_available():
-            raise NotConfigured('missing botocore library')
+            raise NotConfigured("missing botocore library")
         import botocore.session
+
         session = botocore.session.get_session()
         self.s3_client = session.create_client(
-            's3',
+            "s3",
             aws_access_key_id=self.AWS_ACCESS_KEY_ID,
             aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,
             aws_session_token=self.AWS_SESSION_TOKEN,
             endpoint_url=self.AWS_ENDPOINT_URL,
             region_name=self.AWS_REGION_NAME,
             use_ssl=self.AWS_USE_SSL,
-            verify=self.AWS_VERIFY
+            verify=self.AWS_VERIFY,
         )
         if not uri.startswith("s3://"):
             raise ValueError(f"Incorrect URI scheme in {uri}, expected 's3'")
-        self.bucket, self.prefix = uri[5:].split('/', 1)
+        self.bucket, self.prefix = uri[5:].split("/", 1)
 
     def stat_file(self, path, info):
         def _onsuccess(boto_key):
-            checksum = boto_key['ETag'].strip('"')
-            last_modified = boto_key['LastModified']
+            checksum = boto_key["ETag"].strip('"')
+            last_modified = boto_key["LastModified"]
             modified_stamp = time.mktime(last_modified.timetuple())
-            return {'checksum': checksum, 'last_modified': modified_stamp}
+            return {"checksum": checksum, "last_modified": modified_stamp}
 
         return self._get_boto_key(path).addCallback(_onsuccess)
 
     def _get_boto_key(self, path):
-        key_name = f'{self.prefix}{path}'
+        key_name = f"{self.prefix}{path}"
         return threads.deferToThread(
-            self.s3_client.head_object,
-            Bucket=self.bucket,
-            Key=key_name)
+            self.s3_client.head_object, Bucket=self.bucket, Key=key_name
+        )
 
     def persist_file(self, path, buf, info, meta=None, headers=None):
         """Upload file to S3 storage"""
-        key_name = f'{self.prefix}{path}'
+        key_name = f"{self.prefix}{path}"
         buf.seek(0)
         extra = self._headers_to_botocore_kwargs(self.HEADERS)
         if headers:
             extra.update(self._headers_to_botocore_kwargs(headers))
         return threads.deferToThread(
             self.s3_client.put_object,
             Bucket=self.bucket,
             Key=key_name,
             Body=buf,
             Metadata={k: str(v) for k, v in (meta or {}).items()},
             ACL=self.POLICY,
-            **extra)
+            **extra,
+        )
 
     def _headers_to_botocore_kwargs(self, headers):
-        """ Convert headers to botocore keyword arguments.
-        """
+        """Convert headers to botocore keyword arguments."""
         # This is required while we need to support both boto and botocore.
-        mapping = CaselessDict({
-            'Content-Type': 'ContentType',
-            'Cache-Control': 'CacheControl',
-            'Content-Disposition': 'ContentDisposition',
-            'Content-Encoding': 'ContentEncoding',
-            'Content-Language': 'ContentLanguage',
-            'Content-Length': 'ContentLength',
-            'Content-MD5': 'ContentMD5',
-            'Expires': 'Expires',
-            'X-Amz-Grant-Full-Control': 'GrantFullControl',
-            'X-Amz-Grant-Read': 'GrantRead',
-            'X-Amz-Grant-Read-ACP': 'GrantReadACP',
-            'X-Amz-Grant-Write-ACP': 'GrantWriteACP',
-            'X-Amz-Object-Lock-Legal-Hold': 'ObjectLockLegalHoldStatus',
-            'X-Amz-Object-Lock-Mode': 'ObjectLockMode',
-            'X-Amz-Object-Lock-Retain-Until-Date': 'ObjectLockRetainUntilDate',
-            'X-Amz-Request-Payer': 'RequestPayer',
-            'X-Amz-Server-Side-Encryption': 'ServerSideEncryption',
-            'X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id': 'SSEKMSKeyId',
-            'X-Amz-Server-Side-Encryption-Context': 'SSEKMSEncryptionContext',
-            'X-Amz-Server-Side-Encryption-Customer-Algorithm': 'SSECustomerAlgorithm',
-            'X-Amz-Server-Side-Encryption-Customer-Key': 'SSECustomerKey',
-            'X-Amz-Server-Side-Encryption-Customer-Key-Md5': 'SSECustomerKeyMD5',
-            'X-Amz-Storage-Class': 'StorageClass',
-            'X-Amz-Tagging': 'Tagging',
-            'X-Amz-Website-Redirect-Location': 'WebsiteRedirectLocation',
-        })
+        mapping = CaselessDict(
+            {
+                "Content-Type": "ContentType",
+                "Cache-Control": "CacheControl",
+                "Content-Disposition": "ContentDisposition",
+                "Content-Encoding": "ContentEncoding",
+                "Content-Language": "ContentLanguage",
+                "Content-Length": "ContentLength",
+                "Content-MD5": "ContentMD5",
+                "Expires": "Expires",
+                "X-Amz-Grant-Full-Control": "GrantFullControl",
+                "X-Amz-Grant-Read": "GrantRead",
+                "X-Amz-Grant-Read-ACP": "GrantReadACP",
+                "X-Amz-Grant-Write-ACP": "GrantWriteACP",
+                "X-Amz-Object-Lock-Legal-Hold": "ObjectLockLegalHoldStatus",
+                "X-Amz-Object-Lock-Mode": "ObjectLockMode",
+                "X-Amz-Object-Lock-Retain-Until-Date": "ObjectLockRetainUntilDate",
+                "X-Amz-Request-Payer": "RequestPayer",
+                "X-Amz-Server-Side-Encryption": "ServerSideEncryption",
+                "X-Amz-Server-Side-Encryption-Aws-Kms-Key-Id": "SSEKMSKeyId",
+                "X-Amz-Server-Side-Encryption-Context": "SSEKMSEncryptionContext",
+                "X-Amz-Server-Side-Encryption-Customer-Algorithm": "SSECustomerAlgorithm",
+                "X-Amz-Server-Side-Encryption-Customer-Key": "SSECustomerKey",
+                "X-Amz-Server-Side-Encryption-Customer-Key-Md5": "SSECustomerKeyMD5",
+                "X-Amz-Storage-Class": "StorageClass",
+                "X-Amz-Tagging": "Tagging",
+                "X-Amz-Website-Redirect-Location": "WebsiteRedirectLocation",
+            }
+        )
         extra = {}
         for key, value in headers.items():
             try:
                 kwarg = mapping[key]
             except KeyError:
                 raise TypeError(f'Header "{key}" is not supported by botocore')
             else:
@@ -183,71 +186,73 @@
         return extra
 
 
 class GCSFilesStore:
 
     GCS_PROJECT_ID = None
 
-    CACHE_CONTROL = 'max-age=172800'
+    CACHE_CONTROL = "max-age=172800"
 
     # The bucket's default object ACL will be applied to the object.
     # Overridden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.
     POLICY = None
 
     def __init__(self, uri):
         from google.cloud import storage
+
         client = storage.Client(project=self.GCS_PROJECT_ID)
-        bucket, prefix = uri[5:].split('/', 1)
+        bucket, prefix = uri[5:].split("/", 1)
         self.bucket = client.bucket(bucket)
         self.prefix = prefix
         permissions = self.bucket.test_iam_permissions(
-            ['storage.objects.get', 'storage.objects.create']
+            ["storage.objects.get", "storage.objects.create"]
         )
-        if 'storage.objects.get' not in permissions:
+        if "storage.objects.get" not in permissions:
             logger.warning(
                 "No 'storage.objects.get' permission for GSC bucket %(bucket)s. "
                 "Checking if files are up to date will be impossible. Files will be downloaded every time.",
-                {'bucket': bucket}
+                {"bucket": bucket},
             )
-        if 'storage.objects.create' not in permissions:
+        if "storage.objects.create" not in permissions:
             logger.error(
                 "No 'storage.objects.create' permission for GSC bucket %(bucket)s. Saving files will be impossible!",
-                {'bucket': bucket}
+                {"bucket": bucket},
             )
 
     def stat_file(self, path, info):
         def _onsuccess(blob):
             if blob:
                 checksum = blob.md5_hash
                 last_modified = time.mktime(blob.updated.timetuple())
-                return {'checksum': checksum, 'last_modified': last_modified}
-            else:
-                return {}
+                return {"checksum": checksum, "last_modified": last_modified}
+            return {}
+
         blob_path = self._get_blob_path(path)
-        return threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(_onsuccess)
+        return threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(
+            _onsuccess
+        )
 
     def _get_content_type(self, headers):
-        if headers and 'Content-Type' in headers:
-            return headers['Content-Type']
-        else:
-            return 'application/octet-stream'
+        if headers and "Content-Type" in headers:
+            return headers["Content-Type"]
+        return "application/octet-stream"
 
     def _get_blob_path(self, path):
         return self.prefix + path
 
     def persist_file(self, path, buf, info, meta=None, headers=None):
         blob_path = self._get_blob_path(path)
         blob = self.bucket.blob(blob_path)
         blob.cache_control = self.CACHE_CONTROL
         blob.metadata = {k: str(v) for k, v in (meta or {}).items()}
         return threads.deferToThread(
             blob.upload_from_string,
             data=buf.getvalue(),
             content_type=self._get_content_type(headers),
-            predefined_acl=self.POLICY
+            predefined_acl=self.POLICY,
         )
 
 
 class FTPFilesStore:
 
     FTP_USERNAME = None
     FTP_PASSWORD = None
@@ -258,40 +263,46 @@
             raise ValueError(f"Incorrect URI scheme in {uri}, expected 'ftp'")
         u = urlparse(uri)
         self.port = u.port
         self.host = u.hostname
         self.port = int(u.port or 21)
         self.username = u.username or self.FTP_USERNAME
         self.password = u.password or self.FTP_PASSWORD
-        self.basedir = u.path.rstrip('/')
+        self.basedir = u.path.rstrip("/")
 
     def persist_file(self, path, buf, info, meta=None, headers=None):
-        path = f'{self.basedir}/{path}'
+        path = f"{self.basedir}/{path}"
         return threads.deferToThread(
-            ftp_store_file, path=path, file=buf,
-            host=self.host, port=self.port, username=self.username,
-            password=self.password, use_active_mode=self.USE_ACTIVE_MODE
+            ftp_store_file,
+            path=path,
+            file=buf,
+            host=self.host,
+            port=self.port,
+            username=self.username,
+            password=self.password,
+            use_active_mode=self.USE_ACTIVE_MODE,
         )
 
     def stat_file(self, path, info):
         def _stat_file(path):
             try:
                 ftp = FTP()
                 ftp.connect(self.host, self.port)
                 ftp.login(self.username, self.password)
                 if self.USE_ACTIVE_MODE:
                     ftp.set_pasv(False)
                 file_path = f"{self.basedir}/{path}"
                 last_modified = float(ftp.voidcmd(f"MDTM {file_path}")[4:].strip())
                 m = hashlib.md5()
-                ftp.retrbinary(f'RETR {file_path}', m.update)
-                return {'last_modified': last_modified, 'checksum': m.hexdigest()}
+                ftp.retrbinary(f"RETR {file_path}", m.update)
+                return {"last_modified": last_modified, "checksum": m.hexdigest()}
             # The file doesn't exist
             except Exception:
                 return {}
+
         return threads.deferToThread(_stat_file, path)
 
 
 class FilesPipeline(MediaPipeline):
     """Abstract pipeline that implement the file downloading
 
     This pipeline tries to minimize network transfers and file processing,
@@ -309,195 +320,208 @@
         refresh it in case of change.
 
     """
 
     MEDIA_NAME = "file"
     EXPIRES = 90
     STORE_SCHEMES = {
-        '': FSFilesStore,
-        'file': FSFilesStore,
-        's3': S3FilesStore,
-        'gs': GCSFilesStore,
-        'ftp': FTPFilesStore
+        "": FSFilesStore,
+        "file": FSFilesStore,
+        "s3": S3FilesStore,
+        "gs": GCSFilesStore,
+        "ftp": FTPFilesStore,
     }
-    DEFAULT_FILES_URLS_FIELD = 'file_urls'
-    DEFAULT_FILES_RESULT_FIELD = 'files'
+    DEFAULT_FILES_URLS_FIELD = "file_urls"
+    DEFAULT_FILES_RESULT_FIELD = "files"
 
     def __init__(self, store_uri, download_func=None, settings=None):
         if not store_uri:
             raise NotConfigured
 
         if isinstance(settings, dict) or settings is None:
             settings = Settings(settings)
 
         cls_name = "FilesPipeline"
         self.store = self._get_store(store_uri)
-        resolve = functools.partial(self._key_for_pipe,
-                                    base_class_name=cls_name,
-                                    settings=settings)
-        self.expires = settings.getint(
-            resolve('FILES_EXPIRES'), self.EXPIRES
+        resolve = functools.partial(
+            self._key_for_pipe, base_class_name=cls_name, settings=settings
         )
+        self.expires = settings.getint(resolve("FILES_EXPIRES"), self.EXPIRES)
         if not hasattr(self, "FILES_URLS_FIELD"):
             self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD
         if not hasattr(self, "FILES_RESULT_FIELD"):
             self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD
         self.files_urls_field = settings.get(
-            resolve('FILES_URLS_FIELD'), self.FILES_URLS_FIELD
+            resolve("FILES_URLS_FIELD"), self.FILES_URLS_FIELD
         )
         self.files_result_field = settings.get(
-            resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD
+            resolve("FILES_RESULT_FIELD"), self.FILES_RESULT_FIELD
         )
 
         super().__init__(download_func=download_func, settings=settings)
 
     @classmethod
     def from_settings(cls, settings):
-        s3store = cls.STORE_SCHEMES['s3']
-        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']
-        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']
-        s3store.AWS_SESSION_TOKEN = settings['AWS_SESSION_TOKEN']
-        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']
-        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']
-        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']
-        s3store.AWS_VERIFY = settings['AWS_VERIFY']
-        s3store.POLICY = settings['FILES_STORE_S3_ACL']
-
-        gcs_store = cls.STORE_SCHEMES['gs']
-        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']
-        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None
-
-        ftp_store = cls.STORE_SCHEMES['ftp']
-        ftp_store.FTP_USERNAME = settings['FTP_USER']
-        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']
-        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')
+        s3store = cls.STORE_SCHEMES["s3"]
+        s3store.AWS_ACCESS_KEY_ID = settings["AWS_ACCESS_KEY_ID"]
+        s3store.AWS_SECRET_ACCESS_KEY = settings["AWS_SECRET_ACCESS_KEY"]
+        s3store.AWS_SESSION_TOKEN = settings["AWS_SESSION_TOKEN"]
+        s3store.AWS_ENDPOINT_URL = settings["AWS_ENDPOINT_URL"]
+        s3store.AWS_REGION_NAME = settings["AWS_REGION_NAME"]
+        s3store.AWS_USE_SSL = settings["AWS_USE_SSL"]
+        s3store.AWS_VERIFY = settings["AWS_VERIFY"]
+        s3store.POLICY = settings["FILES_STORE_S3_ACL"]
+
+        gcs_store = cls.STORE_SCHEMES["gs"]
+        gcs_store.GCS_PROJECT_ID = settings["GCS_PROJECT_ID"]
+        gcs_store.POLICY = settings["FILES_STORE_GCS_ACL"] or None
+
+        ftp_store = cls.STORE_SCHEMES["ftp"]
+        ftp_store.FTP_USERNAME = settings["FTP_USER"]
+        ftp_store.FTP_PASSWORD = settings["FTP_PASSWORD"]
+        ftp_store.USE_ACTIVE_MODE = settings.getbool("FEED_STORAGE_FTP_ACTIVE")
 
-        store_uri = settings['FILES_STORE']
+        store_uri = settings["FILES_STORE"]
         return cls(store_uri, settings=settings)
 
-    def _get_store(self, uri):
-        if os.path.isabs(uri):  # to support win32 paths like: C:\\some\dir
-            scheme = 'file'
+    def _get_store(self, uri: str):
+        if Path(uri).is_absolute():  # to support win32 paths like: C:\\some\dir
+            scheme = "file"
         else:
             scheme = urlparse(uri).scheme
         store_cls = self.STORE_SCHEMES[scheme]
         return store_cls(uri)
 
     def media_to_download(self, request, info, *, item=None):
         def _onsuccess(result):
             if not result:
                 return  # returning None force download
 
-            last_modified = result.get('last_modified', None)
+            last_modified = result.get("last_modified", None)
             if not last_modified:
                 return  # returning None force download
 
             age_seconds = time.time() - last_modified
             age_days = age_seconds / 60 / 60 / 24
             if age_days > self.expires:
                 return  # returning None force download
 
             referer = referer_str(request)
             logger.debug(
-                'File (uptodate): Downloaded %(medianame)s from %(request)s '
-                'referred in <%(referer)s>',
-                {'medianame': self.MEDIA_NAME, 'request': request,
-                 'referer': referer},
-                extra={'spider': info.spider}
+                "File (uptodate): Downloaded %(medianame)s from %(request)s "
+                "referred in <%(referer)s>",
+                {"medianame": self.MEDIA_NAME, "request": request, "referer": referer},
+                extra={"spider": info.spider},
             )
-            self.inc_stats(info.spider, 'uptodate')
+            self.inc_stats(info.spider, "uptodate")
 
-            checksum = result.get('checksum', None)
-            return {'url': request.url, 'path': path, 'checksum': checksum, 'status': 'uptodate'}
+            checksum = result.get("checksum", None)
+            return {
+                "url": request.url,
+                "path": path,
+                "checksum": checksum,
+                "status": "uptodate",
+            }
 
         path = self.file_path(request, info=info, item=item)
         dfd = defer.maybeDeferred(self.store.stat_file, path, info)
         dfd.addCallbacks(_onsuccess, lambda _: None)
         dfd.addErrback(
-            lambda f:
-            logger.error(self.__class__.__name__ + '.store.stat_file',
-                         exc_info=failure_to_exc_info(f),
-                         extra={'spider': info.spider})
+            lambda f: logger.error(
+                self.__class__.__name__ + ".store.stat_file",
+                exc_info=failure_to_exc_info(f),
+                extra={"spider": info.spider},
+            )
         )
         return dfd
 
     def media_failed(self, failure, request, info):
         if not isinstance(failure.value, IgnoreRequest):
             referer = referer_str(request)
             logger.warning(
-                'File (unknown-error): Error downloading %(medianame)s from '
-                '%(request)s referred in <%(referer)s>: %(exception)s',
-                {'medianame': self.MEDIA_NAME, 'request': request,
-                 'referer': referer, 'exception': failure.value},
-                extra={'spider': info.spider}
+                "File (unknown-error): Error downloading %(medianame)s from "
+                "%(request)s referred in <%(referer)s>: %(exception)s",
+                {
+                    "medianame": self.MEDIA_NAME,
+                    "request": request,
+                    "referer": referer,
+                    "exception": failure.value,
+                },
+                extra={"spider": info.spider},
             )
 
         raise FileException
 
     def media_downloaded(self, response, request, info, *, item=None):
         referer = referer_str(request)
 
         if response.status != 200:
             logger.warning(
-                'File (code: %(status)s): Error downloading file from '
-                '%(request)s referred in <%(referer)s>',
-                {'status': response.status,
-                 'request': request, 'referer': referer},
-                extra={'spider': info.spider}
+                "File (code: %(status)s): Error downloading file from "
+                "%(request)s referred in <%(referer)s>",
+                {"status": response.status, "request": request, "referer": referer},
+                extra={"spider": info.spider},
             )
-            raise FileException('download-error')
+            raise FileException("download-error")
 
         if not response.body:
             logger.warning(
-                'File (empty-content): Empty file from %(request)s referred '
-                'in <%(referer)s>: no-content',
-                {'request': request, 'referer': referer},
-                extra={'spider': info.spider}
+                "File (empty-content): Empty file from %(request)s referred "
+                "in <%(referer)s>: no-content",
+                {"request": request, "referer": referer},
+                extra={"spider": info.spider},
             )
-            raise FileException('empty-content')
+            raise FileException("empty-content")
 
-        status = 'cached' if 'cached' in response.flags else 'downloaded'
+        status = "cached" if "cached" in response.flags else "downloaded"
         logger.debug(
-            'File (%(status)s): Downloaded file from %(request)s referred in '
-            '<%(referer)s>',
-            {'status': status, 'request': request, 'referer': referer},
-            extra={'spider': info.spider}
+            "File (%(status)s): Downloaded file from %(request)s referred in "
+            "<%(referer)s>",
+            {"status": status, "request": request, "referer": referer},
+            extra={"spider": info.spider},
         )
         self.inc_stats(info.spider, status)
 
         try:
             path = self.file_path(request, response=response, info=info, item=item)
             checksum = self.file_downloaded(response, request, info, item=item)
         except FileException as exc:
             logger.warning(
-                'File (error): Error processing file from %(request)s '
-                'referred in <%(referer)s>: %(errormsg)s',
-                {'request': request, 'referer': referer, 'errormsg': str(exc)},
-                extra={'spider': info.spider}, exc_info=True
+                "File (error): Error processing file from %(request)s "
+                "referred in <%(referer)s>: %(errormsg)s",
+                {"request": request, "referer": referer, "errormsg": str(exc)},
+                extra={"spider": info.spider},
+                exc_info=True,
             )
             raise
         except Exception as exc:
             logger.error(
-                'File (unknown-error): Error processing file from %(request)s '
-                'referred in <%(referer)s>',
-                {'request': request, 'referer': referer},
-                exc_info=True, extra={'spider': info.spider}
+                "File (unknown-error): Error processing file from %(request)s "
+                "referred in <%(referer)s>",
+                {"request": request, "referer": referer},
+                exc_info=True,
+                extra={"spider": info.spider},
             )
             raise FileException(str(exc))
 
-        return {'url': request.url, 'path': path, 'checksum': checksum, 'status': status}
+        return {
+            "url": request.url,
+            "path": path,
+            "checksum": checksum,
+            "status": status,
+        }
 
     def inc_stats(self, spider, status):
-        spider.crawler.stats.inc_value('file_count', spider=spider)
-        spider.crawler.stats.inc_value(f'file_status_count/{status}', spider=spider)
+        spider.crawler.stats.inc_value("file_count", spider=spider)
+        spider.crawler.stats.inc_value(f"file_status_count/{status}", spider=spider)
 
     # Overridable Interface
     def get_media_requests(self, item, info):
         urls = ItemAdapter(item).get(self.files_urls_field, [])
-        return [Request(u) for u in urls]
+        return [Request(u, callback=NO_CALLBACK) for u in urls]
 
     def file_downloaded(self, response, request, info, *, item=None):
         path = self.file_path(request, response=response, info=info, item=item)
         buf = BytesIO(response.body)
         checksum = md5sum(buf)
         buf.seek(0)
         self.store.persist_file(path, buf, info)
@@ -506,16 +530,16 @@
     def item_completed(self, results, item, info):
         with suppress(KeyError):
             ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]
         return item
 
     def file_path(self, request, response=None, info=None, *, item=None):
         media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()
-        media_ext = os.path.splitext(request.url)[1]
+        media_ext = Path(request.url).suffix
         # Handles empty and wild extensions by trying to guess the
         # mime type then extension or default to empty string otherwise
         if media_ext not in mimetypes.types_map:
-            media_ext = ''
+            media_ext = ""
             media_type = mimetypes.guess_type(request.url)[0]
             if media_type:
                 media_ext = mimetypes.guess_extension(media_type)
-        return f'full/{media_guid}{media_ext}'
+        return f"full/{media_guid}{media_ext}"
```

### Comparing `Scrapy-2.7.1/scrapy/pipelines/images.py` & `Scrapy-2.8.0/scrapy/pipelines/images.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,191 +1,231 @@
 """
 Images Pipeline
 
 See documentation in topics/media-pipeline.rst
 """
 import functools
 import hashlib
+import warnings
 from contextlib import suppress
 from io import BytesIO
 
 from itemadapter import ItemAdapter
 
-from scrapy.exceptions import DropItem, NotConfigured
+from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning
 from scrapy.http import Request
+from scrapy.http.request import NO_CALLBACK
 from scrapy.pipelines.files import FileException, FilesPipeline
+
 # TODO: from scrapy.pipelines.media import MediaPipeline
 from scrapy.settings import Settings
 from scrapy.utils.misc import md5sum
-from scrapy.utils.python import to_bytes
+from scrapy.utils.python import get_func_args, to_bytes
 
 
 class NoimagesDrop(DropItem):
     """Product with no images exception"""
 
+    def __init__(self, *args, **kwargs):
+        warnings.warn(
+            "The NoimagesDrop class is deprecated",
+            category=ScrapyDeprecationWarning,
+            stacklevel=2,
+        )
+        super().__init__(*args, **kwargs)
+
 
 class ImageException(FileException):
     """General image error exception"""
 
 
 class ImagesPipeline(FilesPipeline):
-    """Abstract pipeline that implement the image thumbnail generation logic
-
-    """
+    """Abstract pipeline that implement the image thumbnail generation logic"""
 
-    MEDIA_NAME = 'image'
+    MEDIA_NAME = "image"
 
     # Uppercase attributes kept for backward compatibility with code that subclasses
     # ImagesPipeline. They may be overridden by settings.
     MIN_WIDTH = 0
     MIN_HEIGHT = 0
     EXPIRES = 90
     THUMBS = {}
-    DEFAULT_IMAGES_URLS_FIELD = 'image_urls'
-    DEFAULT_IMAGES_RESULT_FIELD = 'images'
+    DEFAULT_IMAGES_URLS_FIELD = "image_urls"
+    DEFAULT_IMAGES_RESULT_FIELD = "images"
 
     def __init__(self, store_uri, download_func=None, settings=None):
         try:
             from PIL import Image
+
             self._Image = Image
         except ImportError:
             raise NotConfigured(
-                'ImagesPipeline requires installing Pillow 4.0.0 or later'
+                "ImagesPipeline requires installing Pillow 4.0.0 or later"
             )
 
         super().__init__(store_uri, settings=settings, download_func=download_func)
 
         if isinstance(settings, dict) or settings is None:
             settings = Settings(settings)
 
-        resolve = functools.partial(self._key_for_pipe,
-                                    base_class_name="ImagesPipeline",
-                                    settings=settings)
-        self.expires = settings.getint(
-            resolve("IMAGES_EXPIRES"), self.EXPIRES
+        resolve = functools.partial(
+            self._key_for_pipe,
+            base_class_name="ImagesPipeline",
+            settings=settings,
         )
+        self.expires = settings.getint(resolve("IMAGES_EXPIRES"), self.EXPIRES)
 
         if not hasattr(self, "IMAGES_RESULT_FIELD"):
             self.IMAGES_RESULT_FIELD = self.DEFAULT_IMAGES_RESULT_FIELD
         if not hasattr(self, "IMAGES_URLS_FIELD"):
             self.IMAGES_URLS_FIELD = self.DEFAULT_IMAGES_URLS_FIELD
 
         self.images_urls_field = settings.get(
-            resolve('IMAGES_URLS_FIELD'),
-            self.IMAGES_URLS_FIELD
+            resolve("IMAGES_URLS_FIELD"), self.IMAGES_URLS_FIELD
         )
         self.images_result_field = settings.get(
-            resolve('IMAGES_RESULT_FIELD'),
-            self.IMAGES_RESULT_FIELD
-        )
-        self.min_width = settings.getint(
-            resolve('IMAGES_MIN_WIDTH'), self.MIN_WIDTH
-        )
-        self.min_height = settings.getint(
-            resolve('IMAGES_MIN_HEIGHT'), self.MIN_HEIGHT
-        )
-        self.thumbs = settings.get(
-            resolve('IMAGES_THUMBS'), self.THUMBS
+            resolve("IMAGES_RESULT_FIELD"), self.IMAGES_RESULT_FIELD
         )
+        self.min_width = settings.getint(resolve("IMAGES_MIN_WIDTH"), self.MIN_WIDTH)
+        self.min_height = settings.getint(resolve("IMAGES_MIN_HEIGHT"), self.MIN_HEIGHT)
+        self.thumbs = settings.get(resolve("IMAGES_THUMBS"), self.THUMBS)
+
+        self._deprecated_convert_image = None
 
     @classmethod
     def from_settings(cls, settings):
-        s3store = cls.STORE_SCHEMES['s3']
-        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']
-        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']
-        s3store.AWS_SESSION_TOKEN = settings['AWS_SESSION_TOKEN']
-        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']
-        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']
-        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']
-        s3store.AWS_VERIFY = settings['AWS_VERIFY']
-        s3store.POLICY = settings['IMAGES_STORE_S3_ACL']
-
-        gcs_store = cls.STORE_SCHEMES['gs']
-        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']
-        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None
-
-        ftp_store = cls.STORE_SCHEMES['ftp']
-        ftp_store.FTP_USERNAME = settings['FTP_USER']
-        ftp_store.FTP_PASSWORD = settings['FTP_PASSWORD']
-        ftp_store.USE_ACTIVE_MODE = settings.getbool('FEED_STORAGE_FTP_ACTIVE')
+        s3store = cls.STORE_SCHEMES["s3"]
+        s3store.AWS_ACCESS_KEY_ID = settings["AWS_ACCESS_KEY_ID"]
+        s3store.AWS_SECRET_ACCESS_KEY = settings["AWS_SECRET_ACCESS_KEY"]
+        s3store.AWS_SESSION_TOKEN = settings["AWS_SESSION_TOKEN"]
+        s3store.AWS_ENDPOINT_URL = settings["AWS_ENDPOINT_URL"]
+        s3store.AWS_REGION_NAME = settings["AWS_REGION_NAME"]
+        s3store.AWS_USE_SSL = settings["AWS_USE_SSL"]
+        s3store.AWS_VERIFY = settings["AWS_VERIFY"]
+        s3store.POLICY = settings["IMAGES_STORE_S3_ACL"]
+
+        gcs_store = cls.STORE_SCHEMES["gs"]
+        gcs_store.GCS_PROJECT_ID = settings["GCS_PROJECT_ID"]
+        gcs_store.POLICY = settings["IMAGES_STORE_GCS_ACL"] or None
+
+        ftp_store = cls.STORE_SCHEMES["ftp"]
+        ftp_store.FTP_USERNAME = settings["FTP_USER"]
+        ftp_store.FTP_PASSWORD = settings["FTP_PASSWORD"]
+        ftp_store.USE_ACTIVE_MODE = settings.getbool("FEED_STORAGE_FTP_ACTIVE")
 
-        store_uri = settings['IMAGES_STORE']
+        store_uri = settings["IMAGES_STORE"]
         return cls(store_uri, settings=settings)
 
     def file_downloaded(self, response, request, info, *, item=None):
         return self.image_downloaded(response, request, info, item=item)
 
     def image_downloaded(self, response, request, info, *, item=None):
         checksum = None
         for path, image, buf in self.get_images(response, request, info, item=item):
             if checksum is None:
                 buf.seek(0)
                 checksum = md5sum(buf)
             width, height = image.size
             self.store.persist_file(
-                path, buf, info,
-                meta={'width': width, 'height': height},
-                headers={'Content-Type': 'image/jpeg'})
+                path,
+                buf,
+                info,
+                meta={"width": width, "height": height},
+                headers={"Content-Type": "image/jpeg"},
+            )
         return checksum
 
     def get_images(self, response, request, info, *, item=None):
         path = self.file_path(request, response=response, info=info, item=item)
         orig_image = self._Image.open(BytesIO(response.body))
 
         width, height = orig_image.size
         if width < self.min_width or height < self.min_height:
-            raise ImageException("Image too small "
-                                 f"({width}x{height} < "
-                                 f"{self.min_width}x{self.min_height})")
+            raise ImageException(
+                "Image too small "
+                f"({width}x{height} < "
+                f"{self.min_width}x{self.min_height})"
+            )
 
-        image, buf = self.convert_image(orig_image)
+        if self._deprecated_convert_image is None:
+            self._deprecated_convert_image = "response_body" not in get_func_args(
+                self.convert_image
+            )
+            if self._deprecated_convert_image:
+                warnings.warn(
+                    f"{self.__class__.__name__}.convert_image() method overridden in a deprecated way, "
+                    "overridden method does not accept response_body argument.",
+                    category=ScrapyDeprecationWarning,
+                )
+
+        if self._deprecated_convert_image:
+            image, buf = self.convert_image(orig_image)
+        else:
+            image, buf = self.convert_image(
+                orig_image, response_body=BytesIO(response.body)
+            )
         yield path, image, buf
 
         for thumb_id, size in self.thumbs.items():
-            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info, item=item)
-            thumb_image, thumb_buf = self.convert_image(image, size)
+            thumb_path = self.thumb_path(
+                request, thumb_id, response=response, info=info, item=item
+            )
+            if self._deprecated_convert_image:
+                thumb_image, thumb_buf = self.convert_image(image, size)
+            else:
+                thumb_image, thumb_buf = self.convert_image(image, size, buf)
             yield thumb_path, thumb_image, thumb_buf
 
-    def convert_image(self, image, size=None):
-        if image.format == 'PNG' and image.mode == 'RGBA':
-            background = self._Image.new('RGBA', image.size, (255, 255, 255))
+    def convert_image(self, image, size=None, response_body=None):
+        if response_body is None:
+            warnings.warn(
+                f"{self.__class__.__name__}.convert_image() method called in a deprecated way, "
+                "method called without response_body argument.",
+                category=ScrapyDeprecationWarning,
+                stacklevel=2,
+            )
+
+        if image.format in ("PNG", "WEBP") and image.mode == "RGBA":
+            background = self._Image.new("RGBA", image.size, (255, 255, 255))
             background.paste(image, image)
-            image = background.convert('RGB')
-        elif image.mode == 'P':
+            image = background.convert("RGB")
+        elif image.mode == "P":
             image = image.convert("RGBA")
-            background = self._Image.new('RGBA', image.size, (255, 255, 255))
+            background = self._Image.new("RGBA", image.size, (255, 255, 255))
             background.paste(image, image)
-            image = background.convert('RGB')
-        elif image.mode != 'RGB':
-            image = image.convert('RGB')
+            image = background.convert("RGB")
+        elif image.mode != "RGB":
+            image = image.convert("RGB")
 
         if size:
             image = image.copy()
             try:
                 # Image.Resampling.LANCZOS was added in Pillow 9.1.0
                 # remove this try except block,
                 # when updating the minimum requirements for Pillow.
                 resampling_filter = self._Image.Resampling.LANCZOS
             except AttributeError:
                 resampling_filter = self._Image.ANTIALIAS
             image.thumbnail(size, resampling_filter)
+        elif response_body is not None and image.format == "JPEG":
+            return image, response_body
 
         buf = BytesIO()
-        image.save(buf, 'JPEG')
+        image.save(buf, "JPEG")
         return image, buf
 
     def get_media_requests(self, item, info):
         urls = ItemAdapter(item).get(self.images_urls_field, [])
-        return [Request(u) for u in urls]
+        return [Request(u, callback=NO_CALLBACK) for u in urls]
 
     def item_completed(self, results, item, info):
         with suppress(KeyError):
             ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]
         return item
 
     def file_path(self, request, response=None, info=None, *, item=None):
         image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()
-        return f'full/{image_guid}.jpg'
+        return f"full/{image_guid}.jpg"
 
     def thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):
         thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()
-        return f'thumbs/{thumb_id}/{thumb_guid}.jpg'
+        return f"thumbs/{thumb_id}/{thumb_guid}.jpg"
```

### Comparing `Scrapy-2.7.1/scrapy/pipelines/media.py` & `Scrapy-2.8.0/scrapy/pipelines/media.py`

 * *Files 7% similar despite different names*

```diff
@@ -3,24 +3,29 @@
 from collections import defaultdict
 from inspect import signature
 from warnings import warn
 
 from twisted.internet.defer import Deferred, DeferredList
 from twisted.python.failure import Failure
 
+from scrapy.http.request import NO_CALLBACK
 from scrapy.settings import Settings
 from scrapy.utils.datatypes import SequenceExclude
-from scrapy.utils.defer import mustbe_deferred, defer_result
+from scrapy.utils.defer import defer_result, mustbe_deferred
 from scrapy.utils.deprecate import ScrapyDeprecationWarning
-from scrapy.utils.misc import arg_to_iter
 from scrapy.utils.log import failure_to_exc_info
+from scrapy.utils.misc import arg_to_iter
 
 logger = logging.getLogger(__name__)
 
 
+def _DUMMY_CALLBACK(response):
+    return response
+
+
 class MediaPipeline:
 
     LOG_FAILED_RESULTS = True
 
     class SpiderInfo:
         def __init__(self, spider):
             self.spider = spider
@@ -30,20 +35,18 @@
 
     def __init__(self, download_func=None, settings=None):
         self.download_func = download_func
         self._expects_item = {}
 
         if isinstance(settings, dict) or settings is None:
             settings = Settings(settings)
-        resolve = functools.partial(self._key_for_pipe,
-                                    base_class_name="MediaPipeline",
-                                    settings=settings)
-        self.allow_redirects = settings.getbool(
-            resolve('MEDIA_ALLOW_REDIRECTS'), False
+        resolve = functools.partial(
+            self._key_for_pipe, base_class_name="MediaPipeline", settings=settings
         )
+        self.allow_redirects = settings.getbool(resolve("MEDIA_ALLOW_REDIRECTS"), False)
         self._handle_statuses(self.allow_redirects)
 
         # Check if deprecated methods are being used and make them compatible
         self._make_compatible()
 
     def _handle_statuses(self, allow_redirects):
         self.handle_httpstatus_list = None
@@ -60,15 +63,16 @@
         'MYPIPE_IMAGES'
         """
         class_name = self.__class__.__name__
         formatted_key = f"{class_name.upper()}_{key}"
         if (
             not base_class_name
             or class_name == base_class_name
-            or settings and not settings.get(formatted_key)
+            or settings
+            and not settings.get(formatted_key)
         ):
             return key
         return formatted_key
 
     @classmethod
     def from_crawler(cls, crawler):
         try:
@@ -87,17 +91,20 @@
         requests = arg_to_iter(self.get_media_requests(item, info))
         dlist = [self._process_request(r, info, item) for r in requests]
         dfd = DeferredList(dlist, consumeErrors=True)
         return dfd.addCallback(self.item_completed, item, info)
 
     def _process_request(self, request, info, item):
         fp = self._fingerprinter.fingerprint(request)
-        cb = request.callback or (lambda _: _)
+        if not request.callback or request.callback is NO_CALLBACK:
+            cb = _DUMMY_CALLBACK
+        else:
+            cb = request.callback
         eb = request.errback
-        request.callback = None
+        request.callback = NO_CALLBACK
         request.errback = None
 
         # Return cached result if request was already seen
         if fp in info.downloaded:
             return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)
 
         # Otherwise, wait for result
@@ -109,24 +116,31 @@
             return wad
 
         # Download request checking media_to_download hook output first
         info.downloading.add(fp)
         dfd = mustbe_deferred(self.media_to_download, request, info, item=item)
         dfd.addCallback(self._check_media_to_download, request, info, item=item)
         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)
-        dfd.addErrback(lambda f: logger.error(
-            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})
+        dfd.addErrback(
+            lambda f: logger.error(
+                f.value, exc_info=failure_to_exc_info(f), extra={"spider": info.spider}
+            )
         )
         return dfd.addBoth(lambda _: wad)  # it must return wad at last
 
     def _make_compatible(self):
         """Make overridable methods of MediaPipeline and subclasses backwards compatible"""
         methods = [
-            "file_path", "thumb_path", "media_to_download", "media_downloaded",
-            "file_downloaded", "image_downloaded", "get_images"
+            "file_path",
+            "thumb_path",
+            "media_to_download",
+            "media_downloaded",
+            "file_downloaded",
+            "image_downloaded",
+            "get_images",
         ]
 
         for method_name in methods:
             method = getattr(self, method_name, None)
             if callable(method):
                 setattr(self, method_name, self._compatible(method))
 
@@ -135,52 +149,63 @@
         self._check_signature(func)
 
         @functools.wraps(func)
         def wrapper(*args, **kwargs):
             if self._expects_item[func.__name__]:
                 return func(*args, **kwargs)
 
-            kwargs.pop('item', None)
+            kwargs.pop("item", None)
             return func(*args, **kwargs)
 
         return wrapper
 
     def _check_signature(self, func):
         sig = signature(func)
         self._expects_item[func.__name__] = True
 
-        if 'item' not in sig.parameters:
+        if "item" not in sig.parameters:
             old_params = str(sig)[1:-1]
             new_params = old_params + ", *, item=None"
-            warn(f'{func.__name__}(self, {old_params}) is deprecated, '
-                 f'please use {func.__name__}(self, {new_params})',
-                 ScrapyDeprecationWarning, stacklevel=2)
+            warn(
+                f"{func.__name__}(self, {old_params}) is deprecated, "
+                f"please use {func.__name__}(self, {new_params})",
+                ScrapyDeprecationWarning,
+                stacklevel=2,
+            )
             self._expects_item[func.__name__] = False
 
     def _modify_media_request(self, request):
         if self.handle_httpstatus_list:
-            request.meta['handle_httpstatus_list'] = self.handle_httpstatus_list
+            request.meta["handle_httpstatus_list"] = self.handle_httpstatus_list
         else:
-            request.meta['handle_httpstatus_all'] = True
+            request.meta["handle_httpstatus_all"] = True
 
     def _check_media_to_download(self, result, request, info, item):
         if result is not None:
             return result
         if self.download_func:
             # this ugly code was left only to support tests. TODO: remove
             dfd = mustbe_deferred(self.download_func, request, info.spider)
             dfd.addCallbacks(
-                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},
-                errback=self.media_failed, errbackArgs=(request, info))
+                callback=self.media_downloaded,
+                callbackArgs=(request, info),
+                callbackKeywords={"item": item},
+                errback=self.media_failed,
+                errbackArgs=(request, info),
+            )
         else:
             self._modify_media_request(request)
             dfd = self.crawler.engine.download(request)
             dfd.addCallbacks(
-                callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},
-                errback=self.media_failed, errbackArgs=(request, info))
+                callback=self.media_downloaded,
+                callbackArgs=(request, info),
+                callbackKeywords={"item": item},
+                errback=self.media_failed,
+                errbackArgs=(request, info),
+            )
         return dfd
 
     def _cache_result_and_execute_waiters(self, result, fp, info):
         if isinstance(result, Failure):
             # minimize cached information for failure
             result.cleanFailure()
             result.frames = []
@@ -203,17 +228,17 @@
             #
             # To avoid keeping references to the Response and therefore Request
             # objects on the Media Pipeline cache, we should wipe the context of
             # the encapsulated exception when it is a StopIteration instance
             #
             # This problem does not occur in Python 2.7 since we don't have
             # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).
-            context = getattr(result.value, '__context__', None)
+            context = getattr(result.value, "__context__", None)
             if isinstance(context, StopIteration):
-                setattr(result.value, '__context__', None)
+                setattr(result.value, "__context__", None)
 
         info.downloading.remove(fp)
         info.downloaded[fp] = result  # cache result
         for wad in info.waiting.pop(fp):
             defer_result(result).chainDeferred(wad)
 
     # Overridable Interface
@@ -235,17 +260,17 @@
 
     def item_completed(self, results, item, info):
         """Called per item when all media requests has been processed"""
         if self.LOG_FAILED_RESULTS:
             for ok, value in results:
                 if not ok:
                     logger.error(
-                        '%(class)s found errors processing %(item)s',
-                        {'class': self.__class__.__name__, 'item': item},
+                        "%(class)s found errors processing %(item)s",
+                        {"class": self.__class__.__name__, "item": item},
                         exc_info=failure_to_exc_info(value),
-                        extra={'spider': info.spider}
+                        extra={"spider": info.spider},
                     )
         return item
 
     def file_path(self, request, response=None, info=None, *, item=None):
         """Returns the path where downloaded media should be stored"""
         pass
```

### Comparing `Scrapy-2.7.1/scrapy/pqueues.py` & `Scrapy-2.8.0/scrapy/pqueues.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,32 +1,31 @@
 import hashlib
 import logging
 
 from scrapy.utils.misc import create_instance
 
-
 logger = logging.getLogger(__name__)
 
 
 def _path_safe(text):
     """
     Return a filesystem-safe version of a string ``text``
 
     >>> _path_safe('simple.org').startswith('simple.org')
     True
     >>> _path_safe('dash-underscore_.org').startswith('dash-underscore_.org')
     True
     >>> _path_safe('some@symbol?').startswith('some_symbol_')
     True
     """
-    pathable_slot = "".join([c if c.isalnum() or c in '-._' else '_' for c in text])
+    pathable_slot = "".join([c if c.isalnum() or c in "-._" else "_" for c in text])
     # as we replace some letters we can get collision for different slots
     # add we add unique part
-    unique_slot = hashlib.md5(text.encode('utf8')).hexdigest()
-    return '-'.join([pathable_slot, unique_slot])
+    unique_slot = hashlib.md5(text.encode("utf8")).hexdigest()
+    return "-".join([pathable_slot, unique_slot])
 
 
 class ScrapyPriorityQueue:
     """A priority queue implemented using multiple internal queues (typically,
     FIFO queues). It uses one internal queue for each priority value. The internal
     queue must implement the following methods:
 
@@ -73,15 +72,15 @@
         self.curprio = min(startprios)
 
     def qfactory(self, key):
         return create_instance(
             self.downstream_queue_cls,
             None,
             self.crawler,
-            self.key + '/' + str(key),
+            self.key + "/" + str(key),
         )
 
     def priority(self, request):
         return -request.priority
 
     def push(self, request):
         priority = self.priority(request)
@@ -124,68 +123,71 @@
         return active
 
     def __len__(self):
         return sum(len(x) for x in self.queues.values()) if self.queues else 0
 
 
 class DownloaderInterface:
-
     def __init__(self, crawler):
         self.downloader = crawler.engine.downloader
 
     def stats(self, possible_slots):
         return [(self._active_downloads(slot), slot) for slot in possible_slots]
 
     def get_slot_key(self, request):
         return self.downloader._get_slot_key(request, None)
 
     def _active_downloads(self, slot):
-        """ Return a number of requests in a Downloader for a given slot """
+        """Return a number of requests in a Downloader for a given slot"""
         if slot not in self.downloader.slots:
             return 0
         return len(self.downloader.slots[slot].active)
 
 
 class DownloaderAwarePriorityQueue:
-    """ PriorityQueue which takes Downloader activity into account:
+    """PriorityQueue which takes Downloader activity into account:
     domains (slots) with the least amount of active downloads are dequeued
     first.
     """
 
     @classmethod
     def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):
         return cls(crawler, downstream_queue_cls, key, startprios)
 
     def __init__(self, crawler, downstream_queue_cls, key, slot_startprios=()):
-        if crawler.settings.getint('CONCURRENT_REQUESTS_PER_IP') != 0:
-            raise ValueError(f'"{self.__class__}" does not support CONCURRENT_REQUESTS_PER_IP')
+        if crawler.settings.getint("CONCURRENT_REQUESTS_PER_IP") != 0:
+            raise ValueError(
+                f'"{self.__class__}" does not support CONCURRENT_REQUESTS_PER_IP'
+            )
 
         if slot_startprios and not isinstance(slot_startprios, dict):
-            raise ValueError("DownloaderAwarePriorityQueue accepts "
-                             "``slot_startprios`` as a dict; "
-                             f"{slot_startprios.__class__!r} instance "
-                             "is passed. Most likely, it means the state is"
-                             "created by an incompatible priority queue. "
-                             "Only a crawl started with the same priority "
-                             "queue class can be resumed.")
+            raise ValueError(
+                "DownloaderAwarePriorityQueue accepts "
+                "``slot_startprios`` as a dict; "
+                f"{slot_startprios.__class__!r} instance "
+                "is passed. Most likely, it means the state is"
+                "created by an incompatible priority queue. "
+                "Only a crawl started with the same priority "
+                "queue class can be resumed."
+            )
 
         self._downloader_interface = DownloaderInterface(crawler)
         self.downstream_queue_cls = downstream_queue_cls
         self.key = key
         self.crawler = crawler
 
         self.pqueues = {}  # slot -> priority queue
         for slot, startprios in (slot_startprios or {}).items():
             self.pqueues[slot] = self.pqfactory(slot, startprios)
 
     def pqfactory(self, slot, startprios=()):
         return ScrapyPriorityQueue(
             self.crawler,
             self.downstream_queue_cls,
-            self.key + '/' + _path_safe(slot),
+            self.key + "/" + _path_safe(slot),
             startprios,
         )
 
     def pop(self):
         stats = self._downloader_interface.stats(self.pqueues)
 
         if not stats:
```

### Comparing `Scrapy-2.7.1/scrapy/resolver.py` & `Scrapy-2.8.0/scrapy/resolver.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,19 @@
 from twisted.internet import defer
 from twisted.internet.base import ThreadedResolver
-from twisted.internet.interfaces import IHostResolution, IHostnameResolver, IResolutionReceiver, IResolverSimple
+from twisted.internet.interfaces import (
+    IHostnameResolver,
+    IHostResolution,
+    IResolutionReceiver,
+    IResolverSimple,
+)
 from zope.interface.declarations import implementer, provider
 
 from scrapy.utils.datatypes import LocalCache
 
-
 # TODO: cache misses
 dnscache = LocalCache(10000)
 
 
 @implementer(IResolverSimple)
 class CachingThreadedResolver(ThreadedResolver):
     """
@@ -19,19 +23,19 @@
     def __init__(self, reactor, cache_size, timeout):
         super().__init__(reactor)
         dnscache.limit = cache_size
         self.timeout = timeout
 
     @classmethod
     def from_crawler(cls, crawler, reactor):
-        if crawler.settings.getbool('DNSCACHE_ENABLED'):
-            cache_size = crawler.settings.getint('DNSCACHE_SIZE')
+        if crawler.settings.getbool("DNSCACHE_ENABLED"):
+            cache_size = crawler.settings.getint("DNSCACHE_SIZE")
         else:
             cache_size = 0
-        return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))
+        return cls(reactor, cache_size, crawler.settings.getfloat("DNS_TIMEOUT"))
 
     def install_on_reactor(self):
         self.reactor.installResolver(self)
 
     def getHostByName(self, name, timeout=None):
         if name in dnscache:
             return defer.succeed(dnscache[name])
@@ -90,25 +94,30 @@
     def __init__(self, reactor, cache_size):
         self.reactor = reactor
         self.original_resolver = reactor.nameResolver
         dnscache.limit = cache_size
 
     @classmethod
     def from_crawler(cls, crawler, reactor):
-        if crawler.settings.getbool('DNSCACHE_ENABLED'):
-            cache_size = crawler.settings.getint('DNSCACHE_SIZE')
+        if crawler.settings.getbool("DNSCACHE_ENABLED"):
+            cache_size = crawler.settings.getint("DNSCACHE_SIZE")
         else:
             cache_size = 0
         return cls(reactor, cache_size)
 
     def install_on_reactor(self):
         self.reactor.installNameResolver(self)
 
     def resolveHostName(
-        self, resolutionReceiver, hostName, portNumber=0, addressTypes=None, transportSemantics="TCP"
+        self,
+        resolutionReceiver,
+        hostName,
+        portNumber=0,
+        addressTypes=None,
+        transportSemantics="TCP",
     ):
         try:
             addresses = dnscache[hostName]
         except KeyError:
             return self.original_resolver.resolveHostName(
                 _CachingResolutionReceiver(resolutionReceiver, hostName),
                 hostName,
```

### Comparing `Scrapy-2.7.1/scrapy/responsetypes.py` & `Scrapy-2.8.0/tests/test_responsetypes.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,122 +1,120 @@
-"""
-This module implements a class which returns the appropriate Response class
-based on different criteria.
-"""
-from mimetypes import MimeTypes
-from pkgutil import get_data
-from io import StringIO
-
-from scrapy.http import Response
-from scrapy.utils.misc import load_object
-from scrapy.utils.python import binary_is_text, to_bytes, to_unicode
-
-
-class ResponseTypes:
-
-    CLASSES = {
-        'text/html': 'scrapy.http.HtmlResponse',
-        'application/atom+xml': 'scrapy.http.XmlResponse',
-        'application/rdf+xml': 'scrapy.http.XmlResponse',
-        'application/rss+xml': 'scrapy.http.XmlResponse',
-        'application/xhtml+xml': 'scrapy.http.HtmlResponse',
-        'application/vnd.wap.xhtml+xml': 'scrapy.http.HtmlResponse',
-        'application/xml': 'scrapy.http.XmlResponse',
-        'application/json': 'scrapy.http.TextResponse',
-        'application/x-json': 'scrapy.http.TextResponse',
-        'application/json-amazonui-streaming': 'scrapy.http.TextResponse',
-        'application/javascript': 'scrapy.http.TextResponse',
-        'application/x-javascript': 'scrapy.http.TextResponse',
-        'text/xml': 'scrapy.http.XmlResponse',
-        'text/*': 'scrapy.http.TextResponse',
-    }
-
-    def __init__(self):
-        self.classes = {}
-        self.mimetypes = MimeTypes()
-        mimedata = get_data('scrapy', 'mime.types').decode('utf8')
-        self.mimetypes.readfp(StringIO(mimedata))
-        for mimetype, cls in self.CLASSES.items():
-            self.classes[mimetype] = load_object(cls)
-
-    def from_mimetype(self, mimetype):
-        """Return the most appropriate Response class for the given mimetype"""
-        if mimetype is None:
-            return Response
-        elif mimetype in self.classes:
-            return self.classes[mimetype]
-        else:
-            basetype = f"{mimetype.split('/')[0]}/*"
-            return self.classes.get(basetype, Response)
-
-    def from_content_type(self, content_type, content_encoding=None):
-        """Return the most appropriate Response class from an HTTP Content-Type
-        header """
-        if content_encoding:
-            return Response
-        mimetype = to_unicode(content_type).split(';')[0].strip().lower()
-        return self.from_mimetype(mimetype)
-
-    def from_content_disposition(self, content_disposition):
-        try:
-            filename = to_unicode(
-                content_disposition, encoding='latin-1', errors='replace'
-            ).split(';')[1].split('=')[1].strip('"\'')
-            return self.from_filename(filename)
-        except IndexError:
-            return Response
-
-    def from_headers(self, headers):
-        """Return the most appropriate Response class by looking at the HTTP
-        headers"""
-        cls = Response
-        if b'Content-Type' in headers:
-            cls = self.from_content_type(
-                content_type=headers[b'Content-Type'],
-                content_encoding=headers.get(b'Content-Encoding')
-            )
-        if cls is Response and b'Content-Disposition' in headers:
-            cls = self.from_content_disposition(headers[b'Content-Disposition'])
-        return cls
-
-    def from_filename(self, filename):
-        """Return the most appropriate Response class from a file name"""
-        mimetype, encoding = self.mimetypes.guess_type(filename)
-        if mimetype and not encoding:
-            return self.from_mimetype(mimetype)
-        else:
-            return Response
-
-    def from_body(self, body):
-        """Try to guess the appropriate response based on the body content.
-        This method is a bit magic and could be improved in the future, but
-        it's not meant to be used except for special cases where response types
-        cannot be guess using more straightforward methods."""
-        chunk = body[:5000]
-        chunk = to_bytes(chunk)
-        if not binary_is_text(chunk):
-            return self.from_mimetype('application/octet-stream')
-        lowercase_chunk = chunk.lower()
-        if b"<html>" in lowercase_chunk:
-            return self.from_mimetype('text/html')
-        if b"<?xml" in lowercase_chunk:
-            return self.from_mimetype('text/xml')
-        if b'<!doctype html>' in lowercase_chunk:
-            return self.from_mimetype('text/html')
-        return self.from_mimetype('text')
-
-    def from_args(self, headers=None, url=None, filename=None, body=None):
-        """Guess the most appropriate Response class based on
-        the given arguments."""
-        cls = Response
-        if headers is not None:
-            cls = self.from_headers(headers)
-        if cls is Response and url is not None:
-            cls = self.from_filename(url)
-        if cls is Response and filename is not None:
-            cls = self.from_filename(filename)
-        if cls is Response and body is not None:
-            cls = self.from_body(body)
-        return cls
+import unittest
 
+from scrapy.http import Headers, HtmlResponse, Response, TextResponse, XmlResponse
+from scrapy.responsetypes import responsetypes
 
-responsetypes = ResponseTypes()
+
+class ResponseTypesTest(unittest.TestCase):
+    def test_from_filename(self):
+        mappings = [
+            ("data.bin", Response),
+            ("file.txt", TextResponse),
+            ("file.xml.gz", Response),
+            ("file.xml", XmlResponse),
+            ("file.html", HtmlResponse),
+            ("file.unknownext", Response),
+        ]
+        for source, cls in mappings:
+            retcls = responsetypes.from_filename(source)
+            assert retcls is cls, f"{source} ==> {retcls} != {cls}"
+
+    def test_from_content_disposition(self):
+        mappings = [
+            (b'attachment; filename="data.xml"', XmlResponse),
+            (b"attachment; filename=data.xml", XmlResponse),
+            ("attachment;filename=data.tar.gz".encode("utf-8"), Response),
+            ("attachment;filename=data.tar.gz".encode("latin-1"), Response),
+            ("attachment;filename=data.doc".encode("gbk"), Response),
+            ("attachment;filename=data.html".encode("cp720"), HtmlResponse),
+            ("attachment;filename=Wikipedia.xml".encode("iso2022_jp"), XmlResponse),
+        ]
+        for source, cls in mappings:
+            retcls = responsetypes.from_content_disposition(source)
+            assert retcls is cls, f"{source} ==> {retcls} != {cls}"
+
+    def test_from_content_type(self):
+        mappings = [
+            ("text/html; charset=UTF-8", HtmlResponse),
+            ("text/xml; charset=UTF-8", XmlResponse),
+            ("application/xhtml+xml; charset=UTF-8", HtmlResponse),
+            ("application/vnd.wap.xhtml+xml; charset=utf-8", HtmlResponse),
+            ("application/xml; charset=UTF-8", XmlResponse),
+            ("application/octet-stream", Response),
+            ("application/x-json; encoding=UTF8;charset=UTF-8", TextResponse),
+            ("application/json-amazonui-streaming;charset=UTF-8", TextResponse),
+        ]
+        for source, cls in mappings:
+            retcls = responsetypes.from_content_type(source)
+            assert retcls is cls, f"{source} ==> {retcls} != {cls}"
+
+    def test_from_body(self):
+        mappings = [
+            (b"\x03\x02\xdf\xdd\x23", Response),
+            (b"Some plain text\ndata with tabs\t and null bytes\0", TextResponse),
+            (b"<html><head><title>Hello</title></head>", HtmlResponse),
+            # https://codersblock.com/blog/the-smallest-valid-html5-page/
+            (b"<!DOCTYPE html>\n<title>.</title>", HtmlResponse),
+            (b'<?xml version="1.0" encoding="utf-8"', XmlResponse),
+        ]
+        for source, cls in mappings:
+            retcls = responsetypes.from_body(source)
+            assert retcls is cls, f"{source} ==> {retcls} != {cls}"
+
+    def test_from_headers(self):
+        mappings = [
+            ({"Content-Type": ["text/html; charset=utf-8"]}, HtmlResponse),
+            (
+                {
+                    "Content-Type": ["text/html; charset=utf-8"],
+                    "Content-Encoding": ["gzip"],
+                },
+                Response,
+            ),
+            (
+                {
+                    "Content-Type": ["application/octet-stream"],
+                    "Content-Disposition": ["attachment; filename=data.txt"],
+                },
+                TextResponse,
+            ),
+        ]
+        for source, cls in mappings:
+            source = Headers(source)
+            retcls = responsetypes.from_headers(source)
+            assert retcls is cls, f"{source} ==> {retcls} != {cls}"
+
+    def test_from_args(self):
+        # TODO: add more tests that check precedence between the different arguments
+        mappings = [
+            ({"url": "http://www.example.com/data.csv"}, TextResponse),
+            # headers takes precedence over url
+            (
+                {
+                    "headers": Headers({"Content-Type": ["text/html; charset=utf-8"]}),
+                    "url": "http://www.example.com/item/",
+                },
+                HtmlResponse,
+            ),
+            (
+                {
+                    "headers": Headers(
+                        {"Content-Disposition": ['attachment; filename="data.xml.gz"']}
+                    ),
+                    "url": "http://www.example.com/page/",
+                },
+                Response,
+            ),
+        ]
+        for source, cls in mappings:
+            retcls = responsetypes.from_args(**source)
+            assert retcls is cls, f"{source} ==> {retcls} != {cls}"
+
+    def test_custom_mime_types_loaded(self):
+        # check that mime.types files shipped with scrapy are loaded
+        self.assertEqual(
+            responsetypes.mimetypes.guess_type("x.scrapytest")[0], "x-scrapy/test"
+        )
+
+
+if __name__ == "__main__":
+    unittest.main()
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `Scrapy-2.7.1/scrapy/robotstxt.py` & `Scrapy-2.8.0/scrapy/robotstxt.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,33 +1,32 @@
-import sys
 import logging
+import sys
 from abc import ABCMeta, abstractmethod
 
 from scrapy.utils.python import to_unicode
 
-
 logger = logging.getLogger(__name__)
 
 
 def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):
     try:
         if to_native_str_type:
             robotstxt_body = to_unicode(robotstxt_body)
         else:
-            robotstxt_body = robotstxt_body.decode('utf-8')
+            robotstxt_body = robotstxt_body.decode("utf-8")
     except UnicodeDecodeError:
         # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.
         # Switch to 'allow all' state.
         logger.warning(
             "Failure while parsing robots.txt. File either contains garbage or "
             "is in an encoding other than UTF-8, treating it as an empty file.",
             exc_info=sys.exc_info(),
-            extra={'spider': spider},
+            extra={"spider": spider},
         )
-        robotstxt_body = ''
+        robotstxt_body = ""
     return robotstxt_body
 
 
 class RobotParser(metaclass=ABCMeta):
     @classmethod
     @abstractmethod
     def from_crawler(cls, crawler, robotstxt_body):
@@ -54,16 +53,19 @@
         """
         pass
 
 
 class PythonRobotParser(RobotParser):
     def __init__(self, robotstxt_body, spider):
         from urllib.robotparser import RobotFileParser
+
         self.spider = spider
-        robotstxt_body = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)
+        robotstxt_body = decode_robotstxt(
+            robotstxt_body, spider, to_native_str_type=True
+        )
         self.rp = RobotFileParser()
         self.rp.parse(robotstxt_body.splitlines())
 
     @classmethod
     def from_crawler(cls, crawler, robotstxt_body):
         spider = None if not crawler else crawler.spider
         o = cls(robotstxt_body, spider)
@@ -74,30 +76,32 @@
         url = to_unicode(url)
         return self.rp.can_fetch(user_agent, url)
 
 
 class ReppyRobotParser(RobotParser):
     def __init__(self, robotstxt_body, spider):
         from reppy.robots import Robots
+
         self.spider = spider
-        self.rp = Robots.parse('', robotstxt_body)
+        self.rp = Robots.parse("", robotstxt_body)
 
     @classmethod
     def from_crawler(cls, crawler, robotstxt_body):
         spider = None if not crawler else crawler.spider
         o = cls(robotstxt_body, spider)
         return o
 
     def allowed(self, url, user_agent):
         return self.rp.allowed(url, user_agent)
 
 
 class RerpRobotParser(RobotParser):
     def __init__(self, robotstxt_body, spider):
         from robotexclusionrulesparser import RobotExclusionRulesParser
+
         self.spider = spider
         self.rp = RobotExclusionRulesParser()
         robotstxt_body = decode_robotstxt(robotstxt_body, spider)
         self.rp.parse(robotstxt_body)
 
     @classmethod
     def from_crawler(cls, crawler, robotstxt_body):
@@ -110,14 +114,15 @@
         url = to_unicode(url)
         return self.rp.is_allowed(user_agent, url)
 
 
 class ProtegoRobotParser(RobotParser):
     def __init__(self, robotstxt_body, spider):
         from protego import Protego
+
         self.spider = spider
         robotstxt_body = decode_robotstxt(robotstxt_body, spider)
         self.rp = Protego.parse(robotstxt_body)
 
     @classmethod
     def from_crawler(cls, crawler, robotstxt_body):
         spider = None if not crawler else crawler.spider
```

### Comparing `Scrapy-2.7.1/scrapy/selector/unified.py` & `Scrapy-2.8.0/scrapy/selector/unified.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,30 +1,29 @@
 """
 XPath selectors based on lxml
 """
 
 from parsel import Selector as _ParselSelector
-from scrapy.utils.trackref import object_ref
-from scrapy.utils.python import to_bytes
-from scrapy.http import HtmlResponse, XmlResponse
 
+from scrapy.http import HtmlResponse, XmlResponse
+from scrapy.utils.python import to_bytes
+from scrapy.utils.trackref import object_ref
 
-__all__ = ['Selector', 'SelectorList']
+__all__ = ["Selector", "SelectorList"]
 
 
 def _st(response, st):
     if st is None:
-        return 'xml' if isinstance(response, XmlResponse) else 'html'
+        return "xml" if isinstance(response, XmlResponse) else "html"
     return st
 
 
 def _response_from_text(text, st):
-    rt = XmlResponse if st == 'xml' else HtmlResponse
-    return rt(url='about:blank', encoding='utf-8',
-              body=to_bytes(text, 'utf-8'))
+    rt = XmlResponse if st == "xml" else HtmlResponse
+    return rt(url="about:blank", encoding="utf-8", body=to_bytes(text, "utf-8"))
 
 
 class SelectorList(_ParselSelector.selectorlist_cls, object_ref):
     """
     The :class:`SelectorList` class is a subclass of the builtin ``list``
     class, which provides a few additional methods.
     """
@@ -57,26 +56,28 @@
     * ``"xml"`` for :class:`~scrapy.http.XmlResponse` type
     * ``"html"`` for anything else
 
     Otherwise, if ``type`` is set, the selector type will be forced and no
     detection will occur.
     """
 
-    __slots__ = ['response']
+    __slots__ = ["response"]
     selectorlist_cls = SelectorList
 
     def __init__(self, response=None, text=None, type=None, root=None, **kwargs):
         if response is not None and text is not None:
-            raise ValueError(f'{self.__class__.__name__}.__init__() received '
-                             'both response and text')
+            raise ValueError(
+                f"{self.__class__.__name__}.__init__() received "
+                "both response and text"
+            )
 
         st = _st(response, type)
 
         if text is not None:
             response = _response_from_text(text, st)
 
         if response is not None:
             text = response.text
-            kwargs.setdefault('base_url', response.url)
+            kwargs.setdefault("base_url", response.url)
 
         self.response = response
         super().__init__(text=text, type=st, root=root, **kwargs)
```

### Comparing `Scrapy-2.7.1/scrapy/settings/__init__.py` & `Scrapy-2.8.0/scrapy/settings/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,35 +1,33 @@
-import json
 import copy
+import json
 from collections.abc import MutableMapping
 from importlib import import_module
 from pprint import pformat
 
 from scrapy.settings import default_settings
 
-
 SETTINGS_PRIORITIES = {
-    'default': 0,
-    'command': 10,
-    'project': 20,
-    'spider': 30,
-    'cmdline': 40,
+    "default": 0,
+    "command": 10,
+    "project": 20,
+    "spider": 30,
+    "cmdline": 40,
 }
 
 
 def get_settings_priority(priority):
     """
     Small helper function that looks up a given string priority in the
     :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its
     numerical value, or directly returns a given numerical priority.
     """
     if isinstance(priority, str):
         return SETTINGS_PRIORITIES[priority]
-    else:
-        return priority
+    return priority
 
 
 class SettingsAttribute:
 
     """Class for storing data related to settings attributes.
 
     This class is intended for internal usage, you should try Settings class
@@ -47,19 +45,17 @@
         """Sets value if priority is higher or equal than current priority."""
         if priority >= self.priority:
             if isinstance(self.value, BaseSettings):
                 value = BaseSettings(value, priority=priority)
             self.value = value
             self.priority = priority
 
-    def __str__(self):
+    def __repr__(self):
         return f"<SettingsAttribute value={self.value!r} priority={self.priority}>"
 
-    __repr__ = __str__
-
 
 class BaseSettings(MutableMapping):
     """
     Instances of this class behave like dictionaries, but store priorities
     along with their ``(key, value)`` pairs, and can be frozen (i.e. marked
     immutable).
 
@@ -75,15 +71,15 @@
     :meth:`~scrapy.settings.BaseSettings.set` method, and can be accessed with
     the square bracket notation of dictionaries, or with the
     :meth:`~scrapy.settings.BaseSettings.get` method of the instance and its
     value conversion variants. When requesting a stored key, the value with the
     highest priority will be retrieved.
     """
 
-    def __init__(self, values=None, priority='project'):
+    def __init__(self, values=None, priority="project"):
         self.frozen = False
         self.attributes = {}
         if values:
             self.update(values, priority)
 
     def __getitem__(self, opt_name):
         if opt_name not in self:
@@ -125,17 +121,19 @@
         try:
             return bool(int(got))
         except ValueError:
             if got in ("True", "true"):
                 return True
             if got in ("False", "false"):
                 return False
-            raise ValueError("Supported values for boolean settings "
-                             "are 0/1, True/False, '0'/'1', "
-                             "'True'/'False' and 'true'/'false'")
+            raise ValueError(
+                "Supported values for boolean settings "
+                "are 0/1, True/False, '0'/'1', "
+                "'True'/'False' and 'true'/'false'"
+            )
 
     def getint(self, name, default=0):
         """
         Get a setting value as an int.
 
         :param name: the setting name
         :type name: str
@@ -169,15 +167,15 @@
         :type name: str
 
         :param default: the value to return if no setting is found
         :type default: object
         """
         value = self.get(name, default or [])
         if isinstance(value, str):
-            value = value.split(',')
+            value = value.split(",")
         return list(value)
 
     def getdict(self, name, default=None):
         """
         Get a setting value as a dictionary. If the setting original type is a
         dictionary, a copy of it will be returned. If it is a string it will be
         evaluated as a JSON dictionary. In the case that it is a
@@ -222,26 +220,26 @@
         value = self.get(name, default)
         if value is None:
             return {}
         if isinstance(value, str):
             try:
                 return json.loads(value)
             except ValueError:
-                return value.split(',')
+                return value.split(",")
         return copy.deepcopy(value)
 
     def getwithbase(self, name):
         """Get a composition of a dictionary-like setting and its `_BASE`
         counterpart.
 
         :param name: name of the dictionary-like setting
         :type name: str
         """
         compbs = BaseSettings()
-        compbs.update(self[name + '_BASE'])
+        compbs.update(self[name + "_BASE"])
         compbs.update(self[name])
         return compbs
 
     def getpriority(self, name):
         """
         Return the current numerical priority value of a setting, or ``None`` if
         the given ``name`` does not exist.
@@ -258,21 +256,20 @@
         Return the numerical value of the highest priority present throughout
         all settings, or the numerical value for ``default`` from
         :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings
         stored.
         """
         if len(self) > 0:
             return max(self.getpriority(name) for name in self)
-        else:
-            return get_settings_priority('default')
+        return get_settings_priority("default")
 
     def __setitem__(self, name, value):
         self.set(name, value)
 
-    def set(self, name, value, priority='project'):
+    def set(self, name, value, priority="project"):
         """
         Store a key/value attribute with a given priority.
 
         Settings should be populated *before* configuring the Crawler object
         (through the :meth:`~scrapy.crawler.Crawler.configure` method),
         otherwise they won't have any effect.
 
@@ -292,18 +289,18 @@
             if isinstance(value, SettingsAttribute):
                 self.attributes[name] = value
             else:
                 self.attributes[name] = SettingsAttribute(value, priority)
         else:
             self.attributes[name].set(value, priority)
 
-    def setdict(self, values, priority='project'):
+    def setdict(self, values, priority="project"):
         self.update(values, priority)
 
-    def setmodule(self, module, priority='project'):
+    def setmodule(self, module, priority="project"):
         """
         Store settings from a module with a given priority.
 
         This is a helper function that calls
         :meth:`~scrapy.settings.BaseSettings.set` for every globally declared
         uppercase variable of ``module`` with the provided ``priority``.
 
@@ -317,15 +314,15 @@
         self._assert_mutability()
         if isinstance(module, str):
             module = import_module(module)
         for key in dir(module):
             if key.isupper():
                 self.set(key, getattr(module, key), priority)
 
-    def update(self, values, priority='project'):
+    def update(self, values, priority="project"):
         """
         Store key/value pairs with a given priority.
 
         This is a helper function that calls
         :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``
         with the provided ``priority``.
 
@@ -350,15 +347,15 @@
             if isinstance(values, BaseSettings):
                 for name, value in values.items():
                     self.set(name, value, values.getpriority(name))
             else:
                 for name, value in values.items():
                     self.set(name, value, priority)
 
-    def delete(self, name, priority='project'):
+    def delete(self, name, priority="project"):
         self._assert_mutability()
         priority = get_settings_priority(priority)
         if priority >= self.getpriority(name):
             del self.attributes[name]
 
     def __delitem__(self, name):
         self._assert_mutability()
@@ -403,20 +400,25 @@
     def __iter__(self):
         return iter(self.attributes)
 
     def __len__(self):
         return len(self.attributes)
 
     def _to_dict(self):
-        return {self._get_key(k): (v._to_dict() if isinstance(v, BaseSettings) else v)
-                for k, v in self.items()}
+        return {
+            self._get_key(k): (v._to_dict() if isinstance(v, BaseSettings) else v)
+            for k, v in self.items()
+        }
 
     def _get_key(self, key_value):
-        return (key_value if isinstance(key_value, (bool, float, int, str, type(None)))
-                else str(key_value))
+        return (
+            key_value
+            if isinstance(key_value, (bool, float, int, str, type(None)))
+            else str(key_value)
+        )
 
     def copy_to_dict(self):
         """
         Make a copy of current settings and convert to a dict.
 
         This method returns a new dict populated with the same values
         and their priorities as the current settings.
@@ -433,60 +435,36 @@
     def _repr_pretty_(self, p, cycle):
         if cycle:
             p.text(repr(self))
         else:
             p.text(pformat(self.copy_to_dict()))
 
 
-class _DictProxy(MutableMapping):
-
-    def __init__(self, settings, priority):
-        self.o = {}
-        self.settings = settings
-        self.priority = priority
-
-    def __len__(self):
-        return len(self.o)
-
-    def __getitem__(self, k):
-        return self.o[k]
-
-    def __setitem__(self, k, v):
-        self.settings.set(k, v, priority=self.priority)
-        self.o[k] = v
-
-    def __delitem__(self, k):
-        del self.o[k]
-
-    def __iter__(self, k, v):
-        return iter(self.o)
-
-
 class Settings(BaseSettings):
     """
     This object stores Scrapy settings for the configuration of internal
     components, and can be used for any further customization.
 
     It is a direct subclass and supports all methods of
     :class:`~scrapy.settings.BaseSettings`. Additionally, after instantiation
     of this class, the new object will have the global default settings
     described on :ref:`topics-settings-ref` already populated.
     """
 
-    def __init__(self, values=None, priority='project'):
+    def __init__(self, values=None, priority="project"):
         # Do not pass kwarg values here. We don't want to promote user-defined
         # dicts, and we want to update, not replace, default dicts with the
         # values given by the user
         super().__init__()
-        self.setmodule(default_settings, 'default')
+        self.setmodule(default_settings, "default")
         # Promote default dictionaries to BaseSettings instances for per-key
         # priorities
         for name, val in self.items():
             if isinstance(val, dict):
-                self.set(name, BaseSettings(val, 'default'), 'default')
+                self.set(name, BaseSettings(val, "default"), "default")
         self.update(values, priority)
 
 
 def iter_default_settings():
     """Return the default settings as an iterator of (name, value) tuples"""
     for name in dir(default_settings):
         if name.isupper():
```

### Comparing `Scrapy-2.7.1/scrapy/settings/default_settings.py` & `Scrapy-2.8.0/scrapy/settings/default_settings.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,301 +11,305 @@
 * add its documentation to the available settings documentation
   (docs/topics/settings.rst)
 
 """
 
 import sys
 from importlib import import_module
-from os.path import join, abspath, dirname
+from pathlib import Path
 
 AJAXCRAWL_ENABLED = False
 
 ASYNCIO_EVENT_LOOP = None
 
 AUTOTHROTTLE_ENABLED = False
 AUTOTHROTTLE_DEBUG = False
 AUTOTHROTTLE_MAX_DELAY = 60.0
 AUTOTHROTTLE_START_DELAY = 5.0
 AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
 
-BOT_NAME = 'scrapybot'
+BOT_NAME = "scrapybot"
 
 CLOSESPIDER_TIMEOUT = 0
 CLOSESPIDER_PAGECOUNT = 0
 CLOSESPIDER_ITEMCOUNT = 0
 CLOSESPIDER_ERRORCOUNT = 0
 
-COMMANDS_MODULE = ''
+COMMANDS_MODULE = ""
 
 COMPRESSION_ENABLED = True
 
 CONCURRENT_ITEMS = 100
 
 CONCURRENT_REQUESTS = 16
 CONCURRENT_REQUESTS_PER_DOMAIN = 8
 CONCURRENT_REQUESTS_PER_IP = 0
 
 COOKIES_ENABLED = True
 COOKIES_DEBUG = False
 
-DEFAULT_ITEM_CLASS = 'scrapy.item.Item'
+DEFAULT_ITEM_CLASS = "scrapy.item.Item"
 
 DEFAULT_REQUEST_HEADERS = {
-    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
-    'Accept-Language': 'en',
+    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
+    "Accept-Language": "en",
 }
 
 DEPTH_LIMIT = 0
 DEPTH_STATS_VERBOSE = False
 DEPTH_PRIORITY = 0
 
 DNSCACHE_ENABLED = True
 DNSCACHE_SIZE = 10000
-DNS_RESOLVER = 'scrapy.resolver.CachingThreadedResolver'
+DNS_RESOLVER = "scrapy.resolver.CachingThreadedResolver"
 DNS_TIMEOUT = 60
 
 DOWNLOAD_DELAY = 0
 
 DOWNLOAD_HANDLERS = {}
 DOWNLOAD_HANDLERS_BASE = {
-    'data': 'scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler',
-    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',
-    'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',
-    'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',
-    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',
-    'ftp': 'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler',
+    "data": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler",
+    "file": "scrapy.core.downloader.handlers.file.FileDownloadHandler",
+    "http": "scrapy.core.downloader.handlers.http.HTTPDownloadHandler",
+    "https": "scrapy.core.downloader.handlers.http.HTTPDownloadHandler",
+    "s3": "scrapy.core.downloader.handlers.s3.S3DownloadHandler",
+    "ftp": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler",
 }
 
-DOWNLOAD_TIMEOUT = 180      # 3mins
+DOWNLOAD_TIMEOUT = 180  # 3mins
 
-DOWNLOAD_MAXSIZE = 1024 * 1024 * 1024   # 1024m
-DOWNLOAD_WARNSIZE = 32 * 1024 * 1024    # 32m
+DOWNLOAD_MAXSIZE = 1024 * 1024 * 1024  # 1024m
+DOWNLOAD_WARNSIZE = 32 * 1024 * 1024  # 32m
 
 DOWNLOAD_FAIL_ON_DATALOSS = True
 
-DOWNLOADER = 'scrapy.core.downloader.Downloader'
+DOWNLOADER = "scrapy.core.downloader.Downloader"
 
-DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'
-DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'
-DOWNLOADER_CLIENT_TLS_CIPHERS = 'DEFAULT'
+DOWNLOADER_HTTPCLIENTFACTORY = (
+    "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory"
+)
+DOWNLOADER_CLIENTCONTEXTFACTORY = (
+    "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory"
+)
+DOWNLOADER_CLIENT_TLS_CIPHERS = "DEFAULT"
 # Use highest TLS/SSL protocol version supported by the platform, also allowing negotiation:
-DOWNLOADER_CLIENT_TLS_METHOD = 'TLS'
+DOWNLOADER_CLIENT_TLS_METHOD = "TLS"
 DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False
 
 DOWNLOADER_MIDDLEWARES = {}
 
 DOWNLOADER_MIDDLEWARES_BASE = {
     # Engine side
-    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,
-    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,
-    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,
-    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,
-    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500,
-    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,
-    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,
-    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,
-    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,
-    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,
-    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,
-    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,
-    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,
-    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,
+    "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware": 100,
+    "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware": 300,
+    "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware": 350,
+    "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware": 400,
+    "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware": 500,
+    "scrapy.downloadermiddlewares.retry.RetryMiddleware": 550,
+    "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware": 560,
+    "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware": 580,
+    "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware": 590,
+    "scrapy.downloadermiddlewares.redirect.RedirectMiddleware": 600,
+    "scrapy.downloadermiddlewares.cookies.CookiesMiddleware": 700,
+    "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware": 750,
+    "scrapy.downloadermiddlewares.stats.DownloaderStats": 850,
+    "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware": 900,
     # Downloader side
 }
 
 DOWNLOADER_STATS = True
 
-DUPEFILTER_CLASS = 'scrapy.dupefilters.RFPDupeFilter'
+DUPEFILTER_CLASS = "scrapy.dupefilters.RFPDupeFilter"
 
-EDITOR = 'vi'
-if sys.platform == 'win32':
-    EDITOR = '%s -m idlelib.idle'
+EDITOR = "vi"
+if sys.platform == "win32":
+    EDITOR = "%s -m idlelib.idle"
 
 EXTENSIONS = {}
 
 EXTENSIONS_BASE = {
-    'scrapy.extensions.corestats.CoreStats': 0,
-    'scrapy.extensions.telnet.TelnetConsole': 0,
-    'scrapy.extensions.memusage.MemoryUsage': 0,
-    'scrapy.extensions.memdebug.MemoryDebugger': 0,
-    'scrapy.extensions.closespider.CloseSpider': 0,
-    'scrapy.extensions.feedexport.FeedExporter': 0,
-    'scrapy.extensions.logstats.LogStats': 0,
-    'scrapy.extensions.spiderstate.SpiderState': 0,
-    'scrapy.extensions.throttle.AutoThrottle': 0,
+    "scrapy.extensions.corestats.CoreStats": 0,
+    "scrapy.extensions.telnet.TelnetConsole": 0,
+    "scrapy.extensions.memusage.MemoryUsage": 0,
+    "scrapy.extensions.memdebug.MemoryDebugger": 0,
+    "scrapy.extensions.closespider.CloseSpider": 0,
+    "scrapy.extensions.feedexport.FeedExporter": 0,
+    "scrapy.extensions.logstats.LogStats": 0,
+    "scrapy.extensions.spiderstate.SpiderState": 0,
+    "scrapy.extensions.throttle.AutoThrottle": 0,
 }
 
 FEED_TEMPDIR = None
 FEEDS = {}
 FEED_URI_PARAMS = None  # a function to extend uri arguments
 FEED_STORE_EMPTY = False
 FEED_EXPORT_ENCODING = None
 FEED_EXPORT_FIELDS = None
 FEED_STORAGES = {}
 FEED_STORAGES_BASE = {
-    '': 'scrapy.extensions.feedexport.FileFeedStorage',
-    'file': 'scrapy.extensions.feedexport.FileFeedStorage',
-    'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',
-    'gs': 'scrapy.extensions.feedexport.GCSFeedStorage',
-    's3': 'scrapy.extensions.feedexport.S3FeedStorage',
-    'stdout': 'scrapy.extensions.feedexport.StdoutFeedStorage',
+    "": "scrapy.extensions.feedexport.FileFeedStorage",
+    "file": "scrapy.extensions.feedexport.FileFeedStorage",
+    "ftp": "scrapy.extensions.feedexport.FTPFeedStorage",
+    "gs": "scrapy.extensions.feedexport.GCSFeedStorage",
+    "s3": "scrapy.extensions.feedexport.S3FeedStorage",
+    "stdout": "scrapy.extensions.feedexport.StdoutFeedStorage",
 }
 FEED_EXPORT_BATCH_ITEM_COUNT = 0
 FEED_EXPORTERS = {}
 FEED_EXPORTERS_BASE = {
-    'json': 'scrapy.exporters.JsonItemExporter',
-    'jsonlines': 'scrapy.exporters.JsonLinesItemExporter',
-    'jsonl': 'scrapy.exporters.JsonLinesItemExporter',
-    'jl': 'scrapy.exporters.JsonLinesItemExporter',
-    'csv': 'scrapy.exporters.CsvItemExporter',
-    'xml': 'scrapy.exporters.XmlItemExporter',
-    'marshal': 'scrapy.exporters.MarshalItemExporter',
-    'pickle': 'scrapy.exporters.PickleItemExporter',
+    "json": "scrapy.exporters.JsonItemExporter",
+    "jsonlines": "scrapy.exporters.JsonLinesItemExporter",
+    "jsonl": "scrapy.exporters.JsonLinesItemExporter",
+    "jl": "scrapy.exporters.JsonLinesItemExporter",
+    "csv": "scrapy.exporters.CsvItemExporter",
+    "xml": "scrapy.exporters.XmlItemExporter",
+    "marshal": "scrapy.exporters.MarshalItemExporter",
+    "pickle": "scrapy.exporters.PickleItemExporter",
 }
 FEED_EXPORT_INDENT = 0
 
 FEED_STORAGE_FTP_ACTIVE = False
-FEED_STORAGE_GCS_ACL = ''
-FEED_STORAGE_S3_ACL = ''
+FEED_STORAGE_GCS_ACL = ""
+FEED_STORAGE_S3_ACL = ""
 
-FILES_STORE_S3_ACL = 'private'
-FILES_STORE_GCS_ACL = ''
+FILES_STORE_S3_ACL = "private"
+FILES_STORE_GCS_ACL = ""
 
-FTP_USER = 'anonymous'
-FTP_PASSWORD = 'guest'
+FTP_USER = "anonymous"
+FTP_PASSWORD = "guest"
 FTP_PASSIVE_MODE = True
 
 GCS_PROJECT_ID = None
 
 HTTPCACHE_ENABLED = False
-HTTPCACHE_DIR = 'httpcache'
+HTTPCACHE_DIR = "httpcache"
 HTTPCACHE_IGNORE_MISSING = False
-HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
+HTTPCACHE_STORAGE = "scrapy.extensions.httpcache.FilesystemCacheStorage"
 HTTPCACHE_EXPIRATION_SECS = 0
 HTTPCACHE_ALWAYS_STORE = False
 HTTPCACHE_IGNORE_HTTP_CODES = []
-HTTPCACHE_IGNORE_SCHEMES = ['file']
+HTTPCACHE_IGNORE_SCHEMES = ["file"]
 HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []
-HTTPCACHE_DBM_MODULE = 'dbm'
-HTTPCACHE_POLICY = 'scrapy.extensions.httpcache.DummyPolicy'
+HTTPCACHE_DBM_MODULE = "dbm"
+HTTPCACHE_POLICY = "scrapy.extensions.httpcache.DummyPolicy"
 HTTPCACHE_GZIP = False
 
 HTTPPROXY_ENABLED = True
-HTTPPROXY_AUTH_ENCODING = 'latin-1'
+HTTPPROXY_AUTH_ENCODING = "latin-1"
 
-IMAGES_STORE_S3_ACL = 'private'
-IMAGES_STORE_GCS_ACL = ''
+IMAGES_STORE_S3_ACL = "private"
+IMAGES_STORE_GCS_ACL = ""
 
-ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'
+ITEM_PROCESSOR = "scrapy.pipelines.ItemPipelineManager"
 
 ITEM_PIPELINES = {}
 ITEM_PIPELINES_BASE = {}
 
 LOG_ENABLED = True
-LOG_ENCODING = 'utf-8'
-LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'
-LOG_FORMAT = '%(asctime)s [%(name)s] %(levelname)s: %(message)s'
-LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S'
+LOG_ENCODING = "utf-8"
+LOG_FORMATTER = "scrapy.logformatter.LogFormatter"
+LOG_FORMAT = "%(asctime)s [%(name)s] %(levelname)s: %(message)s"
+LOG_DATEFORMAT = "%Y-%m-%d %H:%M:%S"
 LOG_STDOUT = False
-LOG_LEVEL = 'DEBUG'
+LOG_LEVEL = "DEBUG"
 LOG_FILE = None
 LOG_FILE_APPEND = True
 LOG_SHORT_NAMES = False
 
 SCHEDULER_DEBUG = False
 
 LOGSTATS_INTERVAL = 60.0
 
-MAIL_HOST = 'localhost'
+MAIL_HOST = "localhost"
 MAIL_PORT = 25
-MAIL_FROM = 'scrapy@localhost'
+MAIL_FROM = "scrapy@localhost"
 MAIL_PASS = None
 MAIL_USER = None
 
-MEMDEBUG_ENABLED = False        # enable memory debugging
-MEMDEBUG_NOTIFY = []            # send memory debugging report by mail at engine shutdown
+MEMDEBUG_ENABLED = False  # enable memory debugging
+MEMDEBUG_NOTIFY = []  # send memory debugging report by mail at engine shutdown
 
 MEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0
 MEMUSAGE_ENABLED = True
 MEMUSAGE_LIMIT_MB = 0
 MEMUSAGE_NOTIFY_MAIL = []
 MEMUSAGE_WARNING_MB = 0
 
 METAREFRESH_ENABLED = True
 METAREFRESH_IGNORE_TAGS = []
 METAREFRESH_MAXDELAY = 100
 
-NEWSPIDER_MODULE = ''
+NEWSPIDER_MODULE = ""
 
 RANDOMIZE_DOWNLOAD_DELAY = True
 
 REACTOR_THREADPOOL_MAXSIZE = 10
 
 REDIRECT_ENABLED = True
 REDIRECT_MAX_TIMES = 20  # uses Firefox default setting
 REDIRECT_PRIORITY_ADJUST = +2
 
 REFERER_ENABLED = True
-REFERRER_POLICY = 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'
+REFERRER_POLICY = "scrapy.spidermiddlewares.referer.DefaultReferrerPolicy"
 
-REQUEST_FINGERPRINTER_CLASS = 'scrapy.utils.request.RequestFingerprinter'
-REQUEST_FINGERPRINTER_IMPLEMENTATION = '2.6'
+REQUEST_FINGERPRINTER_CLASS = "scrapy.utils.request.RequestFingerprinter"
+REQUEST_FINGERPRINTER_IMPLEMENTATION = "2.6"
 
 RETRY_ENABLED = True
 RETRY_TIMES = 2  # initial response + 2 retries = 3 requests
 RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]
 RETRY_PRIORITY_ADJUST = -1
 
 ROBOTSTXT_OBEY = False
-ROBOTSTXT_PARSER = 'scrapy.robotstxt.ProtegoRobotParser'
+ROBOTSTXT_PARSER = "scrapy.robotstxt.ProtegoRobotParser"
 ROBOTSTXT_USER_AGENT = None
 
-SCHEDULER = 'scrapy.core.scheduler.Scheduler'
-SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'
-SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'
-SCHEDULER_PRIORITY_QUEUE = 'scrapy.pqueues.ScrapyPriorityQueue'
+SCHEDULER = "scrapy.core.scheduler.Scheduler"
+SCHEDULER_DISK_QUEUE = "scrapy.squeues.PickleLifoDiskQueue"
+SCHEDULER_MEMORY_QUEUE = "scrapy.squeues.LifoMemoryQueue"
+SCHEDULER_PRIORITY_QUEUE = "scrapy.pqueues.ScrapyPriorityQueue"
 
 SCRAPER_SLOT_MAX_ACTIVE_SIZE = 5000000
 
-SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader'
+SPIDER_LOADER_CLASS = "scrapy.spiderloader.SpiderLoader"
 SPIDER_LOADER_WARN_ONLY = False
 
 SPIDER_MIDDLEWARES = {}
 
 SPIDER_MIDDLEWARES_BASE = {
     # Engine side
-    'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': 50,
-    'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': 500,
-    'scrapy.spidermiddlewares.referer.RefererMiddleware': 700,
-    'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware': 800,
-    'scrapy.spidermiddlewares.depth.DepthMiddleware': 900,
+    "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware": 50,
+    "scrapy.spidermiddlewares.offsite.OffsiteMiddleware": 500,
+    "scrapy.spidermiddlewares.referer.RefererMiddleware": 700,
+    "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware": 800,
+    "scrapy.spidermiddlewares.depth.DepthMiddleware": 900,
     # Spider side
 }
 
 SPIDER_MODULES = []
 
-STATS_CLASS = 'scrapy.statscollectors.MemoryStatsCollector'
+STATS_CLASS = "scrapy.statscollectors.MemoryStatsCollector"
 STATS_DUMP = True
 
 STATSMAILER_RCPTS = []
 
-TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))
+TEMPLATES_DIR = str((Path(__file__).parent / ".." / "templates").resolve())
 
 URLLENGTH_LIMIT = 2083
 
 USER_AGENT = f'Scrapy/{import_module("scrapy").__version__} (+https://scrapy.org)'
 
 TELNETCONSOLE_ENABLED = 1
 TELNETCONSOLE_PORT = [6023, 6073]
-TELNETCONSOLE_HOST = '127.0.0.1'
-TELNETCONSOLE_USERNAME = 'scrapy'
+TELNETCONSOLE_HOST = "127.0.0.1"
+TELNETCONSOLE_USERNAME = "scrapy"
 TELNETCONSOLE_PASSWORD = None
 
 TWISTED_REACTOR = None
 
 SPIDER_CONTRACTS = {}
 SPIDER_CONTRACTS_BASE = {
-    'scrapy.contracts.default.UrlContract': 1,
-    'scrapy.contracts.default.CallbackKeywordArgumentsContract': 1,
-    'scrapy.contracts.default.ReturnsContract': 2,
-    'scrapy.contracts.default.ScrapesContract': 3,
+    "scrapy.contracts.default.UrlContract": 1,
+    "scrapy.contracts.default.CallbackKeywordArgumentsContract": 1,
+    "scrapy.contracts.default.ReturnsContract": 2,
+    "scrapy.contracts.default.ScrapesContract": 3,
 }
```

### Comparing `Scrapy-2.7.1/scrapy/shell.py` & `Scrapy-2.8.0/scrapy/shell.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,38 +3,39 @@
 See documentation in docs/topics/shell.rst
 
 """
 import os
 import signal
 
 from itemadapter import is_item
-from twisted.internet import threads, defer
+from twisted.internet import defer, threads
 from twisted.python import threadable
 from w3lib.url import any_to_uri
 
 from scrapy.crawler import Crawler
 from scrapy.exceptions import IgnoreRequest
 from scrapy.http import Request, Response
 from scrapy.settings import Settings
 from scrapy.spiders import Spider
 from scrapy.utils.conf import get_config
 from scrapy.utils.console import DEFAULT_PYTHON_SHELLS, start_python_console
 from scrapy.utils.datatypes import SequenceExclude
 from scrapy.utils.misc import load_object
+from scrapy.utils.reactor import is_asyncio_reactor_installed, set_asyncio_event_loop
 from scrapy.utils.response import open_in_browser
 
 
 class Shell:
 
     relevant_classes = (Crawler, Spider, Request, Response, Settings)
 
     def __init__(self, crawler, update_vars=None, code=None):
         self.crawler = crawler
         self.update_vars = update_vars or (lambda x: None)
-        self.item_class = load_object(crawler.settings['DEFAULT_ITEM_CLASS'])
+        self.item_class = load_object(crawler.settings["DEFAULT_ITEM_CLASS"])
         self.spider = None
         self.inthread = not threadable.isInIOThread()
         self.code = code
         self.vars = {}
 
     def start(self, url=None, request=None, response=None, spider=None, redirect=True):
         # disable accidental Ctrl-C key press from shutting down the engine
@@ -57,29 +58,34 @@
             [settings]
             # shell can be one of ipython, bpython or python;
             # to be used as the interactive python console, if available.
             # (default is ipython, fallbacks in the order listed above)
             shell = python
             """
             cfg = get_config()
-            section, option = 'settings', 'shell'
-            env = os.environ.get('SCRAPY_PYTHON_SHELL')
+            section, option = "settings", "shell"
+            env = os.environ.get("SCRAPY_PYTHON_SHELL")
             shells = []
             if env:
-                shells += env.strip().lower().split(',')
+                shells += env.strip().lower().split(",")
             elif cfg.has_option(section, option):
                 shells += [cfg.get(section, option).strip().lower()]
             else:  # try all by default
                 shells += DEFAULT_PYTHON_SHELLS.keys()
             # always add standard shell as fallback
-            shells += ['python']
-            start_python_console(self.vars, shells=shells,
-                                 banner=self.vars.pop('banner', ''))
+            shells += ["python"]
+            start_python_console(
+                self.vars, shells=shells, banner=self.vars.pop("banner", "")
+            )
 
     def _schedule(self, request, spider):
+        if is_asyncio_reactor_installed():
+            # set the asyncio event loop for the current thread
+            event_loop_path = self.crawler.settings["ASYNCIO_EVENT_LOOP"]
+            set_asyncio_event_loop(event_loop_path)
         spider = self._open_spider(request, spider)
         d = _request_deferred(request)
         d.addCallback(lambda x: (x, spider))
         self.crawler.engine.crawl(request)
         return d
 
     def _open_spider(self, request, spider):
@@ -92,77 +98,91 @@
         self.crawler.spider = spider
         self.crawler.engine.open_spider(spider, close_if_idle=False)
         self.spider = spider
         return spider
 
     def fetch(self, request_or_url, spider=None, redirect=True, **kwargs):
         from twisted.internet import reactor
+
         if isinstance(request_or_url, Request):
             request = request_or_url
         else:
             url = any_to_uri(request_or_url)
             request = Request(url, dont_filter=True, **kwargs)
             if redirect:
-                request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))
+                request.meta["handle_httpstatus_list"] = SequenceExclude(
+                    range(300, 400)
+                )
             else:
-                request.meta['handle_httpstatus_all'] = True
+                request.meta["handle_httpstatus_all"] = True
         response = None
         try:
             response, spider = threads.blockingCallFromThread(
-                reactor, self._schedule, request, spider)
+                reactor, self._schedule, request, spider
+            )
         except IgnoreRequest:
             pass
         self.populate_vars(response, request, spider)
 
     def populate_vars(self, response=None, request=None, spider=None):
         import scrapy
 
-        self.vars['scrapy'] = scrapy
-        self.vars['crawler'] = self.crawler
-        self.vars['item'] = self.item_class()
-        self.vars['settings'] = self.crawler.settings
-        self.vars['spider'] = spider
-        self.vars['request'] = request
-        self.vars['response'] = response
+        self.vars["scrapy"] = scrapy
+        self.vars["crawler"] = self.crawler
+        self.vars["item"] = self.item_class()
+        self.vars["settings"] = self.crawler.settings
+        self.vars["spider"] = spider
+        self.vars["request"] = request
+        self.vars["response"] = response
         if self.inthread:
-            self.vars['fetch'] = self.fetch
-        self.vars['view'] = open_in_browser
-        self.vars['shelp'] = self.print_help
+            self.vars["fetch"] = self.fetch
+        self.vars["view"] = open_in_browser
+        self.vars["shelp"] = self.print_help
         self.update_vars(self.vars)
         if not self.code:
-            self.vars['banner'] = self.get_help()
+            self.vars["banner"] = self.get_help()
 
     def print_help(self):
         print(self.get_help())
 
     def get_help(self):
         b = []
         b.append("Available Scrapy objects:")
-        b.append("  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)")
+        b.append(
+            "  scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)"
+        )
         for k, v in sorted(self.vars.items()):
             if self._is_relevant(v):
                 b.append(f"  {k:<10} {v}")
         b.append("Useful shortcuts:")
         if self.inthread:
-            b.append("  fetch(url[, redirect=True]) "
-                     "Fetch URL and update local objects (by default, redirects are followed)")
-            b.append("  fetch(req)                  "
-                     "Fetch a scrapy.Request and update local objects ")
+            b.append(
+                "  fetch(url[, redirect=True]) "
+                "Fetch URL and update local objects (by default, redirects are followed)"
+            )
+            b.append(
+                "  fetch(req)                  "
+                "Fetch a scrapy.Request and update local objects "
+            )
         b.append("  shelp()           Shell help (print this help)")
         b.append("  view(response)    View response in a browser")
 
         return "\n".join(f"[s] {line}" for line in b)
 
     def _is_relevant(self, value):
         return isinstance(value, self.relevant_classes) or is_item(value)
 
 
 def inspect_response(response, spider):
     """Open a shell to inspect the given response"""
+    # Shell.start removes the SIGINT handler, so save it and re-add it after
+    # the shell has closed
+    sigint_handler = signal.getsignal(signal.SIGINT)
     Shell(spider.crawler).start(response=response, spider=spider)
+    signal.signal(signal.SIGINT, sigint_handler)
 
 
 def _request_deferred(request):
     """Wrap a request inside a Deferred.
 
     This function is harmful, do not use it until you know what you are doing.
```

### Comparing `Scrapy-2.7.1/scrapy/signalmanager.py` & `Scrapy-2.8.0/scrapy/signalmanager.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from pydispatch import dispatcher
+
 from scrapy.utils import signal as _signal
 
 
 class SignalManager:
-
     def __init__(self, sender=dispatcher.Anonymous):
         self.sender = sender
 
     def connect(self, receiver, signal, **kwargs):
         """
         Connect a receiver function to a signal.
 
@@ -17,52 +17,52 @@
 
         :param receiver: the function to be connected
         :type receiver: collections.abc.Callable
 
         :param signal: the signal to connect to
         :type signal: object
         """
-        kwargs.setdefault('sender', self.sender)
+        kwargs.setdefault("sender", self.sender)
         return dispatcher.connect(receiver, signal, **kwargs)
 
     def disconnect(self, receiver, signal, **kwargs):
         """
         Disconnect a receiver function from a signal. This has the
         opposite effect of the :meth:`connect` method, and the arguments
         are the same.
         """
-        kwargs.setdefault('sender', self.sender)
+        kwargs.setdefault("sender", self.sender)
         return dispatcher.disconnect(receiver, signal, **kwargs)
 
     def send_catch_log(self, signal, **kwargs):
         """
         Send a signal, catch exceptions and log them.
 
         The keyword arguments are passed to the signal handlers (connected
         through the :meth:`connect` method).
         """
-        kwargs.setdefault('sender', self.sender)
+        kwargs.setdefault("sender", self.sender)
         return _signal.send_catch_log(signal, **kwargs)
 
     def send_catch_log_deferred(self, signal, **kwargs):
         """
         Like :meth:`send_catch_log` but supports returning
         :class:`~twisted.internet.defer.Deferred` objects from signal handlers.
 
         Returns a Deferred that gets fired once all signal handlers
         deferreds were fired. Send a signal, catch exceptions and log them.
 
         The keyword arguments are passed to the signal handlers (connected
         through the :meth:`connect` method).
         """
-        kwargs.setdefault('sender', self.sender)
+        kwargs.setdefault("sender", self.sender)
         return _signal.send_catch_log_deferred(signal, **kwargs)
 
     def disconnect_all(self, signal, **kwargs):
         """
         Disconnect all receivers from the given signal.
 
         :param signal: the signal to disconnect from
         :type signal: object
         """
-        kwargs.setdefault('sender', self.sender)
+        kwargs.setdefault("sender", self.sender)
         return _signal.disconnect_all(signal, **kwargs)
```

### Comparing `Scrapy-2.7.1/scrapy/signals.py` & `Scrapy-2.8.0/scrapy/signals.py`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/scrapy/spiderloader.py` & `Scrapy-2.8.0/scrapy/spiderloader.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,28 +13,30 @@
 class SpiderLoader:
     """
     SpiderLoader is a class which locates and loads spiders
     in a Scrapy project.
     """
 
     def __init__(self, settings):
-        self.spider_modules = settings.getlist('SPIDER_MODULES')
-        self.warn_only = settings.getbool('SPIDER_LOADER_WARN_ONLY')
+        self.spider_modules = settings.getlist("SPIDER_MODULES")
+        self.warn_only = settings.getbool("SPIDER_LOADER_WARN_ONLY")
         self._spiders = {}
         self._found = defaultdict(list)
         self._load_all_spiders()
 
     def _check_name_duplicates(self):
         dupes = []
         for name, locations in self._found.items():
-            dupes.extend([
-                f"  {cls} named {name!r} (in {mod})"
-                for mod, cls in locations
-                if len(locations) > 1
-            ])
+            dupes.extend(
+                [
+                    f"  {cls} named {name!r} (in {mod})"
+                    for mod, cls in locations
+                    if len(locations) > 1
+                ]
+            )
 
         if dupes:
             dupes_string = "\n\n".join(dupes)
             warnings.warn(
                 "There are several spiders with the same name:\n\n"
                 f"{dupes_string}\n\n  This can cause unexpected behavior.",
                 category=UserWarning,
@@ -77,16 +79,15 @@
             raise KeyError(f"Spider not found: {spider_name}")
 
     def find_by_request(self, request):
         """
         Return the list of spider names that can handle the given request.
         """
         return [
-            name for name, cls in self._spiders.items()
-            if cls.handles_request(request)
+            name for name, cls in self._spiders.items() if cls.handles_request(request)
         ]
 
     def list(self):
         """
         Return a list with the names of all spiders available in the project.
         """
         return list(self._spiders.keys())
```

### Comparing `Scrapy-2.7.1/scrapy/spidermiddlewares/depth.py` & `Scrapy-2.8.0/scrapy/spidermiddlewares/depth.py`

 * *Files 11% similar despite different names*

```diff
@@ -8,59 +8,56 @@
 
 from scrapy.http import Request
 
 logger = logging.getLogger(__name__)
 
 
 class DepthMiddleware:
-
     def __init__(self, maxdepth, stats, verbose_stats=False, prio=1):
         self.maxdepth = maxdepth
         self.stats = stats
         self.verbose_stats = verbose_stats
         self.prio = prio
 
     @classmethod
     def from_crawler(cls, crawler):
         settings = crawler.settings
-        maxdepth = settings.getint('DEPTH_LIMIT')
-        verbose = settings.getbool('DEPTH_STATS_VERBOSE')
-        prio = settings.getint('DEPTH_PRIORITY')
+        maxdepth = settings.getint("DEPTH_LIMIT")
+        verbose = settings.getbool("DEPTH_STATS_VERBOSE")
+        prio = settings.getint("DEPTH_PRIORITY")
         return cls(maxdepth, crawler.stats, verbose, prio)
 
     def process_spider_output(self, response, result, spider):
         self._init_depth(response, spider)
         return (r for r in result or () if self._filter(r, response, spider))
 
     async def process_spider_output_async(self, response, result, spider):
         self._init_depth(response, spider)
         async for r in result or ():
             if self._filter(r, response, spider):
                 yield r
 
     def _init_depth(self, response, spider):
         # base case (depth=0)
-        if 'depth' not in response.meta:
-            response.meta['depth'] = 0
+        if "depth" not in response.meta:
+            response.meta["depth"] = 0
             if self.verbose_stats:
-                self.stats.inc_value('request_depth_count/0', spider=spider)
+                self.stats.inc_value("request_depth_count/0", spider=spider)
 
     def _filter(self, request, response, spider):
         if not isinstance(request, Request):
             return True
-        depth = response.meta['depth'] + 1
-        request.meta['depth'] = depth
+        depth = response.meta["depth"] + 1
+        request.meta["depth"] = depth
         if self.prio:
             request.priority -= depth * self.prio
         if self.maxdepth and depth > self.maxdepth:
             logger.debug(
                 "Ignoring link (depth > %(maxdepth)d): %(requrl)s ",
-                {'maxdepth': self.maxdepth, 'requrl': request.url},
-                extra={'spider': spider}
+                {"maxdepth": self.maxdepth, "requrl": request.url},
+                extra={"spider": spider},
             )
             return False
         if self.verbose_stats:
-            self.stats.inc_value(f'request_depth_count/{depth}',
-                                 spider=spider)
-        self.stats.max_value('request_depth_max', depth,
-                             spider=spider)
+            self.stats.inc_value(f"request_depth_count/{depth}", spider=spider)
+        self.stats.max_value("request_depth_max", depth, spider=spider)
         return True
```

### Comparing `Scrapy-2.7.1/scrapy/spidermiddlewares/httperror.py` & `Scrapy-2.8.0/scrapy/spidermiddlewares/httperror.py`

 * *Files 5% similar despite different names*

```diff
@@ -15,43 +15,45 @@
 
     def __init__(self, response, *args, **kwargs):
         self.response = response
         super().__init__(*args, **kwargs)
 
 
 class HttpErrorMiddleware:
-
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler.settings)
 
     def __init__(self, settings):
-        self.handle_httpstatus_all = settings.getbool('HTTPERROR_ALLOW_ALL')
-        self.handle_httpstatus_list = settings.getlist('HTTPERROR_ALLOWED_CODES')
+        self.handle_httpstatus_all = settings.getbool("HTTPERROR_ALLOW_ALL")
+        self.handle_httpstatus_list = settings.getlist("HTTPERROR_ALLOWED_CODES")
 
     def process_spider_input(self, response, spider):
         if 200 <= response.status < 300:  # common case
             return
         meta = response.meta
-        if meta.get('handle_httpstatus_all', False):
+        if meta.get("handle_httpstatus_all", False):
             return
-        if 'handle_httpstatus_list' in meta:
-            allowed_statuses = meta['handle_httpstatus_list']
+        if "handle_httpstatus_list" in meta:
+            allowed_statuses = meta["handle_httpstatus_list"]
         elif self.handle_httpstatus_all:
             return
         else:
-            allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)
+            allowed_statuses = getattr(
+                spider, "handle_httpstatus_list", self.handle_httpstatus_list
+            )
         if response.status in allowed_statuses:
             return
-        raise HttpError(response, 'Ignoring non-200 response')
+        raise HttpError(response, "Ignoring non-200 response")
 
     def process_spider_exception(self, response, exception, spider):
         if isinstance(exception, HttpError):
-            spider.crawler.stats.inc_value('httperror/response_ignored_count')
+            spider.crawler.stats.inc_value("httperror/response_ignored_count")
             spider.crawler.stats.inc_value(
-                f'httperror/response_ignored_status_count/{response.status}'
+                f"httperror/response_ignored_status_count/{response.status}"
             )
             logger.info(
                 "Ignoring response %(response)r: HTTP status code is not handled or not allowed",
-                {'response': response}, extra={'spider': spider},
+                {"response": response},
+                extra={"spider": spider},
             )
             return []
```

### Comparing `Scrapy-2.7.1/scrapy/spidermiddlewares/offsite.py` & `Scrapy-2.8.0/scrapy/spidermiddlewares/offsite.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,24 @@
 """
 Offsite Spider Middleware
 
 See documentation in docs/topics/spider-middleware.rst
 """
-import re
 import logging
+import re
 import warnings
 
 from scrapy import signals
 from scrapy.http import Request
 from scrapy.utils.httpobj import urlparse_cached
 
 logger = logging.getLogger(__name__)
 
 
 class OffsiteMiddleware:
-
     def __init__(self, stats):
         self.stats = stats
 
     @classmethod
     def from_crawler(cls, crawler):
         o = cls(crawler.stats)
         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
@@ -39,47 +38,53 @@
         if request.dont_filter or self.should_follow(request, spider):
             return True
         domain = urlparse_cached(request).hostname
         if domain and domain not in self.domains_seen:
             self.domains_seen.add(domain)
             logger.debug(
                 "Filtered offsite request to %(domain)r: %(request)s",
-                {'domain': domain, 'request': request}, extra={'spider': spider})
-            self.stats.inc_value('offsite/domains', spider=spider)
-        self.stats.inc_value('offsite/filtered', spider=spider)
+                {"domain": domain, "request": request},
+                extra={"spider": spider},
+            )
+            self.stats.inc_value("offsite/domains", spider=spider)
+        self.stats.inc_value("offsite/filtered", spider=spider)
         return False
 
     def should_follow(self, request, spider):
         regex = self.host_regex
         # hostname can be None for wrong urls (like javascript links)
-        host = urlparse_cached(request).hostname or ''
+        host = urlparse_cached(request).hostname or ""
         return bool(regex.search(host))
 
     def get_host_regex(self, spider):
         """Override this method to implement a different offsite policy"""
-        allowed_domains = getattr(spider, 'allowed_domains', None)
+        allowed_domains = getattr(spider, "allowed_domains", None)
         if not allowed_domains:
-            return re.compile('')  # allow all by default
+            return re.compile("")  # allow all by default
         url_pattern = re.compile(r"^https?://.*$")
         port_pattern = re.compile(r":\d+$")
         domains = []
         for domain in allowed_domains:
             if domain is None:
                 continue
-            elif url_pattern.match(domain):
-                message = ("allowed_domains accepts only domains, not URLs. "
-                           f"Ignoring URL entry {domain} in allowed_domains.")
+            if url_pattern.match(domain):
+                message = (
+                    "allowed_domains accepts only domains, not URLs. "
+                    f"Ignoring URL entry {domain} in allowed_domains."
+                )
                 warnings.warn(message, URLWarning)
             elif port_pattern.search(domain):
-                message = ("allowed_domains accepts only domains without ports. "
-                           f"Ignoring entry {domain} in allowed_domains.")
+                message = (
+                    "allowed_domains accepts only domains without ports. "
+                    f"Ignoring entry {domain} in allowed_domains."
+                )
                 warnings.warn(message, PortWarning)
             else:
                 domains.append(re.escape(domain))
-        regex = fr'^(.*\.)?({"|".join(domains)})$'
+        regex = rf'^(.*\.)?({"|".join(domains)})$'
         return re.compile(regex)
 
     def spider_opened(self, spider):
         self.host_regex = self.get_host_regex(spider)
         self.domains_seen = set()
```

### Comparing `Scrapy-2.7.1/scrapy/spidermiddlewares/referer.py` & `Scrapy-2.8.0/scrapy/spidermiddlewares/referer.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,16 +11,20 @@
 from scrapy import signals
 from scrapy.exceptions import NotConfigured
 from scrapy.http import Request, Response
 from scrapy.utils.misc import load_object
 from scrapy.utils.python import to_unicode
 from scrapy.utils.url import strip_url
 
-
-LOCAL_SCHEMES = ('about', 'blob', 'data', 'filesystem',)
+LOCAL_SCHEMES = (
+    "about",
+    "blob",
+    "data",
+    "filesystem",
+)
 
 POLICY_NO_REFERRER = "no-referrer"
 POLICY_NO_REFERRER_WHEN_DOWNGRADE = "no-referrer-when-downgrade"
 POLICY_SAME_ORIGIN = "same-origin"
 POLICY_ORIGIN = "origin"
 POLICY_STRICT_ORIGIN = "strict-origin"
 POLICY_ORIGIN_WHEN_CROSS_ORIGIN = "origin-when-cross-origin"
@@ -57,43 +61,46 @@
         If the origin-only flag is true, then:
             Set url's path to null.
             Set url's query to null.
         Return url.
         """
         if not url:
             return None
-        return strip_url(url,
-                         strip_credentials=True,
-                         strip_fragment=True,
-                         strip_default_port=True,
-                         origin_only=origin_only)
+        return strip_url(
+            url,
+            strip_credentials=True,
+            strip_fragment=True,
+            strip_default_port=True,
+            origin_only=origin_only,
+        )
 
     def origin(self, url):
         """Return serialized origin (scheme, host, path) for a request or response URL."""
         return self.strip_url(url, origin_only=True)
 
     def potentially_trustworthy(self, url):
         # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy
         parsed_url = urlparse(url)
-        if parsed_url.scheme in ('data',):
+        if parsed_url.scheme in ("data",):
             return False
         return self.tls_protected(url)
 
     def tls_protected(self, url):
-        return urlparse(url).scheme in ('https', 'ftps')
+        return urlparse(url).scheme in ("https", "ftps")
 
 
 class NoReferrerPolicy(ReferrerPolicy):
     """
     https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer
 
     The simplest policy is "no-referrer", which specifies that no referrer information
     is to be sent along with requests made from a particular request client to any origin.
     The header will be omitted entirely.
     """
+
     name: str = POLICY_NO_REFERRER
 
     def referrer(self, response_url, request_url):
         return None
 
 
 class NoReferrerWhenDowngradePolicy(ReferrerPolicy):
@@ -106,14 +113,15 @@
 
     Requests from TLS-protected clients to non-potentially trustworthy URLs,
     on the other hand, will contain no referrer information.
     A Referer HTTP header will not be sent.
 
     This is a user agent's default behavior, if no policy is otherwise specified.
     """
+
     name: str = POLICY_NO_REFERRER_WHEN_DOWNGRADE
 
     def referrer(self, response_url, request_url):
         if not self.tls_protected(response_url) or self.tls_protected(request_url):
             return self.stripped_referrer(response_url)
 
 
@@ -123,14 +131,15 @@
 
     The "same-origin" policy specifies that a full URL, stripped for use as a referrer,
     is sent as referrer information when making same-origin requests from a particular request client.
 
     Cross-origin requests, on the other hand, will contain no referrer information.
     A Referer HTTP header will not be sent.
     """
+
     name: str = POLICY_SAME_ORIGIN
 
     def referrer(self, response_url, request_url):
         if self.origin(response_url) == self.origin(request_url):
             return self.stripped_referrer(response_url)
 
 
@@ -139,14 +148,15 @@
     https://www.w3.org/TR/referrer-policy/#referrer-policy-origin
 
     The "origin" policy specifies that only the ASCII serialization
     of the origin of the request client is sent as referrer information
     when making both same-origin requests and cross-origin requests
     from a particular request client.
     """
+
     name: str = POLICY_ORIGIN
 
     def referrer(self, response_url, request_url):
         return self.origin_referrer(response_url)
 
 
 class StrictOriginPolicy(ReferrerPolicy):
@@ -158,19 +168,21 @@
     - from a TLS-protected environment settings object to a potentially trustworthy URL, and
     - from non-TLS-protected environment settings objects to any origin.
 
     Requests from TLS-protected request clients to non- potentially trustworthy URLs,
     on the other hand, will contain no referrer information.
     A Referer HTTP header will not be sent.
     """
+
     name: str = POLICY_STRICT_ORIGIN
 
     def referrer(self, response_url, request_url):
         if (
-            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)
+            self.tls_protected(response_url)
+            and self.potentially_trustworthy(request_url)
             or not self.tls_protected(response_url)
         ):
             return self.origin_referrer(response_url)
 
 
 class OriginWhenCrossOriginPolicy(ReferrerPolicy):
     """
@@ -179,22 +191,22 @@
     The "origin-when-cross-origin" policy specifies that a full URL,
     stripped for use as a referrer, is sent as referrer information
     when making same-origin requests from a particular request client,
     and only the ASCII serialization of the origin of the request client
     is sent as referrer information when making cross-origin requests
     from a particular request client.
     """
+
     name: str = POLICY_ORIGIN_WHEN_CROSS_ORIGIN
 
     def referrer(self, response_url, request_url):
         origin = self.origin(response_url)
         if origin == self.origin(request_url):
             return self.stripped_referrer(response_url)
-        else:
-            return origin
+        return origin
 
 
 class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):
     """
     https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin
 
     The "strict-origin-when-cross-origin" policy specifies that a full URL,
@@ -206,22 +218,24 @@
     - from a TLS-protected environment settings object to a potentially trustworthy URL, and
     - from non-TLS-protected environment settings objects to any origin.
 
     Requests from TLS-protected clients to non- potentially trustworthy URLs,
     on the other hand, will contain no referrer information.
     A Referer HTTP header will not be sent.
     """
+
     name: str = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN
 
     def referrer(self, response_url, request_url):
         origin = self.origin(response_url)
         if origin == self.origin(request_url):
             return self.stripped_referrer(response_url)
-        elif (
-            self.tls_protected(response_url) and self.potentially_trustworthy(request_url)
+        if (
+            self.tls_protected(response_url)
+            and self.potentially_trustworthy(request_url)
             or not self.tls_protected(response_url)
         ):
             return self.origin_referrer(response_url)
 
 
 class UnsafeUrlPolicy(ReferrerPolicy):
     """
@@ -232,44 +246,49 @@
     and same-origin requests made from a particular request client.
 
     Note: The policy's name doesn't lie; it is unsafe.
     This policy will leak origins and paths from TLS-protected resources
     to insecure origins.
     Carefully consider the impact of setting such a policy for potentially sensitive documents.
     """
+
     name: str = POLICY_UNSAFE_URL
 
     def referrer(self, response_url, request_url):
         return self.stripped_referrer(response_url)
 
 
 class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):
     """
     A variant of "no-referrer-when-downgrade",
     with the addition that "Referer" is not sent if the parent request was
     using ``file://`` or ``s3://`` scheme.
     """
-    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + ('file', 's3')
+
+    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + ("file", "s3")
     name: str = POLICY_SCRAPY_DEFAULT
 
 
-_policy_classes = {p.name: p for p in (
-    NoReferrerPolicy,
-    NoReferrerWhenDowngradePolicy,
-    SameOriginPolicy,
-    OriginPolicy,
-    StrictOriginPolicy,
-    OriginWhenCrossOriginPolicy,
-    StrictOriginWhenCrossOriginPolicy,
-    UnsafeUrlPolicy,
-    DefaultReferrerPolicy,
-)}
+_policy_classes = {
+    p.name: p
+    for p in (
+        NoReferrerPolicy,
+        NoReferrerWhenDowngradePolicy,
+        SameOriginPolicy,
+        OriginPolicy,
+        StrictOriginPolicy,
+        OriginWhenCrossOriginPolicy,
+        StrictOriginWhenCrossOriginPolicy,
+        UnsafeUrlPolicy,
+        DefaultReferrerPolicy,
+    )
+}
 
 # Reference: https://www.w3.org/TR/referrer-policy/#referrer-policy-empty-string
-_policy_classes[''] = NoReferrerWhenDowngradePolicy
+_policy_classes[""] = NoReferrerWhenDowngradePolicy
 
 
 def _load_policy_class(policy, warning_only=False):
     """
     Expect a string for the path to the policy class,
     otherwise try to interpret the string as a standard value
     from https://www.w3.org/TR/referrer-policy/#referrer-policies
@@ -285,24 +304,22 @@
                 raise RuntimeError(msg)
             else:
                 warnings.warn(msg, RuntimeWarning)
                 return None
 
 
 class RefererMiddleware:
-
     def __init__(self, settings=None):
         self.default_policy = DefaultReferrerPolicy
         if settings is not None:
-            self.default_policy = _load_policy_class(
-                settings.get('REFERRER_POLICY'))
+            self.default_policy = _load_policy_class(settings.get("REFERRER_POLICY"))
 
     @classmethod
     def from_crawler(cls, crawler):
-        if not crawler.settings.getbool('REFERER_ENABLED'):
+        if not crawler.settings.getbool("REFERER_ENABLED"):
             raise NotConfigured
         mw = cls(crawler.settings)
 
         # Note: this hook is a bit of a hack to intercept redirections
         crawler.signals.connect(mw.request_scheduled, signal=signals.request_scheduled)
 
         return mw
@@ -316,20 +333,20 @@
         - if the policy is set in meta but is wrong (e.g. a typo error),
           the policy from settings is used
         - if the policy is not set in Request meta,
           but there is a Referrer-policy header in the parent response,
           it is used if valid
         - otherwise, the policy from settings is used.
         """
-        policy_name = request.meta.get('referrer_policy')
+        policy_name = request.meta.get("referrer_policy")
         if policy_name is None:
             if isinstance(resp_or_url, Response):
-                policy_header = resp_or_url.headers.get('Referrer-Policy')
+                policy_header = resp_or_url.headers.get("Referrer-Policy")
                 if policy_header is not None:
-                    policy_name = to_unicode(policy_header.decode('latin1'))
+                    policy_name = to_unicode(policy_header.decode("latin1"))
         if policy_name is None:
             return self.default_policy()
 
         cls = _load_policy_class(policy_name, warning_only=True)
         return cls() if cls else self.default_policy()
 
     def process_spider_output(self, response, result, spider):
@@ -339,30 +356,31 @@
         async for r in result or ():
             yield self._set_referer(r, response)
 
     def _set_referer(self, r, response):
         if isinstance(r, Request):
             referrer = self.policy(response, r).referrer(response.url, r.url)
             if referrer is not None:
-                r.headers.setdefault('Referer', referrer)
+                r.headers.setdefault("Referer", referrer)
         return r
 
     def request_scheduled(self, request, spider):
         # check redirected request to patch "Referer" header if necessary
-        redirected_urls = request.meta.get('redirect_urls', [])
+        redirected_urls = request.meta.get("redirect_urls", [])
         if redirected_urls:
-            request_referrer = request.headers.get('Referer')
+            request_referrer = request.headers.get("Referer")
             # we don't patch the referrer value if there is none
             if request_referrer is not None:
                 # the request's referrer header value acts as a surrogate
                 # for the parent response URL
                 #
                 # Note: if the 3xx response contained a Referrer-Policy header,
                 #       the information is not available using this hook
                 parent_url = safe_url_string(request_referrer)
                 policy_referrer = self.policy(parent_url, request).referrer(
-                    parent_url, request.url)
+                    parent_url, request.url
+                )
                 if policy_referrer != request_referrer:
                     if policy_referrer is None:
-                        request.headers.pop('Referer')
+                        request.headers.pop("Referer")
                     else:
-                        request.headers['Referer'] = policy_referrer
+                        request.headers["Referer"] = policy_referrer
```

### Comparing `Scrapy-2.7.1/scrapy/spidermiddlewares/urllength.py` & `Scrapy-2.8.0/scrapy/spidermiddlewares/urllength.py`

 * *Files 11% similar despite different names*

```diff
@@ -2,28 +2,27 @@
 Url Length Spider Middleware
 
 See documentation in docs/topics/spider-middleware.rst
 """
 
 import logging
 
-from scrapy.http import Request
 from scrapy.exceptions import NotConfigured
+from scrapy.http import Request
 
 logger = logging.getLogger(__name__)
 
 
 class UrlLengthMiddleware:
-
     def __init__(self, maxlength):
         self.maxlength = maxlength
 
     @classmethod
     def from_settings(cls, settings):
-        maxlength = settings.getint('URLLENGTH_LIMIT')
+        maxlength = settings.getint("URLLENGTH_LIMIT")
         if not maxlength:
             raise NotConfigured
         return cls(maxlength)
 
     def process_spider_output(self, response, result, spider):
         return (r for r in result or () if self._filter(r, spider))
 
@@ -32,13 +31,15 @@
             if self._filter(r, spider):
                 yield r
 
     def _filter(self, request, spider):
         if isinstance(request, Request) and len(request.url) > self.maxlength:
             logger.info(
                 "Ignoring link (url length > %(maxlength)d): %(url)s ",
-                {'maxlength': self.maxlength, 'url': request.url},
-                extra={'spider': spider}
+                {"maxlength": self.maxlength, "url": request.url},
+                extra={"spider": spider},
+            )
+            spider.crawler.stats.inc_value(
+                "urllength/request_ignored_count", spider=spider
             )
-            spider.crawler.stats.inc_value('urllength/request_ignored_count', spider=spider)
             return False
         return True
```

### Comparing `Scrapy-2.7.1/scrapy/spiders/__init__.py` & `Scrapy-2.8.0/scrapy/spiders/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,42 +1,47 @@
 """
 Base class for Scrapy spiders
 
 See documentation in docs/topics/spiders.rst
 """
+from __future__ import annotations
+
 import logging
-from typing import Optional
+from typing import TYPE_CHECKING, Optional
 
 from scrapy import signals
 from scrapy.http import Request
 from scrapy.utils.trackref import object_ref
 from scrapy.utils.url import url_is_from_spider
 
+if TYPE_CHECKING:
+    from scrapy.crawler import Crawler
+
 
 class Spider(object_ref):
     """Base class for scrapy spiders. All spiders must inherit from this
     class.
     """
 
-    name: Optional[str] = None
+    name: str
     custom_settings: Optional[dict] = None
 
     def __init__(self, name=None, **kwargs):
         if name is not None:
             self.name = name
-        elif not getattr(self, 'name', None):
+        elif not getattr(self, "name", None):
             raise ValueError(f"{type(self).__name__} must have a name")
         self.__dict__.update(kwargs)
-        if not hasattr(self, 'start_urls'):
+        if not hasattr(self, "start_urls"):
             self.start_urls = []
 
     @property
     def logger(self):
         logger = logging.getLogger(self.name)
-        return logging.LoggerAdapter(logger, {'spider': self})
+        return logging.LoggerAdapter(logger, {"spider": self})
 
     def log(self, message, level=logging.DEBUG, **kw):
         """Log the given message at the given log level
 
         This helper wraps a log call to the logger within the spider, but you
         can use it directly (e.g. Spider.logger.info('msg')) or use any other
         Python logger too.
@@ -45,51 +50,52 @@
 
     @classmethod
     def from_crawler(cls, crawler, *args, **kwargs):
         spider = cls(*args, **kwargs)
         spider._set_crawler(crawler)
         return spider
 
-    def _set_crawler(self, crawler):
+    def _set_crawler(self, crawler: Crawler):
         self.crawler = crawler
         self.settings = crawler.settings
         crawler.signals.connect(self.close, signals.spider_closed)
 
     def start_requests(self):
-        if not self.start_urls and hasattr(self, 'start_url'):
+        if not self.start_urls and hasattr(self, "start_url"):
             raise AttributeError(
                 "Crawling could not start: 'start_urls' not found "
                 "or empty (but found 'start_url' attribute instead, "
-                "did you miss an 's'?)")
+                "did you miss an 's'?)"
+            )
         for url in self.start_urls:
             yield Request(url, dont_filter=True)
 
     def _parse(self, response, **kwargs):
         return self.parse(response, **kwargs)
 
     def parse(self, response, **kwargs):
-        raise NotImplementedError(f'{self.__class__.__name__}.parse callback is not defined')
+        raise NotImplementedError(
+            f"{self.__class__.__name__}.parse callback is not defined"
+        )
 
     @classmethod
     def update_settings(cls, settings):
-        settings.setdict(cls.custom_settings or {}, priority='spider')
+        settings.setdict(cls.custom_settings or {}, priority="spider")
 
     @classmethod
     def handles_request(cls, request):
         return url_is_from_spider(request.url, cls)
 
     @staticmethod
     def close(spider, reason):
-        closed = getattr(spider, 'closed', None)
+        closed = getattr(spider, "closed", None)
         if callable(closed):
             return closed(reason)
 
-    def __str__(self):
+    def __repr__(self):
         return f"<{type(self).__name__} {self.name!r} at 0x{id(self):0x}>"
 
-    __repr__ = __str__
-
 
 # Top-level imports
 from scrapy.spiders.crawl import CrawlSpider, Rule
-from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider
+from scrapy.spiders.feed import CSVFeedSpider, XMLFeedSpider
 from scrapy.spiders.sitemap import SitemapSpider
```

### Comparing `Scrapy-2.7.1/scrapy/spiders/crawl.py` & `Scrapy-2.8.0/scrapy/spiders/crawl.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 
 See documentation in docs/topics/spiders.rst
 """
 
 import copy
 from typing import AsyncIterable, Awaitable, Sequence
 
-from scrapy.http import Request, Response, HtmlResponse
+from scrapy.http import HtmlResponse, Request, Response
 from scrapy.linkextractors import LinkExtractor
 from scrapy.spiders import Spider
 from scrapy.utils.asyncgen import collect_asyncgen
 from scrapy.utils.spider import iterate_spider_output
 
 
 def _identity(x):
@@ -22,23 +22,22 @@
 def _identity_process_request(request, response):
     return request
 
 
 def _get_method(method, spider):
     if callable(method):
         return method
-    elif isinstance(method, str):
+    if isinstance(method, str):
         return getattr(spider, method, None)
 
 
 _default_link_extractor = LinkExtractor()
 
 
 class Rule:
-
     def __init__(
         self,
         link_extractor=None,
         callback=None,
         cb_kwargs=None,
         follow=None,
         process_links=None,
@@ -91,27 +90,32 @@
         )
 
     def _requests_to_follow(self, response):
         if not isinstance(response, HtmlResponse):
             return
         seen = set()
         for rule_index, rule in enumerate(self._rules):
-            links = [lnk for lnk in rule.link_extractor.extract_links(response)
-                     if lnk not in seen]
+            links = [
+                lnk
+                for lnk in rule.link_extractor.extract_links(response)
+                if lnk not in seen
+            ]
             for link in rule.process_links(links):
                 seen.add(link)
                 request = self._build_request(rule_index, link)
                 yield rule.process_request(request, response)
 
-    def _callback(self, response):
-        rule = self._rules[response.meta['rule']]
-        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)
+    def _callback(self, response, **cb_kwargs):
+        rule = self._rules[response.meta["rule"]]
+        return self._parse_response(
+            response, rule.callback, {**rule.cb_kwargs, **cb_kwargs}, rule.follow
+        )
 
     def _errback(self, failure):
-        rule = self._rules[failure.request.meta['rule']]
+        rule = self._rules[failure.request.meta["rule"]]
         return self._handle_failure(failure, rule.errback)
 
     async def _parse_response(self, response, callback, cb_kwargs, follow=True):
         if callback:
             cb_res = callback(response, **cb_kwargs) or ()
             if isinstance(cb_res, AsyncIterable):
                 cb_res = await collect_asyncgen(cb_res)
@@ -136,9 +140,11 @@
         for rule in self.rules:
             self._rules.append(copy.copy(rule))
             self._rules[-1]._compile(self)
 
     @classmethod
     def from_crawler(cls, crawler, *args, **kwargs):
         spider = super().from_crawler(crawler, *args, **kwargs)
-        spider._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)
+        spider._follow_links = crawler.settings.getbool(
+            "CRAWLSPIDER_FOLLOW_LINKS", True
+        )
         return spider
```

### Comparing `Scrapy-2.7.1/scrapy/spiders/feed.py` & `Scrapy-2.8.0/scrapy/spiders/feed.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 """
 This module implements the XMLFeedSpider which is the recommended spider to use
 for scraping from an XML feed.
 
 See documentation in docs/topics/spiders.rst
 """
+from scrapy.exceptions import NotConfigured, NotSupported
+from scrapy.selector import Selector
 from scrapy.spiders import Spider
-from scrapy.utils.iterators import xmliter, csviter
+from scrapy.utils.iterators import csviter, xmliter
 from scrapy.utils.spider import iterate_spider_output
-from scrapy.selector import Selector
-from scrapy.exceptions import NotConfigured, NotSupported
 
 
 class XMLFeedSpider(Spider):
     """
     This class intends to be the base class for spiders that scrape
     from XML feeds.
 
     You can choose whether to parse the file using the 'iternodes' iterator, an
     'xml' selector, or an 'html' selector.  In most cases, it's convenient to
     use iternodes, since it's a faster and cleaner.
     """
 
-    iterator = 'iternodes'
-    itertag = 'item'
+    iterator = "iternodes"
+    itertag = "item"
     namespaces = ()
 
     def process_results(self, response, results):
         """This overridable method is called for each result (item or request)
         returned by the spider, and it's intended to perform any last time
         processing required before returning the results to the framework core,
         for example setting the item GUIDs. It receives a list of results and
@@ -40,15 +40,15 @@
         to into the feed before parsing it. This function must return a
         response.
         """
         return response
 
     def parse_node(self, response, selector):
         """This method must be overridden with your custom spider functionality"""
-        if hasattr(self, 'parse_item'):  # backward compatibility
+        if hasattr(self, "parse_item"):  # backward compatibility
             return self.parse_item(response, selector)
         raise NotImplementedError
 
     def parse_nodes(self, response, nodes):
         """This method is called for the nodes matching the provided tag name
         (itertag). Receives the response and an Selector for each node.
         Overriding this method is mandatory. Otherwise, you spider won't work.
@@ -58,30 +58,32 @@
 
         for selector in nodes:
             ret = iterate_spider_output(self.parse_node(response, selector))
             for result_item in self.process_results(response, ret):
                 yield result_item
 
     def _parse(self, response, **kwargs):
-        if not hasattr(self, 'parse_node'):
-            raise NotConfigured('You must define parse_node method in order to scrape this XML feed')
+        if not hasattr(self, "parse_node"):
+            raise NotConfigured(
+                "You must define parse_node method in order to scrape this XML feed"
+            )
 
         response = self.adapt_response(response)
-        if self.iterator == 'iternodes':
+        if self.iterator == "iternodes":
             nodes = self._iternodes(response)
-        elif self.iterator == 'xml':
-            selector = Selector(response, type='xml')
+        elif self.iterator == "xml":
+            selector = Selector(response, type="xml")
             self._register_namespaces(selector)
-            nodes = selector.xpath(f'//{self.itertag}')
-        elif self.iterator == 'html':
-            selector = Selector(response, type='html')
+            nodes = selector.xpath(f"//{self.itertag}")
+        elif self.iterator == "html":
+            selector = Selector(response, type="html")
             self._register_namespaces(selector)
-            nodes = selector.xpath(f'//{self.itertag}')
+            nodes = selector.xpath(f"//{self.itertag}")
         else:
-            raise NotSupported('Unsupported node iterator')
+            raise NotSupported("Unsupported node iterator")
 
         return self.parse_nodes(response, nodes)
 
     def _iternodes(self, response):
         for node in xmliter(response, self.itertag):
             self._register_namespaces(node)
             yield node
@@ -96,16 +98,20 @@
     It receives a CSV file in a response; iterates through each of its rows,
     and calls parse_row with a dict containing each field's data.
 
     You can set some options regarding the CSV file, such as the delimiter, quotechar
     and the file's headers.
     """
 
-    delimiter = None  # When this is None, python's csv module's default delimiter is used
-    quotechar = None  # When this is None, python's csv module's default quotechar is used
+    delimiter = (
+        None  # When this is None, python's csv module's default delimiter is used
+    )
+    quotechar = (
+        None  # When this is None, python's csv module's default quotechar is used
+    )
     headers = None
 
     def process_results(self, response, results):
         """This method has the same purpose as the one in XMLFeedSpider"""
         return results
 
     def adapt_response(self, response):
@@ -119,17 +125,21 @@
     def parse_rows(self, response):
         """Receives a response and a dict (representing each row) with a key for
         each provided (or detected) header of the CSV file.  This spider also
         gives the opportunity to override adapt_response and
         process_results methods for pre and post-processing purposes.
         """
 
-        for row in csviter(response, self.delimiter, self.headers, quotechar=self.quotechar):
+        for row in csviter(
+            response, self.delimiter, self.headers, quotechar=self.quotechar
+        ):
             ret = iterate_spider_output(self.parse_row(response, row))
             for result_item in self.process_results(response, ret):
                 yield result_item
 
     def _parse(self, response, **kwargs):
-        if not hasattr(self, 'parse_row'):
-            raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')
+        if not hasattr(self, "parse_row"):
+            raise NotConfigured(
+                "You must define parse_row method in order to scrape this CSV feed"
+            )
         response = self.adapt_response(response)
         return self.parse_rows(response)
```

### Comparing `Scrapy-2.7.1/scrapy/spiders/init.py` & `Scrapy-2.8.0/scrapy/spiders/init.py`

 * *Files 0% similar despite different names*

```diff
@@ -9,15 +9,15 @@
         self._postinit_reqs = super().start_requests()
         return iterate_spider_output(self.init_request())
 
     def initialized(self, response=None):
         """This method must be set as the callback of your last initialization
         request. See self.init_request() docstring for more info.
         """
-        return self.__dict__.pop('_postinit_reqs')
+        return self.__dict__.pop("_postinit_reqs")
 
     def init_request(self):
         """This function should return one initialization request, with the
         self.initialized method as callback. When the self.initialized method
         is called this spider is considered initialized. If you need to perform
         several requests for initializing your spider, you can do so by using
         different callbacks. The only requirement is that the final callback
```

### Comparing `Scrapy-2.7.1/scrapy/spiders/sitemap.py` & `Scrapy-2.8.0/scrapy/spiders/sitemap.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,24 +1,23 @@
-import re
 import logging
+import re
 
-from scrapy.spiders import Spider
 from scrapy.http import Request, XmlResponse
-from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots
+from scrapy.spiders import Spider
 from scrapy.utils.gz import gunzip, gzip_magic_number
-
+from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots
 
 logger = logging.getLogger(__name__)
 
 
 class SitemapSpider(Spider):
 
     sitemap_urls = ()
-    sitemap_rules = [('', 'parse')]
-    sitemap_follow = ['']
+    sitemap_rules = [("", "parse")]
+    sitemap_follow = [""]
     sitemap_alternate_links = False
 
     def __init__(self, *a, **kw):
         super().__init__(*a, **kw)
         self._cbs = []
         for r, c in self.sitemap_rules:
             if isinstance(c, str):
@@ -35,65 +34,68 @@
         attributes, for example, you can filter locs with lastmod greater
         than a given date (see docs).
         """
         for entry in entries:
             yield entry
 
     def _parse_sitemap(self, response):
-        if response.url.endswith('/robots.txt'):
+        if response.url.endswith("/robots.txt"):
             for url in sitemap_urls_from_robots(response.text, base_url=response.url):
                 yield Request(url, callback=self._parse_sitemap)
         else:
             body = self._get_sitemap_body(response)
             if body is None:
-                logger.warning("Ignoring invalid sitemap: %(response)s",
-                               {'response': response}, extra={'spider': self})
+                logger.warning(
+                    "Ignoring invalid sitemap: %(response)s",
+                    {"response": response},
+                    extra={"spider": self},
+                )
                 return
 
             s = Sitemap(body)
             it = self.sitemap_filter(s)
 
-            if s.type == 'sitemapindex':
+            if s.type == "sitemapindex":
                 for loc in iterloc(it, self.sitemap_alternate_links):
                     if any(x.search(loc) for x in self._follow):
                         yield Request(loc, callback=self._parse_sitemap)
-            elif s.type == 'urlset':
+            elif s.type == "urlset":
                 for loc in iterloc(it, self.sitemap_alternate_links):
                     for r, c in self._cbs:
                         if r.search(loc):
                             yield Request(loc, callback=c)
                             break
 
     def _get_sitemap_body(self, response):
         """Return the sitemap body contained in the given response,
         or None if the response is not a sitemap.
         """
         if isinstance(response, XmlResponse):
             return response.body
-        elif gzip_magic_number(response):
+        if gzip_magic_number(response):
             return gunzip(response.body)
         # actual gzipped sitemap files are decompressed above ;
         # if we are here (response body is not gzipped)
         # and have a response for .xml.gz,
         # it usually means that it was already gunzipped
         # by HttpCompression middleware,
         # the HTTP response being sent with "Content-Encoding: gzip"
         # without actually being a .xml.gz file in the first place,
         # merely XML gzip-compressed on the fly,
         # in other word, here, we have plain XML
-        elif response.url.endswith('.xml') or response.url.endswith('.xml.gz'):
+        if response.url.endswith(".xml") or response.url.endswith(".xml.gz"):
             return response.body
 
 
 def regex(x):
     if isinstance(x, str):
         return re.compile(x)
     return x
 
 
 def iterloc(it, alt=False):
     for d in it:
-        yield d['loc']
+        yield d["loc"]
 
         # Also consider alternate URLs (xhtml:link rel="alternate")
-        if alt and 'alternate' in d:
-            yield from d['alternate']
+        if alt and "alternate" in d:
+            yield from d["alternate"]
```

### Comparing `Scrapy-2.7.1/scrapy/squeues.py` & `Scrapy-2.8.0/scrapy/squeues.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,38 +1,36 @@
 """
 Scheduler queues
 """
 
 import marshal
-import os
 import pickle
+from os import PathLike
+from pathlib import Path
+from typing import Union
 
 from queuelib import queue
 
 from scrapy.utils.deprecate import create_deprecated_class
 from scrapy.utils.request import request_from_dict
 
 
 def _with_mkdir(queue_class):
-
     class DirectoriesCreated(queue_class):
-
-        def __init__(self, path, *args, **kwargs):
-            dirname = os.path.dirname(path)
-            if not os.path.exists(dirname):
-                os.makedirs(dirname, exist_ok=True)
+        def __init__(self, path: Union[str, PathLike], *args, **kwargs):
+            dirname = Path(path).parent
+            if not dirname.exists():
+                dirname.mkdir(parents=True, exist_ok=True)
             super().__init__(path, *args, **kwargs)
 
     return DirectoriesCreated
 
 
 def _serializable_queue(queue_class, serialize, deserialize):
-
     class SerializableQueue(queue_class):
-
         def push(self, obj):
             s = serialize(obj)
             super().push(s)
 
         def pop(self):
             s = super().pop()
             if s:
@@ -44,25 +42,25 @@
 
             Raises :exc:`NotImplementedError` if the underlying queue class does
             not implement a ``peek`` method, which is optional for queues.
             """
             try:
                 s = super().peek()
             except AttributeError as ex:
-                raise NotImplementedError("The underlying queue class does not implement 'peek'") from ex
+                raise NotImplementedError(
+                    "The underlying queue class does not implement 'peek'"
+                ) from ex
             if s:
                 return deserialize(s)
 
     return SerializableQueue
 
 
 def _scrapy_serialization_queue(queue_class):
-
     class ScrapyRequestQueue(queue_class):
-
         def __init__(self, crawler, key):
             self.spider = crawler.spider
             super().__init__(key)
 
         @classmethod
         def from_crawler(cls, crawler, key, *args, **kwargs):
             return cls(crawler, key)
@@ -89,15 +87,14 @@
                 return None
             return request_from_dict(request, spider=self.spider)
 
     return ScrapyRequestQueue
 
 
 def _scrapy_non_serialization_queue(queue_class):
-
     class ScrapyRequestQueue(queue_class):
         @classmethod
         def from_crawler(cls, crawler, *args, **kwargs):
             return cls()
 
         def peek(self):
             """Returns the next object to be returned by :meth:`pop`,
@@ -105,15 +102,17 @@
 
             Raises :exc:`NotImplementedError` if the underlying queue class does
             not implement a ``peek`` method, which is optional for queues.
             """
             try:
                 s = super().peek()
             except AttributeError as ex:
-                raise NotImplementedError("The underlying queue class does not implement 'peek'") from ex
+                raise NotImplementedError(
+                    "The underlying queue class does not implement 'peek'"
+                ) from ex
             return s
 
     return ScrapyRequestQueue
 
 
 def _pickle_serialize(obj):
     try:
@@ -121,32 +120,24 @@
     # Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s)
     # TypeError is raised from parsel.Selector
     except (pickle.PicklingError, AttributeError, TypeError) as e:
         raise ValueError(str(e)) from e
 
 
 _PickleFifoSerializationDiskQueue = _serializable_queue(
-    _with_mkdir(queue.FifoDiskQueue),
-    _pickle_serialize,
-    pickle.loads
+    _with_mkdir(queue.FifoDiskQueue), _pickle_serialize, pickle.loads
 )
 _PickleLifoSerializationDiskQueue = _serializable_queue(
-    _with_mkdir(queue.LifoDiskQueue),
-    _pickle_serialize,
-    pickle.loads
+    _with_mkdir(queue.LifoDiskQueue), _pickle_serialize, pickle.loads
 )
 _MarshalFifoSerializationDiskQueue = _serializable_queue(
-    _with_mkdir(queue.FifoDiskQueue),
-    marshal.dumps,
-    marshal.loads
+    _with_mkdir(queue.FifoDiskQueue), marshal.dumps, marshal.loads
 )
 _MarshalLifoSerializationDiskQueue = _serializable_queue(
-    _with_mkdir(queue.LifoDiskQueue),
-    marshal.dumps,
-    marshal.loads
+    _with_mkdir(queue.LifoDiskQueue), marshal.dumps, marshal.loads
 )
 
 # public queue classes
 PickleFifoDiskQueue = _scrapy_serialization_queue(_PickleFifoSerializationDiskQueue)
 PickleLifoDiskQueue = _scrapy_serialization_queue(_PickleLifoSerializationDiskQueue)
 MarshalFifoDiskQueue = _scrapy_serialization_queue(_MarshalFifoSerializationDiskQueue)
 MarshalLifoDiskQueue = _scrapy_serialization_queue(_MarshalLifoSerializationDiskQueue)
```

### Comparing `Scrapy-2.7.1/scrapy/statscollectors.py` & `Scrapy-2.8.0/scrapy/statscollectors.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,19 @@
 """
 Scrapy extension for collecting scraping stats
 """
-import pprint
 import logging
+import pprint
 
 logger = logging.getLogger(__name__)
 
 
 class StatsCollector:
-
     def __init__(self, crawler):
-        self._dump = crawler.settings.getbool('STATS_DUMP')
+        self._dump = crawler.settings.getbool("STATS_DUMP")
         self._stats = {}
 
     def get_value(self, key, default=None, spider=None):
         return self._stats.get(key, default)
 
     def get_stats(self, spider=None):
         return self._stats
@@ -39,34 +38,34 @@
         self._stats.clear()
 
     def open_spider(self, spider):
         pass
 
     def close_spider(self, spider, reason):
         if self._dump:
-            logger.info("Dumping Scrapy stats:\n" + pprint.pformat(self._stats),
-                        extra={'spider': spider})
+            logger.info(
+                "Dumping Scrapy stats:\n" + pprint.pformat(self._stats),
+                extra={"spider": spider},
+            )
         self._persist_stats(self._stats, spider)
 
     def _persist_stats(self, stats, spider):
         pass
 
 
 class MemoryStatsCollector(StatsCollector):
-
     def __init__(self, crawler):
         super().__init__(crawler)
         self.spider_stats = {}
 
     def _persist_stats(self, stats, spider):
         self.spider_stats[spider.name] = stats
 
 
 class DummyStatsCollector(StatsCollector):
-
     def get_value(self, key, default=None, spider=None):
         return default
 
     def set_value(self, key, value, spider=None):
         pass
 
     def set_stats(self, stats, spider=None):
```

### Comparing `Scrapy-2.7.1/scrapy/templates/project/module/middlewares.py.tmpl` & `Scrapy-2.8.0/scrapy/templates/project/module/middlewares.py.tmpl`

 * *Files 1% similar despite different names*

```diff
@@ -49,15 +49,15 @@
         # that it doesnt have a response associated.
 
         # Must return only requests (not items).
         for r in start_requests:
             yield r
 
     def spider_opened(self, spider):
-        spider.logger.info('Spider opened: %s' % spider.name)
+        spider.logger.info("Spider opened: %s" % spider.name)
 
 
 class ${ProjectName}DownloaderMiddleware:
     # Not all methods need to be defined. If a method is not defined,
     # scrapy acts as if the downloader middleware does not modify the
     # passed objects.
 
@@ -96,8 +96,8 @@
         # Must either:
         # - return None: continue processing this exception
         # - return a Response object: stops process_exception() chain
         # - return a Request object: stops process_exception() chain
         pass
 
     def spider_opened(self, spider):
-        spider.logger.info('Spider opened: %s' % spider.name)
+        spider.logger.info("Spider opened: %s" % spider.name)
```

### Comparing `Scrapy-2.7.1/scrapy/templates/project/module/settings.py.tmpl` & `Scrapy-2.8.0/scrapy/templates/project/module/settings.py.tmpl`

 * *Files 8% similar despite different names*

```diff
@@ -3,22 +3,22 @@
 # For simplicity, this file contains only settings considered important or
 # commonly used. You can find more settings consulting the documentation:
 #
 #     https://docs.scrapy.org/en/latest/topics/settings.html
 #     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
 #     https://docs.scrapy.org/en/latest/topics/spider-middleware.html
 
-BOT_NAME = '$project_name'
+BOT_NAME = "$project_name"
 
-SPIDER_MODULES = ['$project_name.spiders']
-NEWSPIDER_MODULE = '$project_name.spiders'
+SPIDER_MODULES = ["$project_name.spiders"]
+NEWSPIDER_MODULE = "$project_name.spiders"
 
 
 # Crawl responsibly by identifying yourself (and your website) on the user-agent
-#USER_AGENT = '$project_name (+http://www.yourdomain.com)'
+#USER_AGENT = "$project_name (+http://www.yourdomain.com)"
 
 # Obey robots.txt rules
 ROBOTSTXT_OBEY = True
 
 # Configure maximum concurrent requests performed by Scrapy (default: 16)
 #CONCURRENT_REQUESTS = 32
 
@@ -34,40 +34,40 @@
 #COOKIES_ENABLED = False
 
 # Disable Telnet Console (enabled by default)
 #TELNETCONSOLE_ENABLED = False
 
 # Override the default request headers:
 #DEFAULT_REQUEST_HEADERS = {
-#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
-#   'Accept-Language': 'en',
+#    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
+#    "Accept-Language": "en",
 #}
 
 # Enable or disable spider middlewares
 # See https://docs.scrapy.org/en/latest/topics/spider-middleware.html
 #SPIDER_MIDDLEWARES = {
-#    '$project_name.middlewares.${ProjectName}SpiderMiddleware': 543,
+#    "$project_name.middlewares.${ProjectName}SpiderMiddleware": 543,
 #}
 
 # Enable or disable downloader middlewares
 # See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
 #DOWNLOADER_MIDDLEWARES = {
-#    '$project_name.middlewares.${ProjectName}DownloaderMiddleware': 543,
+#    "$project_name.middlewares.${ProjectName}DownloaderMiddleware": 543,
 #}
 
 # Enable or disable extensions
 # See https://docs.scrapy.org/en/latest/topics/extensions.html
 #EXTENSIONS = {
-#    'scrapy.extensions.telnet.TelnetConsole': None,
+#    "scrapy.extensions.telnet.TelnetConsole": None,
 #}
 
 # Configure item pipelines
 # See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
 #ITEM_PIPELINES = {
-#    '$project_name.pipelines.${ProjectName}Pipeline': 300,
+#    "$project_name.pipelines.${ProjectName}Pipeline": 300,
 #}
 
 # Enable and configure the AutoThrottle extension (disabled by default)
 # See https://docs.scrapy.org/en/latest/topics/autothrottle.html
 #AUTOTHROTTLE_ENABLED = True
 # The initial download delay
 #AUTOTHROTTLE_START_DELAY = 5
@@ -79,14 +79,15 @@
 # Enable showing throttling stats for every response received:
 #AUTOTHROTTLE_DEBUG = False
 
 # Enable and configure HTTP caching (disabled by default)
 # See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
 #HTTPCACHE_ENABLED = True
 #HTTPCACHE_EXPIRATION_SECS = 0
-#HTTPCACHE_DIR = 'httpcache'
+#HTTPCACHE_DIR = "httpcache"
 #HTTPCACHE_IGNORE_HTTP_CODES = []
-#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
+#HTTPCACHE_STORAGE = "scrapy.extensions.httpcache.FilesystemCacheStorage"
 
 # Set settings whose default value is deprecated to a future-proof value
-REQUEST_FINGERPRINTER_IMPLEMENTATION = '2.7'
-TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'
+REQUEST_FINGERPRINTER_IMPLEMENTATION = "2.7"
+TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
+FEED_EXPORT_ENCODING = "utf-8"
```

### Comparing `Scrapy-2.7.1/scrapy/utils/benchserver.py` & `Scrapy-2.8.0/scrapy/utils/benchserver.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,44 +1,47 @@
 import random
 from urllib.parse import urlencode
 
-from twisted.web.server import Site
 from twisted.web.resource import Resource
+from twisted.web.server import Site
 
 
 class Root(Resource):
 
     isLeaf = True
 
     def getChild(self, name, request):
         return self
 
     def render(self, request):
-        total = _getarg(request, b'total', 100, int)
-        show = _getarg(request, b'show', 10, int)
+        total = _getarg(request, b"total", 100, int)
+        show = _getarg(request, b"show", 10, int)
         nlist = [random.randint(1, total) for _ in range(show)]
         request.write(b"<html><head></head><body>")
         args = request.args.copy()
         for nl in nlist:
-            args['n'] = nl
+            args["n"] = nl
             argstr = urlencode(args, doseq=True)
-            request.write(f"<a href='/follow?{argstr}'>follow {nl}</a><br>"
-                          .encode('utf8'))
+            request.write(
+                f"<a href='/follow?{argstr}'>follow {nl}</a><br>".encode("utf8")
+            )
         request.write(b"</body></html>")
-        return b''
+        return b""
 
 
 def _getarg(request, name, default=None, type=str):
     return type(request.args[name][0]) if name in request.args else default
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     from twisted.internet import reactor
+
     root = Root()
     factory = Site(root)
     httpPort = reactor.listenTCP(8998, Site(root))
 
     def _print_listening():
         httpHost = httpPort.getHost()
         print(f"Bench server at http://{httpHost.host}:{httpHost.port}")
+
     reactor.callWhenRunning(_print_listening)
     reactor.run()
```

### Comparing `Scrapy-2.7.1/scrapy/utils/conf.py` & `Scrapy-2.8.0/scrapy/utils/conf.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,147 +1,159 @@
 import numbers
 import os
 import sys
 import warnings
 from configparser import ConfigParser
 from operator import itemgetter
+from pathlib import Path
+from typing import Any, Dict, List, Optional, Union
 
 from scrapy.exceptions import ScrapyDeprecationWarning, UsageError
-
 from scrapy.settings import BaseSettings
 from scrapy.utils.deprecate import update_classpath
 from scrapy.utils.python import without_none_values
 
 
 def build_component_list(compdict, custom=None, convert=update_classpath):
     """Compose a component list from a { class: order } dictionary."""
 
     def _check_components(complist):
         if len({convert(c) for c in complist}) != len(complist):
-            raise ValueError(f'Some paths in {complist!r} convert to the same object, '
-                             'please update your settings')
+            raise ValueError(
+                f"Some paths in {complist!r} convert to the same object, "
+                "please update your settings"
+            )
 
     def _map_keys(compdict):
         if isinstance(compdict, BaseSettings):
             compbs = BaseSettings()
             for k, v in compdict.items():
                 prio = compdict.getpriority(k)
                 if compbs.getpriority(convert(k)) == prio:
-                    raise ValueError(f'Some paths in {list(compdict.keys())!r} '
-                                     'convert to the same '
-                                     'object, please update your settings'
-                                     )
+                    raise ValueError(
+                        f"Some paths in {list(compdict.keys())!r} "
+                        "convert to the same "
+                        "object, please update your settings"
+                    )
                 else:
                     compbs.set(convert(k), v, priority=prio)
             return compbs
-        else:
-            _check_components(compdict)
-            return {convert(k): v for k, v in compdict.items()}
+        _check_components(compdict)
+        return {convert(k): v for k, v in compdict.items()}
 
     def _validate_values(compdict):
         """Fail if a value in the components dict is not a real number or None."""
         for name, value in compdict.items():
             if value is not None and not isinstance(value, numbers.Real):
-                raise ValueError(f'Invalid value {value} for component {name}, '
-                                 'please provide a real number or None instead')
+                raise ValueError(
+                    f"Invalid value {value} for component {name}, "
+                    "please provide a real number or None instead"
+                )
 
-    # BEGIN Backward compatibility for old (base, custom) call signature
     if isinstance(custom, (list, tuple)):
         _check_components(custom)
         return type(custom)(convert(c) for c in custom)
 
     if custom is not None:
         compdict.update(custom)
-    # END Backward compatibility
 
     _validate_values(compdict)
     compdict = without_none_values(_map_keys(compdict))
     return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]
 
 
 def arglist_to_dict(arglist):
     """Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a
     dict
     """
-    return dict(x.split('=', 1) for x in arglist)
+    return dict(x.split("=", 1) for x in arglist)
 
 
-def closest_scrapy_cfg(path='.', prevpath=None):
+def closest_scrapy_cfg(
+    path: Union[str, os.PathLike] = ".",
+    prevpath: Optional[Union[str, os.PathLike]] = None,
+) -> str:
     """Return the path to the closest scrapy.cfg file by traversing the current
     directory and its parents
     """
-    if path == prevpath:
-        return ''
-    path = os.path.abspath(path)
-    cfgfile = os.path.join(path, 'scrapy.cfg')
-    if os.path.exists(cfgfile):
-        return cfgfile
-    return closest_scrapy_cfg(os.path.dirname(path), path)
+    if prevpath is not None and str(path) == str(prevpath):
+        return ""
+    path = Path(path).resolve()
+    cfgfile = path / "scrapy.cfg"
+    if cfgfile.exists():
+        return str(cfgfile)
+    return closest_scrapy_cfg(path.parent, path)
 
 
-def init_env(project='default', set_syspath=True):
+def init_env(project="default", set_syspath=True):
     """Initialize environment to use command-line tool from inside a project
     dir. This sets the Scrapy settings module and modifies the Python path to
     be able to locate the project module.
     """
     cfg = get_config()
-    if cfg.has_option('settings', project):
-        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('settings', project)
+    if cfg.has_option("settings", project):
+        os.environ["SCRAPY_SETTINGS_MODULE"] = cfg.get("settings", project)
     closest = closest_scrapy_cfg()
     if closest:
-        projdir = os.path.dirname(closest)
+        projdir = str(Path(closest).parent)
         if set_syspath and projdir not in sys.path:
             sys.path.append(projdir)
 
 
 def get_config(use_closest=True):
     """Get Scrapy config file as a ConfigParser"""
     sources = get_sources(use_closest)
     cfg = ConfigParser()
     cfg.read(sources)
     return cfg
 
 
-def get_sources(use_closest=True):
-    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')
+def get_sources(use_closest=True) -> List[str]:
+    xdg_config_home = (
+        os.environ.get("XDG_CONFIG_HOME") or Path("~/.config").expanduser()
+    )
     sources = [
-        '/etc/scrapy.cfg',
-        r'c:\scrapy\scrapy.cfg',
-        xdg_config_home + '/scrapy.cfg',
-        os.path.expanduser('~/.scrapy.cfg'),
+        "/etc/scrapy.cfg",
+        r"c:\scrapy\scrapy.cfg",
+        str(Path(xdg_config_home) / "scrapy.cfg"),
+        str(Path("~/.scrapy.cfg").expanduser()),
     ]
     if use_closest:
         sources.append(closest_scrapy_cfg())
     return sources
 
 
 def feed_complete_default_values_from_settings(feed, settings):
     out = feed.copy()
-    out.setdefault("batch_item_count", settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT'))
+    out.setdefault("batch_item_count", settings.getint("FEED_EXPORT_BATCH_ITEM_COUNT"))
     out.setdefault("encoding", settings["FEED_EXPORT_ENCODING"])
     out.setdefault("fields", settings.getdictorlist("FEED_EXPORT_FIELDS") or None)
     out.setdefault("store_empty", settings.getbool("FEED_STORE_EMPTY"))
     out.setdefault("uri_params", settings["FEED_URI_PARAMS"])
     out.setdefault("item_export_kwargs", {})
     if settings["FEED_EXPORT_INDENT"] is None:
         out.setdefault("indent", None)
     else:
         out.setdefault("indent", settings.getint("FEED_EXPORT_INDENT"))
     return out
 
 
-def feed_process_params_from_cli(settings, output, output_format=None,
-                                 overwrite_output=None):
+def feed_process_params_from_cli(
+    settings,
+    output: List[str],
+    output_format=None,
+    overwrite_output: Optional[List[str]] = None,
+):
     """
     Receives feed export params (from the 'crawl' or 'runspider' commands),
     checks for inconsistencies in their quantities and returns a dictionary
     suitable to be used as the FEEDS setting.
     """
     valid_output_formats = without_none_values(
-        settings.getwithbase('FEED_EXPORTERS')
+        settings.getwithbase("FEED_EXPORTERS")
     ).keys()
 
     def check_valid_format(output_format):
         if output_format not in valid_output_formats:
             raise UsageError(
                 f"Unrecognized output format '{output_format}'. "
                 f"Set a supported one ({tuple(valid_output_formats)}) "
@@ -176,33 +188,32 @@
                 " -O/--overwrite-output option (i.e. -o/-O <URI>:<FORMAT>). See the documentation"
                 " of the -o or -O option or the following examples for more information. "
                 "Examples working in the tutorial: "
                 "scrapy crawl quotes -o quotes.csv:csv   or   "
                 "scrapy crawl quotes -O quotes.json:json"
             )
             warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)
-            return {output[0]: {'format': output_format}}
-        else:
-            raise UsageError(
-                'The -t command-line option cannot be used if multiple output '
-                'URIs are specified'
-            )
+            return {output[0]: {"format": output_format}}
+        raise UsageError(
+            "The -t command-line option cannot be used if multiple output "
+            "URIs are specified"
+        )
 
-    result = {}
+    result: Dict[str, Dict[str, Any]] = {}
     for element in output:
         try:
-            feed_uri, feed_format = element.rsplit(':', 1)
+            feed_uri, feed_format = element.rsplit(":", 1)
         except ValueError:
             feed_uri = element
-            feed_format = os.path.splitext(element)[1].replace('.', '')
+            feed_format = Path(element).suffix.replace(".", "")
         else:
-            if feed_uri == '-':
-                feed_uri = 'stdout:'
+            if feed_uri == "-":
+                feed_uri = "stdout:"
         check_valid_format(feed_format)
-        result[feed_uri] = {'format': feed_format}
+        result[feed_uri] = {"format": feed_format}
         if overwrite:
-            result[feed_uri]['overwrite'] = True
+            result[feed_uri]["overwrite"] = True
 
     # FEEDS setting should take precedence over the matching CLI options
-    result.update(settings.getdict('FEEDS'))
+    result.update(settings.getdict("FEEDS"))
 
     return result
```

### Comparing `Scrapy-2.7.1/scrapy/utils/console.py` & `Scrapy-2.8.0/scrapy/utils/console.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,78 +1,84 @@
 from functools import wraps
-from collections import OrderedDict
 
 
-def _embed_ipython_shell(namespace={}, banner=''):
+def _embed_ipython_shell(namespace={}, banner=""):
     """Start an IPython Shell"""
     try:
         from IPython.terminal.embed import InteractiveShellEmbed
         from IPython.terminal.ipapp import load_default_config
     except ImportError:
         from IPython.frontend.terminal.embed import InteractiveShellEmbed
         from IPython.frontend.terminal.ipapp import load_default_config
 
     @wraps(_embed_ipython_shell)
-    def wrapper(namespace=namespace, banner=''):
+    def wrapper(namespace=namespace, banner=""):
         config = load_default_config()
         # Always use .instance() to ensure _instance propagation to all parents
         # this is needed for <TAB> completion works well for new imports
         # and clear the instance to always have the fresh env
         # on repeated breaks like with inspect_response()
         InteractiveShellEmbed.clear_instance()
         shell = InteractiveShellEmbed.instance(
-            banner1=banner, user_ns=namespace, config=config)
+            banner1=banner, user_ns=namespace, config=config
+        )
         shell()
+
     return wrapper
 
 
-def _embed_bpython_shell(namespace={}, banner=''):
+def _embed_bpython_shell(namespace={}, banner=""):
     """Start a bpython shell"""
     import bpython
 
     @wraps(_embed_bpython_shell)
-    def wrapper(namespace=namespace, banner=''):
+    def wrapper(namespace=namespace, banner=""):
         bpython.embed(locals_=namespace, banner=banner)
+
     return wrapper
 
 
-def _embed_ptpython_shell(namespace={}, banner=''):
+def _embed_ptpython_shell(namespace={}, banner=""):
     """Start a ptpython shell"""
     import ptpython.repl
 
     @wraps(_embed_ptpython_shell)
-    def wrapper(namespace=namespace, banner=''):
+    def wrapper(namespace=namespace, banner=""):
         print(banner)
         ptpython.repl.embed(locals=namespace)
+
     return wrapper
 
 
-def _embed_standard_shell(namespace={}, banner=''):
+def _embed_standard_shell(namespace={}, banner=""):
     """Start a standard python shell"""
     import code
+
     try:  # readline module is only available on unix systems
         import readline
     except ImportError:
         pass
     else:
         import rlcompleter  # noqa: F401
+
         readline.parse_and_bind("tab:complete")
 
     @wraps(_embed_standard_shell)
-    def wrapper(namespace=namespace, banner=''):
+    def wrapper(namespace=namespace, banner=""):
         code.interact(banner=banner, local=namespace)
+
     return wrapper
 
 
-DEFAULT_PYTHON_SHELLS = OrderedDict([
-    ('ptpython', _embed_ptpython_shell),
-    ('ipython', _embed_ipython_shell),
-    ('bpython', _embed_bpython_shell),
-    ('python', _embed_standard_shell),
-])
+DEFAULT_PYTHON_SHELLS = {
+    "ptpython": _embed_ptpython_shell,
+    "ipython": _embed_ipython_shell,
+    "bpython": _embed_bpython_shell,
+    "python": _embed_standard_shell,
+}
 
 
 def get_shell_embed_func(shells=None, known_shells=None):
     """Return the first acceptable shell-embed function
     from a given list of shell names.
     """
     if shells is None:  # list, preference order of shells
@@ -85,15 +91,15 @@
                 # function test: run all setup code (imports),
                 # but dont fall into the shell
                 return known_shells[shell]()
             except ImportError:
                 continue
 
 
-def start_python_console(namespace=None, banner='', shells=None):
+def start_python_console(namespace=None, banner="", shells=None):
     """Start Python console bound to the given namespace.
     Readline support and tab completion will be used on Unix, if available.
     """
     if namespace is None:
         namespace = {}
 
     try:
```

### Comparing `Scrapy-2.7.1/scrapy/utils/curl.py` & `Scrapy-2.8.0/scrapy/utils/curl.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,76 +1,78 @@
 import argparse
 import warnings
-from shlex import split
 from http.cookies import SimpleCookie
+from shlex import split
 from urllib.parse import urlparse
 
 from w3lib.http import basic_auth_header
 
 
 class CurlParser(argparse.ArgumentParser):
     def error(self, message):
-        error_msg = f'There was an error parsing the curl command: {message}'
+        error_msg = f"There was an error parsing the curl command: {message}"
         raise ValueError(error_msg)
 
 
 curl_parser = CurlParser()
-curl_parser.add_argument('url')
-curl_parser.add_argument('-H', '--header', dest='headers', action='append')
-curl_parser.add_argument('-X', '--request', dest='method')
-curl_parser.add_argument('-d', '--data', '--data-raw', dest='data')
-curl_parser.add_argument('-u', '--user', dest='auth')
+curl_parser.add_argument("url")
+curl_parser.add_argument("-H", "--header", dest="headers", action="append")
+curl_parser.add_argument("-X", "--request", dest="method")
+curl_parser.add_argument("-d", "--data", "--data-raw", dest="data")
+curl_parser.add_argument("-u", "--user", dest="auth")
 
 
 safe_to_ignore_arguments = [
-    ['--compressed'],
+    ["--compressed"],
     # `--compressed` argument is not safe to ignore, but it's included here
     # because the `HttpCompressionMiddleware` is enabled by default
-    ['-s', '--silent'],
-    ['-v', '--verbose'],
-    ['-#', '--progress-bar']
+    ["-s", "--silent"],
+    ["-v", "--verbose"],
+    ["-#", "--progress-bar"],
 ]
 
 for argument in safe_to_ignore_arguments:
-    curl_parser.add_argument(*argument, action='store_true')
+    curl_parser.add_argument(*argument, action="store_true")
 
 
 def _parse_headers_and_cookies(parsed_args):
     headers = []
     cookies = {}
     for header in parsed_args.headers or ():
-        name, val = header.split(':', 1)
+        name, val = header.split(":", 1)
         name = name.strip()
         val = val.strip()
-        if name.title() == 'Cookie':
+        if name.title() == "Cookie":
             for name, morsel in SimpleCookie(val).items():
                 cookies[name] = morsel.value
         else:
             headers.append((name, val))
 
     if parsed_args.auth:
-        user, password = parsed_args.auth.split(':', 1)
-        headers.append(('Authorization', basic_auth_header(user, password)))
+        user, password = parsed_args.auth.split(":", 1)
+        headers.append(("Authorization", basic_auth_header(user, password)))
 
     return headers, cookies
 
 
-def curl_to_request_kwargs(curl_command: str, ignore_unknown_options: bool = True) -> dict:
+def curl_to_request_kwargs(
+    curl_command: str, ignore_unknown_options: bool = True
+) -> dict:
     """Convert a cURL command syntax to Request kwargs.
 
     :param str curl_command: string containing the curl command
     :param bool ignore_unknown_options: If true, only a warning is emitted when
                                         cURL options are unknown. Otherwise
                                         raises an error. (default: True)
     :return: dictionary of Request kwargs
     """
 
     curl_args = split(curl_command)
 
-    if curl_args[0] != 'curl':
+    if curl_args[0] != "curl":
         raise ValueError('A curl command must start with "curl"')
 
     parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])
 
     if argv:
         msg = f'Unrecognized options: {", ".join(argv)}'
         if ignore_unknown_options:
@@ -80,27 +82,27 @@
 
     url = parsed_args.url
 
     # curl automatically prepends 'http' if the scheme is missing, but Request
     # needs the scheme to work
     parsed_url = urlparse(url)
     if not parsed_url.scheme:
-        url = 'http://' + url
+        url = "http://" + url
 
-    method = parsed_args.method or 'GET'
+    method = parsed_args.method or "GET"
 
-    result = {'method': method.upper(), 'url': url}
+    result = {"method": method.upper(), "url": url}
 
     headers, cookies = _parse_headers_and_cookies(parsed_args)
 
     if headers:
-        result['headers'] = headers
+        result["headers"] = headers
     if cookies:
-        result['cookies'] = cookies
+        result["cookies"] = cookies
     if parsed_args.data:
-        result['body'] = parsed_args.data
+        result["body"] = parsed_args.data
         if not parsed_args.method:
             # if the "data" is specified but the "method" is not specified,
             # the default method is 'POST'
-            result['method'] = 'POST'
+            result["method"] = "POST"
 
     return result
```

### Comparing `Scrapy-2.7.1/scrapy/utils/datatypes.py` & `Scrapy-2.8.0/scrapy/utils/datatypes.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,18 +26,20 @@
         dict.__setitem__(self, self.normkey(key), self.normvalue(value))
 
     def __delitem__(self, key):
         dict.__delitem__(self, self.normkey(key))
 
     def __contains__(self, key):
         return dict.__contains__(self, self.normkey(key))
+
     has_key = __contains__
 
     def __copy__(self):
         return self.__class__(self)
+
     copy = __copy__
 
     def normkey(self, key):
         """Method to normalize dictionary key access"""
         return key.lower()
 
     def normvalue(self, value):
```

### Comparing `Scrapy-2.7.1/scrapy/utils/decorators.py` & `Scrapy-2.8.0/scrapy/utils/decorators.py`

 * *Files 5% similar despite different names*

```diff
@@ -15,31 +15,36 @@
         @wraps(func)
         def wrapped(*args, **kwargs):
             message = f"Call to deprecated function {func.__name__}."
             if use_instead:
                 message += f" Use {use_instead} instead."
             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)
             return func(*args, **kwargs)
+
         return wrapped
 
     if callable(use_instead):
         deco = deco(use_instead)
         use_instead = None
     return deco
 
 
 def defers(func):
     """Decorator to make sure a function always returns a deferred"""
+
     @wraps(func)
     def wrapped(*a, **kw):
         return defer.maybeDeferred(func, *a, **kw)
+
     return wrapped
 
 
 def inthread(func):
     """Decorator to call a function in a thread and return a deferred with the
     result
     """
+
     @wraps(func)
     def wrapped(*a, **kw):
         return threads.deferToThread(func, *a, **kw)
+
     return wrapped
```

### Comparing `Scrapy-2.7.1/scrapy/utils/defer.py` & `Scrapy-2.8.0/scrapy/utils/defer.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,60 +12,61 @@
     Callable,
     Coroutine,
     Generator,
     Iterable,
     Iterator,
     List,
     Optional,
-    Union
+    Union,
 )
 
 from twisted.internet import defer
 from twisted.internet.defer import Deferred, DeferredList, ensureDeferred
 from twisted.internet.task import Cooperator
 from twisted.python import failure
 from twisted.python.failure import Failure
 
 from scrapy.exceptions import IgnoreRequest
-from scrapy.utils.reactor import is_asyncio_reactor_installed, get_asyncio_event_loop_policy
+from scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed
 
 
 def defer_fail(_failure: Failure) -> Deferred:
     """Same as twisted.internet.defer.fail but delay calling errback until
     next reactor loop
 
     It delays by 100ms so reactor has a chance to go through readers and writers
     before attending pending delayed calls, so do not set delay to zero.
     """
     from twisted.internet import reactor
+
     d = Deferred()
     reactor.callLater(0.1, d.errback, _failure)
     return d
 
 
 def defer_succeed(result) -> Deferred:
     """Same as twisted.internet.defer.succeed but delay calling callback until
     next reactor loop
 
     It delays by 100ms so reactor has a chance to go through readers and writers
     before attending pending delayed calls, so do not set delay to zero.
     """
     from twisted.internet import reactor
+
     d = Deferred()
     reactor.callLater(0.1, d.callback, result)
     return d
 
 
 def defer_result(result) -> Deferred:
     if isinstance(result, Deferred):
         return result
-    elif isinstance(result, failure.Failure):
+    if isinstance(result, failure.Failure):
         return defer_fail(result)
-    else:
-        return defer_succeed(result)
+    return defer_succeed(result)
 
 
 def mustbe_deferred(f: Callable, *args, **kw) -> Deferred:
     """Same as twisted.internet.defer.maybeDeferred, but delay calling
     callback/errback to next reactor loop
     """
     try:
@@ -77,27 +78,29 @@
         return defer_fail(failure.Failure(e))
     except Exception:
         return defer_fail(failure.Failure())
     else:
         return defer_result(result)
 
 
-def parallel(iterable: Iterable, count: int, callable: Callable, *args, **named) -> DeferredList:
+def parallel(
+    iterable: Iterable, count: int, callable: Callable, *args, **named
+) -> DeferredList:
     """Execute a callable over the objects in the given iterable, in parallel,
     using no more than ``count`` concurrent calls.
 
     Taken from: https://jcalderone.livejournal.com/24285.html
     """
     coop = Cooperator()
     work = (callable(elem, *args, **named) for elem in iterable)
     return DeferredList([coop.coiterate(work) for _ in range(count)])
 
 
 class _AsyncCooperatorAdapter(Iterator):
-    """ A class that wraps an async iterable into a normal iterator suitable
+    """A class that wraps an async iterable into a normal iterator suitable
     for using in Cooperator.coiterate(). As it's only needed for parallel_async(),
     it calls the callable directly in the callback, instead of providing a more
     generic interface.
 
     On the outside, this class behaves as an iterator that yields Deferreds.
     Each Deferred is fired with the result of the callable which was called on
     the next result from aiterator. It raises StopIteration when aiterator is
@@ -133,15 +136,22 @@
     Note that CooperativeTask ignores the value returned from the Deferred that it waits
     for, so we fire them with None when needed.
 
     It may be possible to write an async iterator-aware replacement for
     Cooperator/CooperativeTask and use it instead of this adapter to achieve the same
     goal.
     """
-    def __init__(self, aiterable: AsyncIterable, callable: Callable, *callable_args, **callable_kwargs):
+
+    def __init__(
+        self,
+        aiterable: AsyncIterable,
+        callable: Callable,
+        *callable_args,
+        **callable_kwargs
+    ):
         self.aiterator = aiterable.__aiter__()
         self.callable = callable
         self.callable_args = callable_args
         self.callable_kwargs = callable_kwargs
         self.finished = False
         self.waiting_deferreds: List[Deferred] = []
         self.anext_deferred: Optional[Deferred] = None
@@ -183,16 +193,18 @@
         d = Deferred()
         self.waiting_deferreds.append(d)
         if not self.anext_deferred:
             self._call_anext()
         return d
 
 
-def parallel_async(async_iterable: AsyncIterable, count: int, callable: Callable, *args, **named) -> DeferredList:
-    """ Like parallel but for async iterators """
+def parallel_async(
+    async_iterable: AsyncIterable, count: int, callable: Callable, *args, **named
+) -> DeferredList:
+    """Like parallel but for async iterators"""
     coop = Cooperator()
     work = _AsyncCooperatorAdapter(async_iterable, callable, *args, **named)
     dl = DeferredList([coop.coiterate(work) for _ in range(count)])
     return dl
 
 
 def process_chain(callbacks: Iterable[Callable], input, *a, **kw) -> Deferred:
@@ -200,22 +212,27 @@
     d = Deferred()
     for x in callbacks:
         d.addCallback(x, *a, **kw)
     d.callback(input)
     return d
 
 
-def process_chain_both(callbacks: Iterable[Callable], errbacks: Iterable[Callable], input, *a, **kw) -> Deferred:
+def process_chain_both(
+    callbacks: Iterable[Callable], errbacks: Iterable[Callable], input, *a, **kw
+) -> Deferred:
     """Return a Deferred built by chaining the given callbacks and errbacks"""
     d = Deferred()
     for cb, eb in zip(callbacks, errbacks):
         d.addCallbacks(
-            callback=cb, errback=eb,
-            callbackArgs=a, callbackKeywords=kw,
-            errbackArgs=a, errbackKeywords=kw,
+            callback=cb,
+            errback=eb,
+            callbackArgs=a,
+            callbackKeywords=kw,
+            errbackArgs=a,
+            errbackKeywords=kw,
         )
     if isinstance(input, failure.Failure):
         d.errback(input)
     else:
         d.callback(input)
     return d
 
@@ -240,15 +257,17 @@
             yield next(it)
         except StopIteration:
             break
         except Exception:
             errback(failure.Failure(), *a, **kw)
 
 
-async def aiter_errback(aiterable: AsyncIterable, errback: Callable, *a, **kw) -> AsyncGenerator:
+async def aiter_errback(
+    aiterable: AsyncIterable, errback: Callable, *a, **kw
+) -> AsyncGenerator:
     """Wraps an async iterable calling an errback if an error is caught while
     iterating it. Similar to scrapy.utils.defer.iter_errback()
     """
     it = aiterable.__aiter__()
     while True:
         try:
             yield await it.__anext__()
@@ -263,48 +282,48 @@
     if isinstance(o, Deferred):
         return o
     if asyncio.isfuture(o) or inspect.isawaitable(o):
         if not is_asyncio_reactor_installed():
             # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines
             # that use asyncio, e.g. "await asyncio.sleep(1)"
             return ensureDeferred(o)
-        else:
-            # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor
-            event_loop = get_asyncio_event_loop_policy().get_event_loop()
-            return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))
+        # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor
+        event_loop = _get_asyncio_event_loop()
+        return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))
     return o
 
 
 def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]) -> Callable:
-    """ Converts a coroutine function into a function that returns a Deferred.
+    """Converts a coroutine function into a function that returns a Deferred.
 
     The coroutine function will be called at the time when the wrapper is called. Wrapper args will be passed to it.
     This is useful for callback chains, as callback functions are called with the previous callback result.
     """
+
     @wraps(coro_f)
     def f(*coro_args, **coro_kwargs):
         return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
+
     return f
 
 
 def maybeDeferred_coro(f: Callable, *args, **kw) -> Deferred:
-    """ Copy of defer.maybeDeferred that also converts coroutines to Deferreds. """
+    """Copy of defer.maybeDeferred that also converts coroutines to Deferreds."""
     try:
         result = f(*args, **kw)
     except:  # noqa: E722
         return defer.fail(failure.Failure(captureVars=Deferred.debug))
 
     if isinstance(result, Deferred):
         return result
-    elif asyncio.isfuture(result) or inspect.isawaitable(result):
+    if asyncio.isfuture(result) or inspect.isawaitable(result):
         return deferred_from_coro(result)
-    elif isinstance(result, failure.Failure):
+    if isinstance(result, failure.Failure):
         return defer.fail(result)
-    else:
-        return defer.succeed(result)
+    return defer.succeed(result)
 
 
 def deferred_to_future(d: Deferred) -> Future:
     """
     .. versionadded:: 2.6.0
 
     Return an :class:`asyncio.Future` object that wraps *d*.
@@ -317,16 +336,15 @@
 
         class MySpider(Spider):
             ...
             async def parse(self, response):
                 d = treq.get('https://example.com/additional')
                 additional_response = await deferred_to_future(d)
     """
-    policy = get_asyncio_event_loop_policy()
-    return d.asFuture(policy.get_event_loop())
+    return d.asFuture(_get_asyncio_event_loop())
 
 
 def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:
     """
     .. versionadded:: 2.6.0
 
     Return *d* as an object that can be awaited from a :ref:`Scrapy callable
@@ -348,9 +366,8 @@
             ...
             async def parse(self, response):
                 d = treq.get('https://example.com/additional')
                 extra_response = await maybe_deferred_to_future(d)
     """
     if not is_asyncio_reactor_installed():
         return d
-    else:
-        return deferred_to_future(d)
+    return deferred_to_future(d)
```

### Comparing `Scrapy-2.7.1/scrapy/utils/deprecate.py` & `Scrapy-2.8.0/scrapy/utils/deprecate.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,33 +1,36 @@
 """Some helpers for deprecation messages"""
 
-import warnings
 import inspect
+import warnings
+from typing import List, Tuple
+
 from scrapy.exceptions import ScrapyDeprecationWarning
 
 
-def attribute(obj, oldattr, newattr, version='0.12'):
+def attribute(obj, oldattr, newattr, version="0.12"):
     cname = obj.__class__.__name__
     warnings.warn(
         f"{cname}.{oldattr} attribute is deprecated and will be no longer supported "
         f"in Scrapy {version}, use {cname}.{newattr} attribute instead",
         ScrapyDeprecationWarning,
-        stacklevel=3)
+        stacklevel=3,
+    )
 
 
 def create_deprecated_class(
     name,
     new_class,
     clsdict=None,
     warn_category=ScrapyDeprecationWarning,
     warn_once=True,
     old_class_path=None,
     new_class_path=None,
     subclass_warn_message="{cls} inherits from deprecated class {old}, please inherit from {new}.",
-    instance_warn_message="{cls} is deprecated, instantiate {new} instead."
+    instance_warn_message="{cls} is deprecated, instantiate {new} instead.",
 ):
     """
     Return a "deprecated" class that causes its subclasses to issue a warning.
     Subclasses of ``new_class`` are considered subclasses of this class.
     It also warns when the deprecated class is instantiated, but do not when
     its subclasses are instantiated.
 
@@ -62,48 +65,51 @@
             return cls
 
         def __init__(cls, name, bases, clsdict_):
             meta = cls.__class__
             old = meta.deprecated_class
             if old in bases and not (warn_once and meta.warned_on_subclass):
                 meta.warned_on_subclass = True
-                msg = subclass_warn_message.format(cls=_clspath(cls),
-                                                   old=_clspath(old, old_class_path),
-                                                   new=_clspath(new_class, new_class_path))
+                msg = subclass_warn_message.format(
+                    cls=_clspath(cls),
+                    old=_clspath(old, old_class_path),
+                    new=_clspath(new_class, new_class_path),
+                )
                 if warn_once:
-                    msg += ' (warning only on first subclass, there may be others)'
+                    msg += " (warning only on first subclass, there may be others)"
                 warnings.warn(msg, warn_category, stacklevel=2)
             super().__init__(name, bases, clsdict_)
 
         # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass
         # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks
         # for implementation details
         def __instancecheck__(cls, inst):
-            return any(cls.__subclasscheck__(c)
-                       for c in (type(inst), inst.__class__))
+            return any(cls.__subclasscheck__(c) for c in (type(inst), inst.__class__))
 
         def __subclasscheck__(cls, sub):
             if cls is not DeprecatedClass.deprecated_class:
                 # we should do the magic only if second `issubclass` argument
                 # is the deprecated class itself - subclasses of the
                 # deprecated class should not use custom `__subclasscheck__`
                 # method.
                 return super().__subclasscheck__(sub)
 
             if not inspect.isclass(sub):
                 raise TypeError("issubclass() arg 1 must be a class")
 
-            mro = getattr(sub, '__mro__', ())
+            mro = getattr(sub, "__mro__", ())
             return any(c in {cls, new_class} for c in mro)
 
         def __call__(cls, *args, **kwargs):
             old = DeprecatedClass.deprecated_class
             if cls is old:
-                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),
-                                                   new=_clspath(new_class, new_class_path))
+                msg = instance_warn_message.format(
+                    cls=_clspath(cls, old_class_path),
+                    new=_clspath(new_class, new_class_path),
+                )
                 warnings.warn(msg, warn_category, stacklevel=2)
             return super().__call__(*args, **kwargs)
 
     deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})
 
     try:
         frm = inspect.stack()[1]
@@ -119,29 +125,29 @@
 
     return deprecated_cls
 
 
 def _clspath(cls, forced=None):
     if forced is not None:
         return forced
-    return f'{cls.__module__}.{cls.__name__}'
+    return f"{cls.__module__}.{cls.__name__}"
 
 
-DEPRECATION_RULES = [
-    ('scrapy.telnet.', 'scrapy.extensions.telnet.'),
-]
+DEPRECATION_RULES: List[Tuple[str, str]] = []
 
 
 def update_classpath(path):
     """Update a deprecated path from an object with its new location"""
     for prefix, replacement in DEPRECATION_RULES:
         if isinstance(path, str) and path.startswith(prefix):
             new_path = path.replace(prefix, replacement, 1)
-            warnings.warn(f"`{path}` class is deprecated, use `{new_path}` instead",
-                          ScrapyDeprecationWarning)
+            warnings.warn(
+                f"`{path}` class is deprecated, use `{new_path}` instead",
+                ScrapyDeprecationWarning,
+            )
             return new_path
     return path
 
 
 def method_is_overridden(subclass, base_class, method_name):
     """
     Return True if a method named ``method_name`` of a ``base_class``
```

### Comparing `Scrapy-2.7.1/scrapy/utils/display.py` & `Scrapy-2.8.0/scrapy/utils/display.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 """
 pprint and pformat wrappers with colorization support
 """
 
 import ctypes
 import platform
 import sys
-from packaging.version import Version as parse_version
 from pprint import pformat as pformat_
 
+from packaging.version import Version as parse_version
+
 
 def _enable_windows_terminal_processing():
     # https://stackoverflow.com/a/36760881
     kernel32 = ctypes.windll.kernel32
     return bool(kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7))
 
 
@@ -33,16 +34,17 @@
     try:
         from pygments import highlight
     except ImportError:
         return text
     else:
         from pygments.formatters import TerminalFormatter
         from pygments.lexers import PythonLexer
+
         return highlight(text, PythonLexer(), TerminalFormatter())
 
 
 def pformat(obj, *args, **kwargs):
-    return _colorize(pformat_(obj), kwargs.pop('colorize', True))
+    return _colorize(pformat_(obj), kwargs.pop("colorize", True))
 
 
 def pprint(obj, *args, **kwargs):
     print(pformat(obj, *args, **kwargs))
```

### Comparing `Scrapy-2.7.1/scrapy/utils/engine.py` & `Scrapy-2.8.0/scrapy/utils/engine.py`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/scrapy/utils/ftp.py` & `Scrapy-2.8.0/scrapy/utils/ftp.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,9 @@
 import posixpath
-
-from ftplib import error_perm, FTP
+from ftplib import FTP, error_perm
 from posixpath import dirname
 
 
 def ftp_makedirs_cwd(ftp, path, first_call=True):
     """Set the current directory of the FTP connection given in the ``ftp``
     argument (as a ftplib.FTP object), creating all parent directories if they
     don't exist. The ftplib.FTP object must be already connected and logged in.
@@ -15,23 +14,23 @@
         ftp_makedirs_cwd(ftp, dirname(path), False)
         ftp.mkd(path)
         if first_call:
             ftp.cwd(path)
 
 
 def ftp_store_file(
-        *, path, file, host, port,
-        username, password, use_active_mode=False, overwrite=True):
+    *, path, file, host, port, username, password, use_active_mode=False, overwrite=True
+):
     """Opens a FTP connection with passed credentials,sets current directory
     to the directory extracted from given path, then uploads the file to server
     """
     with FTP() as ftp:
         ftp.connect(host, port)
         ftp.login(username, password)
         if use_active_mode:
             ftp.set_pasv(False)
         file.seek(0)
         dirname, filename = posixpath.split(path)
         ftp_makedirs_cwd(ftp, dirname)
-        command = 'STOR' if overwrite else 'APPE'
-        ftp.storbinary(f'{command} {filename}', file)
+        command = "STOR" if overwrite else "APPE"
+        ftp.storbinary(f"{command} {filename}", file)
         file.close()
```

### Comparing `Scrapy-2.7.1/scrapy/utils/gz.py` & `Scrapy-2.8.0/scrapy/utils/gz.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,45 +1,34 @@
 import struct
 from gzip import GzipFile
 from io import BytesIO
 
-from scrapy.utils.decorators import deprecated
-
-
-# - GzipFile's read() has issues returning leftover uncompressed data when
-#   input is corrupted
-# - read1(), which fetches data before raising EOFError on next call
-#   works here
-@deprecated('GzipFile.read1')
-def read1(gzf, size=-1):
-    return gzf.read1(size)
-
 
 def gunzip(data):
     """Gunzip the given data and return as much data as possible.
 
     This is resilient to CRC checksum errors.
     """
     f = GzipFile(fileobj=BytesIO(data))
     output_list = []
-    chunk = b'.'
+    chunk = b"."
     while chunk:
         try:
             chunk = f.read1(8196)
             output_list.append(chunk)
         except (IOError, EOFError, struct.error):
             # complete only if there is some data, otherwise re-raise
             # see issue 87 about catching struct.error
             # some pages are quite small so output_list is empty and f.extrabuf
             # contains the whole page content
-            if output_list or getattr(f, 'extrabuf', None):
+            if output_list or getattr(f, "extrabuf", None):
                 try:
-                    output_list.append(f.extrabuf[-f.extrasize:])
+                    output_list.append(f.extrabuf[-f.extrasize :])
                 finally:
                     break
             else:
                 raise
-    return b''.join(output_list)
+    return b"".join(output_list)
 
 
 def gzip_magic_number(response):
-    return response.body[:3] == b'\x1f\x8b\x08'
+    return response.body[:3] == b"\x1f\x8b\x08"
```

### Comparing `Scrapy-2.7.1/scrapy/utils/httpobj.py` & `Scrapy-2.8.0/scrapy/utils/httpobj.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 """Helper functions for scrapy.http objects (Request, Response)"""
 
 from typing import Union
-from urllib.parse import urlparse, ParseResult
+from urllib.parse import ParseResult, urlparse
 from weakref import WeakKeyDictionary
 
 from scrapy.http import Request, Response
 
-
-_urlparse_cache: "WeakKeyDictionary[Union[Request, Response], ParseResult]" = WeakKeyDictionary()
+_urlparse_cache: "WeakKeyDictionary[Union[Request, Response], ParseResult]" = (
+    WeakKeyDictionary()
+)
 
 
 def urlparse_cached(request_or_response: Union[Request, Response]) -> ParseResult:
     """Return urlparse.urlparse caching the result, where the argument can be a
     Request or Response object
     """
     if request_or_response not in _urlparse_cache:
```

### Comparing `Scrapy-2.7.1/scrapy/utils/iterators.py` & `Scrapy-2.8.0/scrapy/utils/iterators.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,119 +1,120 @@
 import csv
 import logging
 import re
 from io import StringIO
 
-from scrapy.http import TextResponse, Response
+from scrapy.http import Response, TextResponse
 from scrapy.selector import Selector
 from scrapy.utils.python import re_rsearch, to_unicode
 
-
 logger = logging.getLogger(__name__)
 
 
 def xmliter(obj, nodename):
     """Return a iterator of Selector's over all nodes of a XML document,
        given the name of the node to iterate. Useful for parsing XML feeds.
 
     obj can be:
     - a Response object
     - a unicode string
     - a string encoded as utf-8
     """
     nodename_patt = re.escape(nodename)
 
-    DOCUMENT_HEADER_RE = re.compile(r'<\?xml[^>]+>\s*', re.S)
-    HEADER_END_RE = re.compile(fr'<\s*/{nodename_patt}\s*>', re.S)
-    END_TAG_RE = re.compile(r'<\s*/([^\s>]+)\s*>', re.S)
-    NAMESPACE_RE = re.compile(r'((xmlns[:A-Za-z]*)=[^>\s]+)', re.S)
+    DOCUMENT_HEADER_RE = re.compile(r"<\?xml[^>]+>\s*", re.S)
+    HEADER_END_RE = re.compile(rf"<\s*/{nodename_patt}\s*>", re.S)
+    END_TAG_RE = re.compile(r"<\s*/([^\s>]+)\s*>", re.S)
+    NAMESPACE_RE = re.compile(r"((xmlns[:A-Za-z]*)=[^>\s]+)", re.S)
     text = _body_or_str(obj)
 
     document_header = re.search(DOCUMENT_HEADER_RE, text)
-    document_header = document_header.group().strip() if document_header else ''
+    document_header = document_header.group().strip() if document_header else ""
     header_end_idx = re_rsearch(HEADER_END_RE, text)
-    header_end = text[header_end_idx[1]:].strip() if header_end_idx else ''
+    header_end = text[header_end_idx[1] :].strip() if header_end_idx else ""
     namespaces = {}
     if header_end:
         for tagname in reversed(re.findall(END_TAG_RE, header_end)):
-            tag = re.search(fr'<\s*{tagname}.*?xmlns[:=][^>]*>', text[:header_end_idx[1]], re.S)
+            tag = re.search(
+                rf"<\s*{tagname}.*?xmlns[:=][^>]*>", text[: header_end_idx[1]], re.S
+            )
             if tag:
-                namespaces.update(reversed(x) for x in re.findall(NAMESPACE_RE, tag.group()))
+                namespaces.update(
+                    reversed(x) for x in re.findall(NAMESPACE_RE, tag.group())
+                )
 
-    r = re.compile(fr'<{nodename_patt}[\s>].*?</{nodename_patt}>', re.DOTALL)
+    r = re.compile(rf"<{nodename_patt}[\s>].*?</{nodename_patt}>", re.DOTALL)
     for match in r.finditer(text):
         nodetext = (
             document_header
             + match.group().replace(
-                nodename,
-                f'{nodename} {" ".join(namespaces.values())}',
-                1
+                nodename, f'{nodename} {" ".join(namespaces.values())}', 1
             )
             + header_end
         )
-        yield Selector(text=nodetext, type='xml')
+        yield Selector(text=nodetext, type="xml")
 
 
-def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):
+def xmliter_lxml(obj, nodename, namespace=None, prefix="x"):
     from lxml import etree
+
     reader = _StreamReader(obj)
-    tag = f'{{{namespace}}}{nodename}' if namespace else nodename
+    tag = f"{{{namespace}}}{nodename}" if namespace else nodename
     iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)
-    selxpath = '//' + (f'{prefix}:{nodename}' if namespace else nodename)
+    selxpath = "//" + (f"{prefix}:{nodename}" if namespace else nodename)
     for _, node in iterable:
-        nodetext = etree.tostring(node, encoding='unicode')
+        nodetext = etree.tostring(node, encoding="unicode")
         node.clear()
-        xs = Selector(text=nodetext, type='xml')
+        xs = Selector(text=nodetext, type="xml")
         if namespace:
             xs.register_namespace(prefix, namespace)
         yield xs.xpath(selxpath)[0]
 
 
 class _StreamReader:
-
     def __init__(self, obj):
         self._ptr = 0
         if isinstance(obj, Response):
             self._text, self.encoding = obj.body, obj.encoding
         else:
-            self._text, self.encoding = obj, 'utf-8'
+            self._text, self.encoding = obj, "utf-8"
         self._is_unicode = isinstance(self._text, str)
 
     def read(self, n=65535):
         self.read = self._read_unicode if self._is_unicode else self._read_string
         return self.read(n).lstrip()
 
     def _read_string(self, n=65535):
         s, e = self._ptr, self._ptr + n
         self._ptr = e
         return self._text[s:e]
 
     def _read_unicode(self, n=65535):
         s, e = self._ptr, self._ptr + n
         self._ptr = e
-        return self._text[s:e].encode('utf-8')
+        return self._text[s:e].encode("utf-8")
 
 
 def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):
-    """ Returns an iterator of dictionaries from the given csv object
+    """Returns an iterator of dictionaries from the given csv object
 
     obj can be:
     - a Response object
     - a unicode string
     - a string encoded as utf-8
 
     delimiter is the character used to separate fields on the given obj.
 
     headers is an iterable that when provided offers the keys
     for the returned dictionaries, if not the first row is used.
 
     quotechar is the character used to enclosure fields on the given obj.
     """
 
-    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'
+    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or "utf-8"
 
     def row_to_unicode(row_):
         return [to_unicode(field, encoding) for field in row_]
 
     lines = StringIO(_body_or_str(obj, unicode=True))
 
     kwargs = {}
@@ -129,34 +130,36 @@
         except StopIteration:
             return
         headers = row_to_unicode(row)
 
     for row in csv_r:
         row = row_to_unicode(row)
         if len(row) != len(headers):
-            logger.warning("ignoring row %(csvlnum)d (length: %(csvrow)d, "
-                           "should be: %(csvheader)d)",
-                           {'csvlnum': csv_r.line_num, 'csvrow': len(row),
-                            'csvheader': len(headers)})
+            logger.warning(
+                "ignoring row %(csvlnum)d (length: %(csvrow)d, "
+                "should be: %(csvheader)d)",
+                {
+                    "csvlnum": csv_r.line_num,
+                    "csvrow": len(row),
+                    "csvheader": len(headers),
+                },
+            )
             continue
-        else:
-            yield dict(zip(headers, row))
+        yield dict(zip(headers, row))
 
 
 def _body_or_str(obj, unicode=True):
     expected_types = (Response, str, bytes)
     if not isinstance(obj, expected_types):
         expected_types_str = " or ".join(t.__name__ for t in expected_types)
         raise TypeError(
             f"Object {obj!r} must be {expected_types_str}, not {type(obj).__name__}"
         )
     if isinstance(obj, Response):
         if not unicode:
             return obj.body
-        elif isinstance(obj, TextResponse):
+        if isinstance(obj, TextResponse):
             return obj.text
-        else:
-            return obj.body.decode('utf-8')
-    elif isinstance(obj, str):
-        return obj if unicode else obj.encode('utf-8')
-    else:
-        return obj.decode('utf-8') if unicode else obj
+        return obj.body.decode("utf-8")
+    if isinstance(obj, str):
+        return obj if unicode else obj.encode("utf-8")
+    return obj.decode("utf-8") if unicode else obj
```

### Comparing `Scrapy-2.7.1/scrapy/utils/log.py` & `Scrapy-2.8.0/scrapy/utils/log.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,15 +7,14 @@
 from twisted.python.failure import Failure
 
 import scrapy
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.settings import Settings
 from scrapy.utils.versions import scrapy_components_versions
 
-
 logger = logging.getLogger(__name__)
 
 
 def failure_to_exc_info(failure):
     """Extract exc_info from Failure instances"""
     if isinstance(failure, Failure):
         return (failure.type, failure.value, failure.getTracebackObject())
@@ -33,33 +32,36 @@
     ``loggers`` list where it should act.
     """
 
     def __init__(self, loggers=None):
         self.loggers = loggers or []
 
     def filter(self, record):
-        if any(record.name.startswith(logger + '.') for logger in self.loggers):
-            record.name = record.name.split('.', 1)[0]
+        if any(record.name.startswith(logger + ".") for logger in self.loggers):
+            record.name = record.name.split(".", 1)[0]
         return True
 
 
 DEFAULT_LOGGING = {
-    'version': 1,
-    'disable_existing_loggers': False,
-    'loggers': {
-        'hpack': {
-            'level': 'ERROR',
+    "version": 1,
+    "disable_existing_loggers": False,
+    "loggers": {
+        "filelock": {
+            "level": "ERROR",
+        },
+        "hpack": {
+            "level": "ERROR",
         },
-        'scrapy': {
-            'level': 'DEBUG',
+        "scrapy": {
+            "level": "DEBUG",
         },
-        'twisted': {
-            'level': 'ERROR',
+        "twisted": {
+            "level": "ERROR",
         },
-    }
+    },
 }
 
 
 def configure_logging(settings=None, install_root_handler=True):
     """
     Initialize logging defaults for Scrapy.
 
@@ -83,103 +85,109 @@
     using ``settings`` argument. When ``settings`` is empty or None, defaults
     are used.
     """
     if not sys.warnoptions:
         # Route warnings through python logging
         logging.captureWarnings(True)
 
-    observer = twisted_log.PythonLoggingObserver('twisted')
+    observer = twisted_log.PythonLoggingObserver("twisted")
     observer.start()
 
     dictConfig(DEFAULT_LOGGING)
 
     if isinstance(settings, dict) or settings is None:
         settings = Settings(settings)
 
-    if settings.getbool('LOG_STDOUT'):
-        sys.stdout = StreamLogger(logging.getLogger('stdout'))
+    if settings.getbool("LOG_STDOUT"):
+        sys.stdout = StreamLogger(logging.getLogger("stdout"))
 
     if install_root_handler:
         install_scrapy_root_handler(settings)
 
 
 def install_scrapy_root_handler(settings):
     global _scrapy_root_handler
 
-    if (_scrapy_root_handler is not None
-            and _scrapy_root_handler in logging.root.handlers):
+    if (
+        _scrapy_root_handler is not None
+        and _scrapy_root_handler in logging.root.handlers
+    ):
         logging.root.removeHandler(_scrapy_root_handler)
     logging.root.setLevel(logging.NOTSET)
     _scrapy_root_handler = _get_handler(settings)
     logging.root.addHandler(_scrapy_root_handler)
 
 
 def get_scrapy_root_handler():
     return _scrapy_root_handler
 
 
 _scrapy_root_handler = None
 
 
 def _get_handler(settings):
-    """ Return a log handler object according to settings """
-    filename = settings.get('LOG_FILE')
+    """Return a log handler object according to settings"""
+    filename = settings.get("LOG_FILE")
     if filename:
-        mode = 'a' if settings.getbool('LOG_FILE_APPEND') else 'w'
-        encoding = settings.get('LOG_ENCODING')
+        mode = "a" if settings.getbool("LOG_FILE_APPEND") else "w"
+        encoding = settings.get("LOG_ENCODING")
         handler = logging.FileHandler(filename, mode=mode, encoding=encoding)
-    elif settings.getbool('LOG_ENABLED'):
+    elif settings.getbool("LOG_ENABLED"):
         handler = logging.StreamHandler()
     else:
         handler = logging.NullHandler()
 
     formatter = logging.Formatter(
-        fmt=settings.get('LOG_FORMAT'),
-        datefmt=settings.get('LOG_DATEFORMAT')
+        fmt=settings.get("LOG_FORMAT"), datefmt=settings.get("LOG_DATEFORMAT")
     )
     handler.setFormatter(formatter)
-    handler.setLevel(settings.get('LOG_LEVEL'))
-    if settings.getbool('LOG_SHORT_NAMES'):
-        handler.addFilter(TopLevelFormatter(['scrapy']))
+    handler.setLevel(settings.get("LOG_LEVEL"))
+    if settings.getbool("LOG_SHORT_NAMES"):
+        handler.addFilter(TopLevelFormatter(["scrapy"]))
     return handler
 
 
 def log_scrapy_info(settings: Settings) -> None:
-    logger.info("Scrapy %(version)s started (bot: %(bot)s)",
-                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})
+    logger.info(
+        "Scrapy %(version)s started (bot: %(bot)s)",
+        {"version": scrapy.__version__, "bot": settings["BOT_NAME"]},
+    )
     versions = [
         f"{name} {version}"
         for name, version in scrapy_components_versions()
         if name != "Scrapy"
     ]
-    logger.info("Versions: %(versions)s", {'versions': ", ".join(versions)})
+    logger.info("Versions: %(versions)s", {"versions": ", ".join(versions)})
 
 
 def log_reactor_info() -> None:
     from twisted.internet import reactor
+
     logger.debug("Using reactor: %s.%s", reactor.__module__, reactor.__class__.__name__)
     from twisted.internet import asyncioreactor
+
     if isinstance(reactor, asyncioreactor.AsyncioSelectorReactor):
         logger.debug(
             "Using asyncio event loop: %s.%s",
             reactor._asyncioEventloop.__module__,
             reactor._asyncioEventloop.__class__.__name__,
         )
 
 
 class StreamLogger:
     """Fake file-like stream object that redirects writes to a logger instance
 
     Taken from:
         https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/
     """
+
     def __init__(self, logger, log_level=logging.INFO):
         self.logger = logger
         self.log_level = log_level
-        self.linebuf = ''
+        self.linebuf = ""
 
     def write(self, buf):
         for line in buf.rstrip().splitlines():
             self.logger.log(self.log_level, line.rstrip())
 
     def flush(self):
         for h in self.logger.handlers:
@@ -190,33 +198,34 @@
     """Record log levels count into a crawler stats"""
 
     def __init__(self, crawler, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.crawler = crawler
 
     def emit(self, record):
-        sname = f'log_count/{record.levelname}'
+        sname = f"log_count/{record.levelname}"
         self.crawler.stats.inc_value(sname)
 
 
 def logformatter_adapter(logkws):
     """
     Helper that takes the dictionary output from the methods in LogFormatter
     and adapts it into a tuple of positional arguments for logger.log calls,
     handling backward compatibility as well.
     """
-    if not {'level', 'msg', 'args'} <= set(logkws):
-        warnings.warn('Missing keys in LogFormatter method',
-                      ScrapyDeprecationWarning)
-
-    if 'format' in logkws:
-        warnings.warn('`format` key in LogFormatter methods has been '
-                      'deprecated, use `msg` instead',
-                      ScrapyDeprecationWarning)
+    if not {"level", "msg", "args"} <= set(logkws):
+        warnings.warn("Missing keys in LogFormatter method", ScrapyDeprecationWarning)
+
+    if "format" in logkws:
+        warnings.warn(
+            "`format` key in LogFormatter methods has been "
+            "deprecated, use `msg` instead",
+            ScrapyDeprecationWarning,
+        )
 
-    level = logkws.get('level', logging.INFO)
-    message = logkws.get('format', logkws.get('msg'))
+    level = logkws.get("level", logging.INFO)
+    message = logkws.get("format", logkws.get("msg"))
     # NOTE: This also handles 'args' being an empty dict, that case doesn't
     # play well in logger.log calls
-    args = logkws if not logkws.get('args') else logkws['args']
+    args = logkws if not logkws.get("args") else logkws["args"]
 
     return (level, message, args)
```

### Comparing `Scrapy-2.7.1/scrapy/utils/misc.py` & `Scrapy-2.8.0/scrapy/utils/misc.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,43 +1,41 @@
 """Helper functions which don't fit anywhere else"""
 import ast
+import hashlib
 import inspect
 import os
 import re
-import hashlib
 import warnings
 from collections import deque
 from contextlib import contextmanager
+from functools import partial
 from importlib import import_module
 from pkgutil import iter_modules
-from functools import partial
 
 from w3lib.html import replace_entities
 
-from scrapy.utils.datatypes import LocalWeakReferencedCache
-from scrapy.utils.python import flatten, to_unicode
 from scrapy.item import Item
+from scrapy.utils.datatypes import LocalWeakReferencedCache
 from scrapy.utils.deprecate import ScrapyDeprecationWarning
-
+from scrapy.utils.python import flatten, to_unicode
 
 _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes
 
 
 def arg_to_iter(arg):
     """Convert an argument to an iterable. The argument can be a None, single
     value, or an iterable.
 
     Exception: if arg is a dict, [arg] will be returned
     """
     if arg is None:
         return []
-    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):
+    if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, "__iter__"):
         return arg
-    else:
-        return [arg]
+    return [arg]
 
 
 def load_object(path):
     """Load an object given its absolute object path, and return it.
 
     The object can be the import path of a class, function, variable or an
     instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.
@@ -45,24 +43,24 @@
     If ``path`` is not a string, but is a callable object, such as a class or
     a function, then return it as is.
     """
 
     if not isinstance(path, str):
         if callable(path):
             return path
-        else:
-            raise TypeError("Unexpected argument type, expected string "
-                            f"or object, got: {type(path)}")
+        raise TypeError(
+            "Unexpected argument type, expected string " f"or object, got: {type(path)}"
+        )
 
     try:
-        dot = path.rindex('.')
+        dot = path.rindex(".")
     except ValueError:
         raise ValueError(f"Error loading object '{path}': not a full path")
 
-    module, name = path[:dot], path[dot + 1:]
+    module, name = path[:dot], path[dot + 1 :]
     mod = import_module(module)
 
     try:
         obj = getattr(mod, name)
     except AttributeError:
         raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
 
@@ -76,52 +74,52 @@
 
     For example: walk_modules('scrapy.utils')
     """
 
     mods = []
     mod = import_module(path)
     mods.append(mod)
-    if hasattr(mod, '__path__'):
+    if hasattr(mod, "__path__"):
         for _, subpath, ispkg in iter_modules(mod.__path__):
-            fullpath = path + '.' + subpath
+            fullpath = path + "." + subpath
             if ispkg:
                 mods += walk_modules(fullpath)
             else:
                 submod = import_module(fullpath)
                 mods.append(submod)
     return mods
 
 
-def extract_regex(regex, text, encoding='utf-8'):
+def extract_regex(regex, text, encoding="utf-8"):
     """Extract a list of unicode strings from the given text/encoding using the following policies:
 
     * if the regex contains a named group called "extract" that will be returned
     * if the regex contains multiple numbered groups, all those will be returned (flattened)
     * if the regex doesn't contain any group the entire regex matching is returned
     """
     warnings.warn(
         "scrapy.utils.misc.extract_regex has moved to parsel.utils.extract_regex.",
         ScrapyDeprecationWarning,
-        stacklevel=2
+        stacklevel=2,
     )
 
     if isinstance(regex, str):
         regex = re.compile(regex, re.UNICODE)
 
     try:
-        strings = [regex.search(text).group('extract')]   # named group
+        strings = [regex.search(text).group("extract")]  # named group
     except Exception:
-        strings = regex.findall(text)    # full regex or numbered groups
+        strings = regex.findall(text)  # full regex or numbered groups
     strings = flatten(strings)
 
     if isinstance(text, str):
-        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]
-    else:
-        return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])
-                for s in strings]
+        return [replace_entities(s, keep=["lt", "amp"]) for s in strings]
+    return [
+        replace_entities(to_unicode(s, encoding), keep=["lt", "amp"]) for s in strings
+    ]
 
 
 def md5sum(file):
     """Calculate the md5 checksum of a file-like object without reading its
     whole content in memory.
 
     >>> from io import BytesIO
@@ -135,15 +133,15 @@
             break
         m.update(d)
     return m.hexdigest()
 
 
 def rel_has_nofollow(rel):
     """Return True if link rel attribute has nofollow type"""
-    return rel is not None and 'nofollow' in rel.replace(',', ' ').split()
+    return rel is not None and "nofollow" in rel.replace(",", " ").split()
 
 
 def create_instance(objcls, settings, crawler, *args, **kwargs):
     """Construct a class instance using its ``from_crawler`` or
     ``from_settings`` constructors, if available.
 
     At least one of ``settings`` and ``crawler`` needs to be different from
@@ -159,23 +157,23 @@
        Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
        extension has not been implemented correctly).
     """
     if settings is None:
         if crawler is None:
             raise ValueError("Specify at least one of settings and crawler.")
         settings = crawler.settings
-    if crawler and hasattr(objcls, 'from_crawler'):
+    if crawler and hasattr(objcls, "from_crawler"):
         instance = objcls.from_crawler(crawler, *args, **kwargs)
-        method_name = 'from_crawler'
-    elif hasattr(objcls, 'from_settings'):
+        method_name = "from_crawler"
+    elif hasattr(objcls, "from_settings"):
         instance = objcls.from_settings(settings, *args, **kwargs)
-        method_name = 'from_settings'
+        method_name = "from_settings"
     else:
         instance = objcls(*args, **kwargs)
-        method_name = '__new__'
+        method_name = "__new__"
     if instance is None:
         raise TypeError(f"{objcls.__qualname__}.{method_name} returned None")
     return instance
 
 
 @contextmanager
 def set_environ(**kwargs):
@@ -220,15 +218,17 @@
     'return' statement with a value different than None, False otherwise
     """
     if callable in _generator_callbacks_cache:
         return _generator_callbacks_cache[callable]
 
     def returns_none(return_node):
         value = return_node.value
-        return value is None or isinstance(value, ast.NameConstant) and value.value is None
+        return (
+            value is None or isinstance(value, ast.NameConstant) and value.value is None
+        )
 
     if inspect.isgeneratorfunction(callable):
         func = callable
         while isinstance(func, partial):
             func = func.func
 
         src = inspect.getsource(func)
@@ -255,22 +255,22 @@
     a 'return' statement with a value different than None
     """
     try:
         if is_generator_with_return_value(callable):
             warnings.warn(
                 f'The "{spider.__class__.__name__}.{callable.__name__}" method is '
                 'a generator and includes a "return" statement with a value '
-                'different than None. This could lead to unexpected behaviour. Please see '
-                'https://docs.python.org/3/reference/simple_stmts.html#the-return-statement '
+                "different than None. This could lead to unexpected behaviour. Please see "
+                "https://docs.python.org/3/reference/simple_stmts.html#the-return-statement "
                 'for details about the semantics of the "return" statement within generators',
                 stacklevel=2,
             )
     except IndentationError:
         callable_name = spider.__class__.__name__ + "." + callable.__name__
         warnings.warn(
             f'Unable to determine whether or not "{callable_name}" is a generator with a return value. '
-            'This will not prevent your code from working, but it prevents Scrapy from detecting '
+            "This will not prevent your code from working, but it prevents Scrapy from detecting "
             f'potential issues in your implementation of "{callable_name}". Please, report this in the '
-            'Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), '
+            "Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), "
             f'including the code of "{callable_name}"',
             stacklevel=2,
         )
```

### Comparing `Scrapy-2.7.1/scrapy/utils/ossignal.py` & `Scrapy-2.8.0/scrapy/utils/ossignal.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 import signal
 
-
 signal_names = {}
 for signame in dir(signal):
-    if signame.startswith('SIG') and not signame.startswith('SIG_'):
+    if signame.startswith("SIG") and not signame.startswith("SIG_"):
         signum = getattr(signal, signame)
         if isinstance(signum, int):
             signal_names[signum] = signame
 
 
 def install_shutdown_handlers(function, override_sigint=True):
     """Install the given function as a signal handler for all common shutdown
     signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the
     SIGINT handler won't be install if there is already a handler in place
     (e.g.  Pdb)
     """
     from twisted.internet import reactor
+
     reactor._handleSignals()
     signal.signal(signal.SIGTERM, function)
     if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:
         signal.signal(signal.SIGINT, function)
     # Catch Ctrl-Break in windows
-    if hasattr(signal, 'SIGBREAK'):
+    if hasattr(signal, "SIGBREAK"):
         signal.signal(signal.SIGBREAK, function)
```

### Comparing `Scrapy-2.7.1/scrapy/utils/project.py` & `Scrapy-2.8.0/scrapy/utils/project.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,89 +1,88 @@
 import os
 import warnings
-
 from importlib import import_module
-from os.path import join, dirname, abspath, isabs, exists
+from pathlib import Path
 
-from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env
+from scrapy.exceptions import NotConfigured
 from scrapy.settings import Settings
-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning
-
+from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env
 
-ENVVAR = 'SCRAPY_SETTINGS_MODULE'
-DATADIR_CFG_SECTION = 'datadir'
+ENVVAR = "SCRAPY_SETTINGS_MODULE"
+DATADIR_CFG_SECTION = "datadir"
 
 
 def inside_project():
-    scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')
+    scrapy_module = os.environ.get("SCRAPY_SETTINGS_MODULE")
     if scrapy_module is not None:
         try:
             import_module(scrapy_module)
         except ImportError as exc:
-            warnings.warn(f"Cannot import scrapy settings module {scrapy_module}: {exc}")
+            warnings.warn(
+                f"Cannot import scrapy settings module {scrapy_module}: {exc}"
+            )
         else:
             return True
     return bool(closest_scrapy_cfg())
 
 
-def project_data_dir(project='default'):
+def project_data_dir(project="default") -> str:
     """Return the current project data dir, creating it if it doesn't exist"""
     if not inside_project():
         raise NotConfigured("Not inside a project")
     cfg = get_config()
     if cfg.has_option(DATADIR_CFG_SECTION, project):
-        d = cfg.get(DATADIR_CFG_SECTION, project)
+        d = Path(cfg.get(DATADIR_CFG_SECTION, project))
     else:
         scrapy_cfg = closest_scrapy_cfg()
         if not scrapy_cfg:
-            raise NotConfigured("Unable to find scrapy.cfg file to infer project data dir")
-        d = abspath(join(dirname(scrapy_cfg), '.scrapy'))
-    if not exists(d):
-        os.makedirs(d)
-    return d
+            raise NotConfigured(
+                "Unable to find scrapy.cfg file to infer project data dir"
+            )
+        d = (Path(scrapy_cfg).parent / ".scrapy").resolve()
+    if not d.exists():
+        d.mkdir(parents=True)
+    return str(d)
 
 
-def data_path(path, createdir=False):
+def data_path(path: str, createdir=False) -> str:
     """
     Return the given path joined with the .scrapy data directory.
     If given an absolute path, return it unmodified.
     """
-    if not isabs(path):
+    path_obj = Path(path)
+    if not path_obj.is_absolute():
         if inside_project():
-            path = join(project_data_dir(), path)
+            path_obj = Path(project_data_dir(), path)
         else:
-            path = join('.scrapy', path)
-    if createdir and not exists(path):
-        os.makedirs(path)
-    return path
+            path_obj = Path(".scrapy", path)
+    if createdir and not path_obj.exists():
+        path_obj.mkdir(parents=True)
+    return str(path_obj)
 
 
 def get_project_settings():
     if ENVVAR not in os.environ:
-        project = os.environ.get('SCRAPY_PROJECT', 'default')
+        project = os.environ.get("SCRAPY_PROJECT", "default")
         init_env(project)
 
     settings = Settings()
     settings_module_path = os.environ.get(ENVVAR)
     if settings_module_path:
-        settings.setmodule(settings_module_path, priority='project')
+        settings.setmodule(settings_module_path, priority="project")
 
-    scrapy_envvars = {k[7:]: v for k, v in os.environ.items() if
-                      k.startswith('SCRAPY_')}
     valid_envvars = {
-        'CHECK',
-        'PROJECT',
-        'PYTHON_SHELL',
-        'SETTINGS_MODULE',
+        "CHECK",
+        "PROJECT",
+        "PYTHON_SHELL",
+        "SETTINGS_MODULE",
     }
-    setting_envvars = {k for k in scrapy_envvars if k not in valid_envvars}
-    if setting_envvars:
-        setting_envvar_list = ', '.join(sorted(setting_envvars))
-        warnings.warn(
-            'Use of environment variables prefixed with SCRAPY_ to override '
-            'settings is deprecated. The following environment variables are '
-            f'currently defined: {setting_envvar_list}',
-            ScrapyDeprecationWarning
-        )
-    settings.setdict(scrapy_envvars, priority='project')
+
+    scrapy_envvars = {
+        k[7:]: v
+        for k, v in os.environ.items()
+        if k.startswith("SCRAPY_") and k.replace("SCRAPY_", "") in valid_envvars
+    }
+
+    settings.setdict(scrapy_envvars, priority="project")
 
     return settings
```

### Comparing `Scrapy-2.7.1/scrapy/utils/python.py` & `Scrapy-2.8.0/scrapy/utils/python.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,24 +1,20 @@
 """
 This module contains essential stuff that should've come with Python itself ;)
 """
-import errno
 import gc
 import inspect
 import re
 import sys
-import warnings
 import weakref
 from functools import partial, wraps
 from itertools import chain
 from typing import AsyncGenerator, AsyncIterable, Iterable, Union
 
-from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.utils.asyncgen import as_async_generator
-from scrapy.utils.decorators import deprecated
 
 
 def flatten(x):
     """flatten(sequence) -> list
 
     Returns a single, flat list which contains all elements retrieved
     from the sequence and all recursively contained sub-sequences
@@ -82,46 +78,43 @@
         if seenkey in seen:
             continue
         seen.add(seenkey)
         result.append(item)
     return result
 
 
-def to_unicode(text, encoding=None, errors='strict'):
+def to_unicode(text, encoding=None, errors="strict"):
     """Return the unicode representation of a bytes object ``text``. If
     ``text`` is already an unicode object, return it as-is."""
     if isinstance(text, str):
         return text
     if not isinstance(text, (bytes, str)):
-        raise TypeError('to_unicode must receive a bytes or str '
-                        f'object, got {type(text).__name__}')
+        raise TypeError(
+            "to_unicode must receive a bytes or str "
+            f"object, got {type(text).__name__}"
+        )
     if encoding is None:
-        encoding = 'utf-8'
+        encoding = "utf-8"
     return text.decode(encoding, errors)
 
 
-def to_bytes(text, encoding=None, errors='strict'):
+def to_bytes(text, encoding=None, errors="strict"):
     """Return the binary representation of ``text``. If ``text``
     is already a bytes object, return it as-is."""
     if isinstance(text, bytes):
         return text
     if not isinstance(text, str):
-        raise TypeError('to_bytes must receive a str or bytes '
-                        f'object, got {type(text).__name__}')
+        raise TypeError(
+            "to_bytes must receive a str or bytes " f"object, got {type(text).__name__}"
+        )
     if encoding is None:
-        encoding = 'utf-8'
+        encoding = "utf-8"
     return text.encode(encoding, errors)
 
 
-@deprecated('to_unicode')
-def to_native_str(text, encoding=None, errors='strict'):
-    """ Return str representation of ``text``. """
-    return to_unicode(text, encoding, errors)
-
-
 def re_rsearch(pattern, text, chunk_size=1024):
     """
     This function does a reverse search in a text using a regular expression
     given in the attribute 'pattern'.
     Since the re module does not provide this functionality, we have to find for
     the expression into chunks of text extracted from the end (for the sake of efficiency).
     At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for
@@ -131,15 +124,15 @@
     In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing
     the start position of the match, and the ending (regarding the entire text).
     """
 
     def _chunk_iter():
         offset = len(text)
         while True:
-            offset -= (chunk_size * 1024)
+            offset -= chunk_size * 1024
             if offset <= 0:
                 break
             yield (text[offset:], offset)
         yield (text, 0)
 
     if isinstance(pattern, str):
         pattern = re.compile(pattern)
@@ -168,15 +161,15 @@
 
 
 _BINARYCHARS = {to_bytes(chr(i)) for i in range(32)} - {b"\0", b"\t", b"\n", b"\r"}
 _BINARYCHARS |= {ord(ch) for ch in _BINARYCHARS}
 
 
 def binary_is_text(data):
-    """ Returns ``True`` if the given ``data`` argument (a ``bytes`` object)
+    """Returns ``True`` if the given ``data`` argument (a ``bytes`` object)
     does not contain unprintable control characters.
     """
     if not isinstance(data, bytes):
         raise TypeError(f"data must be bytes, got '{type(data).__name__}'")
     return all(c not in _BINARYCHARS for c in data)
 
 
@@ -188,25 +181,27 @@
     elif inspect.isclass(func):
         return get_func_args(func.__init__, True)
     elif inspect.ismethod(func):
         return get_func_args(func.__func__, True)
     elif inspect.ismethoddescriptor(func):
         return []
     elif isinstance(func, partial):
-        return [x for x in get_func_args(func.func)[len(func.args):]
-                if not (func.keywords and x in func.keywords)]
-    elif hasattr(func, '__call__'):
+        return [
+            x
+            for x in get_func_args(func.func)[len(func.args) :]
+            if not (func.keywords and x in func.keywords)
+        ]
+    elif hasattr(func, "__call__"):
         if inspect.isroutine(func):
             return []
-        elif getattr(func, '__name__', None) == '__call__':
+        if getattr(func, "__name__", None) == "__call__":
             return []
-        else:
-            return get_func_args(func.__call__, True)
+        return get_func_args(func.__call__, True)
     else:
-        raise TypeError(f'{type(func)} is not callable')
+        raise TypeError(f"{type(func)} is not callable")
     if stripself:
         func_args.pop(0)
     return func_args
 
 
 def get_spec(func):
     """Returns (args, kwargs) tuple for a function
@@ -228,18 +223,18 @@
 
     >>> get_spec(Test().method)
     (['self', 'val'], {'flags': 0})
     """
 
     if inspect.isfunction(func) or inspect.ismethod(func):
         spec = inspect.getfullargspec(func)
-    elif hasattr(func, '__call__'):
+    elif hasattr(func, "__call__"):
         spec = inspect.getfullargspec(func.__call__)
     else:
-        raise TypeError(f'{type(func)} is not callable')
+        raise TypeError(f"{type(func)} is not callable")
 
     defaults = spec.defaults or []
 
     firstdefault = len(spec.args) - len(defaults)
     args = spec.args[:firstdefault]
     kwargs = dict(zip(spec.args[firstdefault:], defaults))
     return args, kwargs
@@ -259,38 +254,14 @@
                 return False
         elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):
             return False
     # all attributes equal
     return True
 
 
-class WeakKeyCache:
-
-    def __init__(self, default_factory):
-        warnings.warn("The WeakKeyCache class is deprecated", category=ScrapyDeprecationWarning, stacklevel=2)
-        self.default_factory = default_factory
-        self._weakdict = weakref.WeakKeyDictionary()
-
-    def __getitem__(self, key):
-        if key not in self._weakdict:
-            self._weakdict[key] = self.default_factory(key)
-        return self._weakdict[key]
-
-
-@deprecated
-def retry_on_eintr(function, *args, **kw):
-    """Run a function and retry it while getting EINTR errors"""
-    while True:
-        try:
-            return function(*args, **kw)
-        except IOError as e:
-            if e.errno != errno.EINTR:
-                raise
-
-
 def without_none_values(iterable):
     """Return a copy of ``iterable`` with all ``None`` entries removed.
 
     If ``iterable`` is a mapping, return a dictionary where all pairs that have
     value ``None`` have been removed.
     """
     try:
@@ -307,19 +278,22 @@
     >>> global_object_name(Request)
     'scrapy.http.request.Request'
     """
     return f"{obj.__module__}.{obj.__name__}"
 
 
 if hasattr(sys, "pypy_version_info"):
+
     def garbage_collect():
         # Collecting weakreferences can take two collections on PyPy.
         gc.collect()
         gc.collect()
+
 else:
+
     def garbage_collect():
         gc.collect()
 
 
 class MutableChain(Iterable):
     """
     Thin wrapper around itertools.chain, allowing to add iterables "in-place"
@@ -333,18 +307,14 @@
 
     def __iter__(self):
         return self
 
     def __next__(self):
         return next(self.data)
 
-    @deprecated("scrapy.utils.python.MutableChain.__next__")
-    def next(self):
-        return self.__next__()
-
 
 async def _async_chain(*iterables: Union[Iterable, AsyncIterable]) -> AsyncGenerator:
     for it in iterables:
         async for o in as_async_generator(it):
             yield o
```

### Comparing `Scrapy-2.7.1/scrapy/utils/reqser.py` & `Scrapy-2.8.0/scrapy/utils/reqser.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,22 +1,27 @@
 import warnings
 from typing import Optional
 
 import scrapy
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.utils.request import request_from_dict as _from_dict
 
-
 warnings.warn(
-    ("Module scrapy.utils.reqser is deprecated, please use request.to_dict method"
-     " and/or scrapy.utils.request.request_from_dict instead"),
+    (
+        "Module scrapy.utils.reqser is deprecated, please use request.to_dict method"
+        " and/or scrapy.utils.request.request_from_dict instead"
+    ),
     category=ScrapyDeprecationWarning,
     stacklevel=2,
 )
 
 
-def request_to_dict(request: "scrapy.Request", spider: Optional["scrapy.Spider"] = None) -> dict:
+def request_to_dict(
+    request: "scrapy.Request", spider: Optional["scrapy.Spider"] = None
+) -> dict:
     return request.to_dict(spider=spider)
 
 
-def request_from_dict(d: dict, spider: Optional["scrapy.Spider"] = None) -> "scrapy.Request":
+def request_from_dict(
+    d: dict, spider: Optional["scrapy.Spider"] = None
+) -> "scrapy.Request":
     return _from_dict(d, spider=spider)
```

### Comparing `Scrapy-2.7.1/scrapy/utils/request.py` & `Scrapy-2.8.0/scrapy/utils/request.py`

 * *Files 20% similar despite different names*

```diff
@@ -15,15 +15,14 @@
 
 from scrapy import Request, Spider
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.misc import load_object
 from scrapy.utils.python import to_bytes, to_unicode
 
-
 _deprecated_fingerprint_cache: "WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]"
 _deprecated_fingerprint_cache = WeakKeyDictionary()
 
 
 def _serialize_headers(headers, request):
     for header in headers:
         if header in request.headers:
@@ -65,76 +64,78 @@
     Also, servers usually ignore fragments in urls when handling requests,
     so they are also ignored by default when calculating the fingerprint.
     If you want to include them, set the keep_fragments argument to True
     (for instance when handling requests with a headless browser).
     """
     if include_headers or keep_fragments:
         message = (
-            'Call to deprecated function '
-            'scrapy.utils.request.request_fingerprint().\n'
-            '\n'
-            'If you are using this function in a Scrapy component because you '
-            'need a non-default fingerprinting algorithm, and you are OK '
-            'with that non-default fingerprinting algorithm being used by '
-            'all Scrapy components and not just the one calling this '
-            'function, use crawler.request_fingerprinter.fingerprint() '
-            'instead in your Scrapy component (you can get the crawler '
-            'object from the \'from_crawler\' class method), and use the '
-            '\'REQUEST_FINGERPRINTER_CLASS\' setting to configure your '
-            'non-default fingerprinting algorithm.\n'
-            '\n'
-            'Otherwise, consider using the '
-            'scrapy.utils.request.fingerprint() function instead.\n'
-            '\n'
-            'If you switch to \'fingerprint()\', or assign the '
-            '\'REQUEST_FINGERPRINTER_CLASS\' setting a class that uses '
-            '\'fingerprint()\', the generated fingerprints will not only be '
-            'bytes instead of a string, but they will also be different from '
-            'those generated by \'request_fingerprint()\'. Before you switch, '
-            'make sure that you understand the consequences of this (e.g. '
-            'cache invalidation) and are OK with them; otherwise, consider '
-            'implementing your own function which returns the same '
-            'fingerprints as the deprecated \'request_fingerprint()\' function.'
+            "Call to deprecated function "
+            "scrapy.utils.request.request_fingerprint().\n"
+            "\n"
+            "If you are using this function in a Scrapy component because you "
+            "need a non-default fingerprinting algorithm, and you are OK "
+            "with that non-default fingerprinting algorithm being used by "
+            "all Scrapy components and not just the one calling this "
+            "function, use crawler.request_fingerprinter.fingerprint() "
+            "instead in your Scrapy component (you can get the crawler "
+            "object from the 'from_crawler' class method), and use the "
+            "'REQUEST_FINGERPRINTER_CLASS' setting to configure your "
+            "non-default fingerprinting algorithm.\n"
+            "\n"
+            "Otherwise, consider using the "
+            "scrapy.utils.request.fingerprint() function instead.\n"
+            "\n"
+            "If you switch to 'fingerprint()', or assign the "
+            "'REQUEST_FINGERPRINTER_CLASS' setting a class that uses "
+            "'fingerprint()', the generated fingerprints will not only be "
+            "bytes instead of a string, but they will also be different from "
+            "those generated by 'request_fingerprint()'. Before you switch, "
+            "make sure that you understand the consequences of this (e.g. "
+            "cache invalidation) and are OK with them; otherwise, consider "
+            "implementing your own function which returns the same "
+            "fingerprints as the deprecated 'request_fingerprint()' function."
         )
     else:
         message = (
-            'Call to deprecated function '
-            'scrapy.utils.request.request_fingerprint().\n'
-            '\n'
-            'If you are using this function in a Scrapy component, and you '
-            'are OK with users of your component changing the fingerprinting '
-            'algorithm through settings, use '
-            'crawler.request_fingerprinter.fingerprint() instead in your '
-            'Scrapy component (you can get the crawler object from the '
-            '\'from_crawler\' class method).\n'
-            '\n'
-            'Otherwise, consider using the '
-            'scrapy.utils.request.fingerprint() function instead.\n'
-            '\n'
-            'Either way, the resulting fingerprints will be returned as '
-            'bytes, not as a string, and they will also be different from '
-            'those generated by \'request_fingerprint()\'. Before you switch, '
-            'make sure that you understand the consequences of this (e.g. '
-            'cache invalidation) and are OK with them; otherwise, consider '
-            'implementing your own function which returns the same '
-            'fingerprints as the deprecated \'request_fingerprint()\' function.'
+            "Call to deprecated function "
+            "scrapy.utils.request.request_fingerprint().\n"
+            "\n"
+            "If you are using this function in a Scrapy component, and you "
+            "are OK with users of your component changing the fingerprinting "
+            "algorithm through settings, use "
+            "crawler.request_fingerprinter.fingerprint() instead in your "
+            "Scrapy component (you can get the crawler object from the "
+            "'from_crawler' class method).\n"
+            "\n"
+            "Otherwise, consider using the "
+            "scrapy.utils.request.fingerprint() function instead.\n"
+            "\n"
+            "Either way, the resulting fingerprints will be returned as "
+            "bytes, not as a string, and they will also be different from "
+            "those generated by 'request_fingerprint()'. Before you switch, "
+            "make sure that you understand the consequences of this (e.g. "
+            "cache invalidation) and are OK with them; otherwise, consider "
+            "implementing your own function which returns the same "
+            "fingerprints as the deprecated 'request_fingerprint()' function."
         )
     warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)
     processed_include_headers: Optional[Tuple[bytes, ...]] = None
     if include_headers:
         processed_include_headers = tuple(
             to_bytes(h.lower()) for h in sorted(include_headers)
         )
     cache = _deprecated_fingerprint_cache.setdefault(request, {})
     cache_key = (processed_include_headers, keep_fragments)
     if cache_key not in cache:
         fp = hashlib.sha1()
         fp.update(to_bytes(request.method))
-        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))
-        fp.update(request.body or b'')
+        fp.update(
+            to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments))
+        )
+        fp.update(request.body or b"")
         if processed_include_headers:
             for part in _serialize_headers(processed_include_headers, request):
                 fp.update(part)
         cache[cache_key] = fp.hexdigest()
     return cache[cache_key]
 
 
@@ -199,18 +200,18 @@
             for header in processed_include_headers:
                 if header in request.headers:
                     headers[header.hex()] = [
                         header_value.hex()
                         for header_value in request.headers.getlist(header)
                     ]
         fingerprint_data = {
-            'method': to_unicode(request.method),
-            'url': canonicalize_url(request.url, keep_fragments=keep_fragments),
-            'body': (request.body or b'').hex(),
-            'headers': headers,
+            "method": to_unicode(request.method),
+            "url": canonicalize_url(request.url, keep_fragments=keep_fragments),
+            "body": (request.body or b"").hex(),
+            "headers": headers,
         }
         fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)
         cache[cache_key] = hashlib.sha1(fingerprint_json.encode()).digest()
     return cache[cache_key]
 
 
 class RequestFingerprinter:
@@ -229,83 +230,83 @@
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler)
 
     def __init__(self, crawler=None):
         if crawler:
             implementation = crawler.settings.get(
-                'REQUEST_FINGERPRINTER_IMPLEMENTATION'
+                "REQUEST_FINGERPRINTER_IMPLEMENTATION"
             )
         else:
-            implementation = '2.6'
-        if implementation == '2.6':
+            implementation = "2.6"
+        if implementation == "2.6":
             message = (
-                '\'2.6\' is a deprecated value for the '
-                '\'REQUEST_FINGERPRINTER_IMPLEMENTATION\' setting.\n'
-                '\n'
-                'It is also the default value. In other words, it is normal '
-                'to get this warning if you have not defined a value for the '
-                '\'REQUEST_FINGERPRINTER_IMPLEMENTATION\' setting. This is so '
-                'for backward compatibility reasons, but it will change in a '
-                'future version of Scrapy.\n'
-                '\n'
-                'See the documentation of the '
-                '\'REQUEST_FINGERPRINTER_IMPLEMENTATION\' setting for '
-                'information on how to handle this deprecation.'
+                "'2.6' is a deprecated value for the "
+                "'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n"
+                "\n"
+                "It is also the default value. In other words, it is normal "
+                "to get this warning if you have not defined a value for the "
+                "'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so "
+                "for backward compatibility reasons, but it will change in a "
+                "future version of Scrapy.\n"
+                "\n"
+                "See the documentation of the "
+                "'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for "
+                "information on how to handle this deprecation."
             )
             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)
             self._fingerprint = _request_fingerprint_as_bytes
-        elif implementation == '2.7':
+        elif implementation == "2.7":
             self._fingerprint = fingerprint
         else:
             raise ValueError(
-                f'Got an invalid value on setting '
-                f'\'REQUEST_FINGERPRINTER_IMPLEMENTATION\': '
-                f'{implementation!r}. Valid values are \'2.6\' (deprecated) '
-                f'and \'2.7\'.'
+                f"Got an invalid value on setting "
+                f"'REQUEST_FINGERPRINTER_IMPLEMENTATION': "
+                f"{implementation!r}. Valid values are '2.6' (deprecated) "
+                f"and '2.7'."
             )
 
-    def fingerprint(self, request):
+    def fingerprint(self, request: Request):
         return self._fingerprint(request)
 
 
 def request_authenticate(
     request: Request,
     username: str,
     password: str,
 ) -> None:
     """Authenticate the given request (in place) using the HTTP basic access
     authentication mechanism (RFC 2617) and the given username and password
     """
-    request.headers['Authorization'] = basic_auth_header(username, password)
+    request.headers["Authorization"] = basic_auth_header(username, password)
 
 
 def request_httprepr(request: Request) -> bytes:
     """Return the raw HTTP representation (as bytes) of the given request.
     This is provided only for reference since it's not the actual stream of
     bytes that will be send when performing the request (that's controlled
     by Twisted).
     """
     parsed = urlparse_cached(request)
-    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))
+    path = urlunparse(("", "", parsed.path or "/", parsed.params, parsed.query, ""))
     s = to_bytes(request.method) + b" " + to_bytes(path) + b" HTTP/1.1\r\n"
-    s += b"Host: " + to_bytes(parsed.hostname or b'') + b"\r\n"
+    s += b"Host: " + to_bytes(parsed.hostname or b"") + b"\r\n"
     if request.headers:
         s += request.headers.to_string() + b"\r\n"
     s += b"\r\n"
     s += request.body
     return s
 
 
 def referer_str(request: Request) -> Optional[str]:
-    """ Return Referer HTTP header suitable for logging. """
-    referrer = request.headers.get('Referer')
+    """Return Referer HTTP header suitable for logging."""
+    referrer = request.headers.get("Referer")
     if referrer is None:
         return referrer
-    return to_unicode(referrer, errors='replace')
+    return to_unicode(referrer, errors="replace")
 
 
 def request_from_dict(d: dict, *, spider: Optional[Spider] = None) -> Request:
     """Create a :class:`~scrapy.Request` object from a dict.
 
     If a spider is given, it will try to resolve the callbacks looking at the
     spider for methods with the same name.
```

### Comparing `Scrapy-2.7.1/scrapy/utils/response.py` & `Scrapy-2.8.0/scrapy/utils/response.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,94 +5,100 @@
 import os
 import re
 import tempfile
 import webbrowser
 from typing import Any, Callable, Iterable, Optional, Tuple, Union
 from weakref import WeakKeyDictionary
 
-import scrapy
-from scrapy.http.response import Response
-
 from twisted.web import http
-from scrapy.utils.python import to_bytes, to_unicode
-from scrapy.utils.decorators import deprecated
 from w3lib import html
 
+import scrapy
+from scrapy.http.response import Response
+from scrapy.utils.decorators import deprecated
+from scrapy.utils.python import to_bytes, to_unicode
 
 _baseurl_cache: "WeakKeyDictionary[Response, str]" = WeakKeyDictionary()
 
 
 def get_base_url(response: "scrapy.http.response.text.TextResponse") -> str:
     """Return the base url of the given response, joined with the response url"""
     if response not in _baseurl_cache:
         text = response.text[0:4096]
-        _baseurl_cache[response] = html.get_base_url(text, response.url, response.encoding)
+        _baseurl_cache[response] = html.get_base_url(
+            text, response.url, response.encoding
+        )
     return _baseurl_cache[response]
 
 
-_metaref_cache: "WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]" = WeakKeyDictionary()
+_metaref_cache: "WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]" = (
+    WeakKeyDictionary()
+)
 
 
 def get_meta_refresh(
     response: "scrapy.http.response.text.TextResponse",
-    ignore_tags: Optional[Iterable[str]] = ('script', 'noscript'),
+    ignore_tags: Optional[Iterable[str]] = ("script", "noscript"),
 ) -> Union[Tuple[None, None], Tuple[float, str]]:
-    """Parse the http-equiv refrsh parameter from the given response"""
+    """Parse the http-equiv refresh parameter from the given response"""
     if response not in _metaref_cache:
         text = response.text[0:4096]
         _metaref_cache[response] = html.get_meta_refresh(
-            text, response.url, response.encoding, ignore_tags=ignore_tags)
+            text, response.url, response.encoding, ignore_tags=ignore_tags
+        )
     return _metaref_cache[response]
 
 
 def response_status_message(status: Union[bytes, float, int, str]) -> str:
-    """Return status code plus status text descriptive message
-    """
+    """Return status code plus status text descriptive message"""
     status_int = int(status)
     message = http.RESPONSES.get(status_int, "Unknown Status")
-    return f'{status_int} {to_unicode(message)}'
+    return f"{status_int} {to_unicode(message)}"
 
 
 @deprecated
 def response_httprepr(response: Response) -> bytes:
     """Return raw HTTP representation (as bytes) of the given response. This
     is provided only for reference, since it's not the exact stream of bytes
     that was received (that's not exposed by Twisted).
     """
     values = [
         b"HTTP/1.1 ",
         to_bytes(str(response.status)),
         b" ",
-        to_bytes(http.RESPONSES.get(response.status, b'')),
+        to_bytes(http.RESPONSES.get(response.status, b"")),
         b"\r\n",
     ]
     if response.headers:
         values.extend([response.headers.to_string(), b"\r\n"])
     values.extend([b"\r\n", response.body])
     return b"".join(values)
 
 
 def open_in_browser(
-    response: Union["scrapy.http.response.html.HtmlResponse", "scrapy.http.response.text.TextResponse"],
+    response: Union[
+        "scrapy.http.response.html.HtmlResponse",
+        "scrapy.http.response.text.TextResponse",
+    ],
     _openfunc: Callable[[str], Any] = webbrowser.open,
 ) -> Any:
     """Open the given response in a local web browser, populating the <base>
     tag for external links to work
     """
     from scrapy.http import HtmlResponse, TextResponse
+
     # XXX: this implementation is a bit dirty and could be improved
     body = response.body
     if isinstance(response, HtmlResponse):
-        if b'<base' not in body:
-            repl = fr'\1<base href="{response.url}">'
+        if b"<base" not in body:
+            repl = rf'\1<base href="{response.url}">'
             body = re.sub(b"<!--.*?-->", b"", body, flags=re.DOTALL)
             body = re.sub(rb"(<head(?:>|\s.*?>))", to_bytes(repl), body)
-        ext = '.html'
+        ext = ".html"
     elif isinstance(response, TextResponse):
-        ext = '.txt'
+        ext = ".txt"
     else:
-        raise TypeError("Unsupported response type: "
-                        f"{response.__class__.__name__}")
+        raise TypeError("Unsupported response type: " f"{response.__class__.__name__}")
     fd, fname = tempfile.mkstemp(ext)
     os.write(fd, body)
     os.close(fd)
     return _openfunc(f"file://{fname}")
```

### Comparing `Scrapy-2.7.1/scrapy/utils/serialize.py` & `Scrapy-2.8.0/scrapy/utils/serialize.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,40 +1,39 @@
-import json
 import datetime
 import decimal
+import json
 
-from itemadapter import is_item, ItemAdapter
+from itemadapter import ItemAdapter, is_item
 from twisted.internet import defer
 
 from scrapy.http import Request, Response
 
 
 class ScrapyJSONEncoder(json.JSONEncoder):
 
     DATE_FORMAT = "%Y-%m-%d"
     TIME_FORMAT = "%H:%M:%S"
 
     def default(self, o):
         if isinstance(o, set):
             return list(o)
-        elif isinstance(o, datetime.datetime):
+        if isinstance(o, datetime.datetime):
             return o.strftime(f"{self.DATE_FORMAT} {self.TIME_FORMAT}")
-        elif isinstance(o, datetime.date):
+        if isinstance(o, datetime.date):
             return o.strftime(self.DATE_FORMAT)
-        elif isinstance(o, datetime.time):
+        if isinstance(o, datetime.time):
             return o.strftime(self.TIME_FORMAT)
-        elif isinstance(o, decimal.Decimal):
+        if isinstance(o, decimal.Decimal):
             return str(o)
-        elif isinstance(o, defer.Deferred):
+        if isinstance(o, defer.Deferred):
             return str(o)
-        elif is_item(o):
+        if is_item(o):
             return ItemAdapter(o).asdict()
-        elif isinstance(o, Request):
+        if isinstance(o, Request):
             return f"<{type(o).__name__} {o.method} {o.url}>"
-        elif isinstance(o, Response):
+        if isinstance(o, Response):
             return f"<{type(o).__name__} {o.status} {o.url}>"
-        else:
-            return super().default(o)
+        return super().default(o)
 
 
 class ScrapyJSONDecoder(json.JSONDecoder):
     pass
```

### Comparing `Scrapy-2.7.1/scrapy/utils/signal.py` & `Scrapy-2.8.0/scrapy/utils/signal.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,71 +1,92 @@
 """Helper functions for working with signals"""
 import collections.abc
 import logging
 
-from twisted.internet.defer import DeferredList, Deferred
-from twisted.python.failure import Failure
-
-from pydispatch.dispatcher import Anonymous, Any, disconnect, getAllReceivers, liveReceivers
+from pydispatch.dispatcher import (
+    Anonymous,
+    Any,
+    disconnect,
+    getAllReceivers,
+    liveReceivers,
+)
 from pydispatch.robustapply import robustApply
+from twisted.internet.defer import Deferred, DeferredList
+from twisted.python.failure import Failure
 
 from scrapy.exceptions import StopDownload
 from scrapy.utils.defer import maybeDeferred_coro
 from scrapy.utils.log import failure_to_exc_info
 
-
 logger = logging.getLogger(__name__)
 
 
 def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):
     """Like pydispatcher.robust.sendRobust but it also logs errors and returns
     Failures instead of exceptions.
     """
-    dont_log = named.pop('dont_log', ())
-    dont_log = tuple(dont_log) if isinstance(dont_log, collections.abc.Sequence) else (dont_log,)
-    dont_log += (StopDownload, )
-    spider = named.get('spider', None)
+    dont_log = named.pop("dont_log", ())
+    dont_log = (
+        tuple(dont_log)
+        if isinstance(dont_log, collections.abc.Sequence)
+        else (dont_log,)
+    )
+    dont_log += (StopDownload,)
+    spider = named.get("spider", None)
     responses = []
     for receiver in liveReceivers(getAllReceivers(sender, signal)):
         try:
-            response = robustApply(receiver, signal=signal, sender=sender, *arguments, **named)
+            response = robustApply(
+                receiver, signal=signal, sender=sender, *arguments, **named
+            )
             if isinstance(response, Deferred):
-                logger.error("Cannot return deferreds from signal handler: %(receiver)s",
-                             {'receiver': receiver}, extra={'spider': spider})
+                logger.error(
+                    "Cannot return deferreds from signal handler: %(receiver)s",
+                    {"receiver": receiver},
+                    extra={"spider": spider},
+                )
         except dont_log:
             result = Failure()
         except Exception:
             result = Failure()
-            logger.error("Error caught on signal handler: %(receiver)s",
-                         {'receiver': receiver},
-                         exc_info=True, extra={'spider': spider})
+            logger.error(
+                "Error caught on signal handler: %(receiver)s",
+                {"receiver": receiver},
+                exc_info=True,
+                extra={"spider": spider},
+            )
         else:
             result = response
         responses.append((receiver, result))
     return responses
 
 
 def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):
     """Like send_catch_log but supports returning deferreds on signal handlers.
     Returns a deferred that gets fired once all signal handlers deferreds were
     fired.
     """
+
     def logerror(failure, recv):
         if dont_log is None or not isinstance(failure.value, dont_log):
-            logger.error("Error caught on signal handler: %(receiver)s",
-                         {'receiver': recv},
-                         exc_info=failure_to_exc_info(failure),
-                         extra={'spider': spider})
+            logger.error(
+                "Error caught on signal handler: %(receiver)s",
+                {"receiver": recv},
+                exc_info=failure_to_exc_info(failure),
+                extra={"spider": spider},
+            )
         return failure
 
-    dont_log = named.pop('dont_log', None)
-    spider = named.get('spider', None)
+    dont_log = named.pop("dont_log", None)
+    spider = named.get("spider", None)
     dfds = []
     for receiver in liveReceivers(getAllReceivers(sender, signal)):
-        d = maybeDeferred_coro(robustApply, receiver, signal=signal, sender=sender, *arguments, **named)
+        d = maybeDeferred_coro(
+            robustApply, receiver, signal=signal, sender=sender, *arguments, **named
+        )
         d.addErrback(logerror, receiver)
         d.addBoth(lambda result: (receiver, result))
         dfds.append(d)
     d = DeferredList(dfds)
     d.addCallback(lambda out: [x[1] for x in out])
     return d
```

### Comparing `Scrapy-2.7.1/scrapy/utils/sitemap.py` & `Scrapy-2.8.0/scrapy/utils/sitemap.py`

 * *Files 14% similar despite different names*

```diff
@@ -11,37 +11,39 @@
 
 
 class Sitemap:
     """Class to parse Sitemap (type=urlset) and Sitemap Index
     (type=sitemapindex) files"""
 
     def __init__(self, xmltext):
-        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)
+        xmlp = lxml.etree.XMLParser(
+            recover=True, remove_comments=True, resolve_entities=False
+        )
         self._root = lxml.etree.fromstring(xmltext, parser=xmlp)
         rt = self._root.tag
-        self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt
+        self.type = self._root.tag.split("}", 1)[1] if "}" in rt else rt
 
     def __iter__(self):
         for elem in self._root.getchildren():
             d = {}
             for el in elem.getchildren():
                 tag = el.tag
-                name = tag.split('}', 1)[1] if '}' in tag else tag
+                name = tag.split("}", 1)[1] if "}" in tag else tag
 
-                if name == 'link':
-                    if 'href' in el.attrib:
-                        d.setdefault('alternate', []).append(el.get('href'))
+                if name == "link":
+                    if "href" in el.attrib:
+                        d.setdefault("alternate", []).append(el.get("href"))
                 else:
-                    d[name] = el.text.strip() if el.text else ''
+                    d[name] = el.text.strip() if el.text else ""
 
-            if 'loc' in d:
+            if "loc" in d:
                 yield d
 
 
 def sitemap_urls_from_robots(robots_text, base_url=None):
     """Return an iterator over all sitemap urls contained in the given
     robots.txt file
     """
     for line in robots_text.splitlines():
-        if line.lstrip().lower().startswith('sitemap:'):
-            url = line.split(':', 1)[1].strip()
+        if line.lstrip().lower().startswith("sitemap:"):
+            url = line.split(":", 1)[1].strip()
             yield urljoin(base_url, url)
```

### Comparing `Scrapy-2.7.1/scrapy/utils/spider.py` & `Scrapy-2.8.0/scrapy/utils/spider.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,27 +1,25 @@
 import inspect
 import logging
 
 from scrapy.spiders import Spider
 from scrapy.utils.defer import deferred_from_coro
 from scrapy.utils.misc import arg_to_iter
 
-
 logger = logging.getLogger(__name__)
 
 
 def iterate_spider_output(result):
     if inspect.isasyncgen(result):
         return result
-    elif inspect.iscoroutine(result):
+    if inspect.iscoroutine(result):
         d = deferred_from_coro(result)
         d.addCallback(iterate_spider_output)
         return d
-    else:
-        return arg_to_iter(deferred_from_coro(result))
+    return arg_to_iter(deferred_from_coro(result))
 
 
 def iter_spider_classes(module):
     """Return an iterator over all spider classes defined in the given module
     that can be instantiated (i.e. which have name)
     """
     # this needs to be imported here until get rid of the spider manager
@@ -29,21 +27,22 @@
     from scrapy.spiders import Spider
 
     for obj in vars(module).values():
         if (
             inspect.isclass(obj)
             and issubclass(obj, Spider)
             and obj.__module__ == module.__name__
-            and getattr(obj, 'name', None)
+            and getattr(obj, "name", None)
         ):
             yield obj
 
 
-def spidercls_for_request(spider_loader, request, default_spidercls=None,
-                          log_none=False, log_multiple=False):
+def spidercls_for_request(
+    spider_loader, request, default_spidercls=None, log_none=False, log_multiple=False
+):
     """Return a spider class that handles the given Request.
 
     This will look for the spiders that can handle the given request (using
     the spider loader) and return a Spider class if (and only if) there is
     only one Spider able to handle the Request.
 
     If multiple spiders (or no spider) are found, it will return the
@@ -51,19 +50,22 @@
     are found.
     """
     snames = spider_loader.find_by_request(request)
     if len(snames) == 1:
         return spider_loader.load(snames[0])
 
     if len(snames) > 1 and log_multiple:
-        logger.error('More than one spider can handle: %(request)s - %(snames)s',
-                     {'request': request, 'snames': ', '.join(snames)})
+        logger.error(
+            "More than one spider can handle: %(request)s - %(snames)s",
+            {"request": request, "snames": ", ".join(snames)},
+        )
 
     if len(snames) == 0 and log_none:
-        logger.error('Unable to find spider that handles: %(request)s',
-                     {'request': request})
+        logger.error(
+            "Unable to find spider that handles: %(request)s", {"request": request}
+        )
 
     return default_spidercls
 
 
 class DefaultSpider(Spider):
-    name = 'default'
+    name = "default"
```

### Comparing `Scrapy-2.7.1/scrapy/utils/ssl.py` & `Scrapy-2.8.0/scrapy/utils/ssl.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,61 +1,57 @@
-import OpenSSL
 import OpenSSL._util as pyOpenSSLutil
+import OpenSSL.SSL
 
 from scrapy.utils.python import to_unicode
 
 
-# The OpenSSL symbol is present since 1.1.1 but it's not currently supported in any version of pyOpenSSL.
-# Using the binding directly, as this code does, requires cryptography 2.4.
-SSL_OP_NO_TLSv1_3 = getattr(pyOpenSSLutil.lib, 'SSL_OP_NO_TLSv1_3', 0)
-
-
 def ffi_buf_to_string(buf):
     return to_unicode(pyOpenSSLutil.ffi.string(buf))
 
 
 def x509name_to_string(x509name):
     # from OpenSSL.crypto.X509Name.__repr__
     result_buffer = pyOpenSSLutil.ffi.new("char[]", 512)
-    pyOpenSSLutil.lib.X509_NAME_oneline(x509name._name, result_buffer, len(result_buffer))
+    pyOpenSSLutil.lib.X509_NAME_oneline(
+        x509name._name, result_buffer, len(result_buffer)
+    )
 
     return ffi_buf_to_string(result_buffer)
 
 
 def get_temp_key_info(ssl_object):
-    if not hasattr(pyOpenSSLutil.lib, 'SSL_get_server_tmp_key'):  # requires OpenSSL 1.0.2
-        return None
-
     # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()
     temp_key_p = pyOpenSSLutil.ffi.new("EVP_PKEY **")
     if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):
         return None
     temp_key = temp_key_p[0]
     if temp_key == pyOpenSSLutil.ffi.NULL:
         return None
     temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)
     key_info = []
     key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)
     if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:
-        key_info.append('RSA')
+        key_info.append("RSA")
     elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:
-        key_info.append('DH')
+        key_info.append("DH")
     elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:
-        key_info.append('ECDH')
+        key_info.append("ECDH")
         ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)
         ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)
-        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key))
+        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(
+            pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key)
+        )
         cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)
         if cname == pyOpenSSLutil.ffi.NULL:
             cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)
         key_info.append(ffi_buf_to_string(cname))
     else:
         key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))
-    key_info.append(f'{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits')
-    return ', '.join(key_info)
+    key_info.append(f"{pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key)} bits")
+    return ", ".join(key_info)
 
 
 def get_openssl_version():
-    system_openssl = OpenSSL.SSL.SSLeay_version(
-        OpenSSL.SSL.SSLEAY_VERSION
-    ).decode('ascii', errors='replace')
-    return f'{OpenSSL.version.__version__} ({system_openssl})'
+    system_openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION).decode(
+        "ascii", errors="replace"
+    )
+    return f"{OpenSSL.version.__version__} ({system_openssl})"
```

### Comparing `Scrapy-2.7.1/scrapy/utils/test.py` & `Scrapy-2.8.0/scrapy/utils/test.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,57 +1,61 @@
 """
 This module contains some assorted functions used in tests
 """
 
 import asyncio
 import os
+from importlib import import_module
+from pathlib import Path
 from posixpath import split
 from unittest import mock
 
-from importlib import import_module
 from twisted.trial.unittest import SkipTest
 
 from scrapy.utils.boto import is_botocore_available
 
 
 def assert_gcs_environ():
-    if 'GCS_PROJECT_ID' not in os.environ:
+    if "GCS_PROJECT_ID" not in os.environ:
         raise SkipTest("GCS_PROJECT_ID not found")
 
 
 def skip_if_no_boto():
     if not is_botocore_available():
-        raise SkipTest('missing botocore library')
+        raise SkipTest("missing botocore library")
 
 
 def get_gcs_content_and_delete(bucket, path):
     from google.cloud import storage
-    client = storage.Client(project=os.environ.get('GCS_PROJECT_ID'))
+
+    client = storage.Client(project=os.environ.get("GCS_PROJECT_ID"))
     bucket = client.get_bucket(bucket)
     blob = bucket.get_blob(path)
     content = blob.download_as_string()
     acl = list(blob.acl)  # loads acl before it will be deleted
     bucket.delete_blob(path)
     return content, acl, blob
 
 
 def get_ftp_content_and_delete(
-        path, host, port, username,
-        password, use_active_mode=False):
+    path, host, port, username, password, use_active_mode=False
+):
     from ftplib import FTP
+
     ftp = FTP()
     ftp.connect(host, port)
     ftp.login(username, password)
     if use_active_mode:
         ftp.set_pasv(False)
     ftp_data = []
 
     def buffer_data(data):
         ftp_data.append(data)
-    ftp.retrbinary(f'RETR {path}', buffer_data)
+
+    ftp.retrbinary(f"RETR {path}", buffer_data)
     dirname, filename = split(path)
     ftp.cwd(dirname)
     ftp.delete(filename)
     return "".join(ftp_data)
 
 
 def get_crawler(spidercls=None, settings_dict=None, prevent_warnings=True):
@@ -61,33 +65,33 @@
     """
     from scrapy.crawler import CrawlerRunner
     from scrapy.spiders import Spider
 
     # Set by default settings that prevent deprecation warnings.
     settings = {}
     if prevent_warnings:
-        settings['REQUEST_FINGERPRINTER_IMPLEMENTATION'] = '2.7'
+        settings["REQUEST_FINGERPRINTER_IMPLEMENTATION"] = "2.7"
     settings.update(settings_dict or {})
     runner = CrawlerRunner(settings)
     return runner.create_crawler(spidercls or Spider)
 
 
-def get_pythonpath():
+def get_pythonpath() -> str:
     """Return a PYTHONPATH suitable to use in processes so that they find this
     installation of Scrapy"""
-    scrapy_path = import_module('scrapy').__path__[0]
-    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')
+    scrapy_path = import_module("scrapy").__path__[0]
+    return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get("PYTHONPATH", "")
 
 
 def get_testenv():
     """Return a OS environment dict suitable to fork processes that need to import
     this installation of Scrapy, instead of a system installed one.
     """
     env = os.environ.copy()
-    env['PYTHONPATH'] = get_pythonpath()
+    env["PYTHONPATH"] = get_pythonpath()
     return env
 
 
 def assert_samelines(testcase, text1, text2, msg=None):
     """Asserts text1 and text2 have the same lines, ignoring differences in
     line endings between platforms
     """
@@ -101,24 +105,26 @@
     return getter
 
 
 def mock_google_cloud_storage():
     """Creates autospec mocks for google-cloud-storage Client, Bucket and Blob
     classes and set their proper return values.
     """
-    from google.cloud.storage import Client, Bucket, Blob
+    from google.cloud.storage import Blob, Bucket, Client
+
     client_mock = mock.create_autospec(Client)
 
     bucket_mock = mock.create_autospec(Bucket)
     client_mock.get_bucket.return_value = bucket_mock
 
     blob_mock = mock.create_autospec(Blob)
     bucket_mock.blob.return_value = blob_mock
 
     return (client_mock, bucket_mock, blob_mock)
 
 
 def get_web_client_agent_req(url):
     from twisted.internet import reactor
     from twisted.web.client import Agent  # imports twisted.internet.reactor
+
     agent = Agent(reactor)
-    return agent.request(b'GET', url.encode('utf-8'))
+    return agent.request(b"GET", url.encode("utf-8"))
```

### Comparing `Scrapy-2.7.1/scrapy/utils/testproc.py` & `Scrapy-2.8.0/scrapy/utils/testproc.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,24 +1,25 @@
-import sys
 import os
+import sys
 
 from twisted.internet import defer, protocol
 
 
 class ProcessTest:
 
     command = None
-    prefix = [sys.executable, '-m', 'scrapy.cmdline']
+    prefix = [sys.executable, "-m", "scrapy.cmdline"]
     cwd = os.getcwd()  # trial chdirs to temp dir
 
     def execute(self, args, check_code=True, settings=None):
         from twisted.internet import reactor
+
         env = os.environ.copy()
         if settings is not None:
-            env['SCRAPY_SETTINGS_MODULE'] = settings
+            env["SCRAPY_SETTINGS_MODULE"] = settings
         cmd = self.prefix + [self.command] + list(args)
         pp = TestProcessProtocol()
         pp.deferred.addBoth(self._process_finished, cmd, check_code)
         reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)
         return pp.deferred
 
     def _process_finished(self, pp, cmd, check_code):
@@ -28,19 +29,18 @@
             msg += "\n"
             msg += f"\n>>> stderr <<<\n{pp.err}"
             raise RuntimeError(msg)
         return pp.exitcode, pp.out, pp.err
 
 
 class TestProcessProtocol(protocol.ProcessProtocol):
-
     def __init__(self):
         self.deferred = defer.Deferred()
-        self.out = b''
-        self.err = b''
+        self.out = b""
+        self.err = b""
         self.exitcode = None
 
     def outReceived(self, data):
         self.out += data
 
     def errReceived(self, data):
         self.err += data
```

### Comparing `Scrapy-2.7.1/scrapy/utils/trackref.py` & `Scrapy-2.8.0/scrapy/utils/trackref.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,15 +11,14 @@
 
 from collections import defaultdict
 from operator import itemgetter
 from time import time
 from typing import DefaultDict
 from weakref import WeakKeyDictionary
 
-
 NoneType = type(None)
 live_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)
 
 
 class object_ref:
     """Inherit from this class to a keep a record of live instances"""
 
@@ -31,16 +30,15 @@
         return obj
 
 
 def format_live_refs(ignore=NoneType):
     """Return a tabular representation of tracked objects"""
     s = "Live References\n\n"
     now = time()
-    for cls, wdict in sorted(live_refs.items(),
-                             key=lambda x: x[0].__name__):
+    for cls, wdict in sorted(live_refs.items(), key=lambda x: x[0].__name__):
         if not wdict:
             continue
         if issubclass(cls, ignore):
             continue
         oldest = min(wdict.values())
         s += f"{cls.__name__:<30} {len(wdict):6}   oldest: {int(now - oldest)}s ago\n"
     return s
```

### Comparing `Scrapy-2.7.1/scrapy/utils/url.py` & `Scrapy-2.8.0/scrapy/utils/url.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,29 +8,32 @@
 import re
 from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse
 
 # scrapy.utils.url was moved to w3lib.url and import * ensures this
 # move doesn't break old code
 from w3lib.url import *
 from w3lib.url import _safe_chars, _unquotepath  # noqa: F401
+
 from scrapy.utils.python import to_unicode
 
 
 def url_is_from_any_domain(url, domains):
     """Return True if the url belongs to any of the given domains"""
     host = parse_url(url).netloc.lower()
     if not host:
         return False
     domains = [d.lower() for d in domains]
-    return any((host == d) or (host.endswith(f'.{d}')) for d in domains)
+    return any((host == d) or (host.endswith(f".{d}")) for d in domains)
 
 
 def url_is_from_spider(url, spider):
     """Return True if the url belongs to the given spider"""
-    return url_is_from_any_domain(url, [spider.name] + list(getattr(spider, 'allowed_domains', [])))
+    return url_is_from_any_domain(
+        url, [spider.name] + list(getattr(spider, "allowed_domains", []))
+    )
 
 
 def url_has_any_extension(url, extensions):
     """Return True if the url ends with one of the extensions provided"""
     lowercase_path = parse_url(url).path.lower()
     return any(lowercase_path.endswith(ext) for ext in extensions)
 
@@ -42,15 +45,15 @@
     if isinstance(url, ParseResult):
         return url
     return urlparse(to_unicode(url, encoding))
 
 
 def escape_ajax(url):
     """
-    Return the crawleable url according to:
+    Return the crawlable url according to:
     https://developers.google.com/webmasters/ajax-crawling/docs/getting-started
 
     >>> escape_ajax("www.example.com/ajax.html#!key=value")
     'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'
     >>> escape_ajax("www.example.com/ajax.html?k1=v1&k2=v2#!key=value")
     'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'
     >>> escape_ajax("www.example.com/ajax.html?#!key=value")
@@ -64,17 +67,17 @@
     'www.example.com/ajax.html#key=value'
     >>> escape_ajax("www.example.com/ajax.html#")
     'www.example.com/ajax.html#'
     >>> escape_ajax("www.example.com/ajax.html")
     'www.example.com/ajax.html'
     """
     defrag, frag = urldefrag(url)
-    if not frag.startswith('!'):
+    if not frag.startswith("!"):
         return url
-    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])
+    return add_or_replace_parameter(defrag, "_escaped_fragment_", frag[1:])
 
 
 def add_http_if_no_scheme(url):
     """Add http as the default scheme if it is missing from the url."""
     match = re.match(r"^\w+://", url, flags=re.I)
     if not match:
         parts = urlparse(url)
@@ -83,43 +86,43 @@
 
     return url
 
 
 def _is_posix_path(string):
     return bool(
         re.match(
-            r'''
+            r"""
             ^                   # start with...
             (
                 \.              # ...a single dot,
                 (
                     \. | [^/\.]+  # optionally followed by
                 )?                # either a second dot or some characters
                 |
                 ~   # $HOME
             )?      # optional match of ".", ".." or ".blabla"
             /       # at least one "/" for a file path,
             .       # and something after the "/"
-            ''',
+            """,
             string,
             flags=re.VERBOSE,
         )
     )
 
 
 def _is_windows_path(string):
     return bool(
         re.match(
-            r'''
+            r"""
             ^
             (
                 [a-z]:\\
                 | \\\\
             )
-            ''',
+            """,
             string,
             flags=re.IGNORECASE | re.VERBOSE,
         )
     )
 
 
 def _is_filesystem_path(string):
@@ -130,36 +133,48 @@
     """Add an URL scheme if missing: file:// for filepath-like input or
     http:// otherwise."""
     if _is_filesystem_path(url):
         return any_to_uri(url)
     return add_http_if_no_scheme(url)
 
 
-def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=False, strip_fragment=True):
+def strip_url(
+    url,
+    strip_credentials=True,
+    strip_default_port=True,
+    origin_only=False,
+    strip_fragment=True,
+):
 
     """Strip URL string from some of its components:
 
     - ``strip_credentials`` removes "user:password@"
     - ``strip_default_port`` removes ":80" (resp. ":443", ":21")
       from http:// (resp. https://, ftp://) URLs
     - ``origin_only`` replaces path component with "/", also dropping
       query and fragment components ; it also strips credentials
     - ``strip_fragment`` drops any #fragment component
     """
 
     parsed_url = urlparse(url)
     netloc = parsed_url.netloc
-    if (strip_credentials or origin_only) and (parsed_url.username or parsed_url.password):
-        netloc = netloc.split('@')[-1]
+    if (strip_credentials or origin_only) and (
+        parsed_url.username or parsed_url.password
+    ):
+        netloc = netloc.split("@")[-1]
     if strip_default_port and parsed_url.port:
-        if (parsed_url.scheme, parsed_url.port) in (('http', 80),
-                                                    ('https', 443),
-                                                    ('ftp', 21)):
-            netloc = netloc.replace(f':{parsed_url.port}', '')
-    return urlunparse((
-        parsed_url.scheme,
-        netloc,
-        '/' if origin_only else parsed_url.path,
-        '' if origin_only else parsed_url.params,
-        '' if origin_only else parsed_url.query,
-        '' if strip_fragment else parsed_url.fragment
-    ))
+        if (parsed_url.scheme, parsed_url.port) in (
+            ("http", 80),
+            ("https", 443),
+            ("ftp", 21),
+        ):
+            netloc = netloc.replace(f":{parsed_url.port}", "")
+    return urlunparse(
+        (
+            parsed_url.scheme,
+            netloc,
+            "/" if origin_only else parsed_url.path,
+            "" if origin_only else parsed_url.params,
+            "" if origin_only else parsed_url.query,
+            "" if strip_fragment else parsed_url.fragment,
+        )
+    )
```

### Comparing `Scrapy-2.7.1/scrapy/utils/versions.py` & `Scrapy-2.8.0/scrapy/utils/versions.py`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/setup.cfg` & `Scrapy-2.8.0/setup.cfg`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/setup.py` & `Scrapy-2.8.0/setup.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,96 +1,97 @@
-from os.path import dirname, join
-from pkg_resources import parse_version
-from setuptools import setup, find_packages, __version__ as setuptools_version
+from pathlib import Path
 
+from pkg_resources import parse_version
+from setuptools import __version__ as setuptools_version
+from setuptools import find_packages, setup
 
-with open(join(dirname(__file__), 'scrapy/VERSION'), 'rb') as f:
-    version = f.read().decode('ascii').strip()
+version = (Path(__file__).parent / "scrapy/VERSION").read_text("ascii").strip()
 
 
 def has_environment_marker_platform_impl_support():
     """Code extracted from 'pytest/setup.py'
     https://github.com/pytest-dev/pytest/blob/7538680c/setup.py#L31
 
     The first known release to support environment marker with range operators
     it is 18.5, see:
     https://setuptools.readthedocs.io/en/latest/history.html#id235
     """
-    return parse_version(setuptools_version) >= parse_version('18.5')
+    return parse_version(setuptools_version) >= parse_version("18.5")
 
 
 install_requires = [
-    'Twisted>=18.9.0',
-    'cryptography>=3.3',
-    'cssselect>=0.9.1',
-    'itemloaders>=1.0.1',
-    'parsel>=1.5.0',
-    'pyOpenSSL>=21.0.0',
-    'queuelib>=1.4.2',
-    'service_identity>=18.1.0',
-    'w3lib>=1.17.0',
-    'zope.interface>=5.1.0',
-    'protego>=0.1.15',
-    'itemadapter>=0.1.0',
-    'setuptools',
-    'packaging',
-    'tldextract',
-    'lxml>=4.3.0',
+    "Twisted>=18.9.0",
+    "cryptography>=3.4.6",
+    "cssselect>=0.9.1",
+    "itemloaders>=1.0.1",
+    "parsel>=1.5.0",
+    "pyOpenSSL>=21.0.0",
+    "queuelib>=1.4.2",
+    "service_identity>=18.1.0",
+    "w3lib>=1.17.0",
+    "zope.interface>=5.1.0",
+    "protego>=0.1.15",
+    "itemadapter>=0.1.0",
+    "setuptools",
+    "packaging",
+    "tldextract",
+    "lxml>=4.3.0",
 ]
 extras_require = {}
 cpython_dependencies = [
-    'PyDispatcher>=2.0.5',
+    "PyDispatcher>=2.0.5",
 ]
 if has_environment_marker_platform_impl_support():
-    extras_require[':platform_python_implementation == "CPython"'] = cpython_dependencies
+    extras_require[
+        ':platform_python_implementation == "CPython"'
+    ] = cpython_dependencies
     extras_require[':platform_python_implementation == "PyPy"'] = [
-        'PyPyDispatcher>=2.1.0',
+        "PyPyDispatcher>=2.1.0",
     ]
 else:
     install_requires.extend(cpython_dependencies)
 
 
 setup(
-    name='Scrapy',
+    name="Scrapy",
     version=version,
-    url='https://scrapy.org',
+    url="https://scrapy.org",
     project_urls={
-        'Documentation': 'https://docs.scrapy.org/',
-        'Source': 'https://github.com/scrapy/scrapy',
-        'Tracker': 'https://github.com/scrapy/scrapy/issues',
+        "Documentation": "https://docs.scrapy.org/",
+        "Source": "https://github.com/scrapy/scrapy",
+        "Tracker": "https://github.com/scrapy/scrapy/issues",
     },
-    description='A high-level Web Crawling and Web Scraping framework',
-    long_description=open('README.rst').read(),
-    author='Scrapy developers',
-    maintainer='Pablo Hoffman',
-    maintainer_email='pablo@pablohoffman.com',
-    license='BSD',
-    packages=find_packages(exclude=('tests', 'tests.*')),
+    description="A high-level Web Crawling and Web Scraping framework",
+    long_description=open("README.rst", encoding="utf-8").read(),
+    author="Scrapy developers",
+    author_email="pablo@pablohoffman.com",
+    maintainer="Pablo Hoffman",
+    maintainer_email="pablo@pablohoffman.com",
+    license="BSD",
+    packages=find_packages(exclude=("tests", "tests.*")),
     include_package_data=True,
     zip_safe=False,
-    entry_points={
-        'console_scripts': ['scrapy = scrapy.cmdline:execute']
-    },
+    entry_points={"console_scripts": ["scrapy = scrapy.cmdline:execute"]},
     classifiers=[
-        'Framework :: Scrapy',
-        'Development Status :: 5 - Production/Stable',
-        'Environment :: Console',
-        'Intended Audience :: Developers',
-        'License :: OSI Approved :: BSD License',
-        'Operating System :: OS Independent',
-        'Programming Language :: Python',
-        'Programming Language :: Python :: 3',
-        'Programming Language :: Python :: 3.7',
-        'Programming Language :: Python :: 3.8',
-        'Programming Language :: Python :: 3.9',
-        'Programming Language :: Python :: 3.10',
-        'Programming Language :: Python :: 3.11',
-        'Programming Language :: Python :: Implementation :: CPython',
-        'Programming Language :: Python :: Implementation :: PyPy',
-        'Topic :: Internet :: WWW/HTTP',
-        'Topic :: Software Development :: Libraries :: Application Frameworks',
-        'Topic :: Software Development :: Libraries :: Python Modules',
+        "Framework :: Scrapy",
+        "Development Status :: 5 - Production/Stable",
+        "Environment :: Console",
+        "Intended Audience :: Developers",
+        "License :: OSI Approved :: BSD License",
+        "Operating System :: OS Independent",
+        "Programming Language :: Python",
+        "Programming Language :: Python :: 3",
+        "Programming Language :: Python :: 3.7",
+        "Programming Language :: Python :: 3.8",
+        "Programming Language :: Python :: 3.9",
+        "Programming Language :: Python :: 3.10",
+        "Programming Language :: Python :: 3.11",
+        "Programming Language :: Python :: Implementation :: CPython",
+        "Programming Language :: Python :: Implementation :: PyPy",
+        "Topic :: Internet :: WWW/HTTP",
+        "Topic :: Software Development :: Libraries :: Application Frameworks",
+        "Topic :: Software Development :: Libraries :: Python Modules",
     ],
-    python_requires='>=3.7',
+    python_requires=">=3.7",
     install_requires=install_requires,
     extras_require=extras_require,
 )
```

### Comparing `Scrapy-2.7.1/tests/CrawlerProcess/asyncio_deferred_signal.py` & `Scrapy-2.8.0/tests/CrawlerProcess/asyncio_deferred_signal.py`

 * *Files 10% similar despite different names*

```diff
@@ -33,13 +33,15 @@
 if __name__ == "__main__":
     ASYNCIO_EVENT_LOOP: Optional[str]
     try:
         ASYNCIO_EVENT_LOOP = sys.argv[1]
     except IndexError:
         ASYNCIO_EVENT_LOOP = None
 
-    process = CrawlerProcess(settings={
-        "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
-        "ASYNCIO_EVENT_LOOP": ASYNCIO_EVENT_LOOP,
-    })
+    process = CrawlerProcess(
+        settings={
+            "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
+            "ASYNCIO_EVENT_LOOP": ASYNCIO_EVENT_LOOP,
+        }
+    )
     process.crawl(UrlSpider)
     process.start()
```

### Comparing `Scrapy-2.7.1/tests/CrawlerProcess/asyncio_enabled_reactor.py` & `Scrapy-2.8.0/tests/CrawlerProcess/asyncio_enabled_reactor_same_loop.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,24 +1,30 @@
 import asyncio
 import sys
 
 from twisted.internet import asyncioreactor
+from uvloop import Loop
+
 if sys.version_info >= (3, 8) and sys.platform == "win32":
     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
+asyncio.set_event_loop(Loop())
 asyncioreactor.install(asyncio.get_event_loop())
 
 import scrapy  # noqa: E402
 from scrapy.crawler import CrawlerProcess  # noqa: E402
 
 
 class NoRequestsSpider(scrapy.Spider):
-    name = 'no_request'
+    name = "no_request"
 
     def start_requests(self):
         return []
 
 
-process = CrawlerProcess(settings={
-    "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
-})
+process = CrawlerProcess(
+    settings={
+        "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
+        "ASYNCIO_EVENT_LOOP": "uvloop.Loop",
+    }
+)
 process.crawl(NoRequestsSpider)
 process.start()
```

### Comparing `Scrapy-2.7.1/tests/CrawlerProcess/asyncio_enabled_reactor_different_loop.py` & `Scrapy-2.8.0/tests/CrawlerProcess/asyncio_enabled_reactor.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 import asyncio
 import sys
 
 from twisted.internet import asyncioreactor
+
 if sys.version_info >= (3, 8) and sys.platform == "win32":
     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
 asyncioreactor.install(asyncio.get_event_loop())
 
 import scrapy  # noqa: E402
 from scrapy.crawler import CrawlerProcess  # noqa: E402
 
 
 class NoRequestsSpider(scrapy.Spider):
-    name = 'no_request'
+    name = "no_request"
 
     def start_requests(self):
         return []
 
 
-process = CrawlerProcess(settings={
-    "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
-    "ASYNCIO_EVENT_LOOP": "uvloop.Loop",
-})
+process = CrawlerProcess(
+    settings={
+        "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
+    }
+)
 process.crawl(NoRequestsSpider)
 process.start()
```

### Comparing `Scrapy-2.7.1/tests/CrawlerProcess/caching_hostname_resolver.py` & `Scrapy-2.8.0/tests/CrawlerProcess/caching_hostname_resolver.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,27 +4,32 @@
 from scrapy.crawler import CrawlerProcess
 
 
 class CachingHostnameResolverSpider(scrapy.Spider):
     """
     Finishes in a finite amount of time (does not hang indefinitely in the DNS resolution)
     """
+
     name = "caching_hostname_resolver_spider"
 
     def start_requests(self):
         yield scrapy.Request(self.url)
 
     def parse(self, response):
         for _ in range(10):
-            yield scrapy.Request(response.url, dont_filter=True, callback=self.ignore_response)
+            yield scrapy.Request(
+                response.url, dont_filter=True, callback=self.ignore_response
+            )
 
     def ignore_response(self, response):
         self.logger.info(repr(response.ip_address))
 
 
 if __name__ == "__main__":
-    process = CrawlerProcess(settings={
-        "RETRY_ENABLED": False,
-        "DNS_RESOLVER": "scrapy.resolver.CachingHostnameResolver",
-    })
+    process = CrawlerProcess(
+        settings={
+            "RETRY_ENABLED": False,
+            "DNS_RESOLVER": "scrapy.resolver.CachingHostnameResolver",
+        }
+    )
     process.crawl(CachingHostnameResolverSpider, url=sys.argv[1])
     process.start()
```

### Comparing `Scrapy-2.7.1/tests/CrawlerProcess/caching_hostname_resolver_ipv6.py` & `Scrapy-2.8.0/tests/CrawlerProcess/caching_hostname_resolver_ipv6.py`

 * *Files 23% similar despite different names*

```diff
@@ -2,18 +2,21 @@
 from scrapy.crawler import CrawlerProcess
 
 
 class CachingHostnameResolverSpider(scrapy.Spider):
     """
     Finishes without a twisted.internet.error.DNSLookupError exception
     """
+
     name = "caching_hostname_resolver_spider"
     start_urls = ["http://[::1]"]
 
 
 if __name__ == "__main__":
-    process = CrawlerProcess(settings={
-        "RETRY_ENABLED": False,
-        "DNS_RESOLVER": "scrapy.resolver.CachingHostnameResolver",
-    })
+    process = CrawlerProcess(
+        settings={
+            "RETRY_ENABLED": False,
+            "DNS_RESOLVER": "scrapy.resolver.CachingHostnameResolver",
+        }
+    )
     process.crawl(CachingHostnameResolverSpider)
     process.start()
```

### Comparing `Scrapy-2.7.1/tests/CrawlerProcess/reactor_select_subclass_twisted_reactor_select.py` & `Scrapy-2.8.0/tests/CrawlerProcess/reactor_select_subclass_twisted_reactor_select.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,27 +1,30 @@
-import scrapy
-from scrapy.crawler import CrawlerProcess
 from twisted.internet.main import installReactor
 from twisted.internet.selectreactor import SelectReactor
 
+import scrapy
+from scrapy.crawler import CrawlerProcess
+
 
 class SelectReactorSubclass(SelectReactor):
     pass
 
 
 reactor = SelectReactorSubclass()
 installReactor(reactor)
 
 
 class NoRequestsSpider(scrapy.Spider):
-    name = 'no_request'
+    name = "no_request"
 
     def start_requests(self):
         return []
 
 
-process = CrawlerProcess(settings={
-    "TWISTED_REACTOR": "twisted.internet.selectreactor.SelectReactor",
-})
+process = CrawlerProcess(
+    settings={
+        "TWISTED_REACTOR": "twisted.internet.selectreactor.SelectReactor",
+    }
+)
 
 process.crawl(NoRequestsSpider)
 process.start()
```

### Comparing `Scrapy-2.7.1/tests/CrawlerProcess/twisted_reactor_custom_settings_conflict.py` & `Scrapy-2.8.0/tests/CrawlerProcess/twisted_reactor_custom_settings_conflict.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 import scrapy
 from scrapy.crawler import CrawlerProcess
 
 
 class SelectReactorSpider(scrapy.Spider):
-    name = 'select_reactor'
+    name = "select_reactor"
     custom_settings = {
         "TWISTED_REACTOR": "twisted.internet.selectreactor.SelectReactor",
     }
 
 
 class AsyncioReactorSpider(scrapy.Spider):
-    name = 'asyncio_reactor'
+    name = "asyncio_reactor"
     custom_settings = {
         "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
     }
 
 
 process = CrawlerProcess()
 process.crawl(SelectReactorSpider)
```

### Comparing `Scrapy-2.7.1/tests/CrawlerProcess/twisted_reactor_custom_settings_same.py` & `Scrapy-2.8.0/tests/CrawlerProcess/twisted_reactor_custom_settings_same.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 import scrapy
 from scrapy.crawler import CrawlerProcess
 
 
 class AsyncioReactorSpider1(scrapy.Spider):
-    name = 'asyncio_reactor1'
+    name = "asyncio_reactor1"
     custom_settings = {
         "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
     }
 
 
 class AsyncioReactorSpider2(scrapy.Spider):
-    name = 'asyncio_reactor2'
+    name = "asyncio_reactor2"
     custom_settings = {
         "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
     }
 
 
 process = CrawlerProcess()
 process.crawl(AsyncioReactorSpider1)
```

### Comparing `Scrapy-2.7.1/tests/CrawlerRunner/ip_address.py` & `Scrapy-2.8.0/tests/CrawlerRunner/ip_address.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,42 +1,44 @@
 from urllib.parse import urlparse
 
 from twisted.internet import reactor
-from twisted.names import cache, hosts as hostsModule, resolve
+from twisted.names import cache
+from twisted.names import hosts as hostsModule
+from twisted.names import resolve
 from twisted.names.client import Resolver
 from twisted.python.runtime import platform
 
-from scrapy import Spider, Request
+from scrapy import Request, Spider
 from scrapy.crawler import CrawlerRunner
 from scrapy.utils.log import configure_logging
-
-from tests.mockserver import MockServer, MockDNSServer
+from tests.mockserver import MockDNSServer, MockServer
 
 
 # https://stackoverflow.com/a/32784190
 def createResolver(servers=None, resolvconf=None, hosts=None):
     if hosts is None:
-        hosts = b'/etc/hosts' if platform.getType() == 'posix' else r'c:\windows\hosts'
+        hosts = b"/etc/hosts" if platform.getType() == "posix" else r"c:\windows\hosts"
     theResolver = Resolver(resolvconf, servers)
     hostResolver = hostsModule.Resolver(hosts)
     chain = [hostResolver, cache.CacheResolver(), theResolver]
     return resolve.ResolverChain(chain)
 
 
 class LocalhostSpider(Spider):
     name = "localhost_spider"
 
     def start_requests(self):
         yield Request(self.url)
 
     def parse(self, response):
         netloc = urlparse(response.url).netloc
-        self.logger.info("Host: %s" % netloc.split(":")[0])
-        self.logger.info("Type: %s" % type(response.ip_address))
-        self.logger.info("IP address: %s" % response.ip_address)
+        host = netloc.split(":")[0]
+        self.logger.info(f"Host: {host}")
+        self.logger.info(f"Type: {type(response.ip_address)}")
+        self.logger.info(f"IP address: {response.ip_address}")
 
 
 if __name__ == "__main__":
     with MockServer() as mock_http_server, MockDNSServer() as mock_dns_server:
         port = urlparse(mock_http_server.http_address).port
         url = f"http://not.a.real.domain:{port}/echo"
```

### Comparing `Scrapy-2.7.1/tests/__init__.py` & `Scrapy-2.8.0/tests/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -2,41 +2,38 @@
 tests: this package contains all Scrapy unittests
 
 see https://docs.scrapy.org/en/latest/contributing.html#running-tests
 """
 
 import os
 import socket
+from pathlib import Path
 
 # ignore system-wide proxies for tests
 # which would send requests to a totally unsuspecting server
 # (e.g. because urllib does not fully understand the proxy spec)
-os.environ['http_proxy'] = ''
-os.environ['https_proxy'] = ''
-os.environ['ftp_proxy'] = ''
+os.environ["http_proxy"] = ""
+os.environ["https_proxy"] = ""
+os.environ["ftp_proxy"] = ""
 
 # Absolutize paths to coverage config and output file because tests that
 # spawn subprocesses also changes current working directory.
-_sourceroot = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
-if 'COV_CORE_CONFIG' in os.environ:
-    os.environ['COVERAGE_FILE'] = os.path.join(_sourceroot, '.coverage')
-    os.environ['COV_CORE_CONFIG'] = os.path.join(_sourceroot,
-                                                 os.environ['COV_CORE_CONFIG'])
+_sourceroot = Path(__file__).resolve().parent.parent
+if "COV_CORE_CONFIG" in os.environ:
+    os.environ["COVERAGE_FILE"] = str(_sourceroot / ".coverage")
+    os.environ["COV_CORE_CONFIG"] = str(_sourceroot / os.environ["COV_CORE_CONFIG"])
 
-tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)),
-                             'sample_data')
+tests_datadir = str(Path(__file__).parent.resolve() / "sample_data")
 
 
 # In some environments accessing a non-existing host doesn't raise an
 # error. In such cases we're going to skip tests which rely on it.
 try:
-    socket.getaddrinfo('non-existing-host', 80)
+    socket.getaddrinfo("non-existing-host", 80)
     NON_EXISTING_RESOLVABLE = True
 except socket.gaierror:
     NON_EXISTING_RESOLVABLE = False
 
 
-def get_testdata(*paths):
+def get_testdata(*paths: str) -> bytes:
     """Return test data"""
-    path = os.path.join(tests_datadir, *paths)
-    with open(path, 'rb') as f:
-        return f.read()
+    return Path(tests_datadir, *paths).read_bytes()
```

### Comparing `Scrapy-2.7.1/tests/ftpserver.py` & `Scrapy-2.8.0/tests/ftpserver.py`

 * *Files 22% similar despite different names*

```diff
@@ -3,22 +3,22 @@
 from pyftpdlib.authorizers import DummyAuthorizer
 from pyftpdlib.handlers import FTPHandler
 from pyftpdlib.servers import FTPServer
 
 
 def main():
     parser = ArgumentParser()
-    parser.add_argument('-d', '--directory')
+    parser.add_argument("-d", "--directory")
     args = parser.parse_args()
 
     authorizer = DummyAuthorizer()
-    full_permissions = 'elradfmwMT'
+    full_permissions = "elradfmwMT"
     authorizer.add_anonymous(args.directory, perm=full_permissions)
     handler = FTPHandler
     handler.authorizer = authorizer
-    address = ('127.0.0.1', 2121)
+    address = ("127.0.0.1", 2121)
     server = FTPServer(address, handler)
     server.serve_forever()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `Scrapy-2.7.1/tests/keys/example-com.cert.pem` & `Scrapy-2.8.0/tests/keys/example-com.cert.pem`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/keys/example-com.conf` & `Scrapy-2.8.0/tests/keys/example-com.conf`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/keys/example-com.gen.README` & `Scrapy-2.8.0/tests/keys/example-com.gen.README`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/keys/example-com.key.pem` & `Scrapy-2.8.0/tests/keys/example-com.key.pem`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/keys/localhost-ip.gen.README` & `Scrapy-2.8.0/tests/keys/localhost-ip.gen.README`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/keys/localhost.gen.README` & `Scrapy-2.8.0/tests/keys/localhost.gen.README`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/keys/localhost.ip.crt` & `Scrapy-2.8.0/tests/keys/localhost.ip.crt`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/keys/localhost.ip.key` & `Scrapy-2.8.0/tests/keys/localhost.ip.key`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/keys/mitmproxy-ca.pem` & `Scrapy-2.8.0/tests/keys/mitmproxy-ca.pem`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/mockserver.py` & `Scrapy-2.8.0/tests/mockserver.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,41 +1,38 @@
 import argparse
 import json
-import os
 import random
 import sys
 from pathlib import Path
 from shutil import rmtree
-from subprocess import Popen, PIPE
+from subprocess import PIPE, Popen
 from tempfile import mkdtemp
 from urllib.parse import urlencode
 
 from OpenSSL import SSL
 from twisted.internet import defer, reactor, ssl
 from twisted.internet.task import deferLater
 from twisted.names import dns, error
 from twisted.names.server import DNSServerFactory
 from twisted.web import resource, server
-from twisted.web.server import GzipEncoderFactory, NOT_DONE_YET, Site
+from twisted.web.server import NOT_DONE_YET, GzipEncoderFactory, Site
 from twisted.web.static import File
 from twisted.web.util import redirectTo
 
 from scrapy.utils.python import to_bytes, to_unicode
-from scrapy.utils.ssl import SSL_OP_NO_TLSv1_3
 from scrapy.utils.test import get_testenv
 
 
 def getarg(request, name, default=None, type=None):
     if name in request.args:
         value = request.args[name][0]
         if type is not None:
             value = type(value)
         return value
-    else:
-        return default
+    return default
 
 
 # most of the following resources are copied from twisted.web.test.test_webclient
 class ForeverTakingResource(resource.Resource):
     """
     L{ForeverTakingResource} is a resource which never finishes responding
     to requests.
@@ -109,15 +106,14 @@
 
         d = deferLater(reactor, delay, f, *a, **kw)
         request.notifyFinish().addErrback(_cancelrequest)
         return d
 
 
 class Follow(LeafResource):
-
     def render(self, request):
         total = getarg(request, b"total", 100, type=int)
         show = getarg(request, b"show", 1, type=int)
         order = getarg(request, b"order", b"desc")
         maxlatency = getarg(request, b"maxlatency", 0, type=float)
         n = getarg(request, b"n", total, type=int)
         if order == b"rand":
@@ -138,157 +134,157 @@
             s += f"<a href='/follow?{argstr}'>follow {nl}</a><br>"
         s += """</body>"""
         request.write(to_bytes(s))
         request.finish()
 
 
 class Delay(LeafResource):
-
     def render_GET(self, request):
         n = getarg(request, b"n", 1, type=float)
         b = getarg(request, b"b", 1, type=int)
         if b:
             # send headers now and delay body
-            request.write('')
+            request.write("")
         self.deferRequest(request, n, self._delayedRender, request, n)
         return NOT_DONE_YET
 
     def _delayedRender(self, request, n):
         request.write(to_bytes(f"Response delayed for {n:.3f} seconds\n"))
         request.finish()
 
 
 class Status(LeafResource):
-
     def render_GET(self, request):
         n = getarg(request, b"n", 200, type=int)
         request.setResponseCode(n)
         return b""
 
 
 class Raw(LeafResource):
-
     def render_GET(self, request):
         request.startedWriting = 1
         self.deferRequest(request, 0, self._delayedRender, request)
         return NOT_DONE_YET
+
     render_POST = render_GET
 
     def _delayedRender(self, request):
-        raw = getarg(request, b'raw', b'HTTP 1.1 200 OK\n')
+        raw = getarg(request, b"raw", b"HTTP 1.1 200 OK\n")
         request.startedWriting = 1
         request.write(raw)
         request.channel.transport.loseConnection()
         request.finish()
 
 
 class Echo(LeafResource):
-
     def render_GET(self, request):
         output = {
-            'headers': dict(
+            "headers": dict(
                 (to_unicode(k), [to_unicode(v) for v in vs])
-                for k, vs in request.requestHeaders.getAllRawHeaders()),
-            'body': to_unicode(request.content.read()),
+                for k, vs in request.requestHeaders.getAllRawHeaders()
+            ),
+            "body": to_unicode(request.content.read()),
         }
         return to_bytes(json.dumps(output))
+
     render_POST = render_GET
 
 
 class RedirectTo(LeafResource):
-
     def render(self, request):
-        goto = getarg(request, b'goto', b'/')
+        goto = getarg(request, b"goto", b"/")
         # we force the body content, otherwise Twisted redirectTo()
         # returns HTML with <meta http-equiv="refresh"
         redirectTo(goto, request)
-        return b'redirecting...'
+        return b"redirecting..."
 
 
 class Partial(LeafResource):
-
     def render_GET(self, request):
         request.setHeader(b"Content-Length", b"1024")
         self.deferRequest(request, 0, self._delayedRender, request)
         return NOT_DONE_YET
 
     def _delayedRender(self, request):
         request.write(b"partial content\n")
         request.finish()
 
 
 class Drop(Partial):
-
     def _delayedRender(self, request):
         abort = getarg(request, b"abort", 0, type=int)
         request.write(b"this connection will be dropped\n")
         tr = request.channel.transport
         try:
-            if abort and hasattr(tr, 'abortConnection'):
+            if abort and hasattr(tr, "abortConnection"):
                 tr.abortConnection()
             else:
                 tr.loseConnection()
         finally:
             request.finish()
 
 
 class ArbitraryLengthPayloadResource(LeafResource):
-
     def render(self, request):
         return request.content.read()
 
 
 class Root(resource.Resource):
-
     def __init__(self):
         resource.Resource.__init__(self)
         self.putChild(b"status", Status())
         self.putChild(b"follow", Follow())
         self.putChild(b"delay", Delay())
         self.putChild(b"partial", Partial())
         self.putChild(b"drop", Drop())
         self.putChild(b"raw", Raw())
         self.putChild(b"echo", Echo())
         self.putChild(b"payload", PayloadResource())
-        self.putChild(b"xpayload", resource.EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))
+        self.putChild(
+            b"xpayload",
+            resource.EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]),
+        )
         self.putChild(b"alpayload", ArbitraryLengthPayloadResource())
         try:
             from tests import tests_datadir
-            self.putChild(b"files", File(os.path.join(tests_datadir, 'test_site/files/')))
+
+            self.putChild(b"files", File(str(Path(tests_datadir, "test_site/files/"))))
         except Exception:
             pass
         self.putChild(b"redirect-to", RedirectTo())
 
     def getChild(self, name, request):
         return self
 
     def render(self, request):
-        return b'Scrapy mock HTTP server\n'
+        return b"Scrapy mock HTTP server\n"
 
 
 class MockServer:
-
     def __enter__(self):
-        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'http'],
-                          stdout=PIPE, env=get_testenv())
-        http_address = self.proc.stdout.readline().strip().decode('ascii')
-        https_address = self.proc.stdout.readline().strip().decode('ascii')
+        self.proc = Popen(
+            [sys.executable, "-u", "-m", "tests.mockserver", "-t", "http"],
+            stdout=PIPE,
+            env=get_testenv(),
+        )
+        http_address = self.proc.stdout.readline().strip().decode("ascii")
+        https_address = self.proc.stdout.readline().strip().decode("ascii")
 
         self.http_address = http_address
         self.https_address = https_address
 
         return self
 
     def __exit__(self, exc_type, exc_value, traceback):
         self.proc.kill()
         self.proc.communicate()
 
     def url(self, path, is_secure=False):
         host = self.https_address if is_secure else self.http_address
-        host = host.replace('0.0.0.0', '127.0.0.1')
+        host = host.replace("0.0.0.0", "127.0.0.1")
         return host + path
 
 
 class MockDNSResolver:
     """
     Implements twisted.internet.interfaces.IResolver partially
     """
@@ -304,80 +300,91 @@
         return defer.fail(error.DomainError())
 
     def lookupAllRecords(self, name, timeout=None):
         return defer.succeed(self._resolve(name))
 
 
 class MockDNSServer:
-
     def __enter__(self):
-        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver', '-t', 'dns'],
-                          stdout=PIPE, env=get_testenv())
-        self.host = '127.0.0.1'
-        self.port = int(self.proc.stdout.readline().strip().decode('ascii').split(":")[1])
+        self.proc = Popen(
+            [sys.executable, "-u", "-m", "tests.mockserver", "-t", "dns"],
+            stdout=PIPE,
+            env=get_testenv(),
+        )
+        self.host = "127.0.0.1"
+        self.port = int(
+            self.proc.stdout.readline().strip().decode("ascii").split(":")[1]
+        )
         return self
 
     def __exit__(self, exc_type, exc_value, traceback):
         self.proc.kill()
         self.proc.communicate()
 
 
 class MockFTPServer:
     """Creates an FTP server on port 2121 with a default passwordless user
     (anonymous) and a temporary root path that you can read from the
     :attr:`path` attribute."""
 
     def __enter__(self):
         self.path = Path(mkdtemp())
-        self.proc = Popen([sys.executable, '-u', '-m', 'tests.ftpserver', '-d', str(self.path)],
-                          stderr=PIPE, env=get_testenv())
+        self.proc = Popen(
+            [sys.executable, "-u", "-m", "tests.ftpserver", "-d", str(self.path)],
+            stderr=PIPE,
+            env=get_testenv(),
+        )
         for line in self.proc.stderr:
-            if b'starting FTP server' in line:
+            if b"starting FTP server" in line:
                 break
         return self
 
     def __exit__(self, exc_type, exc_value, traceback):
         rmtree(str(self.path))
         self.proc.kill()
         self.proc.communicate()
 
     def url(self, path):
-        return 'ftp://127.0.0.1:2121/' + path
+        return "ftp://127.0.0.1:2121/" + path
 
 
-def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt', cipher_string=None):
+def ssl_context_factory(
+    keyfile="keys/localhost.key", certfile="keys/localhost.crt", cipher_string=None
+):
     factory = ssl.DefaultOpenSSLContextFactory(
-        os.path.join(os.path.dirname(__file__), keyfile),
-        os.path.join(os.path.dirname(__file__), certfile),
+        str(Path(__file__).parent / keyfile),
+        str(Path(__file__).parent / certfile),
     )
     if cipher_string:
         ctx = factory.getContext()
         # disabling TLS1.3 because it unconditionally enables some strong ciphers
-        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL_OP_NO_TLSv1_3)
+        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_3)
         ctx.set_cipher_list(to_bytes(cipher_string))
     return factory
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
-    parser.add_argument("-t", "--type", type=str, choices=("http", "dns"), default="http")
+    parser.add_argument(
+        "-t", "--type", type=str, choices=("http", "dns"), default="http"
+    )
     args = parser.parse_args()
 
     if args.type == "http":
         root = Root()
         factory = Site(root)
         httpPort = reactor.listenTCP(0, factory)
         contextFactory = ssl_context_factory()
         httpsPort = reactor.listenSSL(0, factory, contextFactory)
 
         def print_listening():
             httpHost = httpPort.getHost()
             httpsHost = httpsPort.getHost()
-            httpAddress = f'http://{httpHost.host}:{httpHost.port}'
-            httpsAddress = f'https://{httpsHost.host}:{httpsHost.port}'
+            httpAddress = f"http://{httpHost.host}:{httpHost.port}"
+            httpsAddress = f"https://{httpsHost.host}:{httpsHost.port}"
             print(httpAddress)
             print(httpsAddress)
 
     elif args.type == "dns":
         clients = [MockDNSResolver()]
         factory = DNSServerFactory(clients=clients)
         protocol = dns.DNSDatagramProtocol(controller=factory)
```

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.tar` & `Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.tar`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.xml` & `Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.xml`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.xml.bz2` & `Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.xml.bz2`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.xml.gz` & `Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.xml.gz`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/feed-sample1.zip` & `Scrapy-2.8.0/tests/sample_data/compressed/feed-sample1.zip`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/html-br.bin` & `Scrapy-2.8.0/tests/sample_data/compressed/html-br.bin`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/html-gzip.bin` & `Scrapy-2.8.0/tests/sample_data/compressed/html-gzip.bin`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/html-rawdeflate.bin` & `Scrapy-2.8.0/tests/sample_data/compressed/html-rawdeflate.bin`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/html-zlibdeflate.bin` & `Scrapy-2.8.0/tests/sample_data/compressed/html-zlibdeflate.bin`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/html-zstd-static-content-size.bin` & `Scrapy-2.8.0/tests/sample_data/compressed/html-zstd-static-content-size.bin`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/html-zstd-static-no-content-size.bin` & `Scrapy-2.8.0/tests/sample_data/compressed/html-zstd-static-no-content-size.bin`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/html-zstd-streaming-no-content-size.bin` & `Scrapy-2.8.0/tests/sample_data/compressed/html-zstd-streaming-no-content-size.bin`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/truncated-crc-error-short.gz` & `Scrapy-2.8.0/tests/sample_data/compressed/truncated-crc-error-short.gz`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/truncated-crc-error.gz` & `Scrapy-2.8.0/tests/sample_data/compressed/truncated-crc-error.gz`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/unexpected-eof-output.txt` & `Scrapy-2.8.0/tests/sample_data/compressed/unexpected-eof-output.txt`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/compressed/unexpected-eof.gz` & `Scrapy-2.8.0/tests/sample_data/compressed/unexpected-eof.gz`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/feeds/feed-sample1.xml` & `Scrapy-2.8.0/tests/sample_data/feeds/feed-sample1.xml`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/feeds/feed-sample2.xml` & `Scrapy-2.8.0/tests/sample_data/feeds/feed-sample2.xml`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/link_extractor/linkextractor.html` & `Scrapy-2.8.0/tests/sample_data/link_extractor/linkextractor.html`

 * *Files 6% similar despite different names*

```diff
@@ -9,14 +9,15 @@
     <div id='wrapper'>
       <div id='subwrapper'>
         <area href='sample1.html' alt='sample1'/>
         <a href='sample2.html'>sample 2<img src='sample2.jpg' alt='sample2'/></a>
       </div>
       <a href='http://example.com/sample3.html' title='sample 3'>sample 3 text</a>
       <a href='sample3.html'>sample 3 repetition</a>
+      <a href='sample3.html'>sample 3 repetition</a>
       <a href='sample3.html#foo'>sample 3 repetition with fragment</a>
       <a href='http://www.google.com/something'></a>
       <a href='http://example.com/innertag.html'><strong>inner</strong> tag</a>
       <a href='page 4.html'>href with whitespaces</a>
     </div>
   </body>
 </html>
```

#### html2text {}

```diff
@@ -1,3 +1,3 @@
  sample_2[sample2]
-sample_3_text sample_3_repetition sample_3_repetition_with_fragment  inner_tag
-href_with_whitespaces
+sample_3_text sample_3_repetition sample_3_repetition sample_3_repetition_with
+fragment  inner_tag href_with_whitespaces
```

### Comparing `Scrapy-2.7.1/tests/sample_data/link_extractor/linkextractor_latin1.html` & `Scrapy-2.8.0/tests/sample_data/link_extractor/linkextractor_latin1.html`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/link_extractor/linkextractor_no_href.html` & `Scrapy-2.8.0/tests/sample_data/link_extractor/linkextractor_no_href.html`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/test_site/files/images/python-logo-master-v3-TM-flattened.png` & `Scrapy-2.8.0/tests/sample_data/test_site/files/images/python-logo-master-v3-TM-flattened.png`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/test_site/files/images/python-powered-h-50x65.png` & `Scrapy-2.8.0/tests/sample_data/test_site/files/images/python-powered-h-50x65.png`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/sample_data/test_site/files/images/scrapy.png` & `Scrapy-2.8.0/tests/sample_data/test_site/files/images/scrapy.png`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/spiders.py` & `Scrapy-2.8.0/tests/spiders.py`

 * *Files 6% similar despite different names*

```diff
@@ -22,47 +22,49 @@
     def __init__(self, mockserver=None, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.mockserver = mockserver
 
 
 class MetaSpider(MockServerSpider):
 
-    name = 'meta'
+    name = "meta"
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.meta = {}
 
     def closed(self, reason):
-        self.meta['close_reason'] = reason
+        self.meta["close_reason"] = reason
 
 
 class FollowAllSpider(MetaSpider):
 
-    name = 'follow'
+    name = "follow"
     link_extractor = LinkExtractor()
 
-    def __init__(self, total=10, show=20, order="rand", maxlatency=0.0, *args, **kwargs):
+    def __init__(
+        self, total=10, show=20, order="rand", maxlatency=0.0, *args, **kwargs
+    ):
         super().__init__(*args, **kwargs)
         self.urls_visited = []
         self.times = []
-        qargs = {'total': total, 'show': show, 'order': order, 'maxlatency': maxlatency}
+        qargs = {"total": total, "show": show, "order": order, "maxlatency": maxlatency}
         url = self.mockserver.url(f"/follow?{urlencode(qargs, doseq=True)}")
         self.start_urls = [url]
 
     def parse(self, response):
         self.urls_visited.append(response.url)
         self.times.append(time.time())
         for link in self.link_extractor.extract_links(response):
             yield Request(link.url, callback=self.parse)
 
 
 class DelaySpider(MetaSpider):
 
-    name = 'delay'
+    name = "delay"
 
     def __init__(self, n=1, b=0, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.n = n
         self.b = b
         self.t1 = self.t2 = self.t2_err = 0
 
@@ -76,52 +78,52 @@
 
     def errback(self, failure):
         self.t2_err = time.time()
 
 
 class SimpleSpider(MetaSpider):
 
-    name = 'simple'
+    name = "simple"
 
     def __init__(self, url="http://localhost:8998", *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.start_urls = [url]
 
     def parse(self, response):
         self.logger.info(f"Got response {response.status}")
 
 
 class AsyncDefSpider(SimpleSpider):
 
-    name = 'asyncdef'
+    name = "asyncdef"
 
     async def parse(self, response):
         await defer.succeed(42)
         self.logger.info(f"Got response {response.status}")
 
 
 class AsyncDefAsyncioSpider(SimpleSpider):
 
-    name = 'asyncdef_asyncio'
+    name = "asyncdef_asyncio"
 
     async def parse(self, response):
         await asyncio.sleep(0.2)
         status = await get_from_asyncio_queue(response.status)
         self.logger.info(f"Got response {status}")
 
 
 class AsyncDefAsyncioReturnSpider(SimpleSpider):
 
-    name = 'asyncdef_asyncio_return'
+    name = "asyncdef_asyncio_return"
 
     async def parse(self, response):
         await asyncio.sleep(0.2)
         status = await get_from_asyncio_queue(response.status)
         self.logger.info(f"Got response {status}")
-        return [{'id': 1}, {'id': 2}]
+        return [{"id": 1}, {"id": 2}]
 
 
 class AsyncDefAsyncioReturnSingleElementSpider(SimpleSpider):
 
     name = "asyncdef_asyncio_return_single_element"
 
     async def parse(self, response):
@@ -129,140 +131,146 @@
         status = await get_from_asyncio_queue(response.status)
         self.logger.info(f"Got response {status}")
         return {"foo": 42}
 
 
 class AsyncDefAsyncioReqsReturnSpider(SimpleSpider):
 
-    name = 'asyncdef_asyncio_reqs_return'
+    name = "asyncdef_asyncio_reqs_return"
 
     async def parse(self, response):
         await asyncio.sleep(0.2)
-        req_id = response.meta.get('req_id', 0)
+        req_id = response.meta.get("req_id", 0)
         status = await get_from_asyncio_queue(response.status)
         self.logger.info(f"Got response {status}, req_id {req_id}")
         if req_id > 0:
             return
         reqs = []
         for i in range(1, 3):
-            req = Request(self.start_urls[0], dont_filter=True, meta={'req_id': i})
+            req = Request(self.start_urls[0], dont_filter=True, meta={"req_id": i})
             reqs.append(req)
         return reqs
 
 
 class AsyncDefAsyncioGenExcSpider(SimpleSpider):
-    name = 'asyncdef_asyncio_gen_exc'
+    name = "asyncdef_asyncio_gen_exc"
 
     async def parse(self, response):
         for i in range(10):
             await asyncio.sleep(0.1)
-            yield {'foo': i}
+            yield {"foo": i}
             if i > 5:
                 raise ValueError("Stopping the processing")
 
 
 class AsyncDefDeferredDirectSpider(SimpleSpider):
-    name = 'asyncdef_deferred_direct'
+    name = "asyncdef_deferred_direct"
 
     async def parse(self, response):
         resp = await get_web_client_agent_req(self.mockserver.url("/status?n=200"))
-        yield {'code': resp.code}
+        yield {"code": resp.code}
 
 
 class AsyncDefDeferredWrappedSpider(SimpleSpider):
-    name = 'asyncdef_deferred_wrapped'
+    name = "asyncdef_deferred_wrapped"
 
     async def parse(self, response):
-        resp = await deferred_to_future(get_web_client_agent_req(self.mockserver.url("/status?n=200")))
-        yield {'code': resp.code}
+        resp = await deferred_to_future(
+            get_web_client_agent_req(self.mockserver.url("/status?n=200"))
+        )
+        yield {"code": resp.code}
 
 
 class AsyncDefDeferredMaybeWrappedSpider(SimpleSpider):
-    name = 'asyncdef_deferred_wrapped'
+    name = "asyncdef_deferred_wrapped"
 
     async def parse(self, response):
-        resp = await maybe_deferred_to_future(get_web_client_agent_req(self.mockserver.url("/status?n=200")))
-        yield {'code': resp.code}
+        resp = await maybe_deferred_to_future(
+            get_web_client_agent_req(self.mockserver.url("/status?n=200"))
+        )
+        yield {"code": resp.code}
 
 
 class AsyncDefAsyncioGenSpider(SimpleSpider):
 
-    name = 'asyncdef_asyncio_gen'
+    name = "asyncdef_asyncio_gen"
 
     async def parse(self, response):
         await asyncio.sleep(0.2)
-        yield {'foo': 42}
+        yield {"foo": 42}
         self.logger.info(f"Got response {response.status}")
 
 
 class AsyncDefAsyncioGenLoopSpider(SimpleSpider):
 
-    name = 'asyncdef_asyncio_gen_loop'
+    name = "asyncdef_asyncio_gen_loop"
 
     async def parse(self, response):
         for i in range(10):
             await asyncio.sleep(0.1)
-            yield {'foo': i}
+            yield {"foo": i}
         self.logger.info(f"Got response {response.status}")
 
 
 class AsyncDefAsyncioGenComplexSpider(SimpleSpider):
 
-    name = 'asyncdef_asyncio_gen_complex'
+    name = "asyncdef_asyncio_gen_complex"
     initial_reqs = 4
     following_reqs = 3
     depth = 2
 
     def _get_req(self, index, cb=None):
-        return Request(self.mockserver.url(f"/status?n=200&request={index}"),
-                       meta={'index': index},
-                       dont_filter=True,
-                       callback=cb)
+        return Request(
+            self.mockserver.url(f"/status?n=200&request={index}"),
+            meta={"index": index},
+            dont_filter=True,
+            callback=cb,
+        )
 
     def start_requests(self):
         for i in range(1, self.initial_reqs + 1):
             yield self._get_req(i)
 
     async def parse(self, response):
-        index = response.meta['index']
-        yield {'index': index}
-        if index < 10 ** self.depth:
+        index = response.meta["index"]
+        yield {"index": index}
+        if index < 10**self.depth:
             for new_index in range(10 * index, 10 * index + self.following_reqs):
                 yield self._get_req(new_index)
         yield self._get_req(index, cb=self.parse2)
         await asyncio.sleep(0.1)
-        yield {'index': index + 5}
+        yield {"index": index + 5}
 
     async def parse2(self, response):
         await asyncio.sleep(0.1)
-        yield {'index2': response.meta['index']}
+        yield {"index2": response.meta["index"]}
 
 
 class ItemSpider(FollowAllSpider):
 
-    name = 'item'
+    name = "item"
 
     def parse(self, response):
         for request in super().parse(response):
             yield request
             yield Item()
             yield {}
 
 
 class DefaultError(Exception):
     pass
 
 
 class ErrorSpider(FollowAllSpider):
 
-    name = 'error'
+    name = "error"
     exception_cls = DefaultError
 
     def raise_exception(self):
-        raise self.exception_cls('Expected exception')
+        raise self.exception_cls("Expected exception")
 
     def parse(self, response):
         for request in super().parse(response):
             yield request
             self.raise_exception()
 
 
@@ -276,24 +284,26 @@
         self.seedsseen = []
 
     def start_requests(self):
         if self.fail_before_yield:
             1 / 0
 
         for s in range(100):
-            qargs = {'total': 10, 'seed': s}
+            qargs = {"total": 10, "seed": s}
             url = self.mockserver.url(f"/follow?{urlencode(qargs, doseq=True)}")
-            yield Request(url, meta={'seed': s})
+            yield Request(url, meta={"seed": s})
             if self.fail_yielding:
                 2 / 0
 
-        assert self.seedsseen, 'All start requests consumed before any download happened'
+        assert (
+            self.seedsseen
+        ), "All start requests consumed before any download happened"
 
     def parse(self, response):
-        self.seedsseen.append(response.meta.get('seed'))
+        self.seedsseen.append(response.meta.get("seed"))
         for req in super().parse(response):
             yield req
 
 
 class SingleRequestSpider(MetaSpider):
 
     seed = None
@@ -303,29 +313,29 @@
     def start_requests(self):
         if isinstance(self.seed, Request):
             yield self.seed.replace(callback=self.parse, errback=self.on_error)
         else:
             yield Request(self.seed, callback=self.parse, errback=self.on_error)
 
     def parse(self, response):
-        self.meta.setdefault('responses', []).append(response)
+        self.meta.setdefault("responses", []).append(response)
         if callable(self.callback_func):
             return self.callback_func(response)
-        if 'next' in response.meta:
-            return response.meta['next']
+        if "next" in response.meta:
+            return response.meta["next"]
 
     def on_error(self, failure):
-        self.meta['failure'] = failure
+        self.meta["failure"] = failure
         if callable(self.errback_func):
             return self.errback_func(failure)
 
 
 class DuplicateStartRequestsSpider(MockServerSpider):
     dont_filter = True
-    name = 'duplicatestartrequests'
+    name = "duplicatestartrequests"
     distinct_urls = 2
     dupe_factor = 3
 
     def start_requests(self):
         for i in range(0, self.distinct_urls):
             for j in range(0, self.dupe_factor):
                 url = self.mockserver.url(f"/echo?headers=1&body=test{i}")
@@ -339,21 +349,20 @@
         self.visited += 1
 
 
 class CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):
     """
     A CrawlSpider which overrides the 'parse' method
     """
-    name = 'crawl_spider_with_parse_method'
+
+    name = "crawl_spider_with_parse_method"
     custom_settings: dict = {
-        'RETRY_HTTP_CODES': [],  # no need to retry
+        "RETRY_HTTP_CODES": [],  # no need to retry
     }
-    rules = (
-        Rule(LinkExtractor(), callback='parse', follow=True),
-    )
+    rules = (Rule(LinkExtractor(), callback="parse", follow=True),)
 
     def start_requests(self):
         test_body = b"""
         <html>
             <head><title>Page title<title></head>
             <body>
                 <p><a href="/status?n=200">Item 200</a></p>  <!-- callback -->
@@ -361,47 +370,53 @@
             </body>
         </html>
         """
         url = self.mockserver.url("/alpayload")
         yield Request(url, method="POST", body=test_body)
 
     def parse(self, response, foo=None):
-        self.logger.info('[parse] status %i (foo: %s)', response.status, foo)
-        yield Request(self.mockserver.url("/status?n=202"), self.parse, cb_kwargs={"foo": "bar"})
+        self.logger.info("[parse] status %i (foo: %s)", response.status, foo)
+        yield Request(
+            self.mockserver.url("/status?n=202"), self.parse, cb_kwargs={"foo": "bar"}
+        )
 
 
 class CrawlSpiderWithAsyncCallback(CrawlSpiderWithParseMethod):
     """A CrawlSpider with an async def callback"""
-    name = 'crawl_spider_with_async_callback'
-    rules = (
-        Rule(LinkExtractor(), callback='parse_async', follow=True),
-    )
+
+    name = "crawl_spider_with_async_callback"
+    rules = (Rule(LinkExtractor(), callback="parse_async", follow=True),)
 
     async def parse_async(self, response, foo=None):
-        self.logger.info('[parse_async] status %i (foo: %s)', response.status, foo)
-        return Request(self.mockserver.url("/status?n=202"), self.parse_async, cb_kwargs={"foo": "bar"})
+        self.logger.info("[parse_async] status %i (foo: %s)", response.status, foo)
+        return Request(
+            self.mockserver.url("/status?n=202"),
+            self.parse_async,
+            cb_kwargs={"foo": "bar"},
+        )
 
 
 class CrawlSpiderWithAsyncGeneratorCallback(CrawlSpiderWithParseMethod):
     """A CrawlSpider with an async generator callback"""
-    name = 'crawl_spider_with_async_generator_callback'
-    rules = (
-        Rule(LinkExtractor(), callback='parse_async_gen', follow=True),
-    )
+
+    name = "crawl_spider_with_async_generator_callback"
+    rules = (Rule(LinkExtractor(), callback="parse_async_gen", follow=True),)
 
     async def parse_async_gen(self, response, foo=None):
-        self.logger.info('[parse_async_gen] status %i (foo: %s)', response.status, foo)
-        yield Request(self.mockserver.url("/status?n=202"), self.parse_async_gen, cb_kwargs={"foo": "bar"})
+        self.logger.info("[parse_async_gen] status %i (foo: %s)", response.status, foo)
+        yield Request(
+            self.mockserver.url("/status?n=202"),
+            self.parse_async_gen,
+            cb_kwargs={"foo": "bar"},
+        )
 
 
 class CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):
-    name = 'crawl_spider_with_errback'
-    rules = (
-        Rule(LinkExtractor(), callback='parse', errback='errback', follow=True),
-    )
+    name = "crawl_spider_with_errback"
+    rules = (Rule(LinkExtractor(), callback="parse", errback="errback", follow=True),)
 
     def start_requests(self):
         test_body = b"""
         <html>
             <head><title>Page title<title></head>
             <body>
                 <p><a href="/status?n=200">Item 200</a></p>  <!-- callback -->
@@ -412,15 +427,31 @@
             </body>
         </html>
         """
         url = self.mockserver.url("/alpayload")
         yield Request(url, method="POST", body=test_body)
 
     def errback(self, failure):
-        self.logger.info('[errback] status %i', failure.value.response.status)
+        self.logger.info("[errback] status %i", failure.value.response.status)
+
+
+class CrawlSpiderWithProcessRequestCallbackKeywordArguments(CrawlSpiderWithParseMethod):
+    name = "crawl_spider_with_process_request_cb_kwargs"
+    rules = (
+        Rule(
+            LinkExtractor(),
+            callback="parse",
+            follow=True,
+            process_request="process_request",
+        ),
+    )
+
+    def process_request(self, request, response):
+        request.cb_kwargs["foo"] = "process_request"
+        return request
 
 
 class BytesReceivedCallbackSpider(MetaSpider):
 
     full_response_length = 2**18
 
     @classmethod
@@ -442,22 +473,20 @@
 
     def bytes_received(self, data, request, spider):
         self.meta["bytes_received"] = data
         raise StopDownload(fail=False)
 
 
 class BytesReceivedErrbackSpider(BytesReceivedCallbackSpider):
-
     def bytes_received(self, data, request, spider):
         self.meta["bytes_received"] = data
         raise StopDownload(fail=True)
 
 
 class HeadersReceivedCallbackSpider(MetaSpider):
-
     @classmethod
     def from_crawler(cls, crawler, *args, **kwargs):
         spider = super().from_crawler(crawler, *args, **kwargs)
         crawler.signals.connect(spider.headers_received, signals.headers_received)
         return spider
 
     def start_requests(self):
@@ -471,11 +500,10 @@
 
     def headers_received(self, headers, body_length, request, spider):
         self.meta["headers_received"] = headers
         raise StopDownload(fail=False)
 
 
 class HeadersReceivedErrbackSpider(HeadersReceivedCallbackSpider):
-
     def headers_received(self, headers, body_length, request, spider):
         self.meta["headers_received"] = headers
         raise StopDownload(fail=True)
```

### Comparing `Scrapy-2.7.1/tests/test_closespider.py` & `Scrapy-2.8.0/tests/test_closespider.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,56 +1,56 @@
 from twisted.internet import defer
 from twisted.trial.unittest import TestCase
+
 from scrapy.utils.test import get_crawler
-from tests.spiders import FollowAllSpider, ItemSpider, ErrorSpider
 from tests.mockserver import MockServer
+from tests.spiders import ErrorSpider, FollowAllSpider, ItemSpider
 
 
 class TestCloseSpider(TestCase):
-
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
 
     def tearDown(self):
         self.mockserver.__exit__(None, None, None)
 
     @defer.inlineCallbacks
     def test_closespider_itemcount(self):
         close_on = 5
-        crawler = get_crawler(ItemSpider, {'CLOSESPIDER_ITEMCOUNT': close_on})
+        crawler = get_crawler(ItemSpider, {"CLOSESPIDER_ITEMCOUNT": close_on})
         yield crawler.crawl(mockserver=self.mockserver)
-        reason = crawler.spider.meta['close_reason']
-        self.assertEqual(reason, 'closespider_itemcount')
-        itemcount = crawler.stats.get_value('item_scraped_count')
+        reason = crawler.spider.meta["close_reason"]
+        self.assertEqual(reason, "closespider_itemcount")
+        itemcount = crawler.stats.get_value("item_scraped_count")
         self.assertTrue(itemcount >= close_on)
 
     @defer.inlineCallbacks
     def test_closespider_pagecount(self):
         close_on = 5
-        crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_PAGECOUNT': close_on})
+        crawler = get_crawler(FollowAllSpider, {"CLOSESPIDER_PAGECOUNT": close_on})
         yield crawler.crawl(mockserver=self.mockserver)
-        reason = crawler.spider.meta['close_reason']
-        self.assertEqual(reason, 'closespider_pagecount')
-        pagecount = crawler.stats.get_value('response_received_count')
+        reason = crawler.spider.meta["close_reason"]
+        self.assertEqual(reason, "closespider_pagecount")
+        pagecount = crawler.stats.get_value("response_received_count")
         self.assertTrue(pagecount >= close_on)
 
     @defer.inlineCallbacks
     def test_closespider_errorcount(self):
         close_on = 5
-        crawler = get_crawler(ErrorSpider, {'CLOSESPIDER_ERRORCOUNT': close_on})
+        crawler = get_crawler(ErrorSpider, {"CLOSESPIDER_ERRORCOUNT": close_on})
         yield crawler.crawl(total=1000000, mockserver=self.mockserver)
-        reason = crawler.spider.meta['close_reason']
-        self.assertEqual(reason, 'closespider_errorcount')
-        key = f'spider_exceptions/{crawler.spider.exception_cls.__name__}'
+        reason = crawler.spider.meta["close_reason"]
+        self.assertEqual(reason, "closespider_errorcount")
+        key = f"spider_exceptions/{crawler.spider.exception_cls.__name__}"
         errorcount = crawler.stats.get_value(key)
         self.assertTrue(errorcount >= close_on)
 
     @defer.inlineCallbacks
     def test_closespider_timeout(self):
         close_on = 0.1
-        crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_TIMEOUT': close_on})
+        crawler = get_crawler(FollowAllSpider, {"CLOSESPIDER_TIMEOUT": close_on})
         yield crawler.crawl(total=1000000, mockserver=self.mockserver)
-        reason = crawler.spider.meta['close_reason']
-        self.assertEqual(reason, 'closespider_timeout')
-        total_seconds = crawler.stats.get_value('elapsed_time_seconds')
+        reason = crawler.spider.meta["close_reason"]
+        self.assertEqual(reason, "closespider_timeout")
+        total_seconds = crawler.stats.get_value("elapsed_time_seconds")
         self.assertTrue(total_seconds >= close_on)
```

### Comparing `Scrapy-2.7.1/tests/test_cmdline/__init__.py` & `Scrapy-2.8.0/tests/test_cmdline/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,70 +1,73 @@
 import json
-import os
 import pstats
 import shutil
 import sys
 import tempfile
 import unittest
 from io import StringIO
-from subprocess import Popen, PIPE
+from pathlib import Path
+from subprocess import PIPE, Popen
 
 from scrapy.utils.test import get_testenv
 
 
 class CmdlineTest(unittest.TestCase):
-
     def setUp(self):
         self.env = get_testenv()
-        self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'
+        self.env["SCRAPY_SETTINGS_MODULE"] = "tests.test_cmdline.settings"
 
     def _execute(self, *new_args, **kwargs):
-        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'
-        args = (sys.executable, '-m', 'scrapy.cmdline') + new_args
+        encoding = getattr(sys.stdout, "encoding") or "utf-8"
+        args = (sys.executable, "-m", "scrapy.cmdline") + new_args
         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)
         comm = proc.communicate()[0].strip()
         return comm.decode(encoding)
 
     def test_default_settings(self):
-        self.assertEqual(self._execute('settings', '--get', 'TEST1'), 'default')
+        self.assertEqual(self._execute("settings", "--get", "TEST1"), "default")
 
     def test_override_settings_using_set_arg(self):
-        self.assertEqual(self._execute('settings', '--get', 'TEST1', '-s',
-                                       'TEST1=override'), 'override')
-
-    def test_override_settings_using_envvar(self):
-        self.env['SCRAPY_TEST1'] = 'override'
-        self.assertEqual(self._execute('settings', '--get', 'TEST1'), 'override')
+        self.assertEqual(
+            self._execute("settings", "--get", "TEST1", "-s", "TEST1=override"),
+            "override",
+        )
 
     def test_profiling(self):
-        path = tempfile.mkdtemp()
-        filename = os.path.join(path, 'res.prof')
+        path = Path(tempfile.mkdtemp())
+        filename = path / "res.prof"
         try:
-            self._execute('version', '--profile', filename)
-            self.assertTrue(os.path.exists(filename))
+            self._execute("version", "--profile", str(filename))
+            self.assertTrue(filename.exists())
             out = StringIO()
-            stats = pstats.Stats(filename, stream=out)
+            stats = pstats.Stats(str(filename), stream=out)
             stats.print_stats()
             out.seek(0)
             stats = out.read()
-            self.assertIn(os.path.join('scrapy', 'commands', 'version.py'),
-                          stats)
-            self.assertIn('tottime', stats)
+            self.assertIn(str(Path("scrapy", "commands", "version.py")), stats)
+            self.assertIn("tottime", stats)
         finally:
             shutil.rmtree(path)
 
     def test_override_dict_settings(self):
         EXT_PATH = "tests.test_cmdline.extensions.DummyExtension"
         EXTENSIONS = {EXT_PATH: 200}
-        settingsstr = self._execute('settings', '--get', 'EXTENSIONS', '-s',
-                                    'EXTENSIONS=' + json.dumps(EXTENSIONS))
+        settingsstr = self._execute(
+            "settings",
+            "--get",
+            "EXTENSIONS",
+            "-s",
+            "EXTENSIONS=" + json.dumps(EXTENSIONS),
+        )
         # XXX: There's gotta be a smarter way to do this...
         self.assertNotIn("...", settingsstr)
         for char in ("'", "<", ">"):
             settingsstr = settingsstr.replace(char, '"')
         settingsdict = json.loads(settingsstr)
         self.assertCountEqual(settingsdict.keys(), EXTENSIONS.keys())
         self.assertEqual(200, settingsdict[EXT_PATH])
 
     def test_pathlib_path_as_feeds_key(self):
-        self.assertEqual(self._execute('settings', '--get', 'FEEDS'),
-                         json.dumps({"items.csv": {"format": "csv", "fields": ["price", "name"]}}))
+        self.assertEqual(
+            self._execute("settings", "--get", "FEEDS"),
+            json.dumps({"items.csv": {"format": "csv", "fields": ["price", "name"]}}),
+        )
```

### Comparing `Scrapy-2.7.1/tests/test_cmdline_crawl_with_pipeline/__init__.py` & `Scrapy-2.8.0/tests/test_cmdline_crawl_with_pipeline/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,20 +1,19 @@
-import os
 import sys
 import unittest
-from subprocess import Popen, PIPE
+from pathlib import Path
+from subprocess import PIPE, Popen
 
 
 class CmdlineCrawlPipelineTest(unittest.TestCase):
-
     def _execute(self, spname):
-        args = (sys.executable, '-m', 'scrapy.cmdline', 'crawl', spname)
-        cwd = os.path.dirname(os.path.abspath(__file__))
+        args = (sys.executable, "-m", "scrapy.cmdline", "crawl", spname)
+        cwd = Path(__file__).resolve().parent
         proc = Popen(args, stdout=PIPE, stderr=PIPE, cwd=cwd)
         proc.communicate()
         return proc.returncode
 
     def test_open_spider_normally_in_pipeline(self):
-        self.assertEqual(self._execute('normal'), 0)
+        self.assertEqual(self._execute("normal"), 0)
 
     def test_exception_at_open_spider_in_pipeline(self):
-        self.assertEqual(self._execute('exception'), 1)
+        self.assertEqual(self._execute("exception"), 1)
```

### Comparing `Scrapy-2.7.1/tests/test_command_check.py` & `Scrapy-2.8.0/tests/test_command_check.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-from os.path import join, abspath
-
 from tests.test_commands import CommandTest
 
 
 class CheckCommandTest(CommandTest):
 
-    command = 'check'
+    command = "check"
 
     def setUp(self):
-        super(CheckCommandTest, self).setUp()
-        self.spider_name = 'check_spider'
-        self.spider = abspath(join(self.proj_mod_path, 'spiders', 'checkspider.py'))
+        super().setUp()
+        self.spider_name = "check_spider"
+        self.spider = (self.proj_mod_path / "spiders" / "checkspider.py").resolve()
 
     def _write_contract(self, contracts, parse_def):
-        with open(self.spider, 'w') as file:
-            file.write(f"""
+        self.spider.write_text(
+            f"""
 import scrapy
 
 class CheckSpider(scrapy.Spider):
     name = '{self.spider_name}'
     start_urls = ['http://toscrape.com']
 
     def parse(self, response, **cb_kwargs):
         \"\"\"
         @url http://toscrape.com
         {contracts}
         \"\"\"
         {parse_def}
-            """)
+        """,
+            encoding="utf-8",
+        )
 
-    def _test_contract(self, contracts='', parse_def='pass'):
+    def _test_contract(self, contracts="", parse_def="pass"):
         self._write_contract(contracts, parse_def)
-        p, out, err = self.proc('check')
-        self.assertNotIn('F', out)
-        self.assertIn('OK', err)
+        p, out, err = self.proc("check")
+        self.assertNotIn("F", out)
+        self.assertIn("OK", err)
         self.assertEqual(p.returncode, 0)
 
     def test_check_returns_requests_contract(self):
         contracts = """
         @returns requests 1
         """
         parse_def = """
```

### Comparing `Scrapy-2.7.1/tests/test_command_fetch.py` & `Scrapy-2.8.0/tests/test_command_fetch.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,34 +1,36 @@
-from twisted.trial import unittest
 from twisted.internet import defer
+from twisted.trial import unittest
 
-from scrapy.utils.testsite import SiteTest
 from scrapy.utils.testproc import ProcessTest
+from scrapy.utils.testsite import SiteTest
 
 
 class FetchTest(ProcessTest, SiteTest, unittest.TestCase):
 
-    command = 'fetch'
+    command = "fetch"
 
     @defer.inlineCallbacks
     def test_output(self):
-        _, out, _ = yield self.execute([self.url('/text')])
-        self.assertEqual(out.strip(), b'Works')
+        _, out, _ = yield self.execute([self.url("/text")])
+        self.assertEqual(out.strip(), b"Works")
 
     @defer.inlineCallbacks
     def test_redirect_default(self):
-        _, out, _ = yield self.execute([self.url('/redirect')])
-        self.assertEqual(out.strip(), b'Redirected here')
+        _, out, _ = yield self.execute([self.url("/redirect")])
+        self.assertEqual(out.strip(), b"Redirected here")
 
     @defer.inlineCallbacks
     def test_redirect_disabled(self):
-        _, out, err = yield self.execute(['--no-redirect', self.url('/redirect-no-meta-refresh')])
+        _, out, err = yield self.execute(
+            ["--no-redirect", self.url("/redirect-no-meta-refresh")]
+        )
         err = err.strip()
-        self.assertIn(b'downloader/response_status_count/302', err, err)
-        self.assertNotIn(b'downloader/response_status_count/200', err, err)
+        self.assertIn(b"downloader/response_status_count/302", err, err)
+        self.assertNotIn(b"downloader/response_status_count/200", err, err)
 
     @defer.inlineCallbacks
     def test_headers(self):
-        _, out, _ = yield self.execute([self.url('/text'), '--headers'])
-        out = out.replace(b'\r', b'')  # required on win32
-        assert b'Server: TwistedWeb' in out, out
-        assert b'Content-Type: text/plain' in out
+        _, out, _ = yield self.execute([self.url("/text"), "--headers"])
+        out = out.replace(b"\r", b"")  # required on win32
+        assert b"Server: TwistedWeb" in out, out
+        assert b"Content-Type: text/plain" in out
```

### Comparing `Scrapy-2.7.1/tests/test_command_parse.py` & `Scrapy-2.8.0/tests/test_command_parse.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-import os
 import argparse
-from os.path import join, abspath, isfile, exists
+import os
+from pathlib import Path
 
 from twisted.internet import defer
+
 from scrapy.commands import parse
 from scrapy.settings import Settings
-from scrapy.utils.testsite import SiteTest
-from scrapy.utils.testproc import ProcessTest
 from scrapy.utils.python import to_unicode
+from scrapy.utils.testproc import ProcessTest
+from scrapy.utils.testsite import SiteTest
 from tests.test_commands import CommandTest
 
 
 def _textmode(bstr):
     """Normalize input the same as writing to a file
     and reading from it in text mode"""
-    return to_unicode(bstr).replace(os.linesep, '\n')
+    return to_unicode(bstr).replace(os.linesep, "\n")
 
 
 class ParseCommandTest(ProcessTest, SiteTest, CommandTest):
-    command = 'parse'
+    command = "parse"
 
     def setUp(self):
         super().setUp()
-        self.spider_name = 'parse_spider'
-        fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))
-        with open(fname, 'w') as f:
-            f.write(f"""
+        self.spider_name = "parse_spider"
+        (self.proj_mod_path / "spiders" / "myspider.py").write_text(
+            f"""
 import scrapy
 from scrapy.linkextractors import LinkExtractor
 from scrapy.spiders import CrawlSpider, Rule
 from scrapy.utils.test import get_from_asyncio_queue
 
 class AsyncDefAsyncioSpider(scrapy.Spider):
 
@@ -90,192 +90,248 @@
 
     rules = (
         Rule(LinkExtractor(allow=r'/html'), callback='parse_item', follow=True),
     )
 
     def parse(self, response):
         return [scrapy.Item(), dict(foo='bar')]
-""")
+""",
+            encoding="utf-8",
+        )
 
-        fname = abspath(join(self.proj_mod_path, 'pipelines.py'))
-        with open(fname, 'w') as f:
-            f.write("""
+        (self.proj_mod_path / "pipelines.py").write_text(
+            """
 import logging
 
 class MyPipeline:
     component_name = 'my_pipeline'
 
     def process_item(self, item, spider):
         logging.info('It Works!')
         return item
-""")
+""",
+            encoding="utf-8",
+        )
 
-        fname = abspath(join(self.proj_mod_path, 'settings.py'))
-        with open(fname, 'a') as f:
-            f.write(f"""
+        with (self.proj_mod_path / "settings.py").open("a", encoding="utf-8") as f:
+            f.write(
+                f"""
 ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}
-""")
+"""
+            )
 
     @defer.inlineCallbacks
     def test_spider_arguments(self):
-        _, _, stderr = yield self.execute(['--spider', self.spider_name,
-                                           '-a', 'test_arg=1',
-                                           '-c', 'parse',
-                                           '--verbose',
-                                           self.url('/html')])
+        _, _, stderr = yield self.execute(
+            [
+                "--spider",
+                self.spider_name,
+                "-a",
+                "test_arg=1",
+                "-c",
+                "parse",
+                "--verbose",
+                self.url("/html"),
+            ]
+        )
         self.assertIn("DEBUG: It Works!", _textmode(stderr))
 
     @defer.inlineCallbacks
     def test_request_with_meta(self):
         raw_json_string = '{"foo" : "baz"}'
-        _, _, stderr = yield self.execute(['--spider', self.spider_name,
-                                           '--meta', raw_json_string,
-                                           '-c', 'parse_request_with_meta',
-                                           '--verbose',
-                                           self.url('/html')])
+        _, _, stderr = yield self.execute(
+            [
+                "--spider",
+                self.spider_name,
+                "--meta",
+                raw_json_string,
+                "-c",
+                "parse_request_with_meta",
+                "--verbose",
+                self.url("/html"),
+            ]
+        )
         self.assertIn("DEBUG: It Works!", _textmode(stderr))
 
-        _, _, stderr = yield self.execute(['--spider', self.spider_name,
-                                           '-m', raw_json_string,
-                                           '-c', 'parse_request_with_meta',
-                                           '--verbose',
-                                           self.url('/html')])
+        _, _, stderr = yield self.execute(
+            [
+                "--spider",
+                self.spider_name,
+                "-m",
+                raw_json_string,
+                "-c",
+                "parse_request_with_meta",
+                "--verbose",
+                self.url("/html"),
+            ]
+        )
         self.assertIn("DEBUG: It Works!", _textmode(stderr))
 
     @defer.inlineCallbacks
     def test_request_with_cb_kwargs(self):
         raw_json_string = '{"foo" : "bar", "key": "value"}'
-        _, _, stderr = yield self.execute(['--spider', self.spider_name,
-                                           '--cbkwargs', raw_json_string,
-                                           '-c', 'parse_request_with_cb_kwargs',
-                                           '--verbose',
-                                           self.url('/html')])
+        _, _, stderr = yield self.execute(
+            [
+                "--spider",
+                self.spider_name,
+                "--cbkwargs",
+                raw_json_string,
+                "-c",
+                "parse_request_with_cb_kwargs",
+                "--verbose",
+                self.url("/html"),
+            ]
+        )
         self.assertIn("DEBUG: It Works!", _textmode(stderr))
 
     @defer.inlineCallbacks
     def test_request_without_meta(self):
-        _, _, stderr = yield self.execute(['--spider', self.spider_name,
-                                           '-c', 'parse_request_without_meta',
-                                           '--nolinks',
-                                           self.url('/html')])
+        _, _, stderr = yield self.execute(
+            [
+                "--spider",
+                self.spider_name,
+                "-c",
+                "parse_request_without_meta",
+                "--nolinks",
+                self.url("/html"),
+            ]
+        )
         self.assertIn("DEBUG: It Works!", _textmode(stderr))
 
     @defer.inlineCallbacks
     def test_pipelines(self):
-        _, _, stderr = yield self.execute(['--spider', self.spider_name,
-                                           '--pipelines',
-                                           '-c', 'parse',
-                                           '--verbose',
-                                           self.url('/html')])
+        _, _, stderr = yield self.execute(
+            [
+                "--spider",
+                self.spider_name,
+                "--pipelines",
+                "-c",
+                "parse",
+                "--verbose",
+                self.url("/html"),
+            ]
+        )
         self.assertIn("INFO: It Works!", _textmode(stderr))
 
     @defer.inlineCallbacks
     def test_asyncio_parse_items(self):
         status, out, stderr = yield self.execute(
-            ['--spider', 'asyncdef' + self.spider_name, '-c', 'parse', self.url('/html')]
+            [
+                "--spider",
+                "asyncdef" + self.spider_name,
+                "-c",
+                "parse",
+                self.url("/html"),
+            ]
         )
         self.assertIn("""[{}, {'foo': 'bar'}]""", _textmode(out))
 
     @defer.inlineCallbacks
     def test_parse_items(self):
         status, out, stderr = yield self.execute(
-            ['--spider', self.spider_name, '-c', 'parse', self.url('/html')]
+            ["--spider", self.spider_name, "-c", "parse", self.url("/html")]
         )
         self.assertIn("""[{}, {'foo': 'bar'}]""", _textmode(out))
 
     @defer.inlineCallbacks
     def test_parse_items_no_callback_passed(self):
         status, out, stderr = yield self.execute(
-            ['--spider', self.spider_name, self.url('/html')]
+            ["--spider", self.spider_name, self.url("/html")]
         )
         self.assertIn("""[{}, {'foo': 'bar'}]""", _textmode(out))
 
     @defer.inlineCallbacks
     def test_wrong_callback_passed(self):
         status, out, stderr = yield self.execute(
-            ['--spider', self.spider_name, '-c', 'dummy', self.url('/html')]
+            ["--spider", self.spider_name, "-c", "dummy", self.url("/html")]
         )
         self.assertRegex(_textmode(out), r"""# Scraped Items  -+\n\[\]""")
         self.assertIn("""Cannot find callback""", _textmode(stderr))
 
     @defer.inlineCallbacks
     def test_crawlspider_matching_rule_callback_set(self):
         """If a rule matches the URL, use it's defined callback."""
         status, out, stderr = yield self.execute(
-            ['--spider', 'goodcrawl' + self.spider_name, '-r', self.url('/html')]
+            ["--spider", "goodcrawl" + self.spider_name, "-r", self.url("/html")]
         )
         self.assertIn("""[{}, {'foo': 'bar'}]""", _textmode(out))
 
     @defer.inlineCallbacks
     def test_crawlspider_matching_rule_default_callback(self):
         """If a rule match but it has no callback set, use the 'parse' callback."""
         status, out, stderr = yield self.execute(
-            ['--spider', 'goodcrawl' + self.spider_name, '-r', self.url('/text')]
+            ["--spider", "goodcrawl" + self.spider_name, "-r", self.url("/text")]
         )
         self.assertIn("""[{}, {'nomatch': 'default'}]""", _textmode(out))
 
     @defer.inlineCallbacks
     def test_spider_with_no_rules_attribute(self):
         """Using -r with a spider with no rule should not produce items."""
         status, out, stderr = yield self.execute(
-            ['--spider', self.spider_name, '-r', self.url('/html')]
+            ["--spider", self.spider_name, "-r", self.url("/html")]
         )
         self.assertRegex(_textmode(out), r"""# Scraped Items  -+\n\[\]""")
         self.assertIn("""No CrawlSpider rules found""", _textmode(stderr))
 
     @defer.inlineCallbacks
     def test_crawlspider_missing_callback(self):
         status, out, stderr = yield self.execute(
-            ['--spider', 'badcrawl' + self.spider_name, '-r', self.url('/html')]
+            ["--spider", "badcrawl" + self.spider_name, "-r", self.url("/html")]
         )
         self.assertRegex(_textmode(out), r"""# Scraped Items  -+\n\[\]""")
 
     @defer.inlineCallbacks
     def test_crawlspider_no_matching_rule(self):
         """The requested URL has no matching rule, so no items should be scraped"""
         status, out, stderr = yield self.execute(
-            ['--spider', 'badcrawl' + self.spider_name, '-r', self.url('/enc-gb18030')]
+            ["--spider", "badcrawl" + self.spider_name, "-r", self.url("/enc-gb18030")]
         )
         self.assertRegex(_textmode(out), r"""# Scraped Items  -+\n\[\]""")
         self.assertIn("""Cannot find a rule that matches""", _textmode(stderr))
 
     @defer.inlineCallbacks
     def test_crawlspider_not_exists_with_not_matched_url(self):
-        status, out, stderr = yield self.execute([self.url('/invalid_url')])
+        status, out, stderr = yield self.execute([self.url("/invalid_url")])
         self.assertEqual(status, 0)
 
     @defer.inlineCallbacks
     def test_output_flag(self):
         """Checks if a file was created successfully having
         correct format containing correct data in it.
         """
-        file_name = 'data.json'
-        file_path = join(self.proj_path, file_name)
-        yield self.execute([
-            '--spider', self.spider_name,
-            '-c', 'parse',
-            '-o', file_name,
-            self.url('/html')
-        ])
+        file_name = "data.json"
+        file_path = Path(self.proj_path, file_name)
+        yield self.execute(
+            [
+                "--spider",
+                self.spider_name,
+                "-c",
+                "parse",
+                "-o",
+                file_name,
+                self.url("/html"),
+            ]
+        )
 
-        self.assertTrue(exists(file_path))
-        self.assertTrue(isfile(file_path))
+        self.assertTrue(file_path.exists())
+        self.assertTrue(file_path.is_file())
 
         content = '[\n{},\n{"foo": "bar"}\n]'
-        with open(file_path, 'r') as f:
-            self.assertEqual(f.read(), content)
+        self.assertEqual(file_path.read_text(encoding="utf-8"), content)
 
     def test_parse_add_options(self):
         command = parse.Command()
         command.settings = Settings()
         parser = argparse.ArgumentParser(
-            prog='scrapy', formatter_class=argparse.HelpFormatter,
-            conflict_handler='resolve', prefix_chars='-'
+            prog="scrapy",
+            formatter_class=argparse.HelpFormatter,
+            conflict_handler="resolve",
+            prefix_chars="-",
         )
         command.add_options(parser)
         namespace = parser.parse_args(
-            ['--verbose', '--nolinks', '-d', '2', '--spider', self.spider_name]
+            ["--verbose", "--nolinks", "-d", "2", "--spider", self.spider_name]
         )
         self.assertTrue(namespace.nolinks)
         self.assertEqual(namespace.depth, 2)
         self.assertEqual(namespace.spider, self.spider_name)
         self.assertTrue(namespace.verbose)
```

### Comparing `Scrapy-2.7.1/tests/test_command_shell.py` & `Scrapy-2.8.0/tests/test_command_shell.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,117 +1,136 @@
-from os.path import join
+from pathlib import Path
 
-from twisted.trial import unittest
 from twisted.internet import defer
+from twisted.trial import unittest
 
-from scrapy.utils.testsite import SiteTest
 from scrapy.utils.testproc import ProcessTest
-
-from tests import tests_datadir, NON_EXISTING_RESOLVABLE
+from scrapy.utils.testsite import SiteTest
+from tests import NON_EXISTING_RESOLVABLE, tests_datadir
 
 
 class ShellTest(ProcessTest, SiteTest, unittest.TestCase):
 
-    command = 'shell'
+    command = "shell"
 
     @defer.inlineCallbacks
     def test_empty(self):
-        _, out, _ = yield self.execute(['-c', 'item'])
-        assert b'{}' in out
+        _, out, _ = yield self.execute(["-c", "item"])
+        assert b"{}" in out
 
     @defer.inlineCallbacks
     def test_response_body(self):
-        _, out, _ = yield self.execute([self.url('/text'), '-c', 'response.body'])
-        assert b'Works' in out
+        _, out, _ = yield self.execute([self.url("/text"), "-c", "response.body"])
+        assert b"Works" in out
 
     @defer.inlineCallbacks
     def test_response_type_text(self):
-        _, out, _ = yield self.execute([self.url('/text'), '-c', 'type(response)'])
-        assert b'TextResponse' in out
+        _, out, _ = yield self.execute([self.url("/text"), "-c", "type(response)"])
+        assert b"TextResponse" in out
 
     @defer.inlineCallbacks
     def test_response_type_html(self):
-        _, out, _ = yield self.execute([self.url('/html'), '-c', 'type(response)'])
-        assert b'HtmlResponse' in out
+        _, out, _ = yield self.execute([self.url("/html"), "-c", "type(response)"])
+        assert b"HtmlResponse" in out
 
     @defer.inlineCallbacks
     def test_response_selector_html(self):
-        xpath = 'response.xpath("//p[@class=\'one\']/text()").get()'
-        _, out, _ = yield self.execute([self.url('/html'), '-c', xpath])
-        self.assertEqual(out.strip(), b'Works')
+        xpath = "response.xpath(\"//p[@class='one']/text()\").get()"
+        _, out, _ = yield self.execute([self.url("/html"), "-c", xpath])
+        self.assertEqual(out.strip(), b"Works")
 
     @defer.inlineCallbacks
     def test_response_encoding_gb18030(self):
-        _, out, _ = yield self.execute([self.url('/enc-gb18030'), '-c', 'response.encoding'])
-        self.assertEqual(out.strip(), b'gb18030')
+        _, out, _ = yield self.execute(
+            [self.url("/enc-gb18030"), "-c", "response.encoding"]
+        )
+        self.assertEqual(out.strip(), b"gb18030")
 
     @defer.inlineCallbacks
     def test_redirect(self):
-        _, out, _ = yield self.execute([self.url('/redirect'), '-c', 'response.url'])
-        assert out.strip().endswith(b'/redirected')
+        _, out, _ = yield self.execute([self.url("/redirect"), "-c", "response.url"])
+        assert out.strip().endswith(b"/redirected")
 
     @defer.inlineCallbacks
     def test_redirect_follow_302(self):
-        _, out, _ = yield self.execute([self.url('/redirect-no-meta-refresh'), '-c', 'response.status'])
-        assert out.strip().endswith(b'200')
+        _, out, _ = yield self.execute(
+            [self.url("/redirect-no-meta-refresh"), "-c", "response.status"]
+        )
+        assert out.strip().endswith(b"200")
 
     @defer.inlineCallbacks
     def test_redirect_not_follow_302(self):
         _, out, _ = yield self.execute(
-            ['--no-redirect', self.url('/redirect-no-meta-refresh'), '-c', 'response.status']
+            [
+                "--no-redirect",
+                self.url("/redirect-no-meta-refresh"),
+                "-c",
+                "response.status",
+            ]
         )
-        assert out.strip().endswith(b'302')
+        assert out.strip().endswith(b"302")
 
     @defer.inlineCallbacks
     def test_fetch_redirect_follow_302(self):
         """Test that calling ``fetch(url)`` follows HTTP redirects by default."""
-        url = self.url('/redirect-no-meta-refresh')
+        url = self.url("/redirect-no-meta-refresh")
         code = f"fetch('{url}')"
-        errcode, out, errout = yield self.execute(['-c', code])
+        errcode, out, errout = yield self.execute(["-c", code])
         self.assertEqual(errcode, 0, out)
-        assert b'Redirecting (302)' in errout
-        assert b'Crawled (200)' in errout
+        assert b"Redirecting (302)" in errout
+        assert b"Crawled (200)" in errout
 
     @defer.inlineCallbacks
     def test_fetch_redirect_not_follow_302(self):
         """Test that calling ``fetch(url, redirect=False)`` disables automatic redirects."""
-        url = self.url('/redirect-no-meta-refresh')
+        url = self.url("/redirect-no-meta-refresh")
         code = f"fetch('{url}', redirect=False)"
-        errcode, out, errout = yield self.execute(['-c', code])
+        errcode, out, errout = yield self.execute(["-c", code])
         self.assertEqual(errcode, 0, out)
-        assert b'Crawled (302)' in errout
+        assert b"Crawled (302)" in errout
 
     @defer.inlineCallbacks
     def test_request_replace(self):
-        url = self.url('/text')
+        url = self.url("/text")
         code = f"fetch('{url}') or fetch(response.request.replace(method='POST'))"
-        errcode, out, _ = yield self.execute(['-c', code])
+        errcode, out, _ = yield self.execute(["-c", code])
         self.assertEqual(errcode, 0, out)
 
     @defer.inlineCallbacks
     def test_scrapy_import(self):
-        url = self.url('/text')
+        url = self.url("/text")
         code = f"fetch(scrapy.Request('{url}'))"
-        errcode, out, _ = yield self.execute(['-c', code])
+        errcode, out, _ = yield self.execute(["-c", code])
         self.assertEqual(errcode, 0, out)
 
     @defer.inlineCallbacks
     def test_local_file(self):
-        filepath = join(tests_datadir, 'test_site', 'index.html')
-        _, out, _ = yield self.execute([filepath, '-c', 'item'])
-        assert b'{}' in out
+        filepath = Path(tests_datadir, "test_site", "index.html")
+        _, out, _ = yield self.execute([str(filepath), "-c", "item"])
+        assert b"{}" in out
 
     @defer.inlineCallbacks
     def test_local_nofile(self):
-        filepath = 'file:///tests/sample_data/test_site/nothinghere.html'
-        errcode, out, err = yield self.execute([filepath, '-c', 'item'], check_code=False)
+        filepath = "file:///tests/sample_data/test_site/nothinghere.html"
+        errcode, out, err = yield self.execute(
+            [filepath, "-c", "item"], check_code=False
+        )
         self.assertEqual(errcode, 1, out or err)
-        self.assertIn(b'No such file or directory', err)
+        self.assertIn(b"No such file or directory", err)
 
     @defer.inlineCallbacks
     def test_dns_failures(self):
         if NON_EXISTING_RESOLVABLE:
             raise unittest.SkipTest("Non-existing hosts are resolvable")
-        url = 'www.somedomainthatdoesntexi.st'
-        errcode, out, err = yield self.execute([url, '-c', 'item'], check_code=False)
+        url = "www.somedomainthatdoesntexi.st"
+        errcode, out, err = yield self.execute([url, "-c", "item"], check_code=False)
         self.assertEqual(errcode, 1, out or err)
-        self.assertIn(b'DNS lookup failed', err)
+        self.assertIn(b"DNS lookup failed", err)
+
+    @defer.inlineCallbacks
+    def test_shell_fetch_async(self):
+        reactor_path = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
+        url = self.url("/html")
+        code = f"fetch('{url}')"
+        args = ["-c", code, "--set", f"TWISTED_REACTOR={reactor_path}"]
+        _, _, err = yield self.execute(args, check_code=True)
+        self.assertNotIn(b"RuntimeError: There is no current event loop in thread", err)
```

### Comparing `Scrapy-2.7.1/tests/test_command_version.py` & `Scrapy-2.8.0/tests/test_command_version.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,33 +1,46 @@
 import sys
-from twisted.trial import unittest
+
 from twisted.internet import defer
+from twisted.trial import unittest
 
 import scrapy
 from scrapy.utils.testproc import ProcessTest
 
 
 class VersionTest(ProcessTest, unittest.TestCase):
 
-    command = 'version'
+    command = "version"
 
     @defer.inlineCallbacks
     def test_output(self):
-        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'
+        encoding = getattr(sys.stdout, "encoding") or "utf-8"
         _, out, _ = yield self.execute([])
         self.assertEqual(
             out.strip().decode(encoding),
             f"Scrapy {scrapy.__version__}",
         )
 
     @defer.inlineCallbacks
     def test_verbose_output(self):
-        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'
-        _, out, _ = yield self.execute(['-v'])
+        encoding = getattr(sys.stdout, "encoding") or "utf-8"
+        _, out, _ = yield self.execute(["-v"])
         headers = [
             line.partition(":")[0].strip()
             for line in out.strip().decode(encoding).splitlines()
         ]
-        self.assertEqual(headers, ['Scrapy', 'lxml', 'libxml2',
-                                   'cssselect', 'parsel', 'w3lib',
-                                   'Twisted', 'Python', 'pyOpenSSL',
-                                   'cryptography', 'Platform'])
+        self.assertEqual(
+            headers,
+            [
+                "Scrapy",
+                "lxml",
+                "libxml2",
+                "cssselect",
+                "parsel",
+                "w3lib",
+                "Twisted",
+                "Python",
+                "pyOpenSSL",
+                "cryptography",
+                "Platform",
+            ],
+        )
```

### Comparing `Scrapy-2.7.1/tests/test_commands.py` & `Scrapy-2.8.0/tests/test_commands.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,575 +1,615 @@
+import argparse
 import inspect
 import json
-import argparse
 import os
 import platform
 import re
 import subprocess
 import sys
 import tempfile
 from contextlib import contextmanager
 from itertools import chain
-from os.path import exists, join, abspath, getmtime
 from pathlib import Path
-from shutil import rmtree, copytree
+from shutil import copytree, rmtree
 from stat import S_IWRITE as ANYONE_WRITE_PERMISSION
 from tempfile import mkdtemp
 from threading import Timer
+from typing import Dict, Generator, Optional, Union
 from unittest import skipIf
 
 from pytest import mark
 from twisted import version as twisted_version
 from twisted.python.versions import Version
 from twisted.trial import unittest
 
 import scrapy
-from scrapy.commands import view, ScrapyCommand, ScrapyHelpFormatter
+from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view
 from scrapy.commands.startproject import IGNORE
 from scrapy.settings import Settings
 from scrapy.utils.python import to_unicode
 from scrapy.utils.test import get_testenv
-
 from tests.test_crawler import ExceptionSpider, NoRequestsSpider
 
 
 class CommandSettings(unittest.TestCase):
-
     def setUp(self):
         self.command = ScrapyCommand()
         self.command.settings = Settings()
-        self.parser = argparse.ArgumentParser(formatter_class=ScrapyHelpFormatter,
-                                              conflict_handler='resolve')
+        self.parser = argparse.ArgumentParser(
+            formatter_class=ScrapyHelpFormatter, conflict_handler="resolve"
+        )
         self.command.add_options(self.parser)
 
     def test_settings_json_string(self):
         feeds_json = '{"data.json": {"format": "json"}, "data.xml": {"format": "xml"}}'
-        opts, args = self.parser.parse_known_args(args=['-s', f'FEEDS={feeds_json}', 'spider.py'])
+        opts, args = self.parser.parse_known_args(
+            args=["-s", f"FEEDS={feeds_json}", "spider.py"]
+        )
         self.command.process_options(args, opts)
-        self.assertIsInstance(self.command.settings['FEEDS'], scrapy.settings.BaseSettings)
-        self.assertEqual(dict(self.command.settings['FEEDS']), json.loads(feeds_json))
+        self.assertIsInstance(
+            self.command.settings["FEEDS"], scrapy.settings.BaseSettings
+        )
+        self.assertEqual(dict(self.command.settings["FEEDS"]), json.loads(feeds_json))
 
     def test_help_formatter(self):
-        formatter = ScrapyHelpFormatter(prog='scrapy')
-        part_strings = ['usage: scrapy genspider [options] <name> <domain>\n\n',
-                        '\n', 'optional arguments:\n', '\n', 'Global Options:\n']
+        formatter = ScrapyHelpFormatter(prog="scrapy")
+        part_strings = [
+            "usage: scrapy genspider [options] <name> <domain>\n\n",
+            "\n",
+            "optional arguments:\n",
+            "\n",
+            "Global Options:\n",
+        ]
         self.assertEqual(
             formatter._join_parts(part_strings),
-            ('Usage\n=====\n  scrapy genspider [options] <name> <domain>\n\n\n'
-             'Optional Arguments\n==================\n\n'
-             'Global Options\n--------------\n')
+            (
+                "Usage\n=====\n  scrapy genspider [options] <name> <domain>\n\n\n"
+                "Optional Arguments\n==================\n\n"
+                "Global Options\n--------------\n"
+            ),
         )
 
 
 class ProjectTest(unittest.TestCase):
-    project_name = 'testproject'
+    project_name = "testproject"
 
     def setUp(self):
         self.temp_path = mkdtemp()
         self.cwd = self.temp_path
-        self.proj_path = join(self.temp_path, self.project_name)
-        self.proj_mod_path = join(self.proj_path, self.project_name)
+        self.proj_path = Path(self.temp_path, self.project_name)
+        self.proj_mod_path = self.proj_path / self.project_name
         self.env = get_testenv()
 
     def tearDown(self):
         rmtree(self.temp_path)
 
     def call(self, *new_args, **kwargs):
         with tempfile.TemporaryFile() as out:
-            args = (sys.executable, '-m', 'scrapy.cmdline') + new_args
-            return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd,
-                                   env=self.env, **kwargs)
+            args = (sys.executable, "-m", "scrapy.cmdline") + new_args
+            return subprocess.call(
+                args, stdout=out, stderr=out, cwd=self.cwd, env=self.env, **kwargs
+            )
 
     def proc(self, *new_args, **popen_kwargs):
-        args = (sys.executable, '-m', 'scrapy.cmdline') + new_args
+        args = (sys.executable, "-m", "scrapy.cmdline") + new_args
         p = subprocess.Popen(
             args,
-            cwd=popen_kwargs.pop('cwd', self.cwd),
+            cwd=popen_kwargs.pop("cwd", self.cwd),
             env=self.env,
             stdout=subprocess.PIPE,
             stderr=subprocess.PIPE,
             **popen_kwargs,
         )
 
         def kill_proc():
             p.kill()
             p.communicate()
-            assert False, 'Command took too much time to complete'
+            assert False, "Command took too much time to complete"
 
         timer = Timer(15, kill_proc)
         try:
             timer.start()
             stdout, stderr = p.communicate()
         finally:
             timer.cancel()
 
         return p, to_unicode(stdout), to_unicode(stderr)
 
-    def find_in_file(self, filename, regex):
+    def find_in_file(
+        self, filename: Union[str, os.PathLike], regex
+    ) -> Optional[re.Match]:
         """Find first pattern occurrence in file"""
         pattern = re.compile(regex)
-        with open(filename, "r") as f:
+        with Path(filename).open("r", encoding="utf-8") as f:
             for line in f:
                 match = pattern.search(line)
                 if match is not None:
                     return match
+        return None
 
 
 class StartprojectTest(ProjectTest):
-
     def test_startproject(self):
-        p, out, err = self.proc('startproject', self.project_name)
+        p, out, err = self.proc("startproject", self.project_name)
         print(out)
         print(err, file=sys.stderr)
         self.assertEqual(p.returncode, 0)
 
-        assert exists(join(self.proj_path, 'scrapy.cfg'))
-        assert exists(join(self.proj_path, 'testproject'))
-        assert exists(join(self.proj_mod_path, '__init__.py'))
-        assert exists(join(self.proj_mod_path, 'items.py'))
-        assert exists(join(self.proj_mod_path, 'pipelines.py'))
-        assert exists(join(self.proj_mod_path, 'settings.py'))
-        assert exists(join(self.proj_mod_path, 'spiders', '__init__.py'))
-
-        self.assertEqual(1, self.call('startproject', self.project_name))
-        self.assertEqual(1, self.call('startproject', 'wrong---project---name'))
-        self.assertEqual(1, self.call('startproject', 'sys'))
+        assert Path(self.proj_path, "scrapy.cfg").exists()
+        assert Path(self.proj_path, "testproject").exists()
+        assert Path(self.proj_mod_path, "__init__.py").exists()
+        assert Path(self.proj_mod_path, "items.py").exists()
+        assert Path(self.proj_mod_path, "pipelines.py").exists()
+        assert Path(self.proj_mod_path, "settings.py").exists()
+        assert Path(self.proj_mod_path, "spiders", "__init__.py").exists()
+
+        self.assertEqual(1, self.call("startproject", self.project_name))
+        self.assertEqual(1, self.call("startproject", "wrong---project---name"))
+        self.assertEqual(1, self.call("startproject", "sys"))
 
     def test_startproject_with_project_dir(self):
         project_dir = mkdtemp()
-        self.assertEqual(0, self.call('startproject', self.project_name, project_dir))
+        self.assertEqual(0, self.call("startproject", self.project_name, project_dir))
 
-        assert exists(join(abspath(project_dir), 'scrapy.cfg'))
-        assert exists(join(abspath(project_dir), 'testproject'))
-        assert exists(join(join(abspath(project_dir), self.project_name), '__init__.py'))
-        assert exists(join(join(abspath(project_dir), self.project_name), 'items.py'))
-        assert exists(join(join(abspath(project_dir), self.project_name), 'pipelines.py'))
-        assert exists(join(join(abspath(project_dir), self.project_name), 'settings.py'))
-        assert exists(join(join(abspath(project_dir), self.project_name), 'spiders', '__init__.py'))
-
-        self.assertEqual(0, self.call('startproject', self.project_name, project_dir + '2'))
-
-        self.assertEqual(1, self.call('startproject', self.project_name, project_dir))
-        self.assertEqual(1, self.call('startproject', self.project_name + '2', project_dir))
-        self.assertEqual(1, self.call('startproject', 'wrong---project---name'))
-        self.assertEqual(1, self.call('startproject', 'sys'))
-        self.assertEqual(2, self.call('startproject'))
-        self.assertEqual(2, self.call('startproject', self.project_name, project_dir, 'another_params'))
+        assert Path(project_dir, "scrapy.cfg").exists()
+        assert Path(project_dir, "testproject").exists()
+        assert Path(project_dir, self.project_name, "__init__.py").exists()
+        assert Path(project_dir, self.project_name, "items.py").exists()
+        assert Path(project_dir, self.project_name, "pipelines.py").exists()
+        assert Path(project_dir, self.project_name, "settings.py").exists()
+        assert Path(project_dir, self.project_name, "spiders", "__init__.py").exists()
+
+        self.assertEqual(
+            0, self.call("startproject", self.project_name, project_dir + "2")
+        )
+
+        self.assertEqual(1, self.call("startproject", self.project_name, project_dir))
+        self.assertEqual(
+            1, self.call("startproject", self.project_name + "2", project_dir)
+        )
+        self.assertEqual(1, self.call("startproject", "wrong---project---name"))
+        self.assertEqual(1, self.call("startproject", "sys"))
+        self.assertEqual(2, self.call("startproject"))
+        self.assertEqual(
+            2,
+            self.call("startproject", self.project_name, project_dir, "another_params"),
+        )
 
     def test_existing_project_dir(self):
         project_dir = mkdtemp()
-        project_name = self.project_name + '_existing'
-        project_path = os.path.join(project_dir, project_name)
-        os.mkdir(project_path)
+        project_name = self.project_name + "_existing"
+        project_path = Path(project_dir, project_name)
+        project_path.mkdir()
 
-        p, out, err = self.proc('startproject', project_name, cwd=project_dir)
+        p, out, err = self.proc("startproject", project_name, cwd=project_dir)
         print(out)
         print(err, file=sys.stderr)
         self.assertEqual(p.returncode, 0)
 
-        assert exists(join(abspath(project_path), 'scrapy.cfg'))
-        assert exists(join(abspath(project_path), project_name))
-        assert exists(join(join(abspath(project_path), project_name), '__init__.py'))
-        assert exists(join(join(abspath(project_path), project_name), 'items.py'))
-        assert exists(join(join(abspath(project_path), project_name), 'pipelines.py'))
-        assert exists(join(join(abspath(project_path), project_name), 'settings.py'))
-        assert exists(join(join(abspath(project_path), project_name), 'spiders', '__init__.py'))
-
-
-def get_permissions_dict(path, renamings=None, ignore=None):
+        assert Path(project_path, "scrapy.cfg").exists()
+        assert Path(project_path, project_name).exists()
+        assert Path(project_path, project_name, "__init__.py").exists()
+        assert Path(project_path, project_name, "items.py").exists()
+        assert Path(project_path, project_name, "pipelines.py").exists()
+        assert Path(project_path, project_name, "settings.py").exists()
+        assert Path(project_path, project_name, "spiders", "__init__.py").exists()
+
+
+def get_permissions_dict(
+    path: Union[str, os.PathLike], renamings=None, ignore=None
+) -> Dict[str, str]:
+    def get_permissions(path: Path) -> str:
+        return oct(path.stat().st_mode)
 
-    def get_permissions(path):
-        return oct(os.stat(path).st_mode)
+    path_obj = Path(path)
 
     renamings = renamings or tuple()
     permissions_dict = {
-        '.': get_permissions(path),
+        ".": get_permissions(path_obj),
     }
-    for root, dirs, files in os.walk(path):
+    for root, dirs, files in os.walk(path_obj):
         nodes = list(chain(dirs, files))
         if ignore:
             ignored_names = ignore(root, nodes)
             nodes = [node for node in nodes if node not in ignored_names]
         for node in nodes:
-            absolute_path = os.path.join(root, node)
-            relative_path = os.path.relpath(absolute_path, path)
+            absolute_path = Path(root, node)
+            relative_path = str(absolute_path.relative_to(path))
             for search_string, replacement in renamings:
-                relative_path = relative_path.replace(
-                    search_string,
-                    replacement
-                )
+                relative_path = relative_path.replace(search_string, replacement)
             permissions = get_permissions(absolute_path)
             permissions_dict[relative_path] = permissions
     return permissions_dict
 
 
 class StartprojectTemplatesTest(ProjectTest):
 
     maxDiff = None
 
     def setUp(self):
         super().setUp()
-        self.tmpl = join(self.temp_path, 'templates')
-        self.tmpl_proj = join(self.tmpl, 'project')
+        self.tmpl = str(Path(self.temp_path, "templates"))
+        self.tmpl_proj = str(Path(self.tmpl, "project"))
 
     def test_startproject_template_override(self):
-        copytree(join(scrapy.__path__[0], 'templates'), self.tmpl)
-        with open(join(self.tmpl_proj, 'root_template'), 'w'):
-            pass
-        assert exists(join(self.tmpl_proj, 'root_template'))
-
-        args = ['--set', f'TEMPLATES_DIR={self.tmpl}']
-        p, out, err = self.proc('startproject', self.project_name, *args)
-        self.assertIn(f"New Scrapy project '{self.project_name}', "
-                      "using template directory", out)
+        copytree(Path(scrapy.__path__[0], "templates"), self.tmpl)
+        Path(self.tmpl_proj, "root_template").write_bytes(b"")
+        assert Path(self.tmpl_proj, "root_template").exists()
+
+        args = ["--set", f"TEMPLATES_DIR={self.tmpl}"]
+        p, out, err = self.proc("startproject", self.project_name, *args)
+        self.assertIn(
+            f"New Scrapy project '{self.project_name}', " "using template directory",
+            out,
+        )
         self.assertIn(self.tmpl_proj, out)
-        assert exists(join(self.proj_path, 'root_template'))
+        assert Path(self.proj_path, "root_template").exists()
 
     def test_startproject_permissions_from_writable(self):
         """Check that generated files have the right permissions when the
         template folder has the same permissions as in the project, i.e.
         everything is writable."""
         scrapy_path = scrapy.__path__[0]
-        project_template = os.path.join(scrapy_path, 'templates', 'project')
-        project_name = 'startproject1'
+        project_template = Path(scrapy_path, "templates", "project")
+        project_name = "startproject1"
         renamings = (
-            ('module', project_name),
-            ('.tmpl', ''),
+            ("module", project_name),
+            (".tmpl", ""),
         )
         expected_permissions = get_permissions_dict(
             project_template,
             renamings,
             IGNORE,
         )
 
         destination = mkdtemp()
         process = subprocess.Popen(
             (
                 sys.executable,
-                '-m',
-                'scrapy.cmdline',
-                'startproject',
+                "-m",
+                "scrapy.cmdline",
+                "startproject",
                 project_name,
             ),
             cwd=destination,
             env=self.env,
         )
         process.wait()
 
-        project_dir = os.path.join(destination, project_name)
+        project_dir = Path(destination, project_name)
         actual_permissions = get_permissions_dict(project_dir)
 
         self.assertEqual(actual_permissions, expected_permissions)
 
     def test_startproject_permissions_from_read_only(self):
         """Check that generated files have the right permissions when the
         template folder has been made read-only, which is something that some
         systems do.
 
         See https://github.com/scrapy/scrapy/pull/4604
         """
         scrapy_path = scrapy.__path__[0]
-        templates_dir = os.path.join(scrapy_path, 'templates')
-        project_template = os.path.join(templates_dir, 'project')
-        project_name = 'startproject2'
+        templates_dir = Path(scrapy_path, "templates")
+        project_template = Path(templates_dir, "project")
+        project_name = "startproject2"
         renamings = (
-            ('module', project_name),
-            ('.tmpl', ''),
+            ("module", project_name),
+            (".tmpl", ""),
         )
         expected_permissions = get_permissions_dict(
             project_template,
             renamings,
             IGNORE,
         )
 
-        def _make_read_only(path):
-            current_permissions = os.stat(path).st_mode
-            os.chmod(path, current_permissions & ~ANYONE_WRITE_PERMISSION)
+        def _make_read_only(path: Path):
+            current_permissions = path.stat().st_mode
+            path.chmod(current_permissions & ~ANYONE_WRITE_PERMISSION)
 
-        read_only_templates_dir = str(Path(mkdtemp()) / 'templates')
+        read_only_templates_dir = str(Path(mkdtemp()) / "templates")
         copytree(templates_dir, read_only_templates_dir)
 
         for root, dirs, files in os.walk(read_only_templates_dir):
             for node in chain(dirs, files):
-                _make_read_only(os.path.join(root, node))
+                _make_read_only(Path(root, node))
 
         destination = mkdtemp()
         process = subprocess.Popen(
             (
                 sys.executable,
-                '-m',
-                'scrapy.cmdline',
-                'startproject',
+                "-m",
+                "scrapy.cmdline",
+                "startproject",
                 project_name,
-                '--set',
-                f'TEMPLATES_DIR={read_only_templates_dir}',
+                "--set",
+                f"TEMPLATES_DIR={read_only_templates_dir}",
             ),
             cwd=destination,
             env=self.env,
         )
         process.wait()
 
-        project_dir = os.path.join(destination, project_name)
+        project_dir = Path(destination, project_name)
         actual_permissions = get_permissions_dict(project_dir)
 
         self.assertEqual(actual_permissions, expected_permissions)
 
     def test_startproject_permissions_unchanged_in_destination(self):
-        """Check that pre-existing folders and files in the destination folder
+        """Check that preexisting folders and files in the destination folder
         do not see their permissions modified."""
         scrapy_path = scrapy.__path__[0]
-        project_template = os.path.join(scrapy_path, 'templates', 'project')
-        project_name = 'startproject3'
+        project_template = Path(scrapy_path, "templates", "project")
+        project_name = "startproject3"
         renamings = (
-            ('module', project_name),
-            ('.tmpl', ''),
+            ("module", project_name),
+            (".tmpl", ""),
         )
         expected_permissions = get_permissions_dict(
             project_template,
             renamings,
             IGNORE,
         )
 
         destination = mkdtemp()
-        project_dir = os.path.join(destination, project_name)
+        project_dir = Path(destination, project_name)
 
         existing_nodes = {
             oct(permissions)[2:] + extension: permissions
-            for extension in ('', '.d')
+            for extension in ("", ".d")
             for permissions in (
-                0o444, 0o555, 0o644, 0o666, 0o755, 0o777,
+                0o444,
+                0o555,
+                0o644,
+                0o666,
+                0o755,
+                0o777,
             )
         }
-        os.mkdir(project_dir)
-        project_dir_path = Path(project_dir)
+        project_dir.mkdir()
         for node, permissions in existing_nodes.items():
-            path = project_dir_path / node
-            if node.endswith('.d'):
+            path = project_dir / node
+            if node.endswith(".d"):
                 path.mkdir(mode=permissions)
             else:
                 path.touch(mode=permissions)
             expected_permissions[node] = oct(path.stat().st_mode)
 
         process = subprocess.Popen(
             (
                 sys.executable,
-                '-m',
-                'scrapy.cmdline',
-                'startproject',
+                "-m",
+                "scrapy.cmdline",
+                "startproject",
                 project_name,
-                '.',
+                ".",
             ),
             cwd=project_dir,
             env=self.env,
         )
         process.wait()
 
         actual_permissions = get_permissions_dict(project_dir)
 
         self.assertEqual(actual_permissions, expected_permissions)
 
     def test_startproject_permissions_umask_022(self):
         """Check that generated files have the right permissions when the
         system uses a umask value that causes new files to have different
         permissions than those from the template folder."""
+
         @contextmanager
         def umask(new_mask):
             cur_mask = os.umask(new_mask)
             yield
             os.umask(cur_mask)
 
         scrapy_path = scrapy.__path__[0]
-        project_template = os.path.join(
-            scrapy_path,
-            'templates',
-            'project'
-        )
-        project_name = 'umaskproject'
+        project_template = Path(scrapy_path, "templates", "project")
+        project_name = "umaskproject"
         renamings = (
-            ('module', project_name),
-            ('.tmpl', ''),
+            ("module", project_name),
+            (".tmpl", ""),
         )
         expected_permissions = get_permissions_dict(
             project_template,
             renamings,
             IGNORE,
         )
 
         with umask(0o002):
             destination = mkdtemp()
             process = subprocess.Popen(
                 (
                     sys.executable,
-                    '-m',
-                    'scrapy.cmdline',
-                    'startproject',
+                    "-m",
+                    "scrapy.cmdline",
+                    "startproject",
                     project_name,
                 ),
                 cwd=destination,
                 env=self.env,
             )
             process.wait()
 
-            project_dir = os.path.join(destination, project_name)
+            project_dir = Path(destination, project_name)
             actual_permissions = get_permissions_dict(project_dir)
 
             self.assertEqual(actual_permissions, expected_permissions)
 
 
 class CommandTest(ProjectTest):
-
     def setUp(self):
         super().setUp()
-        self.call('startproject', self.project_name)
-        self.cwd = join(self.temp_path, self.project_name)
-        self.env['SCRAPY_SETTINGS_MODULE'] = f'{self.project_name}.settings'
+        self.call("startproject", self.project_name)
+        self.cwd = Path(self.temp_path, self.project_name)
+        self.env["SCRAPY_SETTINGS_MODULE"] = f"{self.project_name}.settings"
 
 
 class GenspiderCommandTest(CommandTest):
-
     def test_arguments(self):
         # only pass one argument. spider script shouldn't be created
-        self.assertEqual(2, self.call('genspider', 'test_name'))
-        assert not exists(join(self.proj_mod_path, 'spiders', 'test_name.py'))
+        self.assertEqual(2, self.call("genspider", "test_name"))
+        assert not Path(self.proj_mod_path, "spiders", "test_name.py").exists()
         # pass two arguments <name> <domain>. spider script should be created
-        self.assertEqual(0, self.call('genspider', 'test_name', 'test.com'))
-        assert exists(join(self.proj_mod_path, 'spiders', 'test_name.py'))
+        self.assertEqual(0, self.call("genspider", "test_name", "test.com"))
+        assert Path(self.proj_mod_path, "spiders", "test_name.py").exists()
 
-    def test_template(self, tplname='crawl'):
-        args = [f'--template={tplname}'] if tplname else []
-        spname = 'test_spider'
+    def test_template(self, tplname="crawl"):
+        args = [f"--template={tplname}"] if tplname else []
+        spname = "test_spider"
         spmodule = f"{self.project_name}.spiders.{spname}"
-        p, out, err = self.proc('genspider', spname, 'test.com', *args)
-        self.assertIn(f"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}", out)
-        self.assertTrue(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))
-        modify_time_before = getmtime(join(self.proj_mod_path, 'spiders', 'test_spider.py'))
-        p, out, err = self.proc('genspider', spname, 'test.com', *args)
+        p, out, err = self.proc("genspider", spname, "test.com", *args)
+        self.assertIn(
+            f"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}",
+            out,
+        )
+        self.assertTrue(Path(self.proj_mod_path, "spiders", "test_spider.py").exists())
+        modify_time_before = (
+            Path(self.proj_mod_path, "spiders", "test_spider.py").stat().st_mtime
+        )
+        p, out, err = self.proc("genspider", spname, "test.com", *args)
         self.assertIn(f"Spider {spname!r} already exists in module", out)
-        modify_time_after = getmtime(join(self.proj_mod_path, 'spiders', 'test_spider.py'))
+        modify_time_after = (
+            Path(self.proj_mod_path, "spiders", "test_spider.py").stat().st_mtime
+        )
         self.assertEqual(modify_time_after, modify_time_before)
 
     def test_template_basic(self):
-        self.test_template('basic')
+        self.test_template("basic")
 
     def test_template_csvfeed(self):
-        self.test_template('csvfeed')
+        self.test_template("csvfeed")
 
     def test_template_xmlfeed(self):
-        self.test_template('xmlfeed')
+        self.test_template("xmlfeed")
 
     def test_list(self):
-        self.assertEqual(0, self.call('genspider', '--list'))
+        self.assertEqual(0, self.call("genspider", "--list"))
 
     def test_dump(self):
-        self.assertEqual(0, self.call('genspider', '--dump=basic'))
-        self.assertEqual(0, self.call('genspider', '-d', 'basic'))
+        self.assertEqual(0, self.call("genspider", "--dump=basic"))
+        self.assertEqual(0, self.call("genspider", "-d", "basic"))
 
     def test_same_name_as_project(self):
-        self.assertEqual(2, self.call('genspider', self.project_name))
-        assert not exists(join(self.proj_mod_path, 'spiders', f'{self.project_name}.py'))
+        self.assertEqual(2, self.call("genspider", self.project_name))
+        assert not Path(
+            self.proj_mod_path, "spiders", f"{self.project_name}.py"
+        ).exists()
 
     def test_same_filename_as_existing_spider(self, force=False):
-        file_name = 'example'
-        file_path = join(self.proj_mod_path, 'spiders', f'{file_name}.py')
-        self.assertEqual(0, self.call('genspider', file_name, 'example.com'))
-        assert exists(file_path)
+        file_name = "example"
+        file_path = Path(self.proj_mod_path, "spiders", f"{file_name}.py")
+        self.assertEqual(0, self.call("genspider", file_name, "example.com"))
+        assert file_path.exists()
 
         # change name of spider but not its file name
-        with open(file_path, 'r+') as spider_file:
+        with file_path.open("r+", encoding="utf-8") as spider_file:
             file_data = spider_file.read()
-            file_data = file_data.replace("name = \'example\'", "name = \'renamed\'")
+            file_data = file_data.replace('name = "example"', 'name = "renamed"')
             spider_file.seek(0)
             spider_file.write(file_data)
             spider_file.truncate()
-        modify_time_before = getmtime(file_path)
+        modify_time_before = file_path.stat().st_mtime
         file_contents_before = file_data
 
         if force:
-            p, out, err = self.proc('genspider', '--force', file_name, 'example.com')
-            self.assertIn(f"Created spider {file_name!r} using template \'basic\' in module", out)
-            modify_time_after = getmtime(file_path)
+            p, out, err = self.proc("genspider", "--force", file_name, "example.com")
+            self.assertIn(
+                f"Created spider {file_name!r} using template 'basic' in module", out
+            )
+            modify_time_after = file_path.stat().st_mtime
             self.assertNotEqual(modify_time_after, modify_time_before)
-            file_contents_after = open(file_path, 'r').read()
+            file_contents_after = file_path.read_text(encoding="utf-8")
             self.assertNotEqual(file_contents_after, file_contents_before)
         else:
-            p, out, err = self.proc('genspider', file_name, 'example.com')
-            self.assertIn(f"{file_path} already exists", out)
-            modify_time_after = getmtime(file_path)
+            p, out, err = self.proc("genspider", file_name, "example.com")
+            self.assertIn(f"{file_path.resolve()} already exists", out)
+            modify_time_after = file_path.stat().st_mtime
             self.assertEqual(modify_time_after, modify_time_before)
-            file_contents_after = open(file_path, 'r').read()
+            file_contents_after = file_path.read_text(encoding="utf-8")
             self.assertEqual(file_contents_after, file_contents_before)
 
     def test_same_filename_as_existing_spider_force(self):
         self.test_same_filename_as_existing_spider(force=True)
 
-    def test_url(self, url='test.com', domain="test.com"):
-        self.assertEqual(0, self.call('genspider', '--force', 'test_name', url))
-        self.assertEqual(domain,
-                         self.find_in_file(join(self.proj_mod_path,
-                                                'spiders', 'test_name.py'),
-                                           r'allowed_domains\s*=\s*\[\'(.+)\'\]').group(1))
-        self.assertEqual(f'http://{domain}/',
-                         self.find_in_file(join(self.proj_mod_path,
-                                                'spiders', 'test_name.py'),
-                                           r'start_urls\s*=\s*\[\'(.+)\'\]').group(1))
+    def test_url(self, url="test.com", domain="test.com"):
+        self.assertEqual(0, self.call("genspider", "--force", "test_name", url))
+        self.assertEqual(
+            domain,
+            self.find_in_file(
+                Path(self.proj_mod_path, "spiders", "test_name.py"),
+                r"allowed_domains\s*=\s*\[['\"](.+)['\"]\]",
+            ).group(1),
+        )
+        self.assertEqual(
+            f"http://{domain}/",
+            self.find_in_file(
+                Path(self.proj_mod_path, "spiders", "test_name.py"),
+                r"start_urls\s*=\s*\[['\"](.+)['\"]\]",
+            ).group(1),
+        )
 
     def test_url_schema(self):
-        self.test_url('http://test.com', 'test.com')
+        self.test_url("http://test.com", "test.com")
 
     def test_url_path(self):
-        self.test_url('test.com/some/other/page', 'test.com')
+        self.test_url("test.com/some/other/page", "test.com")
 
     def test_url_schema_path(self):
-        self.test_url('https://test.com/some/other/page', 'test.com')
+        self.test_url("https://test.com/some/other/page", "test.com")
 
 
 class GenspiderStandaloneCommandTest(ProjectTest):
-
     def test_generate_standalone_spider(self):
-        self.call('genspider', 'example', 'example.com')
-        assert exists(join(self.temp_path, 'example.py'))
+        self.call("genspider", "example", "example.com")
+        assert Path(self.temp_path, "example.py").exists()
 
     def test_same_name_as_existing_file(self, force=False):
-        file_name = 'example'
-        file_path = join(self.temp_path, file_name + '.py')
-        p, out, err = self.proc('genspider', file_name, 'example.com')
-        self.assertIn(f"Created spider {file_name!r} using template \'basic\' ", out)
-        assert exists(file_path)
-        modify_time_before = getmtime(file_path)
-        file_contents_before = open(file_path, 'r').read()
+        file_name = "example"
+        file_path = Path(self.temp_path, file_name + ".py")
+        p, out, err = self.proc("genspider", file_name, "example.com")
+        self.assertIn(f"Created spider {file_name!r} using template 'basic' ", out)
+        assert file_path.exists()
+        modify_time_before = file_path.stat().st_mtime
+        file_contents_before = file_path.read_text(encoding="utf-8")
 
         if force:
             # use different template to ensure contents were changed
-            p, out, err = self.proc('genspider', '--force', '-t', 'crawl', file_name, 'example.com')
-            self.assertIn(f"Created spider {file_name!r} using template \'crawl\' ", out)
-            modify_time_after = getmtime(file_path)
+            p, out, err = self.proc(
+                "genspider", "--force", "-t", "crawl", file_name, "example.com"
+            )
+            self.assertIn(f"Created spider {file_name!r} using template 'crawl' ", out)
+            modify_time_after = file_path.stat().st_mtime
             self.assertNotEqual(modify_time_after, modify_time_before)
-            file_contents_after = open(file_path, 'r').read()
+            file_contents_after = file_path.read_text(encoding="utf-8")
             self.assertNotEqual(file_contents_after, file_contents_before)
         else:
-            p, out, err = self.proc('genspider', file_name, 'example.com')
-            self.assertIn(f"{join(self.temp_path, file_name + '.py')} already exists", out)
-            modify_time_after = getmtime(file_path)
+            p, out, err = self.proc("genspider", file_name, "example.com")
+            self.assertIn(
+                f"{Path(self.temp_path, file_name + '.py').resolve()} already exists",
+                out,
+            )
+            modify_time_after = file_path.stat().st_mtime
             self.assertEqual(modify_time_after, modify_time_before)
-            file_contents_after = open(file_path, 'r').read()
+            file_contents_after = file_path.read_text(encoding="utf-8")
             self.assertEqual(file_contents_after, file_contents_before)
 
     def test_same_name_as_existing_file_force(self):
         self.test_same_name_as_existing_file(force=True)
 
 
 class MiscCommandsTest(CommandTest):
-
     def test_list(self):
-        self.assertEqual(0, self.call('list'))
+        self.assertEqual(0, self.call("list"))
 
 
 class RunSpiderCommandTest(CommandTest):
 
-    spider_filename = 'myspider.py'
+    spider_filename = "myspider.py"
 
     debug_log_spider = """
 import scrapy
 
 class MySpider(scrapy.Spider):
     name = 'myspider'
 
@@ -584,56 +624,58 @@
 class BadSpider(scrapy.Spider):
     name = "bad"
     def start_requests(self):
         raise Exception("oops!")
         """
 
     @contextmanager
-    def _create_file(self, content, name=None):
-        tmpdir = self.mktemp()
-        os.mkdir(tmpdir)
+    def _create_file(self, content, name=None) -> Generator[str, None, None]:
+        tmpdir = Path(self.mktemp())
+        tmpdir.mkdir()
         if name:
-            fname = abspath(join(tmpdir, name))
+            fname = (tmpdir / name).resolve()
         else:
-            fname = abspath(join(tmpdir, self.spider_filename))
-        with open(fname, 'w') as f:
-            f.write(content)
+            fname = (tmpdir / self.spider_filename).resolve()
+        fname.write_text(content, encoding="utf-8")
         try:
-            yield fname
+            yield str(fname)
         finally:
             rmtree(tmpdir)
 
     def runspider(self, code, name=None, args=()):
         with self._create_file(code, name) as fname:
-            return self.proc('runspider', fname, *args)
+            return self.proc("runspider", fname, *args)
 
     def get_log(self, code, name=None, args=()):
         p, stdout, stderr = self.runspider(code, name, args=args)
         return stderr
 
     def test_runspider(self):
         log = self.get_log(self.debug_log_spider)
         self.assertIn("DEBUG: It Works!", log)
         self.assertIn("INFO: Spider opened", log)
         self.assertIn("INFO: Closing spider (finished)", log)
         self.assertIn("INFO: Spider closed (finished)", log)
 
     def test_run_fail_spider(self):
-        proc, _, _ = self.runspider("import scrapy\n" + inspect.getsource(ExceptionSpider))
+        proc, _, _ = self.runspider(
+            "import scrapy\n" + inspect.getsource(ExceptionSpider)
+        )
         ret = proc.returncode
         self.assertNotEqual(ret, 0)
 
     def test_run_good_spider(self):
-        proc, _, _ = self.runspider("import scrapy\n" + inspect.getsource(NoRequestsSpider))
+        proc, _, _ = self.runspider(
+            "import scrapy\n" + inspect.getsource(NoRequestsSpider)
+        )
         ret = proc.returncode
         self.assertEqual(ret, 0)
 
     def test_runspider_log_level(self):
-        log = self.get_log(self.debug_log_spider,
-                           args=('-s', 'LOG_LEVEL=INFO'))
+        log = self.get_log(self.debug_log_spider, args=("-s", "LOG_LEVEL=INFO"))
         self.assertNotIn("DEBUG: It Works!", log)
         self.assertIn("INFO: Spider opened", log)
 
     def test_runspider_dnscache_disabled(self):
         # see https://github.com/scrapy/scrapy/issues/2811
         # The spider below should not be able to connect to localhost:12345,
         # which is intended,
@@ -645,102 +687,134 @@
 class MySpider(scrapy.Spider):
     name = 'myspider'
     start_urls = ['http://localhost:12345']
 
     def parse(self, response):
         return {'test': 'value'}
 """
-        log = self.get_log(dnscache_spider, args=('-s', 'DNSCACHE_ENABLED=False'))
+        log = self.get_log(dnscache_spider, args=("-s", "DNSCACHE_ENABLED=False"))
         self.assertNotIn("DNSLookupError", log)
         self.assertIn("INFO: Spider opened", log)
 
     def test_runspider_log_short_names(self):
-        log1 = self.get_log(self.debug_log_spider,
-                            args=('-s', 'LOG_SHORT_NAMES=1'))
+        log1 = self.get_log(self.debug_log_spider, args=("-s", "LOG_SHORT_NAMES=1"))
         self.assertIn("[myspider] DEBUG: It Works!", log1)
         self.assertIn("[scrapy]", log1)
         self.assertNotIn("[scrapy.core.engine]", log1)
 
-        log2 = self.get_log(self.debug_log_spider,
-                            args=('-s', 'LOG_SHORT_NAMES=0'))
+        log2 = self.get_log(self.debug_log_spider, args=("-s", "LOG_SHORT_NAMES=0"))
         self.assertIn("[myspider] DEBUG: It Works!", log2)
         self.assertNotIn("[scrapy]", log2)
         self.assertIn("[scrapy.core.engine]", log2)
 
     def test_runspider_no_spider_found(self):
         log = self.get_log("from scrapy.spiders import Spider\n")
         self.assertIn("No spider found in file", log)
 
     def test_runspider_file_not_found(self):
-        _, _, log = self.proc('runspider', 'some_non_existent_file')
+        _, _, log = self.proc("runspider", "some_non_existent_file")
         self.assertIn("File not found: some_non_existent_file", log)
 
     def test_runspider_unable_to_load(self):
-        log = self.get_log('', name='myspider.txt')
-        self.assertIn('Unable to load', log)
+        log = self.get_log("", name="myspider.txt")
+        self.assertIn("Unable to load", log)
 
     def test_start_requests_errors(self):
-        log = self.get_log(self.badspider, name='badspider.py')
+        log = self.get_log(self.badspider, name="badspider.py")
         self.assertIn("start_requests", log)
         self.assertIn("badspider.py", log)
 
     def test_asyncio_enabled_true(self):
-        log = self.get_log(self.debug_log_spider, args=[
-            '-s', 'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor'
-        ])
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        log = self.get_log(
+            self.debug_log_spider,
+            args=[
+                "-s",
+                "TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor",
+            ],
+        )
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
 
     def test_asyncio_enabled_default(self):
         log = self.get_log(self.debug_log_spider, args=[])
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
 
     def test_asyncio_enabled_false(self):
-        log = self.get_log(self.debug_log_spider, args=[
-            '-s', 'TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor'
-        ])
-        self.assertIn("Using reactor: twisted.internet.selectreactor.SelectReactor", log)
-        self.assertNotIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
-
-    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')
-    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')
-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')
+        log = self.get_log(
+            self.debug_log_spider,
+            args=["-s", "TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor"],
+        )
+        self.assertIn(
+            "Using reactor: twisted.internet.selectreactor.SelectReactor", log
+        )
+        self.assertNotIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
+
+    @mark.skipif(
+        sys.implementation.name == "pypy",
+        reason="uvloop does not support pypy properly",
+    )
+    @mark.skipif(
+        platform.system() == "Windows", reason="uvloop does not support Windows"
+    )
+    @mark.skipif(
+        twisted_version == Version("twisted", 21, 2, 0),
+        reason="https://twistedmatrix.com/trac/ticket/10106",
+    )
     def test_custom_asyncio_loop_enabled_true(self):
-        log = self.get_log(self.debug_log_spider, args=[
-            '-s',
-            'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor',
-            '-s',
-            'ASYNCIO_EVENT_LOOP=uvloop.Loop',
-        ])
+        log = self.get_log(
+            self.debug_log_spider,
+            args=[
+                "-s",
+                "TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor",
+                "-s",
+                "ASYNCIO_EVENT_LOOP=uvloop.Loop",
+            ],
+        )
         self.assertIn("Using asyncio event loop: uvloop.Loop", log)
 
     def test_custom_asyncio_loop_enabled_false(self):
-        log = self.get_log(self.debug_log_spider, args=[
-            '-s', 'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor'
-        ])
+        log = self.get_log(
+            self.debug_log_spider,
+            args=[
+                "-s",
+                "TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor",
+            ],
+        )
         import asyncio
-        if sys.platform != 'win32':
+
+        if sys.platform != "win32":
             loop = asyncio.new_event_loop()
         else:
             loop = asyncio.SelectorEventLoop()
-        self.assertIn(f"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}", log)
+        self.assertIn(
+            f"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}",
+            log,
+        )
 
     def test_output(self):
         spider_code = """
 import scrapy
 
 class MySpider(scrapy.Spider):
     name = 'myspider'
 
     def start_requests(self):
         self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))
         return []
 """
-        args = ['-o', 'example.json']
+        args = ["-o", "example.json"]
         log = self.get_log(spider_code, args=args)
-        self.assertIn("[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}", log)
+        self.assertIn(
+            "[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}", log
+        )
 
     def test_overwrite_output(self):
         spider_code = """
 import json
 import scrapy
 
 class MySpider(scrapy.Spider):
@@ -750,63 +824,67 @@
         self.logger.debug(
             'FEEDS: {}'.format(
                 json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)
             )
         )
         return []
 """
-        with open(os.path.join(self.cwd, "example.json"), "w") as f1:
-            f1.write("not empty")
-        args = ['-O', 'example.json']
+        Path(self.cwd, "example.json").write_text("not empty", encoding="utf-8")
+        args = ["-O", "example.json"]
         log = self.get_log(spider_code, args=args)
-        self.assertIn('[myspider] DEBUG: FEEDS: {"example.json": {"format": "json", "overwrite": true}}', log)
-        with open(os.path.join(self.cwd, "example.json")) as f2:
+        self.assertIn(
+            '[myspider] DEBUG: FEEDS: {"example.json": {"format": "json", "overwrite": true}}',
+            log,
+        )
+        with Path(self.cwd, "example.json").open(encoding="utf-8") as f2:
             first_line = f2.readline()
         self.assertNotEqual(first_line, "not empty")
 
     def test_output_and_overwrite_output(self):
         spider_code = """
 import scrapy
 
 class MySpider(scrapy.Spider):
     name = 'myspider'
 
     def start_requests(self):
         return []
 """
-        args = ['-o', 'example1.json', '-O', 'example2.json']
+        args = ["-o", "example1.json", "-O", "example2.json"]
         log = self.get_log(spider_code, args=args)
-        self.assertIn("error: Please use only one of -o/--output and -O/--overwrite-output", log)
+        self.assertIn(
+            "error: Please use only one of -o/--output and -O/--overwrite-output", log
+        )
 
     def test_output_stdout(self):
         spider_code = """
 import scrapy
 
 class MySpider(scrapy.Spider):
     name = 'myspider'
 
     def start_requests(self):
         self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))
         return []
 """
-        args = ['-o', '-:json']
+        args = ["-o", "-:json"]
         log = self.get_log(spider_code, args=args)
         self.assertIn("[myspider] DEBUG: FEEDS: {'stdout:': {'format': 'json'}}", log)
 
 
-@skipIf(platform.system() != 'Windows', "Windows required for .pyw files")
+@skipIf(platform.system() != "Windows", "Windows required for .pyw files")
 class WindowsRunSpiderCommandTest(RunSpiderCommandTest):
 
-    spider_filename = 'myspider.pyw'
+    spider_filename = "myspider.pyw"
 
     def setUp(self):
-        super(WindowsRunSpiderCommandTest, self).setUp()
+        super().setUp()
 
     def test_start_requests_errors(self):
-        log = self.get_log(self.badspider, name='badspider.pyw')
+        log = self.get_log(self.badspider, name="badspider.pyw")
         self.assertIn("start_requests", log)
         self.assertIn("badspider.pyw", log)
 
     def test_run_good_spider(self):
         super().test_run_good_spider()
 
     def test_runspider(self):
@@ -831,44 +909,45 @@
         super().test_overwrite_output()
 
     def test_runspider_unable_to_load(self):
         raise unittest.SkipTest("Already Tested in 'RunSpiderCommandTest' ")
 
 
 class BenchCommandTest(CommandTest):
-
     def test_run(self):
-        _, _, log = self.proc('bench', '-s', 'LOGSTATS_INTERVAL=0.001',
-                              '-s', 'CLOSESPIDER_TIMEOUT=0.01')
-        self.assertIn('INFO: Crawled', log)
-        self.assertNotIn('Unhandled Error', log)
+        _, _, log = self.proc(
+            "bench", "-s", "LOGSTATS_INTERVAL=0.001", "-s", "CLOSESPIDER_TIMEOUT=0.01"
+        )
+        self.assertIn("INFO: Crawled", log)
+        self.assertNotIn("Unhandled Error", log)
 
 
 class ViewCommandTest(CommandTest):
-
     def test_methods(self):
         command = view.Command()
         command.settings = Settings()
-        parser = argparse.ArgumentParser(prog='scrapy', prefix_chars='-',
-                                         formatter_class=ScrapyHelpFormatter,
-                                         conflict_handler='resolve')
+        parser = argparse.ArgumentParser(
+            prog="scrapy",
+            prefix_chars="-",
+            formatter_class=ScrapyHelpFormatter,
+            conflict_handler="resolve",
+        )
         command.add_options(parser)
-        self.assertEqual(command.short_desc(),
-                         "Open URL in browser, as seen by Scrapy")
-        self.assertIn("URL using the Scrapy downloader and show its",
-                      command.long_desc())
+        self.assertEqual(command.short_desc(), "Open URL in browser, as seen by Scrapy")
+        self.assertIn(
+            "URL using the Scrapy downloader and show its", command.long_desc()
+        )
 
 
 class CrawlCommandTest(CommandTest):
-
     def crawl(self, code, args=()):
-        fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))
-        with open(fname, 'w') as f:
-            f.write(code)
-        return self.proc('crawl', 'myspider', *args)
+        Path(self.proj_mod_path, "spiders", "myspider.py").write_text(
+            code, encoding="utf-8"
+        )
+        return self.proc("crawl", "myspider", *args)
 
     def get_log(self, code, args=()):
         _, _, stderr = self.crawl(code, args=args)
         return stderr
 
     def test_no_output(self):
         spider_code = """
@@ -891,17 +970,19 @@
 class MySpider(scrapy.Spider):
     name = 'myspider'
 
     def start_requests(self):
         self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))
         return []
 """
-        args = ['-o', 'example.json']
+        args = ["-o", "example.json"]
         log = self.get_log(spider_code, args=args)
-        self.assertIn("[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}", log)
+        self.assertIn(
+            "[myspider] DEBUG: FEEDS: {'example.json': {'format': 'json'}}", log
+        )
 
     def test_overwrite_output(self):
         spider_code = """
 import json
 import scrapy
 
 class MySpider(scrapy.Spider):
@@ -911,43 +992,59 @@
         self.logger.debug(
             'FEEDS: {}'.format(
                 json.dumps(self.settings.getdict('FEEDS'), sort_keys=True)
             )
         )
         return []
 """
-        with open(os.path.join(self.cwd, "example.json"), "w") as f1:
-            f1.write("not empty")
-        args = ['-O', 'example.json']
+        Path(self.cwd, "example.json").write_text("not empty", encoding="utf-8")
+        args = ["-O", "example.json"]
         log = self.get_log(spider_code, args=args)
-        self.assertIn('[myspider] DEBUG: FEEDS: {"example.json": {"format": "json", "overwrite": true}}', log)
-        with open(os.path.join(self.cwd, "example.json")) as f2:
+        self.assertIn(
+            '[myspider] DEBUG: FEEDS: {"example.json": {"format": "json", "overwrite": true}}',
+            log,
+        )
+        with Path(self.cwd, "example.json").open(encoding="utf-8") as f2:
             first_line = f2.readline()
         self.assertNotEqual(first_line, "not empty")
 
     def test_output_and_overwrite_output(self):
         spider_code = """
 import scrapy
 
 class MySpider(scrapy.Spider):
     name = 'myspider'
 
     def start_requests(self):
         return []
 """
-        args = ['-o', 'example1.json', '-O', 'example2.json']
+        args = ["-o", "example1.json", "-O", "example2.json"]
         log = self.get_log(spider_code, args=args)
-        self.assertIn("error: Please use only one of -o/--output and -O/--overwrite-output", log)
+        self.assertIn(
+            "error: Please use only one of -o/--output and -O/--overwrite-output", log
+        )
 
 
 class HelpMessageTest(CommandTest):
-
     def setUp(self):
         super().setUp()
-        self.commands = ["parse", "startproject", "view", "crawl", "edit",
-                         "list", "fetch", "settings", "shell", "runspider",
-                         "version", "genspider", "check", "bench"]
+        self.commands = [
+            "parse",
+            "startproject",
+            "view",
+            "crawl",
+            "edit",
+            "list",
+            "fetch",
+            "settings",
+            "shell",
+            "runspider",
+            "version",
+            "genspider",
+            "check",
+            "bench",
+        ]
 
     def test_help_messages(self):
         for command in self.commands:
             _, out, _ = self.proc(command, "-h")
             self.assertIn("Usage", out)
```

### Comparing `Scrapy-2.7.1/tests/test_contracts.py` & `Scrapy-2.8.0/tests/test_contracts.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,207 +1,207 @@
 from unittest import TextTestResult
 
 from twisted.internet import defer
 from twisted.python import failure
 from twisted.trial import unittest
 
 from scrapy import FormRequest
-from scrapy.spidermiddlewares.httperror import HttpError
-from scrapy.spiders import Spider
-from scrapy.http import Request
-from scrapy.item import Item, Field
-from scrapy.utils.test import get_crawler
-from scrapy.contracts import ContractsManager, Contract
+from scrapy.contracts import Contract, ContractsManager
 from scrapy.contracts.default import (
-    UrlContract,
     CallbackKeywordArgumentsContract,
     ReturnsContract,
     ScrapesContract,
+    UrlContract,
 )
+from scrapy.http import Request
+from scrapy.item import Field, Item
+from scrapy.spidermiddlewares.httperror import HttpError
+from scrapy.spiders import Spider
+from scrapy.utils.test import get_crawler
 from tests.mockserver import MockServer
 
 
 class TestItem(Item):
     name = Field()
     url = Field()
 
 
 class ResponseMock:
-    url = 'http://scrapy.org'
+    url = "http://scrapy.org"
 
 
 class CustomSuccessContract(Contract):
-    name = 'custom_success_contract'
+    name = "custom_success_contract"
 
     def adjust_request_args(self, args):
-        args['url'] = 'http://scrapy.org'
+        args["url"] = "http://scrapy.org"
         return args
 
 
 class CustomFailContract(Contract):
-    name = 'custom_fail_contract'
+    name = "custom_fail_contract"
 
     def adjust_request_args(self, args):
-        raise TypeError('Error in adjust_request_args')
+        raise TypeError("Error in adjust_request_args")
 
 
 class CustomFormContract(Contract):
-    name = 'custom_form'
+    name = "custom_form"
     request_cls = FormRequest
 
     def adjust_request_args(self, args):
-        args['formdata'] = {'name': 'scrapy'}
+        args["formdata"] = {"name": "scrapy"}
         return args
 
 
 class TestSpider(Spider):
-    name = 'demo_spider'
+    name = "demo_spider"
 
     def returns_request(self, response):
-        """ method which returns request
+        """method which returns request
         @url http://scrapy.org
         @returns requests 1
         """
-        return Request('http://scrapy.org', callback=self.returns_item)
+        return Request("http://scrapy.org", callback=self.returns_item)
 
     def returns_item(self, response):
-        """ method which returns item
+        """method which returns item
         @url http://scrapy.org
         @returns items 1 1
         """
         return TestItem(url=response.url)
 
     def returns_request_cb_kwargs(self, response, url):
-        """ method which returns request
+        """method which returns request
         @url https://example.org
         @cb_kwargs {"url": "http://scrapy.org"}
         @returns requests 1
         """
         return Request(url, callback=self.returns_item_cb_kwargs)
 
     def returns_item_cb_kwargs(self, response, name):
-        """ method which returns item
+        """method which returns item
         @url http://scrapy.org
         @cb_kwargs {"name": "Scrapy"}
         @returns items 1 1
         """
         return TestItem(name=name, url=response.url)
 
     def returns_item_cb_kwargs_error_unexpected_keyword(self, response):
-        """ method which returns item
+        """method which returns item
         @url http://scrapy.org
         @cb_kwargs {"arg": "value"}
         @returns items 1 1
         """
         return TestItem(url=response.url)
 
     def returns_item_cb_kwargs_error_missing_argument(self, response, arg):
-        """ method which returns item
+        """method which returns item
         @url http://scrapy.org
         @returns items 1 1
         """
         return TestItem(url=response.url)
 
     def returns_dict_item(self, response):
-        """ method which returns item
+        """method which returns item
         @url http://scrapy.org
         @returns items 1 1
         """
         return {"url": response.url}
 
     def returns_fail(self, response):
-        """ method which returns item
+        """method which returns item
         @url http://scrapy.org
         @returns items 0 0
         """
         return TestItem(url=response.url)
 
     def returns_dict_fail(self, response):
-        """ method which returns item
+        """method which returns item
         @url http://scrapy.org
         @returns items 0 0
         """
-        return {'url': response.url}
+        return {"url": response.url}
 
     def scrapes_item_ok(self, response):
-        """ returns item with name and url
+        """returns item with name and url
         @url http://scrapy.org
         @returns items 1 1
         @scrapes name url
         """
-        return TestItem(name='test', url=response.url)
+        return TestItem(name="test", url=response.url)
 
     def scrapes_dict_item_ok(self, response):
-        """ returns item with name and url
+        """returns item with name and url
         @url http://scrapy.org
         @returns items 1 1
         @scrapes name url
         """
-        return {'name': 'test', 'url': response.url}
+        return {"name": "test", "url": response.url}
 
     def scrapes_item_fail(self, response):
-        """ returns item with no name
+        """returns item with no name
         @url http://scrapy.org
         @returns items 1 1
         @scrapes name url
         """
         return TestItem(url=response.url)
 
     def scrapes_dict_item_fail(self, response):
-        """ returns item with no name
+        """returns item with no name
         @url http://scrapy.org
         @returns items 1 1
         @scrapes name url
         """
-        return {'url': response.url}
+        return {"url": response.url}
 
     def scrapes_multiple_missing_fields(self, response):
-        """ returns item with no name
+        """returns item with no name
         @url http://scrapy.org
         @returns items 1 1
         @scrapes name url
         """
         return {}
 
     def parse_no_url(self, response):
-        """ method with no url
+        """method with no url
         @returns items 1 1
         """
         pass
 
     def custom_form(self, response):
         """
         @url http://scrapy.org
         @custom_form
         """
         pass
 
 
 class CustomContractSuccessSpider(Spider):
-    name = 'custom_contract_success_spider'
+    name = "custom_contract_success_spider"
 
     def parse(self, response):
         """
         @custom_success_contract
         """
         pass
 
 
 class CustomContractFailSpider(Spider):
-    name = 'custom_contract_fail_spider'
+    name = "custom_contract_fail_spider"
 
     def parse(self, response):
         """
         @custom_fail_contract
         """
         pass
 
 
 class InheritsTestSpider(TestSpider):
-    name = 'inherits_demo_spider'
+    name = "inherits_demo_spider"
 
 
 class ContractsManagerTest(unittest.TestCase):
     contracts = [
         UrlContract,
         CallbackKeywordArgumentsContract,
         ReturnsContract,
@@ -230,15 +230,16 @@
         spider = TestSpider()
 
         # extract contracts correctly
         contracts = self.conman.extract_contracts(spider.returns_request)
         self.assertEqual(len(contracts), 2)
         self.assertEqual(
             frozenset(type(x) for x in contracts),
-            frozenset([UrlContract, ReturnsContract]))
+            frozenset([UrlContract, ReturnsContract]),
+        )
 
         # returns request for valid method
         request = self.conman.from_method(spider.returns_request, self.results)
         self.assertNotEqual(request, None)
 
         # no request for missing url
         request = self.conman.from_method(spider.parse_no_url, self.results)
@@ -247,49 +248,67 @@
     def test_cb_kwargs(self):
         spider = TestSpider()
         response = ResponseMock()
 
         # extract contracts correctly
         contracts = self.conman.extract_contracts(spider.returns_request_cb_kwargs)
         self.assertEqual(len(contracts), 3)
-        self.assertEqual(frozenset(type(x) for x in contracts),
-                         frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]))
+        self.assertEqual(
+            frozenset(type(x) for x in contracts),
+            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),
+        )
 
         contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs)
         self.assertEqual(len(contracts), 3)
-        self.assertEqual(frozenset(type(x) for x in contracts),
-                         frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]))
+        self.assertEqual(
+            frozenset(type(x) for x in contracts),
+            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),
+        )
 
-        contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs_error_unexpected_keyword)
+        contracts = self.conman.extract_contracts(
+            spider.returns_item_cb_kwargs_error_unexpected_keyword
+        )
         self.assertEqual(len(contracts), 3)
-        self.assertEqual(frozenset(type(x) for x in contracts),
-                         frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]))
+        self.assertEqual(
+            frozenset(type(x) for x in contracts),
+            frozenset([UrlContract, CallbackKeywordArgumentsContract, ReturnsContract]),
+        )
 
-        contracts = self.conman.extract_contracts(spider.returns_item_cb_kwargs_error_missing_argument)
+        contracts = self.conman.extract_contracts(
+            spider.returns_item_cb_kwargs_error_missing_argument
+        )
         self.assertEqual(len(contracts), 2)
-        self.assertEqual(frozenset(type(x) for x in contracts),
-                         frozenset([UrlContract, ReturnsContract]))
+        self.assertEqual(
+            frozenset(type(x) for x in contracts),
+            frozenset([UrlContract, ReturnsContract]),
+        )
 
         # returns_request
-        request = self.conman.from_method(spider.returns_request_cb_kwargs, self.results)
+        request = self.conman.from_method(
+            spider.returns_request_cb_kwargs, self.results
+        )
         request.callback(response, **request.cb_kwargs)
         self.should_succeed()
 
         # returns_item
         request = self.conman.from_method(spider.returns_item_cb_kwargs, self.results)
         request.callback(response, **request.cb_kwargs)
         self.should_succeed()
 
         # returns_item (error, callback doesn't take keyword arguments)
-        request = self.conman.from_method(spider.returns_item_cb_kwargs_error_unexpected_keyword, self.results)
+        request = self.conman.from_method(
+            spider.returns_item_cb_kwargs_error_unexpected_keyword, self.results
+        )
         request.callback(response, **request.cb_kwargs)
         self.should_error()
 
         # returns_item (error, contract doesn't provide keyword arguments)
-        request = self.conman.from_method(spider.returns_item_cb_kwargs_error_missing_argument, self.results)
+        request = self.conman.from_method(
+            spider.returns_item_cb_kwargs_error_missing_argument, self.results
+        )
         request.callback(response, **request.cb_kwargs)
         self.should_error()
 
     def test_returns(self):
         spider = TestSpider()
         response = ResponseMock()
 
@@ -339,47 +358,48 @@
 
         # scrapes_dict_item_fail
         request = self.conman.from_method(spider.scrapes_dict_item_fail, self.results)
         request.callback(response)
         self.should_fail()
 
         # scrapes_multiple_missing_fields
-        request = self.conman.from_method(spider.scrapes_multiple_missing_fields, self.results)
+        request = self.conman.from_method(
+            spider.scrapes_multiple_missing_fields, self.results
+        )
         request.callback(response)
         self.should_fail()
-        message = 'ContractFail: Missing fields: name, url'
+        message = "ContractFail: Missing fields: name, url"
         assert message in self.results.failures[-1][-1]
 
     def test_custom_contracts(self):
         self.conman.from_spider(CustomContractSuccessSpider(), self.results)
         self.should_succeed()
 
         self.conman.from_spider(CustomContractFailSpider(), self.results)
         self.should_error()
 
     def test_errback(self):
         spider = TestSpider()
         response = ResponseMock()
 
         try:
-            raise HttpError(response, 'Ignoring non-200 response')
+            raise HttpError(response, "Ignoring non-200 response")
         except HttpError:
             failure_mock = failure.Failure()
 
         request = self.conman.from_method(spider.returns_request, self.results)
         request.errback(failure_mock)
 
         self.assertFalse(self.results.failures)
         self.assertTrue(self.results.errors)
 
     @defer.inlineCallbacks
     def test_same_url(self):
-
         class TestSameUrlSpider(Spider):
-            name = 'test_same_url'
+            name = "test_same_url"
 
             def __init__(self, *args, **kwargs):
                 super().__init__(*args, **kwargs)
                 self.visited = 0
 
             def start_requests(s):
                 return self.conman.from_spider(s, self.results)
@@ -402,15 +422,15 @@
             yield crawler.crawl()
 
         self.assertEqual(crawler.spider.visited, 2)
 
     def test_form_contract(self):
         spider = TestSpider()
         request = self.conman.from_method(spider.custom_form, self.results)
-        self.assertEqual(request.method, 'POST')
+        self.assertEqual(request.method, "POST")
         self.assertIsInstance(request, FormRequest)
 
     def test_inherited_contracts(self):
         spider = InheritsTestSpider()
 
         requests = self.conman.from_spider(spider, self.results)
         self.assertTrue(requests)
```

### Comparing `Scrapy-2.7.1/tests/test_crawl.py` & `Scrapy-2.8.0/tests/test_crawl.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import json
 import logging
+import unittest
 from ipaddress import IPv4Address
 from socket import gethostbyname
 from urllib.parse import urlparse
-import unittest
 
 from pytest import mark
 from testfixtures import LogCapture
 from twisted.internet import defer
 from twisted.internet.ssl import Certificate
 from twisted.python.failure import Failure
 from twisted.trial.unittest import TestCase
@@ -37,26 +37,26 @@
     BrokenStartRequestsSpider,
     BytesReceivedCallbackSpider,
     BytesReceivedErrbackSpider,
     CrawlSpiderWithAsyncCallback,
     CrawlSpiderWithAsyncGeneratorCallback,
     CrawlSpiderWithErrback,
     CrawlSpiderWithParseMethod,
+    CrawlSpiderWithProcessRequestCallbackKeywordArguments,
     DelaySpider,
     DuplicateStartRequestsSpider,
     FollowAllSpider,
     HeadersReceivedCallbackSpider,
     HeadersReceivedErrbackSpider,
     SimpleSpider,
     SingleRequestSpider,
 )
 
 
 class CrawlTestCase(TestCase):
-
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
 
     def tearDown(self):
         self.mockserver.__exit__(None, None, None)
 
@@ -77,38 +77,39 @@
     @defer.inlineCallbacks
     def _test_delay(self, total, delay, randomize=False):
         crawl_kwargs = dict(
             maxlatency=delay * 2,
             mockserver=self.mockserver,
             total=total,
         )
-        tolerance = (1 - (0.6 if randomize else 0.2))
+        tolerance = 1 - (0.6 if randomize else 0.2)
 
-        settings = {"DOWNLOAD_DELAY": delay,
-                    'RANDOMIZE_DOWNLOAD_DELAY': randomize}
+        settings = {"DOWNLOAD_DELAY": delay, "RANDOMIZE_DOWNLOAD_DELAY": randomize}
         crawler = get_crawler(FollowAllSpider, settings)
         yield crawler.crawl(**crawl_kwargs)
         times = crawler.spider.times
         total_time = times[-1] - times[0]
         average = total_time / (len(times) - 1)
-        self.assertTrue(average > delay * tolerance,
-                        f"download delay too small: {average}")
+        self.assertTrue(
+            average > delay * tolerance, f"download delay too small: {average}"
+        )
 
         # Ensure that the same test parameters would cause a failure if no
         # download delay is set. Otherwise, it means we are using a combination
         # of ``total`` and ``delay`` values that are too small for the test
         # code above to have any meaning.
         settings["DOWNLOAD_DELAY"] = 0
         crawler = get_crawler(FollowAllSpider, settings)
         yield crawler.crawl(**crawl_kwargs)
         times = crawler.spider.times
         total_time = times[-1] - times[0]
         average = total_time / (len(times) - 1)
-        self.assertFalse(average > delay / tolerance,
-                         "test total or delay values are too small")
+        self.assertFalse(
+            average > delay / tolerance, "test total or delay values are too small"
+        )
 
     @defer.inlineCallbacks
     def test_timeout_success(self):
         crawler = get_crawler(DelaySpider)
         yield crawler.crawl(n=0.5, mockserver=self.mockserver)
         self.assertTrue(crawler.spider.t1 > 0)
         self.assertTrue(crawler.spider.t2 > 0)
@@ -129,81 +130,98 @@
         self.assertTrue(crawler.spider.t2_err > 0)
         self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)
 
     @defer.inlineCallbacks
     def test_retry_503(self):
         crawler = get_crawler(SimpleSpider)
         with LogCapture() as log:
-            yield crawler.crawl(self.mockserver.url("/status?n=503"), mockserver=self.mockserver)
+            yield crawler.crawl(
+                self.mockserver.url("/status?n=503"), mockserver=self.mockserver
+            )
         self._assert_retried(log)
 
     @defer.inlineCallbacks
     def test_retry_conn_failed(self):
         crawler = get_crawler(SimpleSpider)
         with LogCapture() as log:
-            yield crawler.crawl("http://localhost:65432/status?n=503", mockserver=self.mockserver)
+            yield crawler.crawl(
+                "http://localhost:65432/status?n=503", mockserver=self.mockserver
+            )
         self._assert_retried(log)
 
     @defer.inlineCallbacks
     def test_retry_dns_error(self):
         if NON_EXISTING_RESOLVABLE:
             raise unittest.SkipTest("Non-existing hosts are resolvable")
         crawler = get_crawler(SimpleSpider)
         with LogCapture() as log:
-            # try to fetch the homepage of a non-existent domain
-            yield crawler.crawl("http://dns.resolution.invalid./", mockserver=self.mockserver)
+            # try to fetch the homepage of a nonexistent domain
+            yield crawler.crawl(
+                "http://dns.resolution.invalid./", mockserver=self.mockserver
+            )
         self._assert_retried(log)
 
     @defer.inlineCallbacks
     def test_start_requests_bug_before_yield(self):
-        with LogCapture('scrapy', level=logging.ERROR) as log:
+        with LogCapture("scrapy", level=logging.ERROR) as log:
             crawler = get_crawler(BrokenStartRequestsSpider)
             yield crawler.crawl(fail_before_yield=1, mockserver=self.mockserver)
 
         self.assertEqual(len(log.records), 1)
         record = log.records[0]
         self.assertIsNotNone(record.exc_info)
         self.assertIs(record.exc_info[0], ZeroDivisionError)
 
     @defer.inlineCallbacks
     def test_start_requests_bug_yielding(self):
-        with LogCapture('scrapy', level=logging.ERROR) as log:
+        with LogCapture("scrapy", level=logging.ERROR) as log:
             crawler = get_crawler(BrokenStartRequestsSpider)
             yield crawler.crawl(fail_yielding=1, mockserver=self.mockserver)
 
         self.assertEqual(len(log.records), 1)
         record = log.records[0]
         self.assertIsNotNone(record.exc_info)
         self.assertIs(record.exc_info[0], ZeroDivisionError)
 
     @defer.inlineCallbacks
-    def test_start_requests_lazyness(self):
+    def test_start_requests_laziness(self):
         settings = {"CONCURRENT_REQUESTS": 1}
         crawler = get_crawler(BrokenStartRequestsSpider, settings)
         yield crawler.crawl(mockserver=self.mockserver)
         self.assertTrue(
             crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),
-            crawler.spider.seedsseen)
+            crawler.spider.seedsseen,
+        )
 
     @defer.inlineCallbacks
     def test_start_requests_dupes(self):
         settings = {"CONCURRENT_REQUESTS": 1}
         crawler = get_crawler(DuplicateStartRequestsSpider, settings)
-        yield crawler.crawl(dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver)
+        yield crawler.crawl(
+            dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver
+        )
         self.assertEqual(crawler.spider.visited, 6)
 
-        yield crawler.crawl(dont_filter=False, distinct_urls=3, dupe_factor=4, mockserver=self.mockserver)
+        yield crawler.crawl(
+            dont_filter=False,
+            distinct_urls=3,
+            dupe_factor=4,
+            mockserver=self.mockserver,
+        )
         self.assertEqual(crawler.spider.visited, 3)
 
     @defer.inlineCallbacks
     def test_unbounded_response(self):
         # Completeness of responses without Content-Length or Transfer-Encoding
         # can not be determined, we treat them as valid but flagged as "partial"
         from urllib.parse import urlencode
-        query = urlencode({'raw': '''\
+
+        query = urlencode(
+            {
+                "raw": """\
 HTTP/1.1 200 OK
 Server: Apache-Coyote/1.1
 X-Powered-By: Servlet 2.4; JBoss-4.2.3.GA (build: SVNTag=JBoss_4_2_3_GA date=200807181417)/JBossWeb-2.0
 Set-Cookie: JSESSIONID=08515F572832D0E659FD2B0D8031D75F; Path=/
 Pragma: no-cache
 Expires: Thu, 01 Jan 1970 00:00:00 GMT
 Cache-Control: no-cache
@@ -211,104 +229,118 @@
 Content-Type: text/html;charset=UTF-8
 Content-Language: en
 Date: Tue, 27 Aug 2013 13:05:05 GMT
 Connection: close
 
 foo body
 with multiples lines
-'''})
+"""
+            }
+        )
         crawler = get_crawler(SimpleSpider)
         with LogCapture() as log:
-            yield crawler.crawl(self.mockserver.url(f"/raw?{query}"), mockserver=self.mockserver)
+            yield crawler.crawl(
+                self.mockserver.url(f"/raw?{query}"), mockserver=self.mockserver
+            )
         self.assertEqual(str(log).count("Got response 200"), 1)
 
     @defer.inlineCallbacks
     def test_retry_conn_lost(self):
         # connection lost after receiving data
         crawler = get_crawler(SimpleSpider)
         with LogCapture() as log:
-            yield crawler.crawl(self.mockserver.url("/drop?abort=0"), mockserver=self.mockserver)
+            yield crawler.crawl(
+                self.mockserver.url("/drop?abort=0"), mockserver=self.mockserver
+            )
         self._assert_retried(log)
 
     @defer.inlineCallbacks
     def test_retry_conn_aborted(self):
         # connection lost before receiving data
         crawler = get_crawler(SimpleSpider)
         with LogCapture() as log:
-            yield crawler.crawl(self.mockserver.url("/drop?abort=1"), mockserver=self.mockserver)
+            yield crawler.crawl(
+                self.mockserver.url("/drop?abort=1"), mockserver=self.mockserver
+            )
         self._assert_retried(log)
 
     def _assert_retried(self, log):
         self.assertEqual(str(log).count("Retrying"), 2)
         self.assertEqual(str(log).count("Gave up retrying"), 1)
 
     @defer.inlineCallbacks
     def test_referer_header(self):
         """Referer header is set by RefererMiddleware unless it is already set"""
-        req0 = Request(self.mockserver.url('/echo?headers=1&body=0'), dont_filter=1)
+        req0 = Request(self.mockserver.url("/echo?headers=1&body=0"), dont_filter=1)
         req1 = req0.replace()
-        req2 = req0.replace(headers={'Referer': None})
-        req3 = req0.replace(headers={'Referer': 'http://example.com'})
-        req0.meta['next'] = req1
-        req1.meta['next'] = req2
-        req2.meta['next'] = req3
+        req2 = req0.replace(headers={"Referer": None})
+        req3 = req0.replace(headers={"Referer": "http://example.com"})
+        req0.meta["next"] = req1
+        req1.meta["next"] = req2
+        req2.meta["next"] = req3
         crawler = get_crawler(SingleRequestSpider)
         yield crawler.crawl(seed=req0, mockserver=self.mockserver)
         # basic asserts in case of weird communication errors
-        self.assertIn('responses', crawler.spider.meta)
-        self.assertNotIn('failures', crawler.spider.meta)
+        self.assertIn("responses", crawler.spider.meta)
+        self.assertNotIn("failures", crawler.spider.meta)
         # start requests doesn't set Referer header
-        echo0 = json.loads(to_unicode(crawler.spider.meta['responses'][2].body))
-        self.assertNotIn('Referer', echo0['headers'])
+        echo0 = json.loads(to_unicode(crawler.spider.meta["responses"][2].body))
+        self.assertNotIn("Referer", echo0["headers"])
         # following request sets Referer to start request url
-        echo1 = json.loads(to_unicode(crawler.spider.meta['responses'][1].body))
-        self.assertEqual(echo1['headers'].get('Referer'), [req0.url])
+        echo1 = json.loads(to_unicode(crawler.spider.meta["responses"][1].body))
+        self.assertEqual(echo1["headers"].get("Referer"), [req0.url])
         # next request avoids Referer header
-        echo2 = json.loads(to_unicode(crawler.spider.meta['responses'][2].body))
-        self.assertNotIn('Referer', echo2['headers'])
+        echo2 = json.loads(to_unicode(crawler.spider.meta["responses"][2].body))
+        self.assertNotIn("Referer", echo2["headers"])
         # last request explicitly sets a Referer header
-        echo3 = json.loads(to_unicode(crawler.spider.meta['responses'][3].body))
-        self.assertEqual(echo3['headers'].get('Referer'), ['http://example.com'])
+        echo3 = json.loads(to_unicode(crawler.spider.meta["responses"][3].body))
+        self.assertEqual(echo3["headers"].get("Referer"), ["http://example.com"])
 
     @defer.inlineCallbacks
     def test_engine_status(self):
         from scrapy.utils.engine import get_engine_status
+
         est = []
 
         def cb(response):
             est.append(get_engine_status(crawler.engine))
 
         crawler = get_crawler(SingleRequestSpider)
-        yield crawler.crawl(seed=self.mockserver.url('/'), callback_func=cb, mockserver=self.mockserver)
+        yield crawler.crawl(
+            seed=self.mockserver.url("/"), callback_func=cb, mockserver=self.mockserver
+        )
         self.assertEqual(len(est), 1, est)
         s = dict(est[0])
-        self.assertEqual(s['engine.spider.name'], crawler.spider.name)
-        self.assertEqual(s['len(engine.scraper.slot.active)'], 1)
+        self.assertEqual(s["engine.spider.name"], crawler.spider.name)
+        self.assertEqual(s["len(engine.scraper.slot.active)"], 1)
 
     @defer.inlineCallbacks
     def test_format_engine_status(self):
         from scrapy.utils.engine import format_engine_status
+
         est = []
 
         def cb(response):
             est.append(format_engine_status(crawler.engine))
 
         crawler = get_crawler(SingleRequestSpider)
-        yield crawler.crawl(seed=self.mockserver.url('/'), callback_func=cb, mockserver=self.mockserver)
+        yield crawler.crawl(
+            seed=self.mockserver.url("/"), callback_func=cb, mockserver=self.mockserver
+        )
         self.assertEqual(len(est), 1, est)
         est = est[0].split("\n")[2:-2]  # remove header & footer
         # convert to dict
         est = [x.split(":") for x in est]
         est = [x for sublist in est for x in sublist]  # flatten
         est = [x.lstrip().rstrip() for x in est]
         it = iter(est)
         s = dict(zip(it, it))
 
-        self.assertEqual(s['engine.spider.name'], crawler.spider.name)
-        self.assertEqual(s['len(engine.scraper.slot.active)'], '1')
+        self.assertEqual(s["engine.spider.name"], crawler.spider.name)
+        self.assertEqual(s["len(engine.scraper.slot.active)"], "1")
 
     @defer.inlineCallbacks
     def test_graceful_crawl_error_handling(self):
         """
         Test whether errors happening anywhere in Crawler.crawl() are properly
         reported (and not somehow swallowed) after a graceful engine shutdown.
         The errors should not come from within Scrapy's core but from within
@@ -332,41 +364,55 @@
         settings = {
             "ITEM_PIPELINES": {
                 "tests.pipelines.ZeroDivisionErrorPipeline": 300,
             }
         }
         crawler = get_crawler(SimpleSpider, settings)
         yield self.assertFailure(
-            crawler.crawl(self.mockserver.url("/status?n=200"), mockserver=self.mockserver),
-            ZeroDivisionError)
+            crawler.crawl(
+                self.mockserver.url("/status?n=200"), mockserver=self.mockserver
+            ),
+            ZeroDivisionError,
+        )
         self.assertFalse(crawler.crawling)
 
     @defer.inlineCallbacks
     def test_crawlerrunner_accepts_crawler(self):
         crawler = get_crawler(SimpleSpider)
         runner = CrawlerRunner()
         with LogCapture() as log:
-            yield runner.crawl(crawler, self.mockserver.url("/status?n=200"), mockserver=self.mockserver)
+            yield runner.crawl(
+                crawler,
+                self.mockserver.url("/status?n=200"),
+                mockserver=self.mockserver,
+            )
         self.assertIn("Got response 200", str(log))
 
     @defer.inlineCallbacks
     def test_crawl_multiple(self):
-        runner = CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'})
-        runner.crawl(SimpleSpider, self.mockserver.url("/status?n=200"), mockserver=self.mockserver)
-        runner.crawl(SimpleSpider, self.mockserver.url("/status?n=503"), mockserver=self.mockserver)
+        runner = CrawlerRunner({"REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7"})
+        runner.crawl(
+            SimpleSpider,
+            self.mockserver.url("/status?n=200"),
+            mockserver=self.mockserver,
+        )
+        runner.crawl(
+            SimpleSpider,
+            self.mockserver.url("/status?n=503"),
+            mockserver=self.mockserver,
+        )
 
         with LogCapture() as log:
             yield runner.join()
 
         self._assert_retried(log)
         self.assertIn("Got response 200", str(log))
 
 
 class CrawlSpiderTestCase(TestCase):
-
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
 
     def tearDown(self):
         self.mockserver.__exit__(None, None, None)
 
@@ -376,15 +422,17 @@
 
         def _on_item_scraped(item):
             items.append(item)
 
         crawler = get_crawler(spider_cls)
         crawler.signals.connect(_on_item_scraped, signals.item_scraped)
         with LogCapture() as log:
-            yield crawler.crawl(self.mockserver.url("/status?n=200"), mockserver=self.mockserver)
+            yield crawler.crawl(
+                self.mockserver.url("/status?n=200"), mockserver=self.mockserver
+            )
         return log, items, crawler.stats
 
     @defer.inlineCallbacks
     def test_crawlspider_with_parse(self):
         crawler = get_crawler(CrawlSpiderWithParseMethod)
         with LogCapture() as log:
             yield crawler.crawl(mockserver=self.mockserver)
@@ -423,201 +471,232 @@
         self.assertIn("[parse] status 201 (foo: None)", str(log))
         self.assertIn("[parse] status 202 (foo: bar)", str(log))
         self.assertIn("[errback] status 404", str(log))
         self.assertIn("[errback] status 500", str(log))
         self.assertIn("[errback] status 501", str(log))
 
     @defer.inlineCallbacks
+    def test_crawlspider_process_request_cb_kwargs(self):
+        crawler = get_crawler(CrawlSpiderWithProcessRequestCallbackKeywordArguments)
+        with LogCapture() as log:
+            yield crawler.crawl(mockserver=self.mockserver)
+
+        self.assertIn("[parse] status 200 (foo: process_request)", str(log))
+        self.assertIn("[parse] status 201 (foo: process_request)", str(log))
+        self.assertIn("[parse] status 202 (foo: bar)", str(log))
+
+    @defer.inlineCallbacks
     def test_async_def_parse(self):
         crawler = get_crawler(AsyncDefSpider)
         with LogCapture() as log:
-            yield crawler.crawl(self.mockserver.url("/status?n=200"), mockserver=self.mockserver)
+            yield crawler.crawl(
+                self.mockserver.url("/status?n=200"), mockserver=self.mockserver
+            )
         self.assertIn("Got response 200", str(log))
 
     @mark.only_asyncio()
     @defer.inlineCallbacks
     def test_async_def_asyncio_parse(self):
-        crawler = get_crawler(AsyncDefAsyncioSpider, {
-            "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
-        })
+        crawler = get_crawler(
+            AsyncDefAsyncioSpider,
+            {
+                "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
+            },
+        )
         with LogCapture() as log:
-            yield crawler.crawl(self.mockserver.url("/status?n=200"), mockserver=self.mockserver)
+            yield crawler.crawl(
+                self.mockserver.url("/status?n=200"), mockserver=self.mockserver
+            )
         self.assertIn("Got response 200", str(log))
 
     @mark.only_asyncio()
     @defer.inlineCallbacks
     def test_async_def_asyncio_parse_items_list(self):
         log, items, _ = yield self._run_spider(AsyncDefAsyncioReturnSpider)
         self.assertIn("Got response 200", str(log))
-        self.assertIn({'id': 1}, items)
-        self.assertIn({'id': 2}, items)
+        self.assertIn({"id": 1}, items)
+        self.assertIn({"id": 2}, items)
 
     @mark.only_asyncio()
     @defer.inlineCallbacks
     def test_async_def_asyncio_parse_items_single_element(self):
         items = []
 
         def _on_item_scraped(item):
             items.append(item)
 
         crawler = get_crawler(AsyncDefAsyncioReturnSingleElementSpider)
         crawler.signals.connect(_on_item_scraped, signals.item_scraped)
         with LogCapture() as log:
-            yield crawler.crawl(self.mockserver.url("/status?n=200"), mockserver=self.mockserver)
+            yield crawler.crawl(
+                self.mockserver.url("/status?n=200"), mockserver=self.mockserver
+            )
         self.assertIn("Got response 200", str(log))
         self.assertIn({"foo": 42}, items)
 
     @mark.only_asyncio()
     @defer.inlineCallbacks
     def test_async_def_asyncgen_parse(self):
         log, _, stats = yield self._run_spider(AsyncDefAsyncioGenSpider)
         self.assertIn("Got response 200", str(log))
-        itemcount = stats.get_value('item_scraped_count')
+        itemcount = stats.get_value("item_scraped_count")
         self.assertEqual(itemcount, 1)
 
     @mark.only_asyncio()
     @defer.inlineCallbacks
     def test_async_def_asyncgen_parse_loop(self):
         log, items, stats = yield self._run_spider(AsyncDefAsyncioGenLoopSpider)
         self.assertIn("Got response 200", str(log))
-        itemcount = stats.get_value('item_scraped_count')
+        itemcount = stats.get_value("item_scraped_count")
         self.assertEqual(itemcount, 10)
         for i in range(10):
-            self.assertIn({'foo': i}, items)
+            self.assertIn({"foo": i}, items)
 
     @mark.only_asyncio()
     @defer.inlineCallbacks
     def test_async_def_asyncgen_parse_exc(self):
         log, items, stats = yield self._run_spider(AsyncDefAsyncioGenExcSpider)
         log = str(log)
         self.assertIn("Spider error processing", log)
         self.assertIn("ValueError", log)
-        itemcount = stats.get_value('item_scraped_count')
+        itemcount = stats.get_value("item_scraped_count")
         self.assertEqual(itemcount, 7)
         for i in range(7):
-            self.assertIn({'foo': i}, items)
+            self.assertIn({"foo": i}, items)
 
     @mark.only_asyncio()
     @defer.inlineCallbacks
     def test_async_def_asyncgen_parse_complex(self):
         _, items, stats = yield self._run_spider(AsyncDefAsyncioGenComplexSpider)
-        itemcount = stats.get_value('item_scraped_count')
+        itemcount = stats.get_value("item_scraped_count")
         self.assertEqual(itemcount, 156)
         # some random items
         for i in [1, 4, 21, 22, 207, 311]:
-            self.assertIn({'index': i}, items)
+            self.assertIn({"index": i}, items)
         for i in [10, 30, 122]:
-            self.assertIn({'index2': i}, items)
+            self.assertIn({"index2": i}, items)
 
     @mark.only_asyncio()
     @defer.inlineCallbacks
     def test_async_def_asyncio_parse_reqs_list(self):
         log, *_ = yield self._run_spider(AsyncDefAsyncioReqsReturnSpider)
         for req_id in range(3):
             self.assertIn(f"Got response 200, req_id {req_id}", str(log))
 
     @mark.only_not_asyncio()
     @defer.inlineCallbacks
     def test_async_def_deferred_direct(self):
         _, items, _ = yield self._run_spider(AsyncDefDeferredDirectSpider)
-        self.assertEqual(items, [{'code': 200}])
+        self.assertEqual(items, [{"code": 200}])
 
     @mark.only_asyncio()
     @defer.inlineCallbacks
     def test_async_def_deferred_wrapped(self):
         log, items, _ = yield self._run_spider(AsyncDefDeferredWrappedSpider)
-        self.assertEqual(items, [{'code': 200}])
+        self.assertEqual(items, [{"code": 200}])
 
     @defer.inlineCallbacks
     def test_async_def_deferred_maybe_wrapped(self):
         _, items, _ = yield self._run_spider(AsyncDefDeferredMaybeWrappedSpider)
-        self.assertEqual(items, [{'code': 200}])
+        self.assertEqual(items, [{"code": 200}])
 
     @defer.inlineCallbacks
     def test_response_ssl_certificate_none(self):
         crawler = get_crawler(SingleRequestSpider)
         url = self.mockserver.url("/echo?body=test", is_secure=False)
         yield crawler.crawl(seed=url, mockserver=self.mockserver)
-        self.assertIsNone(crawler.spider.meta['responses'][0].certificate)
+        self.assertIsNone(crawler.spider.meta["responses"][0].certificate)
 
     @defer.inlineCallbacks
     def test_response_ssl_certificate(self):
         crawler = get_crawler(SingleRequestSpider)
         url = self.mockserver.url("/echo?body=test", is_secure=True)
         yield crawler.crawl(seed=url, mockserver=self.mockserver)
-        cert = crawler.spider.meta['responses'][0].certificate
+        cert = crawler.spider.meta["responses"][0].certificate
         self.assertIsInstance(cert, Certificate)
         self.assertEqual(cert.getSubject().commonName, b"localhost")
         self.assertEqual(cert.getIssuer().commonName, b"localhost")
 
     @mark.xfail(reason="Responses with no body return early and contain no certificate")
     @defer.inlineCallbacks
     def test_response_ssl_certificate_empty_response(self):
         crawler = get_crawler(SingleRequestSpider)
         url = self.mockserver.url("/status?n=200", is_secure=True)
         yield crawler.crawl(seed=url, mockserver=self.mockserver)
-        cert = crawler.spider.meta['responses'][0].certificate
+        cert = crawler.spider.meta["responses"][0].certificate
         self.assertIsInstance(cert, Certificate)
         self.assertEqual(cert.getSubject().commonName, b"localhost")
         self.assertEqual(cert.getIssuer().commonName, b"localhost")
 
     @defer.inlineCallbacks
     def test_dns_server_ip_address_none(self):
         crawler = get_crawler(SingleRequestSpider)
-        url = self.mockserver.url('/status?n=200')
+        url = self.mockserver.url("/status?n=200")
         yield crawler.crawl(seed=url, mockserver=self.mockserver)
-        ip_address = crawler.spider.meta['responses'][0].ip_address
+        ip_address = crawler.spider.meta["responses"][0].ip_address
         self.assertIsNone(ip_address)
 
     @defer.inlineCallbacks
     def test_dns_server_ip_address(self):
         crawler = get_crawler(SingleRequestSpider)
-        url = self.mockserver.url('/echo?body=test')
-        expected_netloc, _ = urlparse(url).netloc.split(':')
+        url = self.mockserver.url("/echo?body=test")
+        expected_netloc, _ = urlparse(url).netloc.split(":")
         yield crawler.crawl(seed=url, mockserver=self.mockserver)
-        ip_address = crawler.spider.meta['responses'][0].ip_address
+        ip_address = crawler.spider.meta["responses"][0].ip_address
         self.assertIsInstance(ip_address, IPv4Address)
         self.assertEqual(str(ip_address), gethostbyname(expected_netloc))
 
     @defer.inlineCallbacks
     def test_bytes_received_stop_download_callback(self):
         crawler = get_crawler(BytesReceivedCallbackSpider)
         yield crawler.crawl(mockserver=self.mockserver)
         self.assertIsNone(crawler.spider.meta.get("failure"))
         self.assertIsInstance(crawler.spider.meta["response"], Response)
-        self.assertEqual(crawler.spider.meta["response"].body, crawler.spider.meta.get("bytes_received"))
-        self.assertLess(len(crawler.spider.meta["response"].body), crawler.spider.full_response_length)
+        self.assertEqual(
+            crawler.spider.meta["response"].body,
+            crawler.spider.meta.get("bytes_received"),
+        )
+        self.assertLess(
+            len(crawler.spider.meta["response"].body),
+            crawler.spider.full_response_length,
+        )
 
     @defer.inlineCallbacks
     def test_bytes_received_stop_download_errback(self):
         crawler = get_crawler(BytesReceivedErrbackSpider)
         yield crawler.crawl(mockserver=self.mockserver)
         self.assertIsNone(crawler.spider.meta.get("response"))
         self.assertIsInstance(crawler.spider.meta["failure"], Failure)
         self.assertIsInstance(crawler.spider.meta["failure"].value, StopDownload)
         self.assertIsInstance(crawler.spider.meta["failure"].value.response, Response)
         self.assertEqual(
             crawler.spider.meta["failure"].value.response.body,
-            crawler.spider.meta.get("bytes_received"))
+            crawler.spider.meta.get("bytes_received"),
+        )
         self.assertLess(
             len(crawler.spider.meta["failure"].value.response.body),
-            crawler.spider.full_response_length)
+            crawler.spider.full_response_length,
+        )
 
     @defer.inlineCallbacks
     def test_headers_received_stop_download_callback(self):
         crawler = get_crawler(HeadersReceivedCallbackSpider)
         yield crawler.crawl(mockserver=self.mockserver)
         self.assertIsNone(crawler.spider.meta.get("failure"))
         self.assertIsInstance(crawler.spider.meta["response"], Response)
-        self.assertEqual(crawler.spider.meta["response"].headers, crawler.spider.meta.get("headers_received"))
+        self.assertEqual(
+            crawler.spider.meta["response"].headers,
+            crawler.spider.meta.get("headers_received"),
+        )
 
     @defer.inlineCallbacks
     def test_headers_received_stop_download_errback(self):
         crawler = get_crawler(HeadersReceivedErrbackSpider)
         yield crawler.crawl(mockserver=self.mockserver)
         self.assertIsNone(crawler.spider.meta.get("response"))
         self.assertIsInstance(crawler.spider.meta["failure"], Failure)
         self.assertIsInstance(crawler.spider.meta["failure"].value, StopDownload)
         self.assertIsInstance(crawler.spider.meta["failure"].value.response, Response)
         self.assertEqual(
             crawler.spider.meta["failure"].value.response.headers,
-            crawler.spider.meta.get("headers_received"))
+            crawler.spider.meta.get("headers_received"),
+        )
```

### Comparing `Scrapy-2.7.1/tests/test_crawler.py` & `Scrapy-2.8.0/tests/test_crawler.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,245 +1,233 @@
 import logging
-import os
 import platform
 import subprocess
 import sys
 import warnings
+from pathlib import Path
 
-from pytest import raises, mark
+from pkg_resources import parse_version
+from pytest import mark, raises
 from twisted import version as twisted_version
 from twisted.internet import defer
 from twisted.python.versions import Version
 from twisted.trial import unittest
+from w3lib import __version__ as w3lib_version
 
 import scrapy
-from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess
+from scrapy.crawler import Crawler, CrawlerProcess, CrawlerRunner
 from scrapy.exceptions import ScrapyDeprecationWarning
+from scrapy.extensions import telnet
+from scrapy.extensions.throttle import AutoThrottle
 from scrapy.settings import Settings, default_settings
 from scrapy.spiderloader import SpiderLoader
 from scrapy.utils.log import configure_logging, get_scrapy_root_handler
-from scrapy.utils.spider import DefaultSpider
 from scrapy.utils.misc import load_object
-from scrapy.utils.test import get_crawler
-from scrapy.extensions.throttle import AutoThrottle
-from scrapy.extensions import telnet
-from scrapy.utils.test import get_testenv
-from pkg_resources import parse_version
-from w3lib import __version__ as w3lib_version
-
+from scrapy.utils.spider import DefaultSpider
+from scrapy.utils.test import get_crawler, get_testenv
 from tests.mockserver import MockServer
 
 
 class BaseCrawlerTest(unittest.TestCase):
-
     def assertOptionIsDefault(self, settings, key):
         self.assertIsInstance(settings, Settings)
         self.assertEqual(settings[key], getattr(default_settings, key))
 
 
 class CrawlerTestCase(BaseCrawlerTest):
-
     def test_populate_spidercls_settings(self):
-        spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}
-        project_settings = {'TEST1': 'project', 'TEST3': 'project'}
+        spider_settings = {"TEST1": "spider", "TEST2": "spider"}
+        project_settings = {"TEST1": "project", "TEST3": "project"}
 
         class CustomSettingsSpider(DefaultSpider):
             custom_settings = spider_settings
 
         settings = Settings()
-        settings.setdict(project_settings, priority='project')
+        settings.setdict(project_settings, priority="project")
         with warnings.catch_warnings():
             warnings.simplefilter("ignore", ScrapyDeprecationWarning)
             crawler = Crawler(CustomSettingsSpider, settings)
 
-        self.assertEqual(crawler.settings.get('TEST1'), 'spider')
-        self.assertEqual(crawler.settings.get('TEST2'), 'spider')
-        self.assertEqual(crawler.settings.get('TEST3'), 'project')
+        self.assertEqual(crawler.settings.get("TEST1"), "spider")
+        self.assertEqual(crawler.settings.get("TEST2"), "spider")
+        self.assertEqual(crawler.settings.get("TEST3"), "project")
 
         self.assertFalse(settings.frozen)
         self.assertTrue(crawler.settings.frozen)
 
     def test_crawler_accepts_dict(self):
-        crawler = get_crawler(DefaultSpider, {'foo': 'bar'})
-        self.assertEqual(crawler.settings['foo'], 'bar')
-        self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')
+        crawler = get_crawler(DefaultSpider, {"foo": "bar"})
+        self.assertEqual(crawler.settings["foo"], "bar")
+        self.assertOptionIsDefault(crawler.settings, "RETRY_ENABLED")
 
     def test_crawler_accepts_None(self):
         with warnings.catch_warnings():
             warnings.simplefilter("ignore", ScrapyDeprecationWarning)
             crawler = Crawler(DefaultSpider)
-        self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')
+        self.assertOptionIsDefault(crawler.settings, "RETRY_ENABLED")
 
     def test_crawler_rejects_spider_objects(self):
         with raises(ValueError):
             Crawler(DefaultSpider())
 
 
 class SpiderSettingsTestCase(unittest.TestCase):
     def test_spider_custom_settings(self):
         class MySpider(scrapy.Spider):
-            name = 'spider'
-            custom_settings = {
-                'AUTOTHROTTLE_ENABLED': True
-            }
+            name = "spider"
+            custom_settings = {"AUTOTHROTTLE_ENABLED": True}
 
         crawler = get_crawler(MySpider)
         enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]
         self.assertIn(AutoThrottle, enabled_exts)
 
 
 class CrawlerLoggingTestCase(unittest.TestCase):
     def test_no_root_handler_installed(self):
         handler = get_scrapy_root_handler()
         if handler is not None:
             logging.root.removeHandler(handler)
 
         class MySpider(scrapy.Spider):
-            name = 'spider'
+            name = "spider"
 
         get_crawler(MySpider)
         assert get_scrapy_root_handler() is None
 
     def test_spider_custom_settings_log_level(self):
-        log_file = self.mktemp()
-        with open(log_file, 'wb') as fo:
-            fo.write('previous message\n'.encode('utf-8'))
+        log_file = Path(self.mktemp())
+        log_file.write_text("previous message\n", encoding="utf-8")
 
         class MySpider(scrapy.Spider):
-            name = 'spider'
+            name = "spider"
             custom_settings = {
-                'LOG_LEVEL': 'INFO',
-                'LOG_FILE': log_file,
+                "LOG_LEVEL": "INFO",
+                "LOG_FILE": str(log_file),
                 # settings to avoid extra warnings
-                'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
-                'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,
+                "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+                "TELNETCONSOLE_ENABLED": telnet.TWISTED_CONCH_AVAILABLE,
             }
 
         configure_logging()
         self.assertEqual(get_scrapy_root_handler().level, logging.DEBUG)
         crawler = get_crawler(MySpider)
         self.assertEqual(get_scrapy_root_handler().level, logging.INFO)
-        info_count = crawler.stats.get_value('log_count/INFO')
-        logging.debug('debug message')
-        logging.info('info message')
-        logging.warning('warning message')
-        logging.error('error message')
-
-        with open(log_file, 'rb') as fo:
-            logged = fo.read().decode('utf-8')
-
-        self.assertIn('previous message', logged)
-        self.assertNotIn('debug message', logged)
-        self.assertIn('info message', logged)
-        self.assertIn('warning message', logged)
-        self.assertIn('error message', logged)
-        self.assertEqual(crawler.stats.get_value('log_count/ERROR'), 1)
-        self.assertEqual(crawler.stats.get_value('log_count/WARNING'), 1)
-        self.assertEqual(
-            crawler.stats.get_value('log_count/INFO') - info_count, 1)
-        self.assertEqual(crawler.stats.get_value('log_count/DEBUG', 0), 0)
+        info_count = crawler.stats.get_value("log_count/INFO")
+        logging.debug("debug message")
+        logging.info("info message")
+        logging.warning("warning message")
+        logging.error("error message")
+
+        logged = log_file.read_text(encoding="utf-8")
+
+        self.assertIn("previous message", logged)
+        self.assertNotIn("debug message", logged)
+        self.assertIn("info message", logged)
+        self.assertIn("warning message", logged)
+        self.assertIn("error message", logged)
+        self.assertEqual(crawler.stats.get_value("log_count/ERROR"), 1)
+        self.assertEqual(crawler.stats.get_value("log_count/WARNING"), 1)
+        self.assertEqual(crawler.stats.get_value("log_count/INFO") - info_count, 1)
+        self.assertEqual(crawler.stats.get_value("log_count/DEBUG", 0), 0)
 
     def test_spider_custom_settings_log_append(self):
-        log_file = self.mktemp()
-        with open(log_file, 'wb') as fo:
-            fo.write('previous message\n'.encode('utf-8'))
+        log_file = Path(self.mktemp())
+        log_file.write_text("previous message\n", encoding="utf-8")
 
         class MySpider(scrapy.Spider):
-            name = 'spider'
+            name = "spider"
             custom_settings = {
-                'LOG_FILE': log_file,
-                'LOG_FILE_APPEND': False,
+                "LOG_FILE": str(log_file),
+                "LOG_FILE_APPEND": False,
                 # disable telnet if not available to avoid an extra warning
-                'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,
+                "TELNETCONSOLE_ENABLED": telnet.TWISTED_CONCH_AVAILABLE,
             }
 
         configure_logging()
         get_crawler(MySpider)
-        logging.debug('debug message')
+        logging.debug("debug message")
 
-        with open(log_file, 'rb') as fo:
-            logged = fo.read().decode('utf-8')
+        logged = log_file.read_text(encoding="utf-8")
 
-        self.assertNotIn('previous message', logged)
-        self.assertIn('debug message', logged)
+        self.assertNotIn("previous message", logged)
+        self.assertIn("debug message", logged)
 
 
 class SpiderLoaderWithWrongInterface:
-
     def unneeded_method(self):
         pass
 
 
 class CustomSpiderLoader(SpiderLoader):
     pass
 
 
 class CrawlerRunnerTestCase(BaseCrawlerTest):
-
     def test_spider_manager_verify_interface(self):
-        settings = Settings({
-            'SPIDER_LOADER_CLASS': SpiderLoaderWithWrongInterface,
-        })
+        settings = Settings(
+            {
+                "SPIDER_LOADER_CLASS": SpiderLoaderWithWrongInterface,
+            }
+        )
         with warnings.catch_warnings(record=True) as w:
             self.assertRaises(AttributeError, CrawlerRunner, settings)
             self.assertEqual(len(w), 1)
             self.assertIn("SPIDER_LOADER_CLASS", str(w[0].message))
             self.assertIn("scrapy.interfaces.ISpiderLoader", str(w[0].message))
 
     def test_crawler_runner_accepts_dict(self):
-        runner = CrawlerRunner({'foo': 'bar'})
-        self.assertEqual(runner.settings['foo'], 'bar')
-        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')
+        runner = CrawlerRunner({"foo": "bar"})
+        self.assertEqual(runner.settings["foo"], "bar")
+        self.assertOptionIsDefault(runner.settings, "RETRY_ENABLED")
 
     def test_crawler_runner_accepts_None(self):
         runner = CrawlerRunner()
-        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')
+        self.assertOptionIsDefault(runner.settings, "RETRY_ENABLED")
 
     def test_deprecated_attribute_spiders(self):
         with warnings.catch_warnings(record=True) as w:
             runner = CrawlerRunner(Settings())
             spiders = runner.spiders
             self.assertEqual(len(w), 1)
             self.assertIn("CrawlerRunner.spiders", str(w[0].message))
             self.assertIn("CrawlerRunner.spider_loader", str(w[0].message))
-            sl_cls = load_object(runner.settings['SPIDER_LOADER_CLASS'])
+            sl_cls = load_object(runner.settings["SPIDER_LOADER_CLASS"])
             self.assertIsInstance(spiders, sl_cls)
 
 
 class CrawlerProcessTest(BaseCrawlerTest):
     def test_crawler_process_accepts_dict(self):
-        runner = CrawlerProcess({'foo': 'bar'})
-        self.assertEqual(runner.settings['foo'], 'bar')
-        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')
+        runner = CrawlerProcess({"foo": "bar"})
+        self.assertEqual(runner.settings["foo"], "bar")
+        self.assertOptionIsDefault(runner.settings, "RETRY_ENABLED")
 
     def test_crawler_process_accepts_None(self):
         runner = CrawlerProcess()
-        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')
+        self.assertOptionIsDefault(runner.settings, "RETRY_ENABLED")
 
 
 class ExceptionSpider(scrapy.Spider):
-    name = 'exception'
+    name = "exception"
 
     @classmethod
     def from_crawler(cls, crawler, *args, **kwargs):
-        raise ValueError('Exception in from_crawler method')
+        raise ValueError("Exception in from_crawler method")
 
 
 class NoRequestsSpider(scrapy.Spider):
-    name = 'no_request'
+    name = "no_request"
 
     def start_requests(self):
         return []
 
 
-@mark.usefixtures('reactor_pytest')
+@mark.usefixtures("reactor_pytest")
 class CrawlerRunnerHasSpider(unittest.TestCase):
-
     def _runner(self):
-        return CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'})
+        return CrawlerRunner({"REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7"})
 
     @defer.inlineCallbacks
     def test_crawler_runner_bootstrap_successful(self):
         runner = self._runner()
         yield runner.crawl(NoRequestsSpider)
         self.assertEqual(runner.bootstrap_failed, False)
 
@@ -255,144 +243,167 @@
         runner = self._runner()
 
         try:
             yield runner.crawl(ExceptionSpider)
         except ValueError:
             pass
         else:
-            self.fail('Exception should be raised from spider')
+            self.fail("Exception should be raised from spider")
 
         self.assertEqual(runner.bootstrap_failed, True)
 
     @defer.inlineCallbacks
     def test_crawler_runner_bootstrap_failed_for_several(self):
         runner = self._runner()
 
         try:
             yield runner.crawl(ExceptionSpider)
         except ValueError:
             pass
         else:
-            self.fail('Exception should be raised from spider')
+            self.fail("Exception should be raised from spider")
 
         yield runner.crawl(NoRequestsSpider)
 
         self.assertEqual(runner.bootstrap_failed, True)
 
     @defer.inlineCallbacks
     def test_crawler_runner_asyncio_enabled_true(self):
-        if self.reactor_pytest == 'asyncio':
-            CrawlerRunner(settings={
-                "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
-                "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
-            })
+        if self.reactor_pytest == "asyncio":
+            CrawlerRunner(
+                settings={
+                    "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
+                    "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+                }
+            )
         else:
             msg = r"The installed reactor \(.*?\) does not match the requested one \(.*?\)"
             with self.assertRaisesRegex(Exception, msg):
-                runner = CrawlerRunner(settings={
-                    "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
-                    "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
-                })
+                runner = CrawlerRunner(
+                    settings={
+                        "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
+                        "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+                    }
+                )
                 yield runner.crawl(NoRequestsSpider)
 
 
 class ScriptRunnerMixin:
-    def run_script(self, script_name, *script_args):
-        script_path = os.path.join(self.script_dir, script_name)
-        args = [sys.executable, script_path] + list(script_args)
-        p = subprocess.Popen(args, env=get_testenv(),
-                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+    script_dir: Path
+
+    def run_script(self, script_name: str, *script_args):
+        script_path = self.script_dir / script_name
+        args = [sys.executable, str(script_path)] + list(script_args)
+        p = subprocess.Popen(
+            args, env=get_testenv(), stdout=subprocess.PIPE, stderr=subprocess.PIPE
+        )
         stdout, stderr = p.communicate()
-        return stderr.decode('utf-8')
+        return stderr.decode("utf-8")
 
 
 class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):
-    script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerProcess')
+    script_dir = Path(__file__).parent.resolve() / "CrawlerProcess"
 
     def test_simple(self):
-        log = self.run_script('simple.py')
-        self.assertIn('Spider closed (finished)', log)
-        self.assertNotIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        log = self.run_script("simple.py")
+        self.assertIn("Spider closed (finished)", log)
+        self.assertNotIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
 
     def test_multi(self):
-        log = self.run_script('multi.py')
-        self.assertIn('Spider closed (finished)', log)
-        self.assertNotIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        log = self.run_script("multi.py")
+        self.assertIn("Spider closed (finished)", log)
+        self.assertNotIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
         self.assertNotIn("ReactorAlreadyInstalledError", log)
 
     def test_reactor_default(self):
-        log = self.run_script('reactor_default.py')
-        self.assertIn('Spider closed (finished)', log)
-        self.assertNotIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        log = self.run_script("reactor_default.py")
+        self.assertIn("Spider closed (finished)", log)
+        self.assertNotIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
         self.assertNotIn("ReactorAlreadyInstalledError", log)
 
     def test_reactor_default_twisted_reactor_select(self):
-        log = self.run_script('reactor_default_twisted_reactor_select.py')
-        if platform.system() in ['Windows', 'Darwin']:
+        log = self.run_script("reactor_default_twisted_reactor_select.py")
+        if platform.system() in ["Windows", "Darwin"]:
             # The goal of this test function is to test that, when a reactor is
             # installed (the default one here) and a different reactor is
             # configured (select here), an error raises.
             #
             # In Windows the default reactor is the select reactor, so that
             # error does not raise.
             #
             # If that ever becomes the case on more platforms (i.e. if Linux
             # also starts using the select reactor by default in a future
             # version of Twisted), then we will need to rethink this test.
-            self.assertIn('Spider closed (finished)', log)
+            self.assertIn("Spider closed (finished)", log)
         else:
-            self.assertNotIn('Spider closed (finished)', log)
+            self.assertNotIn("Spider closed (finished)", log)
             self.assertIn(
                 (
                     "does not match the requested one "
                     "(twisted.internet.selectreactor.SelectReactor)"
                 ),
                 log,
             )
 
     def test_reactor_select(self):
-        log = self.run_script('reactor_select.py')
-        self.assertIn('Spider closed (finished)', log)
+        log = self.run_script("reactor_select.py")
+        self.assertIn("Spider closed (finished)", log)
         self.assertNotIn("ReactorAlreadyInstalledError", log)
 
     def test_reactor_select_twisted_reactor_select(self):
-        log = self.run_script('reactor_select_twisted_reactor_select.py')
-        self.assertIn('Spider closed (finished)', log)
+        log = self.run_script("reactor_select_twisted_reactor_select.py")
+        self.assertIn("Spider closed (finished)", log)
         self.assertNotIn("ReactorAlreadyInstalledError", log)
 
     def test_reactor_select_subclass_twisted_reactor_select(self):
-        log = self.run_script('reactor_select_subclass_twisted_reactor_select.py')
-        self.assertNotIn('Spider closed (finished)', log)
+        log = self.run_script("reactor_select_subclass_twisted_reactor_select.py")
+        self.assertNotIn("Spider closed (finished)", log)
         self.assertIn(
             (
                 "does not match the requested one "
                 "(twisted.internet.selectreactor.SelectReactor)"
             ),
             log,
         )
 
     def test_asyncio_enabled_no_reactor(self):
-        log = self.run_script('asyncio_enabled_no_reactor.py')
-        self.assertIn('Spider closed (finished)', log)
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        log = self.run_script("asyncio_enabled_no_reactor.py")
+        self.assertIn("Spider closed (finished)", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
 
     def test_asyncio_enabled_reactor(self):
-        log = self.run_script('asyncio_enabled_reactor.py')
-        self.assertIn('Spider closed (finished)', log)
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        log = self.run_script("asyncio_enabled_reactor.py")
+        self.assertIn("Spider closed (finished)", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
 
-    @mark.skipif(parse_version(w3lib_version) >= parse_version("2.0.0"),
-                 reason='w3lib 2.0.0 and later do not allow invalid domains.')
+    @mark.skipif(
+        parse_version(w3lib_version) >= parse_version("2.0.0"),
+        reason="w3lib 2.0.0 and later do not allow invalid domains.",
+    )
     def test_ipv6_default_name_resolver(self):
-        log = self.run_script('default_name_resolver.py')
-        self.assertIn('Spider closed (finished)', log)
-        self.assertIn("'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,", log)
+        log = self.run_script("default_name_resolver.py")
+        self.assertIn("Spider closed (finished)", log)
+        self.assertIn(
+            "'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,",
+            log,
+        )
         self.assertIn(
             "twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.",
-            log)
+            log,
+        )
 
     def test_caching_hostname_resolver_ipv6(self):
         log = self.run_script("caching_hostname_resolver_ipv6.py")
         self.assertIn("Spider closed (finished)", log)
         self.assertNotIn("twisted.internet.error.DNSLookupError", log)
 
     def test_caching_hostname_resolver_finite_execution(self):
@@ -403,94 +414,149 @@
             self.assertNotIn("ERROR: Error downloading", log)
             self.assertNotIn("TimeoutError", log)
             self.assertNotIn("twisted.internet.error.DNSLookupError", log)
 
     def test_twisted_reactor_select(self):
         log = self.run_script("twisted_reactor_select.py")
         self.assertIn("Spider closed (finished)", log)
-        self.assertIn("Using reactor: twisted.internet.selectreactor.SelectReactor", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.selectreactor.SelectReactor", log
+        )
 
-    @mark.skipif(platform.system() == 'Windows', reason="PollReactor is not supported on Windows")
+    @mark.skipif(
+        platform.system() == "Windows", reason="PollReactor is not supported on Windows"
+    )
     def test_twisted_reactor_poll(self):
         log = self.run_script("twisted_reactor_poll.py")
         self.assertIn("Spider closed (finished)", log)
         self.assertIn("Using reactor: twisted.internet.pollreactor.PollReactor", log)
 
     def test_twisted_reactor_asyncio(self):
         log = self.run_script("twisted_reactor_asyncio.py")
         self.assertIn("Spider closed (finished)", log)
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
 
     def test_twisted_reactor_asyncio_custom_settings(self):
         log = self.run_script("twisted_reactor_custom_settings.py")
         self.assertIn("Spider closed (finished)", log)
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
 
     def test_twisted_reactor_asyncio_custom_settings_same(self):
         log = self.run_script("twisted_reactor_custom_settings_same.py")
         self.assertIn("Spider closed (finished)", log)
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
 
     def test_twisted_reactor_asyncio_custom_settings_conflict(self):
         log = self.run_script("twisted_reactor_custom_settings_conflict.py")
-        self.assertIn("Using reactor: twisted.internet.selectreactor.SelectReactor", log)
-        self.assertIn("(twisted.internet.selectreactor.SelectReactor) does not match the requested one", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.selectreactor.SelectReactor", log
+        )
+        self.assertIn(
+            "(twisted.internet.selectreactor.SelectReactor) does not match the requested one",
+            log,
+        )
 
-    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')
-    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')
-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')
+    @mark.skipif(
+        sys.implementation.name == "pypy",
+        reason="uvloop does not support pypy properly",
+    )
+    @mark.skipif(
+        platform.system() == "Windows", reason="uvloop does not support Windows"
+    )
+    @mark.skipif(
+        twisted_version == Version("twisted", 21, 2, 0),
+        reason="https://twistedmatrix.com/trac/ticket/10106",
+    )
     def test_custom_loop_asyncio(self):
         log = self.run_script("asyncio_custom_loop.py")
         self.assertIn("Spider closed (finished)", log)
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
         self.assertIn("Using asyncio event loop: uvloop.Loop", log)
 
-    @mark.skipif(sys.implementation.name == "pypy", reason="uvloop does not support pypy properly")
-    @mark.skipif(platform.system() == "Windows", reason="uvloop does not support Windows")
-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')
+    @mark.skipif(
+        sys.implementation.name == "pypy",
+        reason="uvloop does not support pypy properly",
+    )
+    @mark.skipif(
+        platform.system() == "Windows", reason="uvloop does not support Windows"
+    )
+    @mark.skipif(
+        twisted_version == Version("twisted", 21, 2, 0),
+        reason="https://twistedmatrix.com/trac/ticket/10106",
+    )
     def test_custom_loop_asyncio_deferred_signal(self):
         log = self.run_script("asyncio_deferred_signal.py", "uvloop.Loop")
         self.assertIn("Spider closed (finished)", log)
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
         self.assertIn("Using asyncio event loop: uvloop.Loop", log)
         self.assertIn("async pipeline opened!", log)
 
-    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')
-    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')
-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')
+    @mark.skipif(
+        sys.implementation.name == "pypy",
+        reason="uvloop does not support pypy properly",
+    )
+    @mark.skipif(
+        platform.system() == "Windows", reason="uvloop does not support Windows"
+    )
+    @mark.skipif(
+        twisted_version == Version("twisted", 21, 2, 0),
+        reason="https://twistedmatrix.com/trac/ticket/10106",
+    )
     def test_asyncio_enabled_reactor_same_loop(self):
         log = self.run_script("asyncio_enabled_reactor_same_loop.py")
         self.assertIn("Spider closed (finished)", log)
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
         self.assertIn("Using asyncio event loop: uvloop.Loop", log)
 
-    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')
-    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')
-    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')
+    @mark.skipif(
+        sys.implementation.name == "pypy",
+        reason="uvloop does not support pypy properly",
+    )
+    @mark.skipif(
+        platform.system() == "Windows", reason="uvloop does not support Windows"
+    )
+    @mark.skipif(
+        twisted_version == Version("twisted", 21, 2, 0),
+        reason="https://twistedmatrix.com/trac/ticket/10106",
+    )
     def test_asyncio_enabled_reactor_different_loop(self):
         log = self.run_script("asyncio_enabled_reactor_different_loop.py")
         self.assertNotIn("Spider closed (finished)", log)
         self.assertIn(
             (
                 "does not match the one specified in the ASYNCIO_EVENT_LOOP "
                 "setting (uvloop.Loop)"
             ),
             log,
         )
 
     def test_default_loop_asyncio_deferred_signal(self):
         log = self.run_script("asyncio_deferred_signal.py")
         self.assertIn("Spider closed (finished)", log)
-        self.assertIn("Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log)
+        self.assertIn(
+            "Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor", log
+        )
         self.assertNotIn("Using asyncio event loop: uvloop.Loop", log)
         self.assertIn("async pipeline opened!", log)
 
 
 class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):
-    script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerRunner')
+    script_dir = Path(__file__).parent.resolve() / "CrawlerRunner"
 
     def test_response_ip_address(self):
         log = self.run_script("ip_address.py")
         self.assertIn("INFO: Spider closed (finished)", log)
         self.assertIn("INFO: Host: not.a.real.domain", log)
         self.assertIn("INFO: Type: <class 'ipaddress.IPv4Address'>", log)
         self.assertIn("INFO: IP address: 127.0.0.1", log)
```

### Comparing `Scrapy-2.7.1/tests/test_dependencies.py` & `Scrapy-2.8.0/tests/test_dependencies.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,50 +1,43 @@
 import os
 import re
 from configparser import ConfigParser
 from importlib import import_module
+from pathlib import Path
 
 from twisted import version as twisted_version
 from twisted.trial import unittest
 
 
 class ScrapyUtilsTest(unittest.TestCase):
-
     def test_required_openssl_version(self):
         try:
-            module = import_module('OpenSSL')
+            module = import_module("OpenSSL")
         except ImportError:
             raise unittest.SkipTest("OpenSSL is not available")
 
-        if hasattr(module, '__version__'):
-            installed_version = [int(x) for x in module.__version__.split('.')[:2]]
+        if hasattr(module, "__version__"):
+            installed_version = [int(x) for x in module.__version__.split(".")[:2]]
             assert installed_version >= [0, 6], "OpenSSL >= 0.6 required"
 
     def test_pinned_twisted_version(self):
         """When running tests within a Tox environment with pinned
         dependencies, make sure that the version of Twisted is the pinned
         version.
 
         See https://github.com/scrapy/scrapy/pull/4814#issuecomment-706230011
         """
-        if not os.environ.get('_SCRAPY_PINNED', None):
-            self.skipTest('Not in a pinned environment')
+        if not os.environ.get("_SCRAPY_PINNED", None):
+            self.skipTest("Not in a pinned environment")
 
-        tox_config_file_path = os.path.join(
-            os.path.dirname(__file__),
-            '..',
-            'tox.ini',
-        )
+        tox_config_file_path = Path(__file__).parent / ".." / "tox.ini"
         config_parser = ConfigParser()
         config_parser.read(tox_config_file_path)
-        pattern = r'Twisted\[http2\]==([\d.]+)'
-        match = re.search(pattern, config_parser['pinned']['deps'])
+        pattern = r"Twisted\[http2\]==([\d.]+)"
+        match = re.search(pattern, config_parser["pinned"]["deps"])
         pinned_twisted_version_string = match[1]
 
-        self.assertEqual(
-            twisted_version.short(),
-            pinned_twisted_version_string
-        )
+        self.assertEqual(twisted_version.short(), pinned_twisted_version_string)
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_downloader_handlers.py` & `Scrapy-2.8.0/tests/test_downloader_handlers.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 import contextlib
 import os
 import shutil
 import sys
 import tempfile
+from pathlib import Path
 from typing import Optional, Type
-from unittest import mock, SkipTest
+from unittest import SkipTest, mock
 
 from testfixtures import LogCapture
 from twisted.cred import checkers, credentials, portal
 from twisted.internet import defer, error, reactor
 from twisted.protocols.policies import WrappingFactory
-from twisted.python.filepath import FilePath
 from twisted.trial import unittest
 from twisted.web import resource, server, static, util
 from twisted.web._newclient import ResponseFailed
 from twisted.web.http import _DataLoss
 from w3lib.url import path_to_file_uri
 
 from scrapy.core.downloader.handlers import DownloadHandlers
 from scrapy.core.downloader.handlers.datauri import DataURIDownloadHandler
 from scrapy.core.downloader.handlers.file import FileDownloadHandler
 from scrapy.core.downloader.handlers.http import HTTPDownloadHandler
 from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler
 from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler
 from scrapy.core.downloader.handlers.s3 import S3DownloadHandler
-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning
+from scrapy.exceptions import NotConfigured
 from scrapy.http import Headers, HtmlResponse, Request
 from scrapy.http.response.text import TextResponse
 from scrapy.responsetypes import responsetypes
 from scrapy.spiders import Spider
 from scrapy.utils.misc import create_instance
 from scrapy.utils.python import to_bytes
 from scrapy.utils.test import get_crawler, skip_if_no_boto
@@ -62,134 +62,129 @@
 
     @classmethod
     def from_crawler(cls, crawler):
         return cls(crawler)
 
 
 class LoadTestCase(unittest.TestCase):
-
     def test_enabled_handler(self):
-        handlers = {'scheme': DummyDH}
-        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})
+        handlers = {"scheme": DummyDH}
+        crawler = get_crawler(settings_dict={"DOWNLOAD_HANDLERS": handlers})
         dh = DownloadHandlers(crawler)
-        self.assertIn('scheme', dh._schemes)
-        self.assertIn('scheme', dh._handlers)
-        self.assertNotIn('scheme', dh._notconfigured)
+        self.assertIn("scheme", dh._schemes)
+        self.assertIn("scheme", dh._handlers)
+        self.assertNotIn("scheme", dh._notconfigured)
 
     def test_not_configured_handler(self):
-        handlers = {'scheme': OffDH}
-        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})
+        handlers = {"scheme": OffDH}
+        crawler = get_crawler(settings_dict={"DOWNLOAD_HANDLERS": handlers})
         dh = DownloadHandlers(crawler)
-        self.assertIn('scheme', dh._schemes)
-        self.assertNotIn('scheme', dh._handlers)
-        self.assertIn('scheme', dh._notconfigured)
+        self.assertIn("scheme", dh._schemes)
+        self.assertNotIn("scheme", dh._handlers)
+        self.assertIn("scheme", dh._notconfigured)
 
     def test_disabled_handler(self):
-        handlers = {'scheme': None}
-        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})
+        handlers = {"scheme": None}
+        crawler = get_crawler(settings_dict={"DOWNLOAD_HANDLERS": handlers})
         dh = DownloadHandlers(crawler)
-        self.assertNotIn('scheme', dh._schemes)
+        self.assertNotIn("scheme", dh._schemes)
         for scheme in handlers:  # force load handlers
             dh._get_handler(scheme)
-        self.assertNotIn('scheme', dh._handlers)
-        self.assertIn('scheme', dh._notconfigured)
+        self.assertNotIn("scheme", dh._handlers)
+        self.assertIn("scheme", dh._notconfigured)
 
     def test_lazy_handlers(self):
-        handlers = {'scheme': DummyLazyDH}
-        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})
+        handlers = {"scheme": DummyLazyDH}
+        crawler = get_crawler(settings_dict={"DOWNLOAD_HANDLERS": handlers})
         dh = DownloadHandlers(crawler)
-        self.assertIn('scheme', dh._schemes)
-        self.assertNotIn('scheme', dh._handlers)
+        self.assertIn("scheme", dh._schemes)
+        self.assertNotIn("scheme", dh._handlers)
         for scheme in handlers:  # force load lazy handler
             dh._get_handler(scheme)
-        self.assertIn('scheme', dh._handlers)
-        self.assertNotIn('scheme', dh._notconfigured)
+        self.assertIn("scheme", dh._handlers)
+        self.assertNotIn("scheme", dh._notconfigured)
 
 
 class FileTestCase(unittest.TestCase):
-
     def setUp(self):
-        self.tmpname = self.mktemp()
-        with open(self.tmpname + '^', 'w') as f:
-            f.write('0123456789')
+        # add a special char to check that they are handled correctly
+        self.tmpname = Path(self.mktemp() + "^")
+        Path(self.tmpname).write_text("0123456789", encoding="utf-8")
         handler = create_instance(FileDownloadHandler, None, get_crawler())
         self.download_request = handler.download_request
 
     def tearDown(self):
-        os.unlink(self.tmpname + '^')
+        self.tmpname.unlink()
 
     def test_download(self):
         def _test(response):
             self.assertEqual(response.url, request.url)
             self.assertEqual(response.status, 200)
-            self.assertEqual(response.body, b'0123456789')
+            self.assertEqual(response.body, b"0123456789")
             self.assertEqual(response.protocol, None)
 
-        request = Request(path_to_file_uri(self.tmpname + '^'))
-        assert request.url.upper().endswith('%5E')
-        return self.download_request(request, Spider('foo')).addCallback(_test)
+        request = Request(path_to_file_uri(str(self.tmpname)))
+        assert request.url.upper().endswith("%5E")
+        return self.download_request(request, Spider("foo")).addCallback(_test)
 
     def test_non_existent(self):
-        request = Request(f'file://{self.mktemp()}')
-        d = self.download_request(request, Spider('foo'))
+        request = Request(f"file://{self.mktemp()}")
+        d = self.download_request(request, Spider("foo"))
         return self.assertFailure(d, IOError)
 
 
 class ContentLengthHeaderResource(resource.Resource):
     """
     A testing resource which renders itself as the value of the Content-Length
     header from the request.
     """
 
     def render(self, request):
         return request.requestHeaders.getRawHeaders(b"content-length")[0]
 
 
 class ChunkedResource(resource.Resource):
-
     def render(self, request):
         def response():
             request.write(b"chunked ")
             request.write(b"content\n")
             request.finish()
 
         reactor.callLater(0, response)
         return server.NOT_DONE_YET
 
 
 class BrokenChunkedResource(resource.Resource):
-
     def render(self, request):
         def response():
             request.write(b"chunked ")
             request.write(b"content\n")
             # Disable terminating chunk on finish.
             request.chunked = False
             closeConnection(request)
 
         reactor.callLater(0, response)
         return server.NOT_DONE_YET
 
 
 class BrokenDownloadResource(resource.Resource):
-
     def render(self, request):
         def response():
             request.setHeader(b"Content-Length", b"20")
             request.write(b"partial")
             closeConnection(request)
 
         reactor.callLater(0, response)
         return server.NOT_DONE_YET
 
 
 def closeConnection(request):
     # We have to force a disconnection for HTTP/1.1 clients. Otherwise
     # client keeps the connection open waiting for more data.
-    if hasattr(request.channel, 'loseConnection'):  # twisted >=16.3.0
+    if hasattr(request.channel, "loseConnection"):  # twisted >=16.3.0
         request.channel.loseConnection()
     else:
         request.channel.transport.loseConnection()
     request.finish()
 
 
 class EmptyContentTypeHeaderResource(resource.Resource):
@@ -210,144 +205,155 @@
                 request.write(b"x" * 1024)
             request.finish()
 
         reactor.callLater(0, response)
         return server.NOT_DONE_YET
 
 
+class DuplicateHeaderResource(resource.Resource):
+    def render(self, request):
+        request.responseHeaders.setRawHeaders(b"Set-Cookie", [b"a=b", b"c=d"])
+        return b""
+
+
 class HttpTestCase(unittest.TestCase):
-    scheme = 'http'
+    scheme = "http"
     download_handler_cls: Type = HTTPDownloadHandler
 
     # only used for HTTPS tests
-    keyfile = 'keys/localhost.key'
-    certfile = 'keys/localhost.crt'
+    keyfile = "keys/localhost.key"
+    certfile = "keys/localhost.crt"
 
     def setUp(self):
-        self.tmpname = self.mktemp()
-        os.mkdir(self.tmpname)
-        FilePath(self.tmpname).child("file").setContent(b"0123456789")
-        r = static.File(self.tmpname)
+        self.tmpname = Path(self.mktemp())
+        self.tmpname.mkdir()
+        (self.tmpname / "file").write_bytes(b"0123456789")
+        r = static.File(str(self.tmpname))
         r.putChild(b"redirect", util.Redirect(b"/file"))
         r.putChild(b"wait", ForeverTakingResource())
         r.putChild(b"hang-after-headers", ForeverTakingResource(write=True))
         r.putChild(b"nolength", NoLengthResource())
         r.putChild(b"host", HostHeaderResource())
         r.putChild(b"payload", PayloadResource())
         r.putChild(b"broken", BrokenDownloadResource())
         r.putChild(b"chunked", ChunkedResource())
         r.putChild(b"broken-chunked", BrokenChunkedResource())
         r.putChild(b"contentlength", ContentLengthHeaderResource())
         r.putChild(b"nocontenttype", EmptyContentTypeHeaderResource())
         r.putChild(b"largechunkedfile", LargeChunkedFileResource())
+        r.putChild(b"duplicate-header", DuplicateHeaderResource())
         r.putChild(b"echo", Echo())
         self.site = server.Site(r, timeout=None)
         self.wrapper = WrappingFactory(self.site)
-        self.host = 'localhost'
-        if self.scheme == 'https':
+        self.host = "localhost"
+        if self.scheme == "https":
             # Using WrappingFactory do not enable HTTP/2 failing all the
             # tests with H2DownloadHandler
             self.port = reactor.listenSSL(
-                0, self.site, ssl_context_factory(self.keyfile, self.certfile),
-                interface=self.host)
+                0,
+                self.site,
+                ssl_context_factory(self.keyfile, self.certfile),
+                interface=self.host,
+            )
         else:
             self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)
         self.portno = self.port.getHost().port
-        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())
+        self.download_handler = create_instance(
+            self.download_handler_cls, None, get_crawler()
+        )
         self.download_request = self.download_handler.download_request
 
     @defer.inlineCallbacks
     def tearDown(self):
         yield self.port.stopListening()
-        if hasattr(self.download_handler, 'close'):
+        if hasattr(self.download_handler, "close"):
             yield self.download_handler.close()
         shutil.rmtree(self.tmpname)
 
     def getURL(self, path):
         return f"{self.scheme}://{self.host}:{self.portno}/{path}"
 
     def test_download(self):
-        request = Request(self.getURL('file'))
-        d = self.download_request(request, Spider('foo'))
+        request = Request(self.getURL("file"))
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.body)
         d.addCallback(self.assertEqual, b"0123456789")
         return d
 
     def test_download_head(self):
-        request = Request(self.getURL('file'), method='HEAD')
-        d = self.download_request(request, Spider('foo'))
+        request = Request(self.getURL("file"), method="HEAD")
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.body)
-        d.addCallback(self.assertEqual, b'')
+        d.addCallback(self.assertEqual, b"")
         return d
 
     def test_redirect_status(self):
-        request = Request(self.getURL('redirect'))
-        d = self.download_request(request, Spider('foo'))
+        request = Request(self.getURL("redirect"))
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.status)
         d.addCallback(self.assertEqual, 302)
         return d
 
     def test_redirect_status_head(self):
-        request = Request(self.getURL('redirect'), method='HEAD')
-        d = self.download_request(request, Spider('foo'))
+        request = Request(self.getURL("redirect"), method="HEAD")
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.status)
         d.addCallback(self.assertEqual, 302)
         return d
 
     @defer.inlineCallbacks
     def test_timeout_download_from_spider_nodata_rcvd(self):
         if self.reactor_pytest == "asyncio" and sys.platform == "win32":
             # https://twistedmatrix.com/trac/ticket/10279
             raise unittest.SkipTest(
                 "This test produces DirtyReactorAggregateError on Windows with asyncio"
             )
 
         # client connects but no data is received
-        spider = Spider('foo')
-        meta = {'download_timeout': 0.5}
-        request = Request(self.getURL('wait'), meta=meta)
+        spider = Spider("foo")
+        meta = {"download_timeout": 0.5}
+        request = Request(self.getURL("wait"), meta=meta)
         d = self.download_request(request, spider)
         yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)
 
     @defer.inlineCallbacks
     def test_timeout_download_from_spider_server_hangs(self):
         if self.reactor_pytest == "asyncio" and sys.platform == "win32":
             # https://twistedmatrix.com/trac/ticket/10279
             raise unittest.SkipTest(
                 "This test produces DirtyReactorAggregateError on Windows with asyncio"
             )
         # client connects, server send headers and some body bytes but hangs
-        spider = Spider('foo')
-        meta = {'download_timeout': 0.5}
-        request = Request(self.getURL('hang-after-headers'), meta=meta)
+        spider = Spider("foo")
+        meta = {"download_timeout": 0.5}
+        request = Request(self.getURL("hang-after-headers"), meta=meta)
         d = self.download_request(request, spider)
         yield self.assertFailure(d, defer.TimeoutError, error.TimeoutError)
 
     def test_host_header_not_in_request_headers(self):
         def _test(response):
-            self.assertEqual(
-                response.body, to_bytes(f'{self.host}:{self.portno}'))
+            self.assertEqual(response.body, to_bytes(f"{self.host}:{self.portno}"))
             self.assertEqual(request.headers, {})
 
-        request = Request(self.getURL('host'))
-        return self.download_request(request, Spider('foo')).addCallback(_test)
+        request = Request(self.getURL("host"))
+        return self.download_request(request, Spider("foo")).addCallback(_test)
 
     def test_host_header_seted_in_request_headers(self):
-        host = self.host + ':' + str(self.portno)
+        host = self.host + ":" + str(self.portno)
 
         def _test(response):
             self.assertEqual(response.body, host.encode())
-            self.assertEqual(request.headers.get('Host'), host.encode())
+            self.assertEqual(request.headers.get("Host"), host.encode())
 
-        request = Request(self.getURL('host'), headers={'Host': host})
-        return self.download_request(request, Spider('foo')).addCallback(_test)
+        request = Request(self.getURL("host"), headers={"Host": host})
+        return self.download_request(request, Spider("foo")).addCallback(_test)
 
-        d = self.download_request(request, Spider('foo'))
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.body)
-        d.addCallback(self.assertEqual, b'localhost')
+        d.addCallback(self.assertEqual, b"localhost")
         return d
 
     def test_content_length_zero_bodyless_post_request_headers(self):
         """Tests if "Content-Length: 0" is sent for bodyless POST requests.
 
         This is not strictly required by HTTP RFCs but can cause trouble
         for some web servers.
@@ -355,536 +361,558 @@
         https://github.com/scrapy/scrapy/issues/823
         https://issues.apache.org/jira/browse/TS-2902
         https://github.com/kennethreitz/requests/issues/405
         https://bugs.python.org/issue14721
         """
 
         def _test(response):
-            self.assertEqual(response.body, b'0')
+            self.assertEqual(response.body, b"0")
 
-        request = Request(self.getURL('contentlength'), method='POST')
-        return self.download_request(request, Spider('foo')).addCallback(_test)
+        request = Request(self.getURL("contentlength"), method="POST")
+        return self.download_request(request, Spider("foo")).addCallback(_test)
 
     def test_content_length_zero_bodyless_post_only_one(self):
         def _test(response):
             import json
-            headers = Headers(json.loads(response.text)['headers'])
-            contentlengths = headers.getlist('Content-Length')
+
+            headers = Headers(json.loads(response.text)["headers"])
+            contentlengths = headers.getlist("Content-Length")
             self.assertEqual(len(contentlengths), 1)
             self.assertEqual(contentlengths, [b"0"])
 
-        request = Request(self.getURL('echo'), method='POST')
-        return self.download_request(request, Spider('foo')).addCallback(_test)
+        request = Request(self.getURL("echo"), method="POST")
+        return self.download_request(request, Spider("foo")).addCallback(_test)
 
     def test_payload(self):
-        body = b'1' * 100  # PayloadResource requires body length to be 100
-        request = Request(self.getURL('payload'), method='POST', body=body)
-        d = self.download_request(request, Spider('foo'))
+        body = b"1" * 100  # PayloadResource requires body length to be 100
+        request = Request(self.getURL("payload"), method="POST", body=body)
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.body)
         d.addCallback(self.assertEqual, body)
         return d
 
     def test_response_header_content_length(self):
         request = Request(self.getURL("file"), method=b"GET")
         d = self.download_request(request, Spider("foo"))
-        d.addCallback(lambda r: r.headers[b'content-length'])
-        d.addCallback(self.assertEqual, b'159')
+        d.addCallback(lambda r: r.headers[b"content-length"])
+        d.addCallback(self.assertEqual, b"159")
         return d
 
     def _test_response_class(self, filename, body, response_class):
         def _test(response):
             self.assertEqual(type(response), response_class)
 
         request = Request(self.getURL(filename), body=body)
-        return self.download_request(request, Spider('foo')).addCallback(_test)
+        return self.download_request(request, Spider("foo")).addCallback(_test)
 
     def test_response_class_from_url(self):
-        return self._test_response_class('foo.html', b'', HtmlResponse)
+        return self._test_response_class("foo.html", b"", HtmlResponse)
 
     def test_response_class_from_body(self):
         return self._test_response_class(
-            'foo',
+            "foo",
             b"<!DOCTYPE html>\n<title>.</title>",
             HtmlResponse,
         )
 
+    def test_get_duplicate_header(self):
+        def _test(response):
+            self.assertEqual(
+                response.headers.getlist(b"Set-Cookie"),
+                [b"a=b", b"c=d"],
+            )
+
+        request = Request(self.getURL("duplicate-header"))
+        return self.download_request(request, Spider("foo")).addCallback(_test)
+
 
 class Http10TestCase(HttpTestCase):
     """HTTP 1.0 test case"""
+
     download_handler_cls: Type = HTTP10DownloadHandler
 
     def test_protocol(self):
         request = Request(self.getURL("host"), method="GET")
         d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.protocol)
         d.addCallback(self.assertEqual, "HTTP/1.0")
         return d
 
 
 class Https10TestCase(Http10TestCase):
-    scheme = 'https'
+    scheme = "https"
 
 
 class Http11TestCase(HttpTestCase):
     """HTTP 1.1 test case"""
+
     download_handler_cls: Type = HTTP11DownloadHandler
 
     def test_download_without_maxsize_limit(self):
-        request = Request(self.getURL('file'))
-        d = self.download_request(request, Spider('foo'))
+        request = Request(self.getURL("file"))
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.body)
         d.addCallback(self.assertEqual, b"0123456789")
         return d
 
     def test_response_class_choosing_request(self):
         """Tests choosing of correct response type
-         in case of Content-Type is empty but body contains text.
+        in case of Content-Type is empty but body contains text.
         """
-        body = b'Some plain text\ndata with tabs\t and null bytes\0'
+        body = b"Some plain text\ndata with tabs\t and null bytes\0"
 
         def _test_type(response):
             self.assertEqual(type(response), TextResponse)
 
-        request = Request(self.getURL('nocontenttype'), body=body)
-        d = self.download_request(request, Spider('foo'))
+        request = Request(self.getURL("nocontenttype"), body=body)
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(_test_type)
         return d
 
     @defer.inlineCallbacks
     def test_download_with_maxsize(self):
-        request = Request(self.getURL('file'))
+        request = Request(self.getURL("file"))
 
         # 10 is minimal size for this request and the limit is only counted on
         # response body. (regardless of headers)
-        d = self.download_request(request, Spider('foo', download_maxsize=10))
+        d = self.download_request(request, Spider("foo", download_maxsize=10))
         d.addCallback(lambda r: r.body)
         d.addCallback(self.assertEqual, b"0123456789")
         yield d
 
-        d = self.download_request(request, Spider('foo', download_maxsize=9))
+        d = self.download_request(request, Spider("foo", download_maxsize=9))
         yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)
 
     @defer.inlineCallbacks
     def test_download_with_maxsize_very_large_file(self):
-        with mock.patch('scrapy.core.downloader.handlers.http11.logger') as logger:
-            request = Request(self.getURL('largechunkedfile'))
+        with mock.patch("scrapy.core.downloader.handlers.http11.logger") as logger:
+            request = Request(self.getURL("largechunkedfile"))
 
             def check(logger):
                 logger.warning.assert_called_once_with(mock.ANY, mock.ANY)
 
-            d = self.download_request(request, Spider('foo', download_maxsize=1500))
+            d = self.download_request(request, Spider("foo", download_maxsize=1500))
             yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)
 
             # As the error message is logged in the dataReceived callback, we
             # have to give a bit of time to the reactor to process the queue
             # after closing the connection.
             d = defer.Deferred()
             d.addCallback(check)
-            reactor.callLater(.1, d.callback, logger)
+            reactor.callLater(0.1, d.callback, logger)
             yield d
 
     @defer.inlineCallbacks
     def test_download_with_maxsize_per_req(self):
-        meta = {'download_maxsize': 2}
-        request = Request(self.getURL('file'), meta=meta)
-        d = self.download_request(request, Spider('foo'))
+        meta = {"download_maxsize": 2}
+        request = Request(self.getURL("file"), meta=meta)
+        d = self.download_request(request, Spider("foo"))
         yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)
 
     @defer.inlineCallbacks
     def test_download_with_small_maxsize_per_spider(self):
-        request = Request(self.getURL('file'))
-        d = self.download_request(request, Spider('foo', download_maxsize=2))
+        request = Request(self.getURL("file"))
+        d = self.download_request(request, Spider("foo", download_maxsize=2))
         yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)
 
     def test_download_with_large_maxsize_per_spider(self):
-        request = Request(self.getURL('file'))
-        d = self.download_request(request, Spider('foo', download_maxsize=100))
+        request = Request(self.getURL("file"))
+        d = self.download_request(request, Spider("foo", download_maxsize=100))
         d.addCallback(lambda r: r.body)
         d.addCallback(self.assertEqual, b"0123456789")
         return d
 
     def test_download_chunked_content(self):
-        request = Request(self.getURL('chunked'))
-        d = self.download_request(request, Spider('foo'))
+        request = Request(self.getURL("chunked"))
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.body)
         d.addCallback(self.assertEqual, b"chunked content\n")
         return d
 
-    def test_download_broken_content_cause_data_loss(self, url='broken'):
+    def test_download_broken_content_cause_data_loss(self, url="broken"):
         request = Request(self.getURL(url))
-        d = self.download_request(request, Spider('foo'))
+        d = self.download_request(request, Spider("foo"))
 
         def checkDataLoss(failure):
             if failure.check(ResponseFailed):
                 if any(r.check(_DataLoss) for r in failure.value.reasons):
                     return None
             return failure
 
         d.addCallback(lambda _: self.fail("No DataLoss exception"))
         d.addErrback(checkDataLoss)
         return d
 
     def test_download_broken_chunked_content_cause_data_loss(self):
-        return self.test_download_broken_content_cause_data_loss('broken-chunked')
+        return self.test_download_broken_content_cause_data_loss("broken-chunked")
 
-    def test_download_broken_content_allow_data_loss(self, url='broken'):
-        request = Request(self.getURL(url), meta={'download_fail_on_dataloss': False})
-        d = self.download_request(request, Spider('foo'))
+    def test_download_broken_content_allow_data_loss(self, url="broken"):
+        request = Request(self.getURL(url), meta={"download_fail_on_dataloss": False})
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.flags)
-        d.addCallback(self.assertEqual, ['dataloss'])
+        d.addCallback(self.assertEqual, ["dataloss"])
         return d
 
     def test_download_broken_chunked_content_allow_data_loss(self):
-        return self.test_download_broken_content_allow_data_loss('broken-chunked')
+        return self.test_download_broken_content_allow_data_loss("broken-chunked")
 
-    def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):
-        crawler = get_crawler(settings_dict={'DOWNLOAD_FAIL_ON_DATALOSS': False})
+    def test_download_broken_content_allow_data_loss_via_setting(self, url="broken"):
+        crawler = get_crawler(settings_dict={"DOWNLOAD_FAIL_ON_DATALOSS": False})
         download_handler = create_instance(self.download_handler_cls, None, crawler)
         request = Request(self.getURL(url))
-        d = download_handler.download_request(request, Spider('foo'))
+        d = download_handler.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.flags)
-        d.addCallback(self.assertEqual, ['dataloss'])
+        d.addCallback(self.assertEqual, ["dataloss"])
         return d
 
     def test_download_broken_chunked_content_allow_data_loss_via_setting(self):
-        return self.test_download_broken_content_allow_data_loss_via_setting('broken-chunked')
+        return self.test_download_broken_content_allow_data_loss_via_setting(
+            "broken-chunked"
+        )
 
     def test_protocol(self):
         request = Request(self.getURL("host"), method="GET")
         d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.protocol)
         d.addCallback(self.assertEqual, "HTTP/1.1")
         return d
 
 
 class Https11TestCase(Http11TestCase):
-    scheme = 'https'
+    scheme = "https"
 
     tls_log_message = (
         'SSL connection certificate: issuer "/C=IE/O=Scrapy/CN=localhost", '
         'subject "/C=IE/O=Scrapy/CN=localhost"'
     )
 
     @defer.inlineCallbacks
     def test_tls_logging(self):
-        crawler = get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True})
+        crawler = get_crawler(
+            settings_dict={"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING": True}
+        )
         download_handler = create_instance(self.download_handler_cls, None, crawler)
         try:
             with LogCapture() as log_capture:
-                request = Request(self.getURL('file'))
-                d = download_handler.download_request(request, Spider('foo'))
+                request = Request(self.getURL("file"))
+                d = download_handler.download_request(request, Spider("foo"))
                 d.addCallback(lambda r: r.body)
                 d.addCallback(self.assertEqual, b"0123456789")
                 yield d
-                log_capture.check_present(('scrapy.core.downloader.tls', 'DEBUG', self.tls_log_message))
+                log_capture.check_present(
+                    ("scrapy.core.downloader.tls", "DEBUG", self.tls_log_message)
+                )
         finally:
             yield download_handler.close()
 
 
 class Https11WrongHostnameTestCase(Http11TestCase):
-    scheme = 'https'
+    scheme = "https"
 
     # above tests use a server certificate for "localhost",
     # client connection to "localhost" too.
     # here we test that even if the server certificate is for another domain,
     # "www.example.com" in this case,
     # the tests still pass
-    keyfile = 'keys/example-com.key.pem'
-    certfile = 'keys/example-com.cert.pem'
+    keyfile = "keys/example-com.key.pem"
+    certfile = "keys/example-com.cert.pem"
 
 
 class Https11InvalidDNSId(Https11TestCase):
     """Connect to HTTPS hosts with IP while certificate uses domain names IDs."""
 
     def setUp(self):
         super().setUp()
-        self.host = '127.0.0.1'
+        self.host = "127.0.0.1"
 
 
 class Https11InvalidDNSPattern(Https11TestCase):
     """Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain."""
 
-    keyfile = 'keys/localhost.ip.key'
-    certfile = 'keys/localhost.ip.crt'
+    keyfile = "keys/localhost.ip.key"
+    certfile = "keys/localhost.ip.crt"
 
     def setUp(self):
         try:
             from service_identity.exceptions import CertificateError  # noqa: F401
         except ImportError:
             raise unittest.SkipTest("cryptography lib is too old")
         self.tls_log_message = (
             'SSL connection certificate: issuer "/C=IE/O=Scrapy/CN=127.0.0.1", '
             'subject "/C=IE/O=Scrapy/CN=127.0.0.1"'
         )
         super().setUp()
 
 
 class Https11CustomCiphers(unittest.TestCase):
-    scheme = 'https'
+    scheme = "https"
     download_handler_cls: Type = HTTP11DownloadHandler
 
-    keyfile = 'keys/localhost.key'
-    certfile = 'keys/localhost.crt'
+    keyfile = "keys/localhost.key"
+    certfile = "keys/localhost.crt"
 
     def setUp(self):
-        self.tmpname = self.mktemp()
-        os.mkdir(self.tmpname)
-        FilePath(self.tmpname).child("file").setContent(b"0123456789")
-        r = static.File(self.tmpname)
+        self.tmpname = Path(self.mktemp())
+        self.tmpname.mkdir()
+        (self.tmpname / "file").write_bytes(b"0123456789")
+        r = static.File(str(self.tmpname))
         self.site = server.Site(r, timeout=None)
-        self.host = 'localhost'
+        self.host = "localhost"
         self.port = reactor.listenSSL(
-            0, self.site, ssl_context_factory(self.keyfile, self.certfile, cipher_string='CAMELLIA256-SHA'),
-            interface=self.host)
+            0,
+            self.site,
+            ssl_context_factory(
+                self.keyfile, self.certfile, cipher_string="CAMELLIA256-SHA"
+            ),
+            interface=self.host,
+        )
         self.portno = self.port.getHost().port
-        crawler = get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'})
-        self.download_handler = create_instance(self.download_handler_cls, None, crawler)
+        crawler = get_crawler(
+            settings_dict={"DOWNLOADER_CLIENT_TLS_CIPHERS": "CAMELLIA256-SHA"}
+        )
+        self.download_handler = create_instance(
+            self.download_handler_cls, None, crawler
+        )
         self.download_request = self.download_handler.download_request
 
     @defer.inlineCallbacks
     def tearDown(self):
         yield self.port.stopListening()
-        if hasattr(self.download_handler, 'close'):
+        if hasattr(self.download_handler, "close"):
             yield self.download_handler.close()
         shutil.rmtree(self.tmpname)
 
     def getURL(self, path):
         return f"{self.scheme}://{self.host}:{self.portno}/{path}"
 
     def test_download(self):
-        request = Request(self.getURL('file'))
-        d = self.download_request(request, Spider('foo'))
+        request = Request(self.getURL("file"))
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.body)
         d.addCallback(self.assertEqual, b"0123456789")
         return d
 
 
 class Http11MockServerTestCase(unittest.TestCase):
     """HTTP 1.1 test case with MockServer"""
+
     settings_dict: Optional[dict] = None
 
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
 
     def tearDown(self):
         self.mockserver.__exit__(None, None, None)
 
     @defer.inlineCallbacks
     def test_download_with_content_length(self):
         crawler = get_crawler(SingleRequestSpider, self.settings_dict)
         # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid
         # download it
-        yield crawler.crawl(seed=Request(url=self.mockserver.url('/partial'), meta={'download_maxsize': 1000}))
-        failure = crawler.spider.meta['failure']
+        yield crawler.crawl(
+            seed=Request(
+                url=self.mockserver.url("/partial"), meta={"download_maxsize": 1000}
+            )
+        )
+        failure = crawler.spider.meta["failure"]
         self.assertIsInstance(failure.value, defer.CancelledError)
 
     @defer.inlineCallbacks
     def test_download(self):
         crawler = get_crawler(SingleRequestSpider, self.settings_dict)
-        yield crawler.crawl(seed=Request(url=self.mockserver.url('')))
-        failure = crawler.spider.meta.get('failure')
+        yield crawler.crawl(seed=Request(url=self.mockserver.url("")))
+        failure = crawler.spider.meta.get("failure")
         self.assertTrue(failure is None)
-        reason = crawler.spider.meta['close_reason']
-        self.assertTrue(reason, 'finished')
+        reason = crawler.spider.meta["close_reason"]
+        self.assertTrue(reason, "finished")
 
     @defer.inlineCallbacks
     def test_download_gzip_response(self):
         crawler = get_crawler(SingleRequestSpider, self.settings_dict)
-        body = b'1' * 100  # PayloadResource requires body length to be 100
-        request = Request(self.mockserver.url('/payload'), method='POST',
-                          body=body, meta={'download_maxsize': 50})
+        body = b"1" * 100  # PayloadResource requires body length to be 100
+        request = Request(
+            self.mockserver.url("/payload"),
+            method="POST",
+            body=body,
+            meta={"download_maxsize": 50},
+        )
         yield crawler.crawl(seed=request)
-        failure = crawler.spider.meta['failure']
+        failure = crawler.spider.meta["failure"]
         # download_maxsize < 100, hence the CancelledError
         self.assertIsInstance(failure.value, defer.CancelledError)
 
         # See issue https://twistedmatrix.com/trac/ticket/8175
         raise unittest.SkipTest("xpayload fails on PY3")
-        request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')
-        request = request.replace(url=self.mockserver.url('/xpayload'))
+        request.headers.setdefault(b"Accept-Encoding", b"gzip,deflate")
+        request = request.replace(url=self.mockserver.url("/xpayload"))
         yield crawler.crawl(seed=request)
         # download_maxsize = 50 is enough for the gzipped response
-        failure = crawler.spider.meta.get('failure')
+        failure = crawler.spider.meta.get("failure")
         self.assertTrue(failure is None)
-        reason = crawler.spider.meta['close_reason']
-        self.assertTrue(reason, 'finished')
+        reason = crawler.spider.meta["close_reason"]
+        self.assertTrue(reason, "finished")
 
 
 class UriResource(resource.Resource):
     """Return the full uri that was requested"""
 
     def getChild(self, path, request):
         return self
 
     def render(self, request):
         # Note: this is an ugly hack for CONNECT request timeout test.
         #       Returning some data here fail SSL/TLS handshake
         # ToDo: implement proper HTTPS proxy tests, not faking them.
-        if request.method != b'CONNECT':
+        if request.method != b"CONNECT":
             return request.uri
-        else:
-            return b''
+        return b""
 
 
 class HttpProxyTestCase(unittest.TestCase):
     download_handler_cls: Type = HTTPDownloadHandler
-    expected_http_proxy_request_body = b'http://example.com'
+    expected_http_proxy_request_body = b"http://example.com"
 
     def setUp(self):
         site = server.Site(UriResource(), timeout=None)
         wrapper = WrappingFactory(site)
-        self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')
+        self.port = reactor.listenTCP(0, wrapper, interface="127.0.0.1")
         self.portno = self.port.getHost().port
-        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())
+        self.download_handler = create_instance(
+            self.download_handler_cls, None, get_crawler()
+        )
         self.download_request = self.download_handler.download_request
 
     @defer.inlineCallbacks
     def tearDown(self):
         yield self.port.stopListening()
-        if hasattr(self.download_handler, 'close'):
+        if hasattr(self.download_handler, "close"):
             yield self.download_handler.close()
 
     def getURL(self, path):
         return f"http://127.0.0.1:{self.portno}/{path}"
 
     def test_download_with_proxy(self):
         def _test(response):
             self.assertEqual(response.status, 200)
             self.assertEqual(response.url, request.url)
             self.assertEqual(response.body, self.expected_http_proxy_request_body)
 
-        http_proxy = self.getURL('')
-        request = Request('http://example.com', meta={'proxy': http_proxy})
-        return self.download_request(request, Spider('foo')).addCallback(_test)
-
-    def test_download_with_proxy_https_noconnect(self):
-        def _test(response):
-            self.assertEqual(response.status, 200)
-            self.assertEqual(response.url, request.url)
-            self.assertEqual(response.body, b'https://example.com')
-
-        http_proxy = f'{self.getURL("")}?noconnect'
-        request = Request('https://example.com', meta={'proxy': http_proxy})
-        with self.assertWarnsRegex(ScrapyDeprecationWarning,
-                                   r'Using HTTPS proxies in the noconnect mode is deprecated'):
-            return self.download_request(request, Spider('foo')).addCallback(_test)
+        http_proxy = self.getURL("")
+        request = Request("http://example.com", meta={"proxy": http_proxy})
+        return self.download_request(request, Spider("foo")).addCallback(_test)
 
     def test_download_without_proxy(self):
         def _test(response):
             self.assertEqual(response.status, 200)
             self.assertEqual(response.url, request.url)
-            self.assertEqual(response.body, b'/path/to/resource')
+            self.assertEqual(response.body, b"/path/to/resource")
 
-        request = Request(self.getURL('path/to/resource'))
-        return self.download_request(request, Spider('foo')).addCallback(_test)
+        request = Request(self.getURL("path/to/resource"))
+        return self.download_request(request, Spider("foo")).addCallback(_test)
 
 
 class Http10ProxyTestCase(HttpProxyTestCase):
     download_handler_cls: Type = HTTP10DownloadHandler
 
     def test_download_with_proxy_https_noconnect(self):
-        raise unittest.SkipTest('noconnect is not supported in HTTP10DownloadHandler')
+        raise unittest.SkipTest("noconnect is not supported in HTTP10DownloadHandler")
 
 
 class Http11ProxyTestCase(HttpProxyTestCase):
     download_handler_cls: Type = HTTP11DownloadHandler
 
     @defer.inlineCallbacks
     def test_download_with_proxy_https_timeout(self):
-        """ Test TunnelingTCP4ClientEndpoint """
+        """Test TunnelingTCP4ClientEndpoint"""
         if NON_EXISTING_RESOLVABLE:
             raise SkipTest("Non-existing hosts are resolvable")
-        http_proxy = self.getURL('')
-        domain = 'https://no-such-domain.nosuch'
-        request = Request(
-            domain, meta={'proxy': http_proxy, 'download_timeout': 0.2})
-        d = self.download_request(request, Spider('foo'))
+        http_proxy = self.getURL("")
+        domain = "https://no-such-domain.nosuch"
+        request = Request(domain, meta={"proxy": http_proxy, "download_timeout": 0.2})
+        d = self.download_request(request, Spider("foo"))
         timeout = yield self.assertFailure(d, error.TimeoutError)
         self.assertIn(domain, timeout.osError)
 
     def test_download_with_proxy_without_http_scheme(self):
         def _test(response):
             self.assertEqual(response.status, 200)
             self.assertEqual(response.url, request.url)
             self.assertEqual(response.body, self.expected_http_proxy_request_body)
 
-        http_proxy = self.getURL('').replace('http://', '')
-        request = Request('http://example.com', meta={'proxy': http_proxy})
-        return self.download_request(request, Spider('foo')).addCallback(_test)
+        http_proxy = self.getURL("").replace("http://", "")
+        request = Request("http://example.com", meta={"proxy": http_proxy})
+        return self.download_request(request, Spider("foo")).addCallback(_test)
 
 
 class HttpDownloadHandlerMock:
-
     def __init__(self, *args, **kwargs):
         pass
 
     def download_request(self, request, spider):
         return request
 
 
 class S3AnonTestCase(unittest.TestCase):
-
     def setUp(self):
         skip_if_no_boto()
         crawler = get_crawler()
         self.s3reqh = create_instance(
             objcls=S3DownloadHandler,
             settings=None,
             crawler=crawler,
             httpdownloadhandler=HttpDownloadHandlerMock,
             # anon=True, # implicit
         )
         self.download_request = self.s3reqh.download_request
-        self.spider = Spider('foo')
+        self.spider = Spider("foo")
 
     def test_anon_request(self):
-        req = Request('s3://aws-publicdatasets/')
+        req = Request("s3://aws-publicdatasets/")
         httpreq = self.download_request(req, self.spider)
-        self.assertEqual(hasattr(self.s3reqh, 'anon'), True)
+        self.assertEqual(hasattr(self.s3reqh, "anon"), True)
         self.assertEqual(self.s3reqh.anon, True)
-        self.assertEqual(
-            httpreq.url, 'http://aws-publicdatasets.s3.amazonaws.com/')
+        self.assertEqual(httpreq.url, "http://aws-publicdatasets.s3.amazonaws.com/")
 
 
 class S3TestCase(unittest.TestCase):
     download_handler_cls: Type = S3DownloadHandler
 
     # test use same example keys than amazon developer guide
     # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf
     # and the tests described here are the examples from that manual
 
-    AWS_ACCESS_KEY_ID = '0PN5J17HBGZHT7JJ3X82'
-    AWS_SECRET_ACCESS_KEY = 'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'
+    AWS_ACCESS_KEY_ID = "0PN5J17HBGZHT7JJ3X82"
+    AWS_SECRET_ACCESS_KEY = "uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o"
 
     def setUp(self):
         skip_if_no_boto()
         crawler = get_crawler()
         s3reqh = create_instance(
             objcls=S3DownloadHandler,
             settings=None,
             crawler=crawler,
             aws_access_key_id=self.AWS_ACCESS_KEY_ID,
             aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,
             httpdownloadhandler=HttpDownloadHandlerMock,
         )
         self.download_request = s3reqh.download_request
-        self.spider = Spider('foo')
+        self.spider = Spider("foo")
 
     @contextlib.contextmanager
     def _mocked_date(self, date):
         try:
             import botocore.auth  # noqa: F401
         except ImportError:
             yield
         else:
             # We need to mock botocore.auth.formatdate, because otherwise
             # botocore overrides Date header with current date and time
             # and Authorization header is different each time
-            with mock.patch('botocore.auth.formatdate') as mock_formatdate:
+            with mock.patch("botocore.auth.formatdate") as mock_formatdate:
                 mock_formatdate.return_value = date
                 yield
 
     def test_extra_kw(self):
         try:
             crawler = get_crawler()
             create_instance(
@@ -896,135 +924,154 @@
         except Exception as e:
             self.assertIsInstance(e, (TypeError, NotConfigured))
         else:
             assert False
 
     def test_request_signing1(self):
         # gets an object from the johnsmith bucket.
-        date = 'Tue, 27 Mar 2007 19:36:42 +0000'
-        req = Request('s3://johnsmith/photos/puppy.jpg', headers={'Date': date})
+        date = "Tue, 27 Mar 2007 19:36:42 +0000"
+        req = Request("s3://johnsmith/photos/puppy.jpg", headers={"Date": date})
         with self._mocked_date(date):
             httpreq = self.download_request(req, self.spider)
-        self.assertEqual(httpreq.headers['Authorization'],
-                         b'AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')
+        self.assertEqual(
+            httpreq.headers["Authorization"],
+            b"AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=",
+        )
 
     def test_request_signing2(self):
         # puts an object into the johnsmith bucket.
-        date = 'Tue, 27 Mar 2007 21:15:45 +0000'
+        date = "Tue, 27 Mar 2007 21:15:45 +0000"
         req = Request(
-            's3://johnsmith/photos/puppy.jpg',
-            method='PUT',
+            "s3://johnsmith/photos/puppy.jpg",
+            method="PUT",
             headers={
-                'Content-Type': 'image/jpeg',
-                'Date': date,
-                'Content-Length': '94328',
+                "Content-Type": "image/jpeg",
+                "Date": date,
+                "Content-Length": "94328",
             },
         )
         with self._mocked_date(date):
             httpreq = self.download_request(req, self.spider)
-        self.assertEqual(httpreq.headers['Authorization'],
-                         b'AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')
+        self.assertEqual(
+            httpreq.headers["Authorization"],
+            b"AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=",
+        )
 
     def test_request_signing3(self):
         # lists the content of the johnsmith bucket.
-        date = 'Tue, 27 Mar 2007 19:42:41 +0000'
+        date = "Tue, 27 Mar 2007 19:42:41 +0000"
         req = Request(
-            's3://johnsmith/?prefix=photos&max-keys=50&marker=puppy',
-            method='GET', headers={
-                'User-Agent': 'Mozilla/5.0',
-                'Date': date,
-            })
+            "s3://johnsmith/?prefix=photos&max-keys=50&marker=puppy",
+            method="GET",
+            headers={
+                "User-Agent": "Mozilla/5.0",
+                "Date": date,
+            },
+        )
         with self._mocked_date(date):
             httpreq = self.download_request(req, self.spider)
-        self.assertEqual(httpreq.headers['Authorization'],
-                         b'AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')
+        self.assertEqual(
+            httpreq.headers["Authorization"],
+            b"AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=",
+        )
 
     def test_request_signing4(self):
         # fetches the access control policy sub-resource for the 'johnsmith' bucket.
-        date = 'Tue, 27 Mar 2007 19:44:46 +0000'
-        req = Request('s3://johnsmith/?acl', method='GET', headers={'Date': date})
+        date = "Tue, 27 Mar 2007 19:44:46 +0000"
+        req = Request("s3://johnsmith/?acl", method="GET", headers={"Date": date})
         with self._mocked_date(date):
             httpreq = self.download_request(req, self.spider)
-        self.assertEqual(httpreq.headers['Authorization'],
-                         b'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')
+        self.assertEqual(
+            httpreq.headers["Authorization"],
+            b"AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=",
+        )
 
     def test_request_signing6(self):
         # uploads an object to a CNAME style virtual hosted bucket with metadata.
-        date = 'Tue, 27 Mar 2007 21:06:08 +0000'
+        date = "Tue, 27 Mar 2007 21:06:08 +0000"
         req = Request(
-            's3://static.johnsmith.net:8080/db-backup.dat.gz',
-            method='PUT', headers={
-                'User-Agent': 'curl/7.15.5',
-                'Host': 'static.johnsmith.net:8080',
-                'Date': date,
-                'x-amz-acl': 'public-read',
-                'content-type': 'application/x-download',
-                'Content-MD5': '4gJE4saaMU4BqNR0kLY+lw==',
-                'X-Amz-Meta-ReviewedBy': 'joe@johnsmith.net,jane@johnsmith.net',
-                'X-Amz-Meta-FileChecksum': '0x02661779',
-                'X-Amz-Meta-ChecksumAlgorithm': 'crc32',
-                'Content-Disposition': 'attachment; filename=database.dat',
-                'Content-Encoding': 'gzip',
-                'Content-Length': '5913339',
-            })
+            "s3://static.johnsmith.net:8080/db-backup.dat.gz",
+            method="PUT",
+            headers={
+                "User-Agent": "curl/7.15.5",
+                "Host": "static.johnsmith.net:8080",
+                "Date": date,
+                "x-amz-acl": "public-read",
+                "content-type": "application/x-download",
+                "Content-MD5": "4gJE4saaMU4BqNR0kLY+lw==",
+                "X-Amz-Meta-ReviewedBy": "joe@johnsmith.net,jane@johnsmith.net",
+                "X-Amz-Meta-FileChecksum": "0x02661779",
+                "X-Amz-Meta-ChecksumAlgorithm": "crc32",
+                "Content-Disposition": "attachment; filename=database.dat",
+                "Content-Encoding": "gzip",
+                "Content-Length": "5913339",
+            },
+        )
         with self._mocked_date(date):
             httpreq = self.download_request(req, self.spider)
-        self.assertEqual(httpreq.headers['Authorization'],
-                         b'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')
+        self.assertEqual(
+            httpreq.headers["Authorization"],
+            b"AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=",
+        )
 
     def test_request_signing7(self):
         # ensure that spaces are quoted properly before signing
-        date = 'Tue, 27 Mar 2007 19:42:41 +0000'
+        date = "Tue, 27 Mar 2007 19:42:41 +0000"
         req = Request(
             "s3://johnsmith/photos/my puppy.jpg?response-content-disposition=my puppy.jpg",
-            method='GET',
-            headers={'Date': date},
+            method="GET",
+            headers={"Date": date},
         )
         with self._mocked_date(date):
             httpreq = self.download_request(req, self.spider)
         self.assertEqual(
-            httpreq.headers['Authorization'],
-            b'AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=')
+            httpreq.headers["Authorization"],
+            b"AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=",
+        )
 
 
 class BaseFTPTestCase(unittest.TestCase):
     username = "scrapy"
     password = "passwd"
     req_meta = {"ftp_user": username, "ftp_password": password}
 
     test_files = (
-        ('file.txt', b"I have the power!"),
-        ('file with spaces.txt', b"Moooooooooo power!"),
-        ('html-file-without-extension', b"<!DOCTYPE html>\n<title>.</title>"),
+        ("file.txt", b"I have the power!"),
+        ("file with spaces.txt", b"Moooooooooo power!"),
+        ("html-file-without-extension", b"<!DOCTYPE html>\n<title>.</title>"),
     )
 
     def setUp(self):
-        from twisted.protocols.ftp import FTPRealm, FTPFactory
+        from twisted.protocols.ftp import FTPFactory, FTPRealm
+
         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler
 
         # setup dirs and test file
-        self.directory = self.mktemp()
-        os.mkdir(self.directory)
-        userdir = os.path.join(self.directory, self.username)
-        os.mkdir(userdir)
-        fp = FilePath(userdir)
+        self.directory = Path(self.mktemp())
+        self.directory.mkdir()
+        userdir = self.directory / self.username
+        userdir.mkdir()
         for filename, content in self.test_files:
-            fp.child(filename).setContent(content)
+            (userdir / filename).write_bytes(content)
 
         # setup server
-        realm = FTPRealm(anonymousRoot=self.directory, userHome=self.directory)
+        realm = FTPRealm(
+            anonymousRoot=str(self.directory), userHome=str(self.directory)
+        )
         p = portal.Portal(realm)
         users_checker = checkers.InMemoryUsernamePasswordDatabaseDontUse()
         users_checker.addUser(self.username, self.password)
         p.registerChecker(users_checker, credentials.IUsernamePassword)
         self.factory = FTPFactory(portal=p)
         self.port = reactor.listenTCP(0, self.factory, interface="127.0.0.1")
         self.portNum = self.port.getHost().port
         crawler = get_crawler()
-        self.download_handler = create_instance(FTPDownloadHandler, crawler.settings, crawler)
+        self.download_handler = create_instance(
+            FTPDownloadHandler, crawler.settings, crawler
+        )
         self.addCleanup(self.port.stopListening)
 
     def tearDown(self):
         shutil.rmtree(self.directory)
 
     def _add_test_callbacks(self, deferred, callback=None, errback=None):
         def _clean(data):
@@ -1035,208 +1082,212 @@
         if callback:
             deferred.addCallback(callback)
         if errback:
             deferred.addErrback(errback)
         return deferred
 
     def test_ftp_download_success(self):
-        request = Request(url=f"ftp://127.0.0.1:{self.portNum}/file.txt",
-                          meta=self.req_meta)
+        request = Request(
+            url=f"ftp://127.0.0.1:{self.portNum}/file.txt", meta=self.req_meta
+        )
         d = self.download_handler.download_request(request, None)
 
         def _test(r):
             self.assertEqual(r.status, 200)
-            self.assertEqual(r.body, b'I have the power!')
-            self.assertEqual(r.headers, {b'Local Filename': [b''], b'Size': [b'17']})
+            self.assertEqual(r.body, b"I have the power!")
+            self.assertEqual(r.headers, {b"Local Filename": [b""], b"Size": [b"17"]})
             self.assertIsNone(r.protocol)
+
         return self._add_test_callbacks(d, _test)
 
     def test_ftp_download_path_with_spaces(self):
         request = Request(
             url=f"ftp://127.0.0.1:{self.portNum}/file with spaces.txt",
-            meta=self.req_meta
+            meta=self.req_meta,
         )
         d = self.download_handler.download_request(request, None)
 
         def _test(r):
             self.assertEqual(r.status, 200)
-            self.assertEqual(r.body, b'Moooooooooo power!')
-            self.assertEqual(r.headers, {b'Local Filename': [b''], b'Size': [b'18']})
+            self.assertEqual(r.body, b"Moooooooooo power!")
+            self.assertEqual(r.headers, {b"Local Filename": [b""], b"Size": [b"18"]})
 
         return self._add_test_callbacks(d, _test)
 
-    def test_ftp_download_notexist(self):
-        request = Request(url=f"ftp://127.0.0.1:{self.portNum}/notexist.txt",
-                          meta=self.req_meta)
+    def test_ftp_download_nonexistent(self):
+        request = Request(
+            url=f"ftp://127.0.0.1:{self.portNum}/nonexistent.txt", meta=self.req_meta
+        )
         d = self.download_handler.download_request(request, None)
 
         def _test(r):
             self.assertEqual(r.status, 404)
 
         return self._add_test_callbacks(d, _test)
 
     def test_ftp_local_filename(self):
         f, local_fname = tempfile.mkstemp()
-        local_fname = to_bytes(local_fname)
+        fname_bytes = to_bytes(local_fname)
+        local_fname = Path(local_fname)
         os.close(f)
-        meta = {"ftp_local_filename": local_fname}
+        meta = {"ftp_local_filename": fname_bytes}
         meta.update(self.req_meta)
-        request = Request(url=f"ftp://127.0.0.1:{self.portNum}/file.txt",
-                          meta=meta)
+        request = Request(url=f"ftp://127.0.0.1:{self.portNum}/file.txt", meta=meta)
         d = self.download_handler.download_request(request, None)
 
         def _test(r):
-            self.assertEqual(r.body, local_fname)
-            self.assertEqual(r.headers, {b'Local Filename': [local_fname],
-                                         b'Size': [b'17']})
-            self.assertTrue(os.path.exists(local_fname))
-            with open(local_fname, "rb") as f:
-                self.assertEqual(f.read(), b"I have the power!")
-            os.remove(local_fname)
+            self.assertEqual(r.body, fname_bytes)
+            self.assertEqual(
+                r.headers, {b"Local Filename": [fname_bytes], b"Size": [b"17"]}
+            )
+            self.assertTrue(local_fname.exists())
+            self.assertEqual(local_fname.read_bytes(), b"I have the power!")
+            local_fname.unlink()
 
         return self._add_test_callbacks(d, _test)
 
     def _test_response_class(self, filename, response_class):
         f, local_fname = tempfile.mkstemp()
-        local_fname = to_bytes(local_fname)
+        local_fname = Path(local_fname)
         os.close(f)
         meta = {}
         meta.update(self.req_meta)
-        request = Request(url=f"ftp://127.0.0.1:{self.portNum}/{filename}",
-                          meta=meta)
+        request = Request(url=f"ftp://127.0.0.1:{self.portNum}/{filename}", meta=meta)
         d = self.download_handler.download_request(request, None)
 
         def _test(r):
             self.assertEqual(type(r), response_class)
-            os.remove(local_fname)
+            local_fname.unlink()
+
         return self._add_test_callbacks(d, _test)
 
     def test_response_class_from_url(self):
-        return self._test_response_class('file.txt', TextResponse)
+        return self._test_response_class("file.txt", TextResponse)
 
     def test_response_class_from_body(self):
-        return self._test_response_class('html-file-without-extension', HtmlResponse)
+        return self._test_response_class("html-file-without-extension", HtmlResponse)
 
 
 class FTPTestCase(BaseFTPTestCase):
-
     def test_invalid_credentials(self):
         if self.reactor_pytest == "asyncio" and sys.platform == "win32":
             raise unittest.SkipTest(
                 "This test produces DirtyReactorAggregateError on Windows with asyncio"
             )
         from twisted.protocols.ftp import ConnectionLost
 
         meta = dict(self.req_meta)
-        meta.update({"ftp_password": 'invalid'})
-        request = Request(url=f"ftp://127.0.0.1:{self.portNum}/file.txt",
-                          meta=meta)
+        meta.update({"ftp_password": "invalid"})
+        request = Request(url=f"ftp://127.0.0.1:{self.portNum}/file.txt", meta=meta)
         d = self.download_handler.download_request(request, None)
 
         def _test(r):
             self.assertEqual(r.type, ConnectionLost)
 
         return self._add_test_callbacks(d, errback=_test)
 
 
 class AnonymousFTPTestCase(BaseFTPTestCase):
     username = "anonymous"
     req_meta = {}
 
     def setUp(self):
-        from twisted.protocols.ftp import FTPRealm, FTPFactory
+        from twisted.protocols.ftp import FTPFactory, FTPRealm
+
         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler
 
         # setup dir and test file
-        self.directory = self.mktemp()
-        os.mkdir(self.directory)
+        self.directory = Path(self.mktemp())
+        self.directory.mkdir()
 
-        fp = FilePath(self.directory)
         for filename, content in self.test_files:
-            fp.child(filename).setContent(content)
+            (self.directory / filename).write_bytes(content)
 
         # setup server for anonymous access
-        realm = FTPRealm(anonymousRoot=self.directory)
+        realm = FTPRealm(anonymousRoot=str(self.directory))
         p = portal.Portal(realm)
-        p.registerChecker(checkers.AllowAnonymousAccess(),
-                          credentials.IAnonymous)
+        p.registerChecker(checkers.AllowAnonymousAccess(), credentials.IAnonymous)
 
-        self.factory = FTPFactory(portal=p,
-                                  userAnonymous=self.username)
+        self.factory = FTPFactory(portal=p, userAnonymous=self.username)
         self.port = reactor.listenTCP(0, self.factory, interface="127.0.0.1")
         self.portNum = self.port.getHost().port
         crawler = get_crawler()
-        self.download_handler = create_instance(FTPDownloadHandler, crawler.settings, crawler)
+        self.download_handler = create_instance(
+            FTPDownloadHandler, crawler.settings, crawler
+        )
         self.addCleanup(self.port.stopListening)
 
     def tearDown(self):
         shutil.rmtree(self.directory)
 
 
 class DataURITestCase(unittest.TestCase):
-
     def setUp(self):
         crawler = get_crawler()
-        self.download_handler = create_instance(DataURIDownloadHandler, crawler.settings, crawler)
+        self.download_handler = create_instance(
+            DataURIDownloadHandler, crawler.settings, crawler
+        )
         self.download_request = self.download_handler.download_request
-        self.spider = Spider('foo')
+        self.spider = Spider("foo")
 
     def test_response_attrs(self):
         uri = "data:,A%20brief%20note"
 
         def _test(response):
             self.assertEqual(response.url, uri)
             self.assertFalse(response.headers)
 
         request = Request(uri)
         return self.download_request(request, self.spider).addCallback(_test)
 
     def test_default_mediatype_encoding(self):
         def _test(response):
-            self.assertEqual(response.text, 'A brief note')
+            self.assertEqual(response.text, "A brief note")
             self.assertEqual(type(response), responsetypes.from_mimetype("text/plain"))
             self.assertEqual(response.encoding, "US-ASCII")
 
         request = Request("data:,A%20brief%20note")
         return self.download_request(request, self.spider).addCallback(_test)
 
     def test_default_mediatype(self):
         def _test(response):
-            self.assertEqual(response.text, '\u038e\u03a3\u038e')
+            self.assertEqual(response.text, "\u038e\u03a3\u038e")
             self.assertEqual(type(response), responsetypes.from_mimetype("text/plain"))
             self.assertEqual(response.encoding, "iso-8859-7")
 
         request = Request("data:;charset=iso-8859-7,%be%d3%be")
         return self.download_request(request, self.spider).addCallback(_test)
 
     def test_text_charset(self):
         def _test(response):
-            self.assertEqual(response.text, '\u038e\u03a3\u038e')
-            self.assertEqual(response.body, b'\xbe\xd3\xbe')
+            self.assertEqual(response.text, "\u038e\u03a3\u038e")
+            self.assertEqual(response.body, b"\xbe\xd3\xbe")
             self.assertEqual(response.encoding, "iso-8859-7")
 
         request = Request("data:text/plain;charset=iso-8859-7,%be%d3%be")
         return self.download_request(request, self.spider).addCallback(_test)
 
     def test_mediatype_parameters(self):
         def _test(response):
-            self.assertEqual(response.text, '\u038e\u03a3\u038e')
+            self.assertEqual(response.text, "\u038e\u03a3\u038e")
             self.assertEqual(type(response), responsetypes.from_mimetype("text/plain"))
             self.assertEqual(response.encoding, "utf-8")
 
-        request = Request('data:text/plain;foo=%22foo;bar%5C%22%22;'
-                          'charset=utf-8;bar=%22foo;%5C%22 foo ;/,%22'
-                          ',%CE%8E%CE%A3%CE%8E')
+        request = Request(
+            "data:text/plain;foo=%22foo;bar%5C%22%22;"
+            "charset=utf-8;bar=%22foo;%5C%22 foo ;/,%22"
+            ",%CE%8E%CE%A3%CE%8E"
+        )
         return self.download_request(request, self.spider).addCallback(_test)
 
     def test_base64(self):
         def _test(response):
-            self.assertEqual(response.text, 'Hello, world.')
+            self.assertEqual(response.text, "Hello, world.")
 
-        request = Request('data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D')
+        request = Request("data:text/plain;base64,SGVsbG8sIHdvcmxkLg%3D%3D")
         return self.download_request(request, self.spider).addCallback(_test)
 
     def test_protocol(self):
         def _test(response):
             self.assertIsNone(response.protocol)
 
         request = Request("data:,")
```

### Comparing `Scrapy-2.7.1/tests/test_downloader_handlers_http2.py` & `Scrapy-2.8.0/tests/test_downloader_handlers_http2.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,145 +11,146 @@
 
 from scrapy.http import Request
 from scrapy.spiders import Spider
 from scrapy.utils.misc import create_instance
 from scrapy.utils.test import get_crawler
 from tests.mockserver import ssl_context_factory
 from tests.test_downloader_handlers import (
-    Https11TestCase, Https11CustomCiphers,
-    Http11MockServerTestCase, Http11ProxyTestCase,
-    UriResource
+    Http11MockServerTestCase,
+    Http11ProxyTestCase,
+    Https11CustomCiphers,
+    Https11TestCase,
+    UriResource,
 )
 
 
 @skipIf(not H2_ENABLED, "HTTP/2 support in Twisted is not enabled")
 class Https2TestCase(Https11TestCase):
 
-    scheme = 'https'
+    scheme = "https"
     HTTP2_DATALOSS_SKIP_REASON = "Content-Length mismatch raises InvalidBodyLengthError"
 
     @classmethod
     def setUpClass(cls):
         from scrapy.core.downloader.handlers.http2 import H2DownloadHandler
+
         cls.download_handler_cls = H2DownloadHandler
 
     def test_protocol(self):
         request = Request(self.getURL("host"), method="GET")
         d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.protocol)
         d.addCallback(self.assertEqual, "h2")
         return d
 
     @defer.inlineCallbacks
     def test_download_with_maxsize_very_large_file(self):
-        with mock.patch('scrapy.core.http2.stream.logger') as logger:
-            request = Request(self.getURL('largechunkedfile'))
+        with mock.patch("scrapy.core.http2.stream.logger") as logger:
+            request = Request(self.getURL("largechunkedfile"))
 
             def check(logger):
                 logger.error.assert_called_once_with(mock.ANY)
 
-            d = self.download_request(request, Spider('foo', download_maxsize=1500))
+            d = self.download_request(request, Spider("foo", download_maxsize=1500))
             yield self.assertFailure(d, defer.CancelledError, error.ConnectionAborted)
 
             # As the error message is logged in the dataReceived callback, we
             # have to give a bit of time to the reactor to process the queue
             # after closing the connection.
             d = defer.Deferred()
             d.addCallback(check)
-            reactor.callLater(.1, d.callback, logger)
+            reactor.callLater(0.1, d.callback, logger)
             yield d
 
     @defer.inlineCallbacks
     def test_unsupported_scheme(self):
         request = Request("ftp://unsupported.scheme")
         d = self.download_request(request, Spider("foo"))
         yield self.assertFailure(d, SchemeNotSupported)
 
-    def test_download_broken_content_cause_data_loss(self, url='broken'):
+    def test_download_broken_content_cause_data_loss(self, url="broken"):
         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)
 
     def test_download_broken_chunked_content_cause_data_loss(self):
         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)
 
-    def test_download_broken_content_allow_data_loss(self, url='broken'):
+    def test_download_broken_content_allow_data_loss(self, url="broken"):
         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)
 
     def test_download_broken_chunked_content_allow_data_loss(self):
         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)
 
-    def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):
+    def test_download_broken_content_allow_data_loss_via_setting(self, url="broken"):
         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)
 
     def test_download_broken_chunked_content_allow_data_loss_via_setting(self):
         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)
 
     def test_concurrent_requests_same_domain(self):
-        spider = Spider('foo')
+        spider = Spider("foo")
 
-        request1 = Request(self.getURL('file'))
+        request1 = Request(self.getURL("file"))
         d1 = self.download_request(request1, spider)
         d1.addCallback(lambda r: r.body)
         d1.addCallback(self.assertEqual, b"0123456789")
 
-        request2 = Request(self.getURL('echo'), method='POST')
+        request2 = Request(self.getURL("echo"), method="POST")
         d2 = self.download_request(request2, spider)
-        d2.addCallback(lambda r: r.headers['Content-Length'])
+        d2.addCallback(lambda r: r.headers["Content-Length"])
         d2.addCallback(self.assertEqual, b"79")
 
         return defer.DeferredList([d1, d2])
 
     @mark.xfail(reason="https://github.com/python-hyper/h2/issues/1247")
     def test_connect_request(self):
-        request = Request(self.getURL('file'), method='CONNECT')
-        d = self.download_request(request, Spider('foo'))
+        request = Request(self.getURL("file"), method="CONNECT")
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.body)
-        d.addCallback(self.assertEqual, b'')
+        d.addCallback(self.assertEqual, b"")
         return d
 
     def test_custom_content_length_good(self):
-        request = Request(self.getURL('contentlength'))
+        request = Request(self.getURL("contentlength"))
         custom_content_length = str(len(request.body))
-        request.headers['Content-Length'] = custom_content_length
-        d = self.download_request(request, Spider('foo'))
+        request.headers["Content-Length"] = custom_content_length
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.text)
         d.addCallback(self.assertEqual, custom_content_length)
         return d
 
     def test_custom_content_length_bad(self):
-        request = Request(self.getURL('contentlength'))
+        request = Request(self.getURL("contentlength"))
         actual_content_length = str(len(request.body))
         bad_content_length = str(len(request.body) + 1)
-        request.headers['Content-Length'] = bad_content_length
+        request.headers["Content-Length"] = bad_content_length
         log = LogCapture()
-        d = self.download_request(request, Spider('foo'))
+        d = self.download_request(request, Spider("foo"))
         d.addCallback(lambda r: r.text)
         d.addCallback(self.assertEqual, actual_content_length)
         d.addCallback(
             lambda _: log.check_present(
                 (
-                    'scrapy.core.http2.stream',
-                    'WARNING',
-                    f'Ignoring bad Content-Length header '
-                    f'{bad_content_length!r} of request {request}, sending '
-                    f'{actual_content_length!r} instead',
+                    "scrapy.core.http2.stream",
+                    "WARNING",
+                    f"Ignoring bad Content-Length header "
+                    f"{bad_content_length!r} of request {request}, sending "
+                    f"{actual_content_length!r} instead",
                 )
             )
         )
-        d.addCallback(
-            lambda _: log.uninstall()
-        )
+        d.addCallback(lambda _: log.uninstall())
         return d
 
     def test_duplicate_header(self):
-        request = Request(self.getURL('echo'))
-        header, value1, value2 = 'Custom-Header', 'foo', 'bar'
+        request = Request(self.getURL("echo"))
+        header, value1, value2 = "Custom-Header", "foo", "bar"
         request.headers.appendlist(header, value1)
         request.headers.appendlist(header, value2)
-        d = self.download_request(request, Spider('foo'))
-        d.addCallback(lambda r: json.loads(r.text)['headers'][header])
+        d = self.download_request(request, Spider("foo"))
+        d.addCallback(lambda r: json.loads(r.text)["headers"][header])
         d.addCallback(self.assertEqual, [value1, value2])
         return d
 
 
 class Https2WrongHostnameTestCase(Https2TestCase):
     tls_log_message = (
         'SSL connection certificate: issuer "/C=XW/ST=XW/L=The '
@@ -159,105 +160,96 @@
     )
 
     # above tests use a server certificate for "localhost",
     # client connection to "localhost" too.
     # here we test that even if the server certificate is for another domain,
     # "www.example.com" in this case,
     # the tests still pass
-    keyfile = 'keys/example-com.key.pem'
-    certfile = 'keys/example-com.cert.pem'
+    keyfile = "keys/example-com.key.pem"
+    certfile = "keys/example-com.cert.pem"
 
 
 class Https2InvalidDNSId(Https2TestCase):
     """Connect to HTTPS hosts with IP while certificate uses domain names IDs."""
 
     def setUp(self):
-        super(Https2InvalidDNSId, self).setUp()
-        self.host = '127.0.0.1'
+        super().setUp()
+        self.host = "127.0.0.1"
 
 
 class Https2InvalidDNSPattern(Https2TestCase):
     """Connect to HTTPS hosts where the certificate are issued to an ip instead of a domain."""
 
-    keyfile = 'keys/localhost.ip.key'
-    certfile = 'keys/localhost.ip.crt'
+    keyfile = "keys/localhost.ip.key"
+    certfile = "keys/localhost.ip.crt"
 
     def setUp(self):
         try:
             from service_identity.exceptions import CertificateError  # noqa: F401
         except ImportError:
             raise unittest.SkipTest("cryptography lib is too old")
         self.tls_log_message = (
             'SSL connection certificate: issuer "/C=IE/O=Scrapy/CN=127.0.0.1", '
             'subject "/C=IE/O=Scrapy/CN=127.0.0.1"'
         )
-        super(Https2InvalidDNSPattern, self).setUp()
+        super().setUp()
 
 
 @skipIf(not H2_ENABLED, "HTTP/2 support in Twisted is not enabled")
 class Https2CustomCiphers(Https11CustomCiphers):
-    scheme = 'https'
+    scheme = "https"
 
     @classmethod
     def setUpClass(cls):
         from scrapy.core.downloader.handlers.http2 import H2DownloadHandler
+
         cls.download_handler_cls = H2DownloadHandler
 
 
 class Http2MockServerTestCase(Http11MockServerTestCase):
     """HTTP 2.0 test case with MockServer"""
+
     settings_dict = {
-        'DOWNLOAD_HANDLERS': {
-            'https': 'scrapy.core.downloader.handlers.http2.H2DownloadHandler'
+        "DOWNLOAD_HANDLERS": {
+            "https": "scrapy.core.downloader.handlers.http2.H2DownloadHandler"
         }
     }
 
 
 @skipIf(not H2_ENABLED, "HTTP/2 support in Twisted is not enabled")
 class Https2ProxyTestCase(Http11ProxyTestCase):
     # only used for HTTPS tests
-    keyfile = 'keys/localhost.key'
-    certfile = 'keys/localhost.crt'
+    keyfile = "keys/localhost.key"
+    certfile = "keys/localhost.crt"
 
-    scheme = 'https'
-    host = '127.0.0.1'
+    scheme = "https"
+    host = "127.0.0.1"
 
-    expected_http_proxy_request_body = b'/'
+    expected_http_proxy_request_body = b"/"
 
     @classmethod
     def setUpClass(cls):
         from scrapy.core.downloader.handlers.http2 import H2DownloadHandler
+
         cls.download_handler_cls = H2DownloadHandler
 
     def setUp(self):
         site = server.Site(UriResource(), timeout=None)
         self.port = reactor.listenSSL(
-            0, site,
+            0,
+            site,
             ssl_context_factory(self.keyfile, self.certfile),
-            interface=self.host
+            interface=self.host,
         )
         self.portno = self.port.getHost().port
-        self.download_handler = create_instance(self.download_handler_cls, None, get_crawler())
+        self.download_handler = create_instance(
+            self.download_handler_cls, None, get_crawler()
+        )
         self.download_request = self.download_handler.download_request
 
     def getURL(self, path):
         return f"{self.scheme}://{self.host}:{self.portno}/{path}"
 
-    def test_download_with_proxy_https_noconnect(self):
-        def _test(response):
-            self.assertEqual(response.status, 200)
-            self.assertEqual(response.url, request.url)
-            self.assertEqual(response.body, b'/')
-
-        http_proxy = f"{self.getURL('')}?noconnect"
-        request = Request('https://example.com', meta={'proxy': http_proxy})
-        with self.assertWarnsRegex(
-            Warning,
-            r'Using HTTPS proxies in the noconnect mode is not supported by the '
-            r'downloader handler.'
-        ):
-            return self.download_request(request, Spider('foo')).addCallback(_test)
-
     @defer.inlineCallbacks
     def test_download_with_proxy_https_timeout(self):
         with self.assertRaises(NotImplementedError):
-            yield super(Https2ProxyTestCase, self).test_download_with_proxy_https_timeout()
+            yield super().test_download_with_proxy_https_timeout()
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 import asyncio
 from unittest import mock
 
 from pytest import mark
 from twisted.internet import defer
 from twisted.internet.defer import Deferred
-from twisted.trial.unittest import TestCase
 from twisted.python.failure import Failure
+from twisted.trial.unittest import TestCase
 
+from scrapy.core.downloader.middleware import DownloaderMiddlewareManager
+from scrapy.exceptions import _InvalidOutput
 from scrapy.http import Request, Response
 from scrapy.spiders import Spider
-from scrapy.exceptions import _InvalidOutput
-from scrapy.core.downloader.middleware import DownloaderMiddlewareManager
-from scrapy.utils.test import get_crawler, get_from_asyncio_queue
 from scrapy.utils.python import to_bytes
+from scrapy.utils.test import get_crawler, get_from_asyncio_queue
 
 
 class ManagerTestCase(TestCase):
 
     settings_dict = None
 
     def setUp(self):
         self.crawler = get_crawler(Spider, self.settings_dict)
-        self.spider = self.crawler._create_spider('foo')
+        self.spider = self.crawler._create_spider("foo")
         self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)
         # some mw depends on stats collector
         self.crawler.stats.open_spider(self.spider)
         return self.mwman.open_spider(self.spider)
 
     def tearDown(self):
-        self.crawler.stats.close_spider(self.spider, '')
+        self.crawler.stats.close_spider(self.spider, "")
         return self.mwman.close_spider(self.spider)
 
     def _download(self, request, response=None):
         """Executes downloader mw manager's download method and returns
         the result (Request or Response) or raise exception in case of
         failure.
         """
@@ -53,15 +53,15 @@
         return ret
 
 
 class DefaultsTest(ManagerTestCase):
     """Tests default behavior with default settings"""
 
     def test_request_response(self):
-        req = Request('http://example.com/index.html')
+        req = Request("http://example.com/index.html")
         resp = Response(req.url, status=200)
         ret = self._download(req, resp)
         self.assertTrue(isinstance(ret, Response), "Non-response returned")
 
     def test_3xx_and_invalid_gzipped_body_must_redirect(self):
         """Regression test for a failure when redirecting a compressed
         request.
@@ -70,68 +70,80 @@
         middleware and attempts to decompress a non-compressed body.
         In particular when some website returns a 30x response with header
         'Content-Encoding: gzip' giving as result the error below:
 
             exceptions.IOError: Not a gzipped file
 
         """
-        req = Request('http://example.com')
-        body = b'<p>You are being redirected</p>'
-        resp = Response(req.url, status=302, body=body, headers={
-            'Content-Length': str(len(body)),
-            'Content-Type': 'text/html',
-            'Content-Encoding': 'gzip',
-            'Location': 'http://example.com/login',
-        })
+        req = Request("http://example.com")
+        body = b"<p>You are being redirected</p>"
+        resp = Response(
+            req.url,
+            status=302,
+            body=body,
+            headers={
+                "Content-Length": str(len(body)),
+                "Content-Type": "text/html",
+                "Content-Encoding": "gzip",
+                "Location": "http://example.com/login",
+            },
+        )
         ret = self._download(request=req, response=resp)
-        self.assertTrue(isinstance(ret, Request),
-                        f"Not redirected: {ret!r}")
-        self.assertEqual(to_bytes(ret.url), resp.headers['Location'],
-                         "Not redirected to location header")
+        self.assertTrue(isinstance(ret, Request), f"Not redirected: {ret!r}")
+        self.assertEqual(
+            to_bytes(ret.url),
+            resp.headers["Location"],
+            "Not redirected to location header",
+        )
 
     def test_200_and_invalid_gzipped_body_must_fail(self):
-        req = Request('http://example.com')
-        body = b'<p>You are being redirected</p>'
-        resp = Response(req.url, status=200, body=body, headers={
-            'Content-Length': str(len(body)),
-            'Content-Type': 'text/html',
-            'Content-Encoding': 'gzip',
-            'Location': 'http://example.com/login',
-        })
+        req = Request("http://example.com")
+        body = b"<p>You are being redirected</p>"
+        resp = Response(
+            req.url,
+            status=200,
+            body=body,
+            headers={
+                "Content-Length": str(len(body)),
+                "Content-Type": "text/html",
+                "Content-Encoding": "gzip",
+                "Location": "http://example.com/login",
+            },
+        )
         self.assertRaises(IOError, self._download, request=req, response=resp)
 
 
 class ResponseFromProcessRequestTest(ManagerTestCase):
     """Tests middleware returning a response from process_request."""
 
     def test_download_func_not_called(self):
-        resp = Response('http://example.com/index.html')
+        resp = Response("http://example.com/index.html")
 
         class ResponseMiddleware:
             def process_request(self, request, spider):
                 return resp
 
         self.mwman._add_middleware(ResponseMiddleware())
 
-        req = Request('http://example.com/index.html')
+        req = Request("http://example.com/index.html")
         download_func = mock.MagicMock()
         dfd = self.mwman.download(download_func, req, self.spider)
         results = []
         dfd.addBoth(results.append)
         self._wait(dfd)
 
         self.assertIs(results[0], resp)
         self.assertFalse(download_func.called)
 
 
 class ProcessRequestInvalidOutput(ManagerTestCase):
     """Invalid return value for process_request method should raise an exception"""
 
     def test_invalid_process_request(self):
-        req = Request('http://example.com/index.html')
+        req = Request("http://example.com/index.html")
 
         class InvalidProcessRequestMiddleware:
             def process_request(self, request, spider):
                 return 1
 
         self.mwman._add_middleware(InvalidProcessRequestMiddleware())
         download_func = mock.MagicMock()
@@ -142,15 +154,15 @@
         self.assertIsInstance(results[0].value, _InvalidOutput)
 
 
 class ProcessResponseInvalidOutput(ManagerTestCase):
     """Invalid return value for process_response method should raise an exception"""
 
     def test_invalid_process_response(self):
-        req = Request('http://example.com/index.html')
+        req = Request("http://example.com/index.html")
 
         class InvalidProcessResponseMiddleware:
             def process_response(self, request, response, spider):
                 return 1
 
         self.mwman._add_middleware(InvalidProcessResponseMiddleware())
         download_func = mock.MagicMock()
@@ -161,15 +173,15 @@
         self.assertIsInstance(results[0].value, _InvalidOutput)
 
 
 class ProcessExceptionInvalidOutput(ManagerTestCase):
     """Invalid return value for process_exception method should raise an exception"""
 
     def test_invalid_process_exception(self):
-        req = Request('http://example.com/index.html')
+        req = Request("http://example.com/index.html")
 
         class InvalidProcessExceptionMiddleware:
             def process_request(self, request, spider):
                 raise Exception()
 
             def process_exception(self, request, exception, spider):
                 return 1
@@ -183,73 +195,73 @@
         self.assertIsInstance(results[0].value, _InvalidOutput)
 
 
 class MiddlewareUsingDeferreds(ManagerTestCase):
     """Middlewares using Deferreds should work"""
 
     def test_deferred(self):
-        resp = Response('http://example.com/index.html')
+        resp = Response("http://example.com/index.html")
 
         class DeferredMiddleware:
             def cb(self, result):
                 return result
 
             def process_request(self, request, spider):
                 d = Deferred()
                 d.addCallback(self.cb)
                 d.callback(resp)
                 return d
 
         self.mwman._add_middleware(DeferredMiddleware())
-        req = Request('http://example.com/index.html')
+        req = Request("http://example.com/index.html")
         download_func = mock.MagicMock()
         dfd = self.mwman.download(download_func, req, self.spider)
         results = []
         dfd.addBoth(results.append)
         self._wait(dfd)
 
         self.assertIs(results[0], resp)
         self.assertFalse(download_func.called)
 
 
-@mark.usefixtures('reactor_pytest')
+@mark.usefixtures("reactor_pytest")
 class MiddlewareUsingCoro(ManagerTestCase):
     """Middlewares using asyncio coroutines should work"""
 
     def test_asyncdef(self):
-        resp = Response('http://example.com/index.html')
+        resp = Response("http://example.com/index.html")
 
         class CoroMiddleware:
             async def process_request(self, request, spider):
                 await defer.succeed(42)
                 return resp
 
         self.mwman._add_middleware(CoroMiddleware())
-        req = Request('http://example.com/index.html')
+        req = Request("http://example.com/index.html")
         download_func = mock.MagicMock()
         dfd = self.mwman.download(download_func, req, self.spider)
         results = []
         dfd.addBoth(results.append)
         self._wait(dfd)
 
         self.assertIs(results[0], resp)
         self.assertFalse(download_func.called)
 
     @mark.only_asyncio()
     def test_asyncdef_asyncio(self):
-        resp = Response('http://example.com/index.html')
+        resp = Response("http://example.com/index.html")
 
         class CoroMiddleware:
             async def process_request(self, request, spider):
                 await asyncio.sleep(0.1)
                 result = await get_from_asyncio_queue(resp)
                 return result
 
         self.mwman._add_middleware(CoroMiddleware())
-        req = Request('http://example.com/index.html')
+        req = Request("http://example.com/index.html")
         download_func = mock.MagicMock()
         dfd = self.mwman.download(download_func, req, self.spider)
         results = []
         dfd.addBoth(results.append)
         self._wait(dfd)
 
         self.assertIs(results[0], resp)
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_ajaxcrawlable.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_ajaxcrawlable.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,60 +1,63 @@
 import unittest
 
 from scrapy.downloadermiddlewares.ajaxcrawl import AjaxCrawlMiddleware
+from scrapy.http import HtmlResponse, Request, Response
 from scrapy.spiders import Spider
-from scrapy.http import Request, HtmlResponse, Response
 from scrapy.utils.test import get_crawler
 
-
-__doctests__ = ['scrapy.downloadermiddlewares.ajaxcrawl']
+__doctests__ = ["scrapy.downloadermiddlewares.ajaxcrawl"]
 
 
 class AjaxCrawlMiddlewareTest(unittest.TestCase):
     def setUp(self):
-        crawler = get_crawler(Spider, {'AJAXCRAWL_ENABLED': True})
-        self.spider = crawler._create_spider('foo')
+        crawler = get_crawler(Spider, {"AJAXCRAWL_ENABLED": True})
+        self.spider = crawler._create_spider("foo")
         self.mw = AjaxCrawlMiddleware.from_crawler(crawler)
 
     def _ajaxcrawlable_body(self):
         return b'<html><head><meta name="fragment" content="!"/></head><body></body></html>'
 
     def _req_resp(self, url, req_kwargs=None, resp_kwargs=None):
         req = Request(url, **(req_kwargs or {}))
         resp = HtmlResponse(url, request=req, **(resp_kwargs or {}))
         return req, resp
 
     def test_non_get(self):
-        req, resp = self._req_resp('http://example.com/', {'method': 'HEAD'})
+        req, resp = self._req_resp("http://example.com/", {"method": "HEAD"})
         resp2 = self.mw.process_response(req, resp, self.spider)
         self.assertEqual(resp, resp2)
 
     def test_binary_response(self):
-        req = Request('http://example.com/')
-        resp = Response('http://example.com/', body=b'foobar\x00\x01\x02', request=req)
+        req = Request("http://example.com/")
+        resp = Response("http://example.com/", body=b"foobar\x00\x01\x02", request=req)
         resp2 = self.mw.process_response(req, resp, self.spider)
         self.assertIs(resp, resp2)
 
     def test_ajaxcrawl(self):
         req, resp = self._req_resp(
-            'http://example.com/',
-            {'meta': {'foo': 'bar'}},
-            {'body': self._ajaxcrawlable_body()}
+            "http://example.com/",
+            {"meta": {"foo": "bar"}},
+            {"body": self._ajaxcrawlable_body()},
         )
         req2 = self.mw.process_response(req, resp, self.spider)
-        self.assertEqual(req2.url, 'http://example.com/?_escaped_fragment_=')
-        self.assertEqual(req2.meta['foo'], 'bar')
+        self.assertEqual(req2.url, "http://example.com/?_escaped_fragment_=")
+        self.assertEqual(req2.meta["foo"], "bar")
 
     def test_ajaxcrawl_loop(self):
-        req, resp = self._req_resp('http://example.com/', {}, {'body': self._ajaxcrawlable_body()})
+        req, resp = self._req_resp(
+            "http://example.com/", {}, {"body": self._ajaxcrawlable_body()}
+        )
         req2 = self.mw.process_response(req, resp, self.spider)
         resp2 = HtmlResponse(req2.url, body=resp.body, request=req2)
         resp3 = self.mw.process_response(req2, resp2, self.spider)
 
         assert isinstance(resp3, HtmlResponse), (resp3.__class__, resp3)
-        self.assertEqual(resp3.request.url, 'http://example.com/?_escaped_fragment_=')
+        self.assertEqual(resp3.request.url, "http://example.com/?_escaped_fragment_=")
         assert resp3 is resp2
 
     def test_noncrawlable_body(self):
-        req, resp = self._req_resp('http://example.com/', {}, {'body': b'<html></html>'})
+        req, resp = self._req_resp(
+            "http://example.com/", {}, {"body": b"<html></html>"}
+        )
         resp2 = self.mw.process_response(req, resp, self.spider)
         self.assertIs(resp, resp2)
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_cookies.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_cookies.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 import logging
-from testfixtures import LogCapture
 from unittest import TestCase
 
 import pytest
+from testfixtures import LogCapture
 
 from scrapy.downloadermiddlewares.cookies import CookiesMiddleware
 from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware
 from scrapy.downloadermiddlewares.redirect import RedirectMiddleware
 from scrapy.exceptions import NotConfigured
-from scrapy.http import Response, Request
+from scrapy.http import Request, Response
 from scrapy.settings import Settings
 from scrapy.spiders import Spider
 from scrapy.utils.python import to_bytes
 from scrapy.utils.test import get_crawler
 
 
 def _cookie_to_set_cookie_value(cookie):
@@ -44,405 +44,444 @@
     dictionaries (i.e. in a format supported by the cookies parameter of
     Request), return the equivalen list of strings that can be associated to a
     ``Set-Cookie`` header."""
     if not cookies:
         return []
     if isinstance(cookies, dict):
         cookies = ({"name": k, "value": v} for k, v in cookies.items())
-    return filter(
-        None,
-        (
-            _cookie_to_set_cookie_value(cookie)
-            for cookie in cookies
-        )
-    )
+    return filter(None, (_cookie_to_set_cookie_value(cookie) for cookie in cookies))
 
 
 class CookiesMiddlewareTest(TestCase):
-
     def assertCookieValEqual(self, first, second, msg=None):
         def split_cookies(cookies):
             return sorted([s.strip() for s in to_bytes(cookies).split(b";")])
+
         return self.assertEqual(split_cookies(first), split_cookies(second), msg=msg)
 
     def setUp(self):
-        self.spider = Spider('foo')
+        self.spider = Spider("foo")
         self.mw = CookiesMiddleware()
         self.redirect_middleware = RedirectMiddleware(settings=Settings())
 
     def tearDown(self):
         del self.mw
         del self.redirect_middleware
 
     def test_basic(self):
-        req = Request('http://scrapytest.org/')
+        req = Request("http://scrapytest.org/")
         assert self.mw.process_request(req, self.spider) is None
-        assert 'Cookie' not in req.headers
+        assert "Cookie" not in req.headers
 
-        headers = {'Set-Cookie': 'C1=value1; path=/'}
-        res = Response('http://scrapytest.org/', headers=headers)
+        headers = {"Set-Cookie": "C1=value1; path=/"}
+        res = Response("http://scrapytest.org/", headers=headers)
         assert self.mw.process_response(req, res, self.spider) is res
 
-        req2 = Request('http://scrapytest.org/sub1/')
+        req2 = Request("http://scrapytest.org/sub1/")
         assert self.mw.process_request(req2, self.spider) is None
-        self.assertEqual(req2.headers.get('Cookie'), b"C1=value1")
+        self.assertEqual(req2.headers.get("Cookie"), b"C1=value1")
 
     def test_setting_false_cookies_enabled(self):
         self.assertRaises(
             NotConfigured,
             CookiesMiddleware.from_crawler,
-            get_crawler(settings_dict={'COOKIES_ENABLED': False})
+            get_crawler(settings_dict={"COOKIES_ENABLED": False}),
         )
 
     def test_setting_default_cookies_enabled(self):
         self.assertIsInstance(
-            CookiesMiddleware.from_crawler(get_crawler()),
-            CookiesMiddleware
+            CookiesMiddleware.from_crawler(get_crawler()), CookiesMiddleware
         )
 
     def test_setting_true_cookies_enabled(self):
         self.assertIsInstance(
             CookiesMiddleware.from_crawler(
-                get_crawler(settings_dict={'COOKIES_ENABLED': True})
+                get_crawler(settings_dict={"COOKIES_ENABLED": True})
             ),
-            CookiesMiddleware
+            CookiesMiddleware,
         )
 
     def test_setting_enabled_cookies_debug(self):
-        crawler = get_crawler(settings_dict={'COOKIES_DEBUG': True})
+        crawler = get_crawler(settings_dict={"COOKIES_DEBUG": True})
         mw = CookiesMiddleware.from_crawler(crawler)
         with LogCapture(
-            'scrapy.downloadermiddlewares.cookies',
+            "scrapy.downloadermiddlewares.cookies",
             propagate=False,
             level=logging.DEBUG,
         ) as log:
-            req = Request('http://scrapytest.org/')
-            res = Response('http://scrapytest.org/', headers={'Set-Cookie': 'C1=value1; path=/'})
+            req = Request("http://scrapytest.org/")
+            res = Response(
+                "http://scrapytest.org/", headers={"Set-Cookie": "C1=value1; path=/"}
+            )
             mw.process_response(req, res, crawler.spider)
-            req2 = Request('http://scrapytest.org/sub1/')
+            req2 = Request("http://scrapytest.org/sub1/")
             mw.process_request(req2, crawler.spider)
 
             log.check(
-                ('scrapy.downloadermiddlewares.cookies',
-                 'DEBUG',
-                 'Received cookies from: <200 http://scrapytest.org/>\n'
-                 'Set-Cookie: C1=value1; path=/\n'),
-                ('scrapy.downloadermiddlewares.cookies',
-                 'DEBUG',
-                 'Sending cookies to: <GET http://scrapytest.org/sub1/>\n'
-                 'Cookie: C1=value1\n'),
+                (
+                    "scrapy.downloadermiddlewares.cookies",
+                    "DEBUG",
+                    "Received cookies from: <200 http://scrapytest.org/>\n"
+                    "Set-Cookie: C1=value1; path=/\n",
+                ),
+                (
+                    "scrapy.downloadermiddlewares.cookies",
+                    "DEBUG",
+                    "Sending cookies to: <GET http://scrapytest.org/sub1/>\n"
+                    "Cookie: C1=value1\n",
+                ),
             )
 
     def test_setting_disabled_cookies_debug(self):
-        crawler = get_crawler(settings_dict={'COOKIES_DEBUG': False})
+        crawler = get_crawler(settings_dict={"COOKIES_DEBUG": False})
         mw = CookiesMiddleware.from_crawler(crawler)
         with LogCapture(
-            'scrapy.downloadermiddlewares.cookies',
+            "scrapy.downloadermiddlewares.cookies",
             propagate=False,
             level=logging.DEBUG,
         ) as log:
-            req = Request('http://scrapytest.org/')
-            res = Response('http://scrapytest.org/', headers={'Set-Cookie': 'C1=value1; path=/'})
+            req = Request("http://scrapytest.org/")
+            res = Response(
+                "http://scrapytest.org/", headers={"Set-Cookie": "C1=value1; path=/"}
+            )
             mw.process_response(req, res, crawler.spider)
-            req2 = Request('http://scrapytest.org/sub1/')
+            req2 = Request("http://scrapytest.org/sub1/")
             mw.process_request(req2, crawler.spider)
 
             log.check()
 
     def test_do_not_break_on_non_utf8_header(self):
-        req = Request('http://scrapytest.org/')
+        req = Request("http://scrapytest.org/")
         assert self.mw.process_request(req, self.spider) is None
-        assert 'Cookie' not in req.headers
+        assert "Cookie" not in req.headers
 
-        headers = {'Set-Cookie': b'C1=in\xa3valid; path=/', 'Other': b'ignore\xa3me'}
-        res = Response('http://scrapytest.org/', headers=headers)
+        headers = {"Set-Cookie": b"C1=in\xa3valid; path=/", "Other": b"ignore\xa3me"}
+        res = Response("http://scrapytest.org/", headers=headers)
         assert self.mw.process_response(req, res, self.spider) is res
 
-        req2 = Request('http://scrapytest.org/sub1/')
+        req2 = Request("http://scrapytest.org/sub1/")
         assert self.mw.process_request(req2, self.spider) is None
-        self.assertIn('Cookie', req2.headers)
+        self.assertIn("Cookie", req2.headers)
 
     def test_dont_merge_cookies(self):
         # merge some cookies into jar
-        headers = {'Set-Cookie': 'C1=value1; path=/'}
-        req = Request('http://scrapytest.org/')
-        res = Response('http://scrapytest.org/', headers=headers)
+        headers = {"Set-Cookie": "C1=value1; path=/"}
+        req = Request("http://scrapytest.org/")
+        res = Response("http://scrapytest.org/", headers=headers)
         assert self.mw.process_response(req, res, self.spider) is res
 
         # test Cookie header is not seted to request
-        req = Request('http://scrapytest.org/dontmerge', meta={'dont_merge_cookies': 1})
+        req = Request("http://scrapytest.org/dontmerge", meta={"dont_merge_cookies": 1})
         assert self.mw.process_request(req, self.spider) is None
-        assert 'Cookie' not in req.headers
+        assert "Cookie" not in req.headers
 
         # check that returned cookies are not merged back to jar
         res = Response(
-            'http://scrapytest.org/dontmerge',
-            headers={'Set-Cookie': 'dont=mergeme; path=/'},
+            "http://scrapytest.org/dontmerge",
+            headers={"Set-Cookie": "dont=mergeme; path=/"},
         )
         assert self.mw.process_response(req, res, self.spider) is res
 
         # check that cookies are merged back
-        req = Request('http://scrapytest.org/mergeme')
+        req = Request("http://scrapytest.org/mergeme")
         assert self.mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers.get('Cookie'), b'C1=value1')
+        self.assertEqual(req.headers.get("Cookie"), b"C1=value1")
 
         # check that cookies are merged when dont_merge_cookies is passed as 0
-        req = Request('http://scrapytest.org/mergeme', meta={'dont_merge_cookies': 0})
+        req = Request("http://scrapytest.org/mergeme", meta={"dont_merge_cookies": 0})
         assert self.mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers.get('Cookie'), b'C1=value1')
+        self.assertEqual(req.headers.get("Cookie"), b"C1=value1")
 
     def test_complex_cookies(self):
         # merge some cookies into jar
         cookies = [
-            {'name': 'C1', 'value': 'value1', 'path': '/foo', 'domain': 'scrapytest.org'},
-            {'name': 'C2', 'value': 'value2', 'path': '/bar', 'domain': 'scrapytest.org'},
-            {'name': 'C3', 'value': 'value3', 'path': '/foo', 'domain': 'scrapytest.org'},
-            {'name': 'C4', 'value': 'value4', 'path': '/foo', 'domain': 'scrapy.org'},
+            {
+                "name": "C1",
+                "value": "value1",
+                "path": "/foo",
+                "domain": "scrapytest.org",
+            },
+            {
+                "name": "C2",
+                "value": "value2",
+                "path": "/bar",
+                "domain": "scrapytest.org",
+            },
+            {
+                "name": "C3",
+                "value": "value3",
+                "path": "/foo",
+                "domain": "scrapytest.org",
+            },
+            {"name": "C4", "value": "value4", "path": "/foo", "domain": "scrapy.org"},
         ]
 
-        req = Request('http://scrapytest.org/', cookies=cookies)
+        req = Request("http://scrapytest.org/", cookies=cookies)
         self.mw.process_request(req, self.spider)
 
         # embed C1 and C3 for scrapytest.org/foo
-        req = Request('http://scrapytest.org/foo')
+        req = Request("http://scrapytest.org/foo")
         self.mw.process_request(req, self.spider)
-        assert req.headers.get('Cookie') in (b'C1=value1; C3=value3', b'C3=value3; C1=value1')
+        assert req.headers.get("Cookie") in (
+            b"C1=value1; C3=value3",
+            b"C3=value3; C1=value1",
+        )
 
         # embed C2 for scrapytest.org/bar
-        req = Request('http://scrapytest.org/bar')
+        req = Request("http://scrapytest.org/bar")
         self.mw.process_request(req, self.spider)
-        self.assertEqual(req.headers.get('Cookie'), b'C2=value2')
+        self.assertEqual(req.headers.get("Cookie"), b"C2=value2")
 
         # embed nothing for scrapytest.org/baz
-        req = Request('http://scrapytest.org/baz')
+        req = Request("http://scrapytest.org/baz")
         self.mw.process_request(req, self.spider)
-        assert 'Cookie' not in req.headers
+        assert "Cookie" not in req.headers
 
     def test_merge_request_cookies(self):
-        req = Request('http://scrapytest.org/', cookies={'galleta': 'salada'})
+        req = Request("http://scrapytest.org/", cookies={"galleta": "salada"})
         assert self.mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers.get('Cookie'), b'galleta=salada')
+        self.assertEqual(req.headers.get("Cookie"), b"galleta=salada")
 
-        headers = {'Set-Cookie': 'C1=value1; path=/'}
-        res = Response('http://scrapytest.org/', headers=headers)
+        headers = {"Set-Cookie": "C1=value1; path=/"}
+        res = Response("http://scrapytest.org/", headers=headers)
         assert self.mw.process_response(req, res, self.spider) is res
 
-        req2 = Request('http://scrapytest.org/sub1/')
+        req2 = Request("http://scrapytest.org/sub1/")
         assert self.mw.process_request(req2, self.spider) is None
 
-        self.assertCookieValEqual(req2.headers.get('Cookie'), b"C1=value1; galleta=salada")
+        self.assertCookieValEqual(
+            req2.headers.get("Cookie"), b"C1=value1; galleta=salada"
+        )
 
     def test_cookiejar_key(self):
         req = Request(
-            'http://scrapytest.org/',
-            cookies={'galleta': 'salada'},
-            meta={'cookiejar': "store1"},
+            "http://scrapytest.org/",
+            cookies={"galleta": "salada"},
+            meta={"cookiejar": "store1"},
         )
         assert self.mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers.get('Cookie'), b'galleta=salada')
+        self.assertEqual(req.headers.get("Cookie"), b"galleta=salada")
 
-        headers = {'Set-Cookie': 'C1=value1; path=/'}
-        res = Response('http://scrapytest.org/', headers=headers, request=req)
+        headers = {"Set-Cookie": "C1=value1; path=/"}
+        res = Response("http://scrapytest.org/", headers=headers, request=req)
         assert self.mw.process_response(req, res, self.spider) is res
 
-        req2 = Request('http://scrapytest.org/', meta=res.meta)
+        req2 = Request("http://scrapytest.org/", meta=res.meta)
         assert self.mw.process_request(req2, self.spider) is None
-        self.assertCookieValEqual(req2.headers.get('Cookie'), b'C1=value1; galleta=salada')
+        self.assertCookieValEqual(
+            req2.headers.get("Cookie"), b"C1=value1; galleta=salada"
+        )
 
         req3 = Request(
-            'http://scrapytest.org/',
-            cookies={'galleta': 'dulce'},
-            meta={'cookiejar': "store2"},
+            "http://scrapytest.org/",
+            cookies={"galleta": "dulce"},
+            meta={"cookiejar": "store2"},
         )
         assert self.mw.process_request(req3, self.spider) is None
-        self.assertEqual(req3.headers.get('Cookie'), b'galleta=dulce')
+        self.assertEqual(req3.headers.get("Cookie"), b"galleta=dulce")
 
-        headers = {'Set-Cookie': 'C2=value2; path=/'}
-        res2 = Response('http://scrapytest.org/', headers=headers, request=req3)
+        headers = {"Set-Cookie": "C2=value2; path=/"}
+        res2 = Response("http://scrapytest.org/", headers=headers, request=req3)
         assert self.mw.process_response(req3, res2, self.spider) is res2
 
-        req4 = Request('http://scrapytest.org/', meta=res2.meta)
+        req4 = Request("http://scrapytest.org/", meta=res2.meta)
         assert self.mw.process_request(req4, self.spider) is None
-        self.assertCookieValEqual(req4.headers.get('Cookie'), b'C2=value2; galleta=dulce')
+        self.assertCookieValEqual(
+            req4.headers.get("Cookie"), b"C2=value2; galleta=dulce"
+        )
 
         # cookies from hosts with port
-        req5_1 = Request('http://scrapytest.org:1104/')
+        req5_1 = Request("http://scrapytest.org:1104/")
         assert self.mw.process_request(req5_1, self.spider) is None
 
-        headers = {'Set-Cookie': 'C1=value1; path=/'}
-        res5_1 = Response('http://scrapytest.org:1104/', headers=headers, request=req5_1)
+        headers = {"Set-Cookie": "C1=value1; path=/"}
+        res5_1 = Response(
+            "http://scrapytest.org:1104/", headers=headers, request=req5_1
+        )
         assert self.mw.process_response(req5_1, res5_1, self.spider) is res5_1
 
-        req5_2 = Request('http://scrapytest.org:1104/some-redirected-path')
+        req5_2 = Request("http://scrapytest.org:1104/some-redirected-path")
         assert self.mw.process_request(req5_2, self.spider) is None
-        self.assertEqual(req5_2.headers.get('Cookie'), b'C1=value1')
+        self.assertEqual(req5_2.headers.get("Cookie"), b"C1=value1")
 
-        req5_3 = Request('http://scrapytest.org/some-redirected-path')
+        req5_3 = Request("http://scrapytest.org/some-redirected-path")
         assert self.mw.process_request(req5_3, self.spider) is None
-        self.assertEqual(req5_3.headers.get('Cookie'), b'C1=value1')
+        self.assertEqual(req5_3.headers.get("Cookie"), b"C1=value1")
 
         # skip cookie retrieval for not http request
-        req6 = Request('file:///scrapy/sometempfile')
+        req6 = Request("file:///scrapy/sometempfile")
         assert self.mw.process_request(req6, self.spider) is None
-        self.assertEqual(req6.headers.get('Cookie'), None)
+        self.assertEqual(req6.headers.get("Cookie"), None)
 
     def test_local_domain(self):
-        request = Request("http://example-host/", cookies={'currencyCookie': 'USD'})
+        request = Request("http://example-host/", cookies={"currencyCookie": "USD"})
         assert self.mw.process_request(request, self.spider) is None
-        self.assertIn('Cookie', request.headers)
-        self.assertEqual(b'currencyCookie=USD', request.headers['Cookie'])
+        self.assertIn("Cookie", request.headers)
+        self.assertEqual(b"currencyCookie=USD", request.headers["Cookie"])
 
     @pytest.mark.xfail(reason="Cookie header is not currently being processed")
     def test_keep_cookie_from_default_request_headers_middleware(self):
-        DEFAULT_REQUEST_HEADERS = dict(Cookie='default=value; asdf=qwerty')
+        DEFAULT_REQUEST_HEADERS = dict(Cookie="default=value; asdf=qwerty")
         mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())
         # overwrite with values from 'cookies' request argument
-        req1 = Request('http://example.org', cookies={'default': 'something'})
+        req1 = Request("http://example.org", cookies={"default": "something"})
         assert mw_default_headers.process_request(req1, self.spider) is None
         assert self.mw.process_request(req1, self.spider) is None
-        self.assertCookieValEqual(req1.headers['Cookie'], b'default=something; asdf=qwerty')
+        self.assertCookieValEqual(
+            req1.headers["Cookie"], b"default=something; asdf=qwerty"
+        )
         # keep both
-        req2 = Request('http://example.com', cookies={'a': 'b'})
+        req2 = Request("http://example.com", cookies={"a": "b"})
         assert mw_default_headers.process_request(req2, self.spider) is None
         assert self.mw.process_request(req2, self.spider) is None
-        self.assertCookieValEqual(req2.headers['Cookie'], b'default=value; a=b; asdf=qwerty')
+        self.assertCookieValEqual(
+            req2.headers["Cookie"], b"default=value; a=b; asdf=qwerty"
+        )
 
     @pytest.mark.xfail(reason="Cookie header is not currently being processed")
     def test_keep_cookie_header(self):
         # keep only cookies from 'Cookie' request header
-        req1 = Request('http://scrapytest.org', headers={'Cookie': 'a=b; c=d'})
+        req1 = Request("http://scrapytest.org", headers={"Cookie": "a=b; c=d"})
         assert self.mw.process_request(req1, self.spider) is None
-        self.assertCookieValEqual(req1.headers['Cookie'], 'a=b; c=d')
+        self.assertCookieValEqual(req1.headers["Cookie"], "a=b; c=d")
         # keep cookies from both 'Cookie' request header and 'cookies' keyword
-        req2 = Request('http://scrapytest.org', headers={'Cookie': 'a=b; c=d'}, cookies={'e': 'f'})
+        req2 = Request(
+            "http://scrapytest.org", headers={"Cookie": "a=b; c=d"}, cookies={"e": "f"}
+        )
         assert self.mw.process_request(req2, self.spider) is None
-        self.assertCookieValEqual(req2.headers['Cookie'], 'a=b; c=d; e=f')
+        self.assertCookieValEqual(req2.headers["Cookie"], "a=b; c=d; e=f")
         # overwrite values from 'Cookie' request header with 'cookies' keyword
         req3 = Request(
-            'http://scrapytest.org',
-            headers={'Cookie': 'a=b; c=d'},
-            cookies={'a': 'new', 'e': 'f'},
+            "http://scrapytest.org",
+            headers={"Cookie": "a=b; c=d"},
+            cookies={"a": "new", "e": "f"},
         )
         assert self.mw.process_request(req3, self.spider) is None
-        self.assertCookieValEqual(req3.headers['Cookie'], 'a=new; c=d; e=f')
+        self.assertCookieValEqual(req3.headers["Cookie"], "a=new; c=d; e=f")
 
     def test_request_cookies_encoding(self):
         # 1) UTF8-encoded bytes
-        req1 = Request('http://example.org', cookies={'a': ''.encode('utf8')})
+        req1 = Request("http://example.org", cookies={"a": "".encode("utf8")})
         assert self.mw.process_request(req1, self.spider) is None
-        self.assertCookieValEqual(req1.headers['Cookie'], b'a=\xc3\xa1')
+        self.assertCookieValEqual(req1.headers["Cookie"], b"a=\xc3\xa1")
 
         # 2) Non UTF8-encoded bytes
-        req2 = Request('http://example.org', cookies={'a': ''.encode('latin1')})
+        req2 = Request("http://example.org", cookies={"a": "".encode("latin1")})
         assert self.mw.process_request(req2, self.spider) is None
-        self.assertCookieValEqual(req2.headers['Cookie'], b'a=\xc3\xa1')
+        self.assertCookieValEqual(req2.headers["Cookie"], b"a=\xc3\xa1")
 
         # 3) String
-        req3 = Request('http://example.org', cookies={'a': ''})
+        req3 = Request("http://example.org", cookies={"a": ""})
         assert self.mw.process_request(req3, self.spider) is None
-        self.assertCookieValEqual(req3.headers['Cookie'], b'a=\xc3\xa1')
+        self.assertCookieValEqual(req3.headers["Cookie"], b"a=\xc3\xa1")
 
     @pytest.mark.xfail(reason="Cookie header is not currently being processed")
     def test_request_headers_cookie_encoding(self):
         # 1) UTF8-encoded bytes
-        req1 = Request('http://example.org', headers={'Cookie': 'a='.encode('utf8')})
+        req1 = Request("http://example.org", headers={"Cookie": "a=".encode("utf8")})
         assert self.mw.process_request(req1, self.spider) is None
-        self.assertCookieValEqual(req1.headers['Cookie'], b'a=\xc3\xa1')
+        self.assertCookieValEqual(req1.headers["Cookie"], b"a=\xc3\xa1")
 
         # 2) Non UTF8-encoded bytes
-        req2 = Request('http://example.org', headers={'Cookie': 'a='.encode('latin1')})
+        req2 = Request("http://example.org", headers={"Cookie": "a=".encode("latin1")})
         assert self.mw.process_request(req2, self.spider) is None
-        self.assertCookieValEqual(req2.headers['Cookie'], b'a=\xc3\xa1')
+        self.assertCookieValEqual(req2.headers["Cookie"], b"a=\xc3\xa1")
 
         # 3) String
-        req3 = Request('http://example.org', headers={'Cookie': 'a='})
+        req3 = Request("http://example.org", headers={"Cookie": "a="})
         assert self.mw.process_request(req3, self.spider) is None
-        self.assertCookieValEqual(req3.headers['Cookie'], b'a=\xc3\xa1')
+        self.assertCookieValEqual(req3.headers["Cookie"], b"a=\xc3\xa1")
 
     def test_invalid_cookies(self):
         """
         Invalid cookies are logged as warnings and discarded
         """
         with LogCapture(
-            'scrapy.downloadermiddlewares.cookies',
+            "scrapy.downloadermiddlewares.cookies",
             propagate=False,
             level=logging.INFO,
         ) as lc:
-            cookies1 = [{'value': 'bar'}, {'name': 'key', 'value': 'value1'}]
-            req1 = Request('http://example.org/1', cookies=cookies1)
+            cookies1 = [{"value": "bar"}, {"name": "key", "value": "value1"}]
+            req1 = Request("http://example.org/1", cookies=cookies1)
             assert self.mw.process_request(req1, self.spider) is None
-            cookies2 = [{'name': 'foo'}, {'name': 'key', 'value': 'value2'}]
-            req2 = Request('http://example.org/2', cookies=cookies2)
+            cookies2 = [{"name": "foo"}, {"name": "key", "value": "value2"}]
+            req2 = Request("http://example.org/2", cookies=cookies2)
             assert self.mw.process_request(req2, self.spider) is None
-            cookies3 = [{'name': 'foo', 'value': None}, {'name': 'key', 'value': ''}]
-            req3 = Request('http://example.org/3', cookies=cookies3)
+            cookies3 = [{"name": "foo", "value": None}, {"name": "key", "value": ""}]
+            req3 = Request("http://example.org/3", cookies=cookies3)
             assert self.mw.process_request(req3, self.spider) is None
             lc.check(
-                ("scrapy.downloadermiddlewares.cookies",
-                 "WARNING",
-                 "Invalid cookie found in request <GET http://example.org/1>:"
-                 " {'value': 'bar'} ('name' is missing)"),
-                ("scrapy.downloadermiddlewares.cookies",
-                 "WARNING",
-                 "Invalid cookie found in request <GET http://example.org/2>:"
-                 " {'name': 'foo'} ('value' is missing)"),
-                ("scrapy.downloadermiddlewares.cookies",
-                 "WARNING",
-                 "Invalid cookie found in request <GET http://example.org/3>:"
-                 " {'name': 'foo', 'value': None} ('value' is missing)"),
+                (
+                    "scrapy.downloadermiddlewares.cookies",
+                    "WARNING",
+                    "Invalid cookie found in request <GET http://example.org/1>:"
+                    " {'value': 'bar'} ('name' is missing)",
+                ),
+                (
+                    "scrapy.downloadermiddlewares.cookies",
+                    "WARNING",
+                    "Invalid cookie found in request <GET http://example.org/2>:"
+                    " {'name': 'foo'} ('value' is missing)",
+                ),
+                (
+                    "scrapy.downloadermiddlewares.cookies",
+                    "WARNING",
+                    "Invalid cookie found in request <GET http://example.org/3>:"
+                    " {'name': 'foo', 'value': None} ('value' is missing)",
+                ),
             )
-        self.assertCookieValEqual(req1.headers['Cookie'], 'key=value1')
-        self.assertCookieValEqual(req2.headers['Cookie'], 'key=value2')
-        self.assertCookieValEqual(req3.headers['Cookie'], 'key=')
+        self.assertCookieValEqual(req1.headers["Cookie"], "key=value1")
+        self.assertCookieValEqual(req2.headers["Cookie"], "key=value2")
+        self.assertCookieValEqual(req3.headers["Cookie"], "key=")
 
     def test_primitive_type_cookies(self):
         # Boolean
-        req1 = Request('http://example.org', cookies={'a': True})
+        req1 = Request("http://example.org", cookies={"a": True})
         assert self.mw.process_request(req1, self.spider) is None
-        self.assertCookieValEqual(req1.headers['Cookie'], b'a=True')
+        self.assertCookieValEqual(req1.headers["Cookie"], b"a=True")
 
         # Float
-        req2 = Request('http://example.org', cookies={'a': 9.5})
+        req2 = Request("http://example.org", cookies={"a": 9.5})
         assert self.mw.process_request(req2, self.spider) is None
-        self.assertCookieValEqual(req2.headers['Cookie'], b'a=9.5')
+        self.assertCookieValEqual(req2.headers["Cookie"], b"a=9.5")
 
         # Integer
-        req3 = Request('http://example.org', cookies={'a': 10})
+        req3 = Request("http://example.org", cookies={"a": 10})
         assert self.mw.process_request(req3, self.spider) is None
-        self.assertCookieValEqual(req3.headers['Cookie'], b'a=10')
+        self.assertCookieValEqual(req3.headers["Cookie"], b"a=10")
 
         # String
-        req4 = Request('http://example.org', cookies={'a': 'b'})
+        req4 = Request("http://example.org", cookies={"a": "b"})
         assert self.mw.process_request(req4, self.spider) is None
-        self.assertCookieValEqual(req4.headers['Cookie'], b'a=b')
+        self.assertCookieValEqual(req4.headers["Cookie"], b"a=b")
 
     def _test_cookie_redirect(
         self,
         source,
         target,
         *,
         cookies1,
         cookies2,
     ):
-        input_cookies = {'a': 'b'}
+        input_cookies = {"a": "b"}
 
         if not isinstance(source, dict):
-            source = {'url': source}
+            source = {"url": source}
         if not isinstance(target, dict):
-            target = {'url': target}
-        target.setdefault('status', 301)
+            target = {"url": target}
+        target.setdefault("status", 301)
 
         request1 = Request(cookies=input_cookies, **source)
         self.mw.process_request(request1, self.spider)
-        cookies = request1.headers.get('Cookie')
+        cookies = request1.headers.get("Cookie")
         self.assertEqual(cookies, b"a=b" if cookies1 else None)
 
         response = Response(
             headers={
-                'Location': target['url'],
+                "Location": target["url"],
             },
             **target,
         )
         self.assertEqual(
             self.mw.process_response(request1, response, self.spider),
             response,
         )
@@ -451,45 +490,45 @@
             request1,
             response,
             self.spider,
         )
         self.assertIsInstance(request2, Request)
 
         self.mw.process_request(request2, self.spider)
-        cookies = request2.headers.get('Cookie')
+        cookies = request2.headers.get("Cookie")
         self.assertEqual(cookies, b"a=b" if cookies2 else None)
 
     def test_cookie_redirect_same_domain(self):
         self._test_cookie_redirect(
-            'https://toscrape.com',
-            'https://toscrape.com',
+            "https://toscrape.com",
+            "https://toscrape.com",
             cookies1=True,
             cookies2=True,
         )
 
     def test_cookie_redirect_same_domain_forcing_get(self):
         self._test_cookie_redirect(
-            'https://toscrape.com',
-            {'url': 'https://toscrape.com', 'status': 302},
+            "https://toscrape.com",
+            {"url": "https://toscrape.com", "status": 302},
             cookies1=True,
             cookies2=True,
         )
 
     def test_cookie_redirect_different_domain(self):
         self._test_cookie_redirect(
-            'https://toscrape.com',
-            'https://example.com',
+            "https://toscrape.com",
+            "https://example.com",
             cookies1=True,
             cookies2=False,
         )
 
     def test_cookie_redirect_different_domain_forcing_get(self):
         self._test_cookie_redirect(
-            'https://toscrape.com',
-            {'url': 'https://example.com', 'status': 302},
+            "https://toscrape.com",
+            {"url": "https://example.com", "status": 302},
             cookies1=True,
             cookies2=False,
         )
 
     def _test_cookie_header_redirect(
         self,
         source,
@@ -510,125 +549,125 @@
         .. note:: This method tests the scenario where the cookie middleware is
                   disabled. Because of known issue #1992, when the cookies
                   middleware is enabled we do not need to be concerned about
                   the Cookie header getting leaked to unintended domains,
                   because the middleware empties the header from every request.
         """
         if not isinstance(source, dict):
-            source = {'url': source}
+            source = {"url": source}
         if not isinstance(target, dict):
-            target = {'url': target}
-        target.setdefault('status', 301)
+            target = {"url": target}
+        target.setdefault("status", 301)
 
-        request1 = Request(headers={'Cookie': b'a=b'}, **source)
+        request1 = Request(headers={"Cookie": b"a=b"}, **source)
 
         response = Response(
             headers={
-                'Location': target['url'],
+                "Location": target["url"],
             },
             **target,
         )
 
         request2 = self.redirect_middleware.process_response(
             request1,
             response,
             self.spider,
         )
         self.assertIsInstance(request2, Request)
 
-        cookies = request2.headers.get('Cookie')
+        cookies = request2.headers.get("Cookie")
         self.assertEqual(cookies, b"a=b" if cookies2 else None)
 
     def test_cookie_header_redirect_same_domain(self):
         self._test_cookie_header_redirect(
-            'https://toscrape.com',
-            'https://toscrape.com',
+            "https://toscrape.com",
+            "https://toscrape.com",
             cookies2=True,
         )
 
     def test_cookie_header_redirect_same_domain_forcing_get(self):
         self._test_cookie_header_redirect(
-            'https://toscrape.com',
-            {'url': 'https://toscrape.com', 'status': 302},
+            "https://toscrape.com",
+            {"url": "https://toscrape.com", "status": 302},
             cookies2=True,
         )
 
     def test_cookie_header_redirect_different_domain(self):
         self._test_cookie_header_redirect(
-            'https://toscrape.com',
-            'https://example.com',
+            "https://toscrape.com",
+            "https://example.com",
             cookies2=False,
         )
 
     def test_cookie_header_redirect_different_domain_forcing_get(self):
         self._test_cookie_header_redirect(
-            'https://toscrape.com',
-            {'url': 'https://example.com', 'status': 302},
+            "https://toscrape.com",
+            {"url": "https://example.com", "status": 302},
             cookies2=False,
         )
 
     def _test_user_set_cookie_domain_followup(
         self,
         url1,
         url2,
         domain,
         *,
         cookies1,
         cookies2,
     ):
         input_cookies = [
             {
-                'name': 'a',
-                'value': 'b',
-                'domain': domain,
+                "name": "a",
+                "value": "b",
+                "domain": domain,
             }
         ]
 
         request1 = Request(url1, cookies=input_cookies)
         self.mw.process_request(request1, self.spider)
-        cookies = request1.headers.get('Cookie')
+        cookies = request1.headers.get("Cookie")
         self.assertEqual(cookies, b"a=b" if cookies1 else None)
 
         request2 = Request(url2)
         self.mw.process_request(request2, self.spider)
-        cookies = request2.headers.get('Cookie')
+        cookies = request2.headers.get("Cookie")
         self.assertEqual(cookies, b"a=b" if cookies2 else None)
 
     def test_user_set_cookie_domain_suffix_private(self):
         self._test_user_set_cookie_domain_followup(
-            'https://books.toscrape.com',
-            'https://quotes.toscrape.com',
-            'toscrape.com',
+            "https://books.toscrape.com",
+            "https://quotes.toscrape.com",
+            "toscrape.com",
             cookies1=True,
             cookies2=True,
         )
 
     def test_user_set_cookie_domain_suffix_public_period(self):
         self._test_user_set_cookie_domain_followup(
-            'https://foo.co.uk',
-            'https://bar.co.uk',
-            'co.uk',
+            "https://foo.co.uk",
+            "https://bar.co.uk",
+            "co.uk",
             cookies1=False,
             cookies2=False,
         )
 
     def test_user_set_cookie_domain_suffix_public_private(self):
         self._test_user_set_cookie_domain_followup(
-            'https://foo.blogspot.com',
-            'https://bar.blogspot.com',
-            'blogspot.com',
+            "https://foo.blogspot.com",
+            "https://bar.blogspot.com",
+            "blogspot.com",
             cookies1=False,
             cookies2=False,
         )
 
     def test_user_set_cookie_domain_public_period(self):
         self._test_user_set_cookie_domain_followup(
-            'https://co.uk',
-            'https://co.uk',
-            'co.uk',
+            "https://co.uk",
+            "https://co.uk",
+            "co.uk",
             cookies1=True,
             cookies2=True,
         )
 
     def _test_server_set_cookie_domain_followup(
         self,
         url1,
@@ -638,58 +677,58 @@
         cookies,
     ):
         request1 = Request(url1)
         self.mw.process_request(request1, self.spider)
 
         input_cookies = [
             {
-                'name': 'a',
-                'value': 'b',
-                'domain': domain,
+                "name": "a",
+                "value": "b",
+                "domain": domain,
             }
         ]
 
         headers = {
-            'Set-Cookie': _cookies_to_set_cookie_list(input_cookies),
+            "Set-Cookie": _cookies_to_set_cookie_list(input_cookies),
         }
         response = Response(url1, status=200, headers=headers)
         self.assertEqual(
             self.mw.process_response(request1, response, self.spider),
             response,
         )
 
         request2 = Request(url2)
         self.mw.process_request(request2, self.spider)
-        actual_cookies = request2.headers.get('Cookie')
+        actual_cookies = request2.headers.get("Cookie")
         self.assertEqual(actual_cookies, b"a=b" if cookies else None)
 
     def test_server_set_cookie_domain_suffix_private(self):
         self._test_server_set_cookie_domain_followup(
-            'https://books.toscrape.com',
-            'https://quotes.toscrape.com',
-            'toscrape.com',
+            "https://books.toscrape.com",
+            "https://quotes.toscrape.com",
+            "toscrape.com",
             cookies=True,
         )
 
     def test_server_set_cookie_domain_suffix_public_period(self):
         self._test_server_set_cookie_domain_followup(
-            'https://foo.co.uk',
-            'https://bar.co.uk',
-            'co.uk',
+            "https://foo.co.uk",
+            "https://bar.co.uk",
+            "co.uk",
             cookies=False,
         )
 
     def test_server_set_cookie_domain_suffix_public_private(self):
         self._test_server_set_cookie_domain_followup(
-            'https://foo.blogspot.com',
-            'https://bar.blogspot.com',
-            'blogspot.com',
+            "https://foo.blogspot.com",
+            "https://bar.blogspot.com",
+            "blogspot.com",
             cookies=False,
         )
 
     def test_server_set_cookie_domain_public_period(self):
         self._test_server_set_cookie_domain_followup(
-            'https://co.uk',
-            'https://co.uk',
-            'co.uk',
+            "https://co.uk",
+            "https://co.uk",
+            "co.uk",
             cookies=True,
         )
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_decompression.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_decompression.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,53 +1,54 @@
 from unittest import TestCase, main
-from scrapy.http import Response, XmlResponse
+
 from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware
+from scrapy.http import Response, XmlResponse
 from scrapy.spiders import Spider
-from tests import get_testdata
 from scrapy.utils.test import assert_samelines
+from tests import get_testdata
 
 
 def _test_data(formats):
-    uncompressed_body = get_testdata('compressed', 'feed-sample1.xml')
+    uncompressed_body = get_testdata("compressed", "feed-sample1.xml")
     test_responses = {}
     for format in formats:
-        body = get_testdata('compressed', 'feed-sample1.' + format)
-        test_responses[format] = Response('http://foo.com/bar', body=body)
+        body = get_testdata("compressed", "feed-sample1." + format)
+        test_responses[format] = Response("http://foo.com/bar", body=body)
     return uncompressed_body, test_responses
 
 
 class DecompressionMiddlewareTest(TestCase):
 
-    test_formats = ['tar', 'xml.bz2', 'xml.gz', 'zip']
+    test_formats = ["tar", "xml.bz2", "xml.gz", "zip"]
     uncompressed_body, test_responses = _test_data(test_formats)
 
     def setUp(self):
         self.mw = DecompressionMiddleware()
-        self.spider = Spider('foo')
+        self.spider = Spider("foo")
 
     def test_known_compression_formats(self):
         for fmt in self.test_formats:
             rsp = self.test_responses[fmt]
             new = self.mw.process_response(None, rsp, self.spider)
-            error_msg = f'Failed {fmt}, response type {type(new).__name__}'
+            error_msg = f"Failed {fmt}, response type {type(new).__name__}"
             assert isinstance(new, XmlResponse), error_msg
             assert_samelines(self, new.body, self.uncompressed_body, fmt)
 
     def test_plain_response(self):
-        rsp = Response(url='http://test.com', body=self.uncompressed_body)
+        rsp = Response(url="http://test.com", body=self.uncompressed_body)
         new = self.mw.process_response(None, rsp, self.spider)
         assert new is rsp
         assert_samelines(self, new.body, rsp.body)
 
     def test_empty_response(self):
-        rsp = Response(url='http://test.com', body=b'')
+        rsp = Response(url="http://test.com", body=b"")
         new = self.mw.process_response(None, rsp, self.spider)
         assert new is rsp
         assert not rsp.body
         assert not new.body
 
     def tearDown(self):
         del self.mw
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_defaultheaders.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_defaultheaders.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,36 +1,35 @@
 from unittest import TestCase
 
 from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware
 from scrapy.http import Request
 from scrapy.spiders import Spider
-from scrapy.utils.test import get_crawler
 from scrapy.utils.python import to_bytes
+from scrapy.utils.test import get_crawler
 
 
 class TestDefaultHeadersMiddleware(TestCase):
-
     def get_defaults_spider_mw(self):
         crawler = get_crawler(Spider)
-        spider = crawler._create_spider('foo')
+        spider = crawler._create_spider("foo")
         defaults = {
             to_bytes(k): [to_bytes(v)]
-            for k, v in crawler.settings.get('DEFAULT_REQUEST_HEADERS').items()
+            for k, v in crawler.settings.get("DEFAULT_REQUEST_HEADERS").items()
         }
         return defaults, spider, DefaultHeadersMiddleware.from_crawler(crawler)
 
     def test_process_request(self):
         defaults, spider, mw = self.get_defaults_spider_mw()
-        req = Request('http://www.scrapytest.org')
+        req = Request("http://www.scrapytest.org")
         mw.process_request(req, spider)
         self.assertEqual(req.headers, defaults)
 
     def test_update_headers(self):
         defaults, spider, mw = self.get_defaults_spider_mw()
-        headers = {'Accept-Language': ['es'], 'Test-Header': ['test']}
-        bytes_headers = {b'Accept-Language': [b'es'], b'Test-Header': [b'test']}
-        req = Request('http://www.scrapytest.org', headers=headers)
+        headers = {"Accept-Language": ["es"], "Test-Header": ["test"]}
+        bytes_headers = {b"Accept-Language": [b"es"], b"Test-Header": [b"test"]}
+        req = Request("http://www.scrapytest.org", headers=headers)
         self.assertEqual(req.headers, bytes_headers)
 
         mw.process_request(req, spider)
         defaults.update(bytes_headers)
         self.assertEqual(req.headers, defaults)
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_downloadtimeout.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_downloadtimeout.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 import unittest
 
 from scrapy.downloadermiddlewares.downloadtimeout import DownloadTimeoutMiddleware
-from scrapy.spiders import Spider
 from scrapy.http import Request
+from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
 
 
 class DownloadTimeoutMiddlewareTest(unittest.TestCase):
-
     def get_request_spider_mw(self, settings=None):
         crawler = get_crawler(Spider, settings)
-        spider = crawler._create_spider('foo')
-        request = Request('http://scrapytest.org/')
+        spider = crawler._create_spider("foo")
+        request = Request("http://scrapytest.org/")
         return request, spider, DownloadTimeoutMiddleware.from_crawler(crawler)
 
     def test_default_download_timeout(self):
         req, spider, mw = self.get_request_spider_mw()
         mw.spider_opened(spider)
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta.get('download_timeout'), 180)
+        self.assertEqual(req.meta.get("download_timeout"), 180)
 
     def test_string_download_timeout(self):
-        req, spider, mw = self.get_request_spider_mw({'DOWNLOAD_TIMEOUT': '20.1'})
+        req, spider, mw = self.get_request_spider_mw({"DOWNLOAD_TIMEOUT": "20.1"})
         mw.spider_opened(spider)
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta.get('download_timeout'), 20.1)
+        self.assertEqual(req.meta.get("download_timeout"), 20.1)
 
     def test_spider_has_download_timeout(self):
         req, spider, mw = self.get_request_spider_mw()
         spider.download_timeout = 2
         mw.spider_opened(spider)
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta.get('download_timeout'), 2)
+        self.assertEqual(req.meta.get("download_timeout"), 2)
 
     def test_request_has_download_timeout(self):
         req, spider, mw = self.get_request_spider_mw()
         spider.download_timeout = 2
         mw.spider_opened(spider)
-        req.meta['download_timeout'] = 1
+        req.meta["download_timeout"] = 1
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta.get('download_timeout'), 1)
+        self.assertEqual(req.meta.get("download_timeout"), 1)
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_httpauth.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_httpauth.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,117 +1,115 @@
 import unittest
 
 import pytest
 from w3lib.http import basic_auth_header
 
+from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.http import Request
-from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware
 from scrapy.spiders import Spider
 
 
 class TestSpiderLegacy(Spider):
-    http_user = 'foo'
-    http_pass = 'bar'
+    http_user = "foo"
+    http_pass = "bar"
 
 
 class TestSpider(Spider):
-    http_user = 'foo'
-    http_pass = 'bar'
-    http_auth_domain = 'example.com'
+    http_user = "foo"
+    http_pass = "bar"
+    http_auth_domain = "example.com"
 
 
 class TestSpiderAny(Spider):
-    http_user = 'foo'
-    http_pass = 'bar'
+    http_user = "foo"
+    http_pass = "bar"
     http_auth_domain = None
 
 
 class HttpAuthMiddlewareLegacyTest(unittest.TestCase):
-
     def setUp(self):
-        self.spider = TestSpiderLegacy('foo')
+        self.spider = TestSpiderLegacy("foo")
 
     def test_auth(self):
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="Using HttpAuthMiddleware without http_auth_domain is deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="Using HttpAuthMiddleware without http_auth_domain is deprecated",
+        ):
             mw = HttpAuthMiddleware()
             mw.spider_opened(self.spider)
 
         # initial request, sets the domain and sends the header
-        req = Request('http://example.com/')
+        req = Request("http://example.com/")
         assert mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))
+        self.assertEqual(req.headers["Authorization"], basic_auth_header("foo", "bar"))
 
         # subsequent request to the same domain, should send the header
-        req = Request('http://example.com/')
+        req = Request("http://example.com/")
         assert mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))
+        self.assertEqual(req.headers["Authorization"], basic_auth_header("foo", "bar"))
 
         # subsequent request to a different domain, shouldn't send the header
-        req = Request('http://example-noauth.com/')
+        req = Request("http://example-noauth.com/")
         assert mw.process_request(req, self.spider) is None
-        self.assertNotIn('Authorization', req.headers)
+        self.assertNotIn("Authorization", req.headers)
 
     def test_auth_already_set(self):
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="Using HttpAuthMiddleware without http_auth_domain is deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="Using HttpAuthMiddleware without http_auth_domain is deprecated",
+        ):
             mw = HttpAuthMiddleware()
             mw.spider_opened(self.spider)
-        req = Request('http://example.com/',
-                      headers=dict(Authorization='Digest 123'))
+        req = Request("http://example.com/", headers=dict(Authorization="Digest 123"))
         assert mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers['Authorization'], b'Digest 123')
+        self.assertEqual(req.headers["Authorization"], b"Digest 123")
 
 
 class HttpAuthMiddlewareTest(unittest.TestCase):
-
     def setUp(self):
         self.mw = HttpAuthMiddleware()
-        self.spider = TestSpider('foo')
+        self.spider = TestSpider("foo")
         self.mw.spider_opened(self.spider)
 
     def tearDown(self):
         del self.mw
 
     def test_no_auth(self):
-        req = Request('http://example-noauth.com/')
+        req = Request("http://example-noauth.com/")
         assert self.mw.process_request(req, self.spider) is None
-        self.assertNotIn('Authorization', req.headers)
+        self.assertNotIn("Authorization", req.headers)
 
     def test_auth_domain(self):
-        req = Request('http://example.com/')
+        req = Request("http://example.com/")
         assert self.mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))
+        self.assertEqual(req.headers["Authorization"], basic_auth_header("foo", "bar"))
 
     def test_auth_subdomain(self):
-        req = Request('http://foo.example.com/')
+        req = Request("http://foo.example.com/")
         assert self.mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))
+        self.assertEqual(req.headers["Authorization"], basic_auth_header("foo", "bar"))
 
     def test_auth_already_set(self):
-        req = Request('http://example.com/',
-                      headers=dict(Authorization='Digest 123'))
+        req = Request("http://example.com/", headers=dict(Authorization="Digest 123"))
         assert self.mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers['Authorization'], b'Digest 123')
+        self.assertEqual(req.headers["Authorization"], b"Digest 123")
 
 
 class HttpAuthAnyMiddlewareTest(unittest.TestCase):
-
     def setUp(self):
         self.mw = HttpAuthMiddleware()
-        self.spider = TestSpiderAny('foo')
+        self.spider = TestSpiderAny("foo")
         self.mw.spider_opened(self.spider)
 
     def tearDown(self):
         del self.mw
 
     def test_auth(self):
-        req = Request('http://example.com/')
+        req = Request("http://example.com/")
         assert self.mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers['Authorization'], basic_auth_header('foo', 'bar'))
+        self.assertEqual(req.headers["Authorization"], basic_auth_header("foo", "bar"))
 
     def test_auth_already_set(self):
-        req = Request('http://example.com/',
-                      headers=dict(Authorization='Digest 123'))
+        req = Request("http://example.com/", headers=dict(Authorization="Digest 123"))
         assert self.mw.process_request(req, self.spider) is None
-        self.assertEqual(req.headers['Authorization'], b'Digest 123')
+        self.assertEqual(req.headers["Authorization"], b"Digest 123")
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_httpcache.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_httpcache.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,54 +1,55 @@
-import time
-import tempfile
+import email.utils
 import shutil
+import tempfile
+import time
 import unittest
-import email.utils
 from contextlib import contextmanager
 
-from scrapy.http import Response, HtmlResponse, Request
-from scrapy.spiders import Spider
-from scrapy.settings import Settings
+from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware
 from scrapy.exceptions import IgnoreRequest
+from scrapy.http import HtmlResponse, Request, Response
+from scrapy.settings import Settings
+from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
-from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware
 
 
 class _BaseTest(unittest.TestCase):
 
-    storage_class = 'scrapy.extensions.httpcache.DbmCacheStorage'
-    policy_class = 'scrapy.extensions.httpcache.RFC2616Policy'
+    storage_class = "scrapy.extensions.httpcache.DbmCacheStorage"
+    policy_class = "scrapy.extensions.httpcache.RFC2616Policy"
 
     def setUp(self):
         self.yesterday = email.utils.formatdate(time.time() - 86400)
         self.today = email.utils.formatdate()
         self.tomorrow = email.utils.formatdate(time.time() + 86400)
         self.crawler = get_crawler(Spider)
-        self.spider = self.crawler._create_spider('example.com')
+        self.spider = self.crawler._create_spider("example.com")
         self.tmpdir = tempfile.mkdtemp()
-        self.request = Request('http://www.example.com',
-                               headers={'User-Agent': 'test'})
-        self.response = Response('http://www.example.com',
-                                 headers={'Content-Type': 'text/html'},
-                                 body=b'test body',
-                                 status=202)
+        self.request = Request("http://www.example.com", headers={"User-Agent": "test"})
+        self.response = Response(
+            "http://www.example.com",
+            headers={"Content-Type": "text/html"},
+            body=b"test body",
+            status=202,
+        )
         self.crawler.stats.open_spider(self.spider)
 
     def tearDown(self):
-        self.crawler.stats.close_spider(self.spider, '')
+        self.crawler.stats.close_spider(self.spider, "")
         shutil.rmtree(self.tmpdir)
 
     def _get_settings(self, **new_settings):
         settings = {
-            'HTTPCACHE_ENABLED': True,
-            'HTTPCACHE_DIR': self.tmpdir,
-            'HTTPCACHE_EXPIRATION_SECS': 1,
-            'HTTPCACHE_IGNORE_HTTP_CODES': [],
-            'HTTPCACHE_POLICY': self.policy_class,
-            'HTTPCACHE_STORAGE': self.storage_class,
+            "HTTPCACHE_ENABLED": True,
+            "HTTPCACHE_DIR": self.tmpdir,
+            "HTTPCACHE_EXPIRATION_SECS": 1,
+            "HTTPCACHE_IGNORE_HTTP_CODES": [],
+            "HTTPCACHE_POLICY": self.policy_class,
+            "HTTPCACHE_STORAGE": self.storage_class,
         }
         settings.update(new_settings)
         return Settings(settings)
 
     @contextmanager
     def _storage(self, **new_settings):
         with self._middleware(**new_settings) as mw:
@@ -78,34 +79,40 @@
     def assertEqualRequest(self, request1, request2):
         self.assertEqual(request1.url, request2.url)
         self.assertEqual(request1.headers, request2.headers)
         self.assertEqual(request1.body, request2.body)
 
     def assertEqualRequestButWithCacheValidators(self, request1, request2):
         self.assertEqual(request1.url, request2.url)
-        assert b'If-None-Match' not in request1.headers
-        assert b'If-Modified-Since' not in request1.headers
-        assert any(h in request2.headers for h in (b'If-None-Match', b'If-Modified-Since'))
+        assert b"If-None-Match" not in request1.headers
+        assert b"If-Modified-Since" not in request1.headers
+        assert any(
+            h in request2.headers for h in (b"If-None-Match", b"If-Modified-Since")
+        )
         self.assertEqual(request1.body, request2.body)
 
     def test_dont_cache(self):
         with self._middleware() as mw:
-            self.request.meta['dont_cache'] = True
+            self.request.meta["dont_cache"] = True
             mw.process_response(self.request, self.response, self.spider)
-            self.assertEqual(mw.storage.retrieve_response(self.spider, self.request), None)
+            self.assertEqual(
+                mw.storage.retrieve_response(self.spider, self.request), None
+            )
 
         with self._middleware() as mw:
-            self.request.meta['dont_cache'] = False
+            self.request.meta["dont_cache"] = False
             mw.process_response(self.request, self.response, self.spider)
             if mw.policy.should_cache_response(self.response, self.request):
-                self.assertIsInstance(mw.storage.retrieve_response(self.spider, self.request), self.response.__class__)
+                self.assertIsInstance(
+                    mw.storage.retrieve_response(self.spider, self.request),
+                    self.response.__class__,
+                )
 
 
 class DefaultStorageTest(_BaseTest):
-
     def test_storage(self):
         with self._storage() as storage:
             request2 = self.request.copy()
             assert storage.retrieve_response(self.spider, request2) is None
 
             storage.store_response(self.spider, self.request, self.response)
             response2 = storage.retrieve_response(self.spider, request2)
@@ -124,123 +131,124 @@
 
     def test_storage_no_content_type_header(self):
         """Test that the response body is used to get the right response class
         even if there is no Content-Type header"""
         with self._storage() as storage:
             assert storage.retrieve_response(self.spider, self.request) is None
             response = Response(
-                'http://www.example.com',
-                body=b'<!DOCTYPE html>\n<title>.</title>',
+                "http://www.example.com",
+                body=b"<!DOCTYPE html>\n<title>.</title>",
                 status=202,
             )
             storage.store_response(self.spider, self.request, response)
             cached_response = storage.retrieve_response(self.spider, self.request)
             self.assertIsInstance(cached_response, HtmlResponse)
             self.assertEqualResponse(response, cached_response)
 
 
 class DbmStorageTest(DefaultStorageTest):
 
-    storage_class = 'scrapy.extensions.httpcache.DbmCacheStorage'
+    storage_class = "scrapy.extensions.httpcache.DbmCacheStorage"
 
 
 class DbmStorageWithCustomDbmModuleTest(DbmStorageTest):
 
-    dbm_module = 'tests.mocks.dummydbm'
+    dbm_module = "tests.mocks.dummydbm"
 
     def _get_settings(self, **new_settings):
-        new_settings.setdefault('HTTPCACHE_DBM_MODULE', self.dbm_module)
+        new_settings.setdefault("HTTPCACHE_DBM_MODULE", self.dbm_module)
         return super()._get_settings(**new_settings)
 
     def test_custom_dbm_module_loaded(self):
         # make sure our dbm module has been loaded
         with self._storage() as storage:
             self.assertEqual(storage.dbmodule.__name__, self.dbm_module)
 
 
 class FilesystemStorageTest(DefaultStorageTest):
 
-    storage_class = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
+    storage_class = "scrapy.extensions.httpcache.FilesystemCacheStorage"
 
 
 class FilesystemStorageGzipTest(FilesystemStorageTest):
-
     def _get_settings(self, **new_settings):
-        new_settings.setdefault('HTTPCACHE_GZIP', True)
+        new_settings.setdefault("HTTPCACHE_GZIP", True)
         return super()._get_settings(**new_settings)
 
 
 class DummyPolicyTest(_BaseTest):
 
-    policy_class = 'scrapy.extensions.httpcache.DummyPolicy'
+    policy_class = "scrapy.extensions.httpcache.DummyPolicy"
 
     def test_middleware(self):
         with self._middleware() as mw:
             assert mw.process_request(self.request, self.spider) is None
             mw.process_response(self.request, self.response, self.spider)
             response = mw.process_request(self.request, self.spider)
             assert isinstance(response, HtmlResponse)
             self.assertEqualResponse(self.response, response)
-            assert 'cached' in response.flags
+            assert "cached" in response.flags
 
     def test_different_request_response_urls(self):
         with self._middleware() as mw:
-            req = Request('http://host.com/path')
-            res = Response('http://host2.net/test.html')
+            req = Request("http://host.com/path")
+            res = Response("http://host2.net/test.html")
             assert mw.process_request(req, self.spider) is None
             mw.process_response(req, res, self.spider)
             cached = mw.process_request(req, self.spider)
             assert isinstance(cached, Response)
             self.assertEqualResponse(res, cached)
-            assert 'cached' in cached.flags
+            assert "cached" in cached.flags
 
     def test_middleware_ignore_missing(self):
         with self._middleware(HTTPCACHE_IGNORE_MISSING=True) as mw:
-            self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)
+            self.assertRaises(
+                IgnoreRequest, mw.process_request, self.request, self.spider
+            )
             mw.process_response(self.request, self.response, self.spider)
             response = mw.process_request(self.request, self.spider)
             assert isinstance(response, HtmlResponse)
             self.assertEqualResponse(self.response, response)
-            assert 'cached' in response.flags
+            assert "cached" in response.flags
 
     def test_middleware_ignore_schemes(self):
         # http responses are cached by default
-        req, res = Request('http://test.com/'), Response('http://test.com/')
+        req, res = Request("http://test.com/"), Response("http://test.com/")
         with self._middleware() as mw:
             assert mw.process_request(req, self.spider) is None
             mw.process_response(req, res, self.spider)
 
             cached = mw.process_request(req, self.spider)
             assert isinstance(cached, Response), type(cached)
             self.assertEqualResponse(res, cached)
-            assert 'cached' in cached.flags
+            assert "cached" in cached.flags
 
         # file response is not cached by default
-        req, res = Request('file:///tmp/t.txt'), Response('file:///tmp/t.txt')
+        req, res = Request("file:///tmp/t.txt"), Response("file:///tmp/t.txt")
         with self._middleware() as mw:
             assert mw.process_request(req, self.spider) is None
             mw.process_response(req, res, self.spider)
 
             assert mw.storage.retrieve_response(self.spider, req) is None
             assert mw.process_request(req, self.spider) is None
 
         # s3 scheme response is cached by default
-        req, res = Request('s3://bucket/key'), Response('http://bucket/key')
+        req, res = Request("s3://bucket/key"), Response("http://bucket/key")
         with self._middleware() as mw:
             assert mw.process_request(req, self.spider) is None
             mw.process_response(req, res, self.spider)
 
             cached = mw.process_request(req, self.spider)
             assert isinstance(cached, Response), type(cached)
             self.assertEqualResponse(res, cached)
-            assert 'cached' in cached.flags
+            assert "cached" in cached.flags
 
         # ignore s3 scheme
-        req, res = Request('s3://bucket/key2'), Response('http://bucket/key2')
-        with self._middleware(HTTPCACHE_IGNORE_SCHEMES=['s3']) as mw:
+        req, res = Request("s3://bucket/key2"), Response("http://bucket/key2")
+        with self._middleware(HTTPCACHE_IGNORE_SCHEMES=["s3"]) as mw:
             assert mw.process_request(req, self.spider) is None
             mw.process_response(req, res, self.spider)
 
             assert mw.storage.retrieve_response(self.spider, req) is None
             assert mw.process_request(req, self.spider) is None
 
     def test_middleware_ignore_http_codes(self):
@@ -254,268 +262,317 @@
 
         # test response is cached
         with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[203]) as mw:
             mw.process_response(self.request, self.response, self.spider)
             response = mw.process_request(self.request, self.spider)
             assert isinstance(response, HtmlResponse)
             self.assertEqualResponse(self.response, response)
-            assert 'cached' in response.flags
+            assert "cached" in response.flags
 
 
 class RFC2616PolicyTest(DefaultStorageTest):
 
-    policy_class = 'scrapy.extensions.httpcache.RFC2616Policy'
+    policy_class = "scrapy.extensions.httpcache.RFC2616Policy"
 
     def _process_requestresponse(self, mw, request, response):
         result = None
         try:
             result = mw.process_request(request, self.spider)
             if result:
                 assert isinstance(result, (Request, Response))
                 return result
-            else:
-                result = mw.process_response(request, response, self.spider)
-                assert isinstance(result, Response)
-                return result
+            result = mw.process_response(request, response, self.spider)
+            assert isinstance(result, Response)
+            return result
         except Exception:
-            print('Request', request)
-            print('Response', response)
-            print('Result', result)
+            print("Request", request)
+            print("Response", response)
+            print("Result", result)
             raise
 
     def test_request_cacheability(self):
-        res0 = Response(self.request.url, status=200,
-                        headers={'Expires': self.tomorrow})
-        req0 = Request('http://example.com')
-        req1 = req0.replace(headers={'Cache-Control': 'no-store'})
-        req2 = req0.replace(headers={'Cache-Control': 'no-cache'})
+        res0 = Response(
+            self.request.url, status=200, headers={"Expires": self.tomorrow}
+        )
+        req0 = Request("http://example.com")
+        req1 = req0.replace(headers={"Cache-Control": "no-store"})
+        req2 = req0.replace(headers={"Cache-Control": "no-cache"})
         with self._middleware() as mw:
             # response for a request with no-store must not be cached
             res1 = self._process_requestresponse(mw, req1, res0)
             self.assertEqualResponse(res1, res0)
             assert mw.storage.retrieve_response(self.spider, req1) is None
             # Re-do request without no-store and expect it to be cached
             res2 = self._process_requestresponse(mw, req0, res0)
-            assert 'cached' not in res2.flags
+            assert "cached" not in res2.flags
             res3 = mw.process_request(req0, self.spider)
-            assert 'cached' in res3.flags
+            assert "cached" in res3.flags
             self.assertEqualResponse(res2, res3)
             # request with no-cache directive must not return cached response
             # but it allows new response to be stored
-            res0b = res0.replace(body=b'foo')
+            res0b = res0.replace(body=b"foo")
             res4 = self._process_requestresponse(mw, req2, res0b)
             self.assertEqualResponse(res4, res0b)
-            assert 'cached' not in res4.flags
+            assert "cached" not in res4.flags
             res5 = self._process_requestresponse(mw, req0, None)
             self.assertEqualResponse(res5, res0b)
-            assert 'cached' in res5.flags
+            assert "cached" in res5.flags
 
     def test_response_cacheability(self):
         responses = [
             # 304 is not cacheable no matter what servers sends
             (False, 304, {}),
-            (False, 304, {'Last-Modified': self.yesterday}),
-            (False, 304, {'Expires': self.tomorrow}),
-            (False, 304, {'Etag': 'bar'}),
-            (False, 304, {'Cache-Control': 'max-age=3600'}),
+            (False, 304, {"Last-Modified": self.yesterday}),
+            (False, 304, {"Expires": self.tomorrow}),
+            (False, 304, {"Etag": "bar"}),
+            (False, 304, {"Cache-Control": "max-age=3600"}),
             # Always obey no-store cache control
-            (False, 200, {'Cache-Control': 'no-store'}),
-            (False, 200, {'Cache-Control': 'no-store, max-age=300'}),  # invalid
-            (False, 200, {'Cache-Control': 'no-store', 'Expires': self.tomorrow}),  # invalid
+            (False, 200, {"Cache-Control": "no-store"}),
+            (False, 200, {"Cache-Control": "no-store, max-age=300"}),  # invalid
+            (
+                False,
+                200,
+                {"Cache-Control": "no-store", "Expires": self.tomorrow},
+            ),  # invalid
             # Ignore responses missing expiration and/or validation headers
             (False, 200, {}),
             (False, 302, {}),
             (False, 307, {}),
             (False, 404, {}),
             # Cache responses with expiration and/or validation headers
-            (True, 200, {'Last-Modified': self.yesterday}),
-            (True, 203, {'Last-Modified': self.yesterday}),
-            (True, 300, {'Last-Modified': self.yesterday}),
-            (True, 301, {'Last-Modified': self.yesterday}),
-            (True, 308, {'Last-Modified': self.yesterday}),
-            (True, 401, {'Last-Modified': self.yesterday}),
-            (True, 404, {'Cache-Control': 'public, max-age=600'}),
-            (True, 302, {'Expires': self.tomorrow}),
-            (True, 200, {'Etag': 'foo'}),
+            (True, 200, {"Last-Modified": self.yesterday}),
+            (True, 203, {"Last-Modified": self.yesterday}),
+            (True, 300, {"Last-Modified": self.yesterday}),
+            (True, 301, {"Last-Modified": self.yesterday}),
+            (True, 308, {"Last-Modified": self.yesterday}),
+            (True, 401, {"Last-Modified": self.yesterday}),
+            (True, 404, {"Cache-Control": "public, max-age=600"}),
+            (True, 302, {"Expires": self.tomorrow}),
+            (True, 200, {"Etag": "foo"}),
         ]
         with self._middleware() as mw:
             for idx, (shouldcache, status, headers) in enumerate(responses):
-                req0 = Request(f'http://example-{idx}.com')
+                req0 = Request(f"http://example-{idx}.com")
                 res0 = Response(req0.url, status=status, headers=headers)
                 res1 = self._process_requestresponse(mw, req0, res0)
                 res304 = res0.replace(status=304)
-                res2 = self._process_requestresponse(mw, req0, res304 if shouldcache else res0)
+                res2 = self._process_requestresponse(
+                    mw, req0, res304 if shouldcache else res0
+                )
                 self.assertEqualResponse(res1, res0)
                 self.assertEqualResponse(res2, res0)
                 resc = mw.storage.retrieve_response(self.spider, req0)
                 if shouldcache:
                     self.assertEqualResponse(resc, res1)
-                    assert 'cached' in res2.flags and res2.status != 304
+                    assert "cached" in res2.flags and res2.status != 304
                 else:
                     self.assertFalse(resc)
-                    assert 'cached' not in res2.flags
+                    assert "cached" not in res2.flags
 
         # cache unconditionally unless response contains no-store or is a 304
         with self._middleware(HTTPCACHE_ALWAYS_STORE=True) as mw:
             for idx, (_, status, headers) in enumerate(responses):
-                shouldcache = 'no-store' not in headers.get('Cache-Control', '') and status != 304
-                req0 = Request(f'http://example2-{idx}.com')
+                shouldcache = (
+                    "no-store" not in headers.get("Cache-Control", "") and status != 304
+                )
+                req0 = Request(f"http://example2-{idx}.com")
                 res0 = Response(req0.url, status=status, headers=headers)
                 res1 = self._process_requestresponse(mw, req0, res0)
                 res304 = res0.replace(status=304)
-                res2 = self._process_requestresponse(mw, req0, res304 if shouldcache else res0)
+                res2 = self._process_requestresponse(
+                    mw, req0, res304 if shouldcache else res0
+                )
                 self.assertEqualResponse(res1, res0)
                 self.assertEqualResponse(res2, res0)
                 resc = mw.storage.retrieve_response(self.spider, req0)
                 if shouldcache:
                     self.assertEqualResponse(resc, res1)
-                    assert 'cached' in res2.flags and res2.status != 304
+                    assert "cached" in res2.flags and res2.status != 304
                 else:
                     self.assertFalse(resc)
-                    assert 'cached' not in res2.flags
+                    assert "cached" not in res2.flags
 
     def test_cached_and_fresh(self):
         sampledata = [
-            (200, {'Date': self.yesterday, 'Expires': self.tomorrow}),
-            (200, {'Date': self.yesterday, 'Cache-Control': 'max-age=86405'}),
-            (200, {'Age': '299', 'Cache-Control': 'max-age=300'}),
+            (200, {"Date": self.yesterday, "Expires": self.tomorrow}),
+            (200, {"Date": self.yesterday, "Cache-Control": "max-age=86405"}),
+            (200, {"Age": "299", "Cache-Control": "max-age=300"}),
             # Obey max-age if present over any others
-            (200, {'Date': self.today,
-                   'Age': '86405',
-                   'Cache-Control': 'max-age=' + str(86400 * 3),
-                   'Expires': self.yesterday,
-                   'Last-Modified': self.yesterday,
-                   }),
+            (
+                200,
+                {
+                    "Date": self.today,
+                    "Age": "86405",
+                    "Cache-Control": "max-age=" + str(86400 * 3),
+                    "Expires": self.yesterday,
+                    "Last-Modified": self.yesterday,
+                },
+            ),
             # obey Expires if max-age is not present
-            (200, {'Date': self.yesterday,
-                   'Age': '86400',
-                   'Cache-Control': 'public',
-                   'Expires': self.tomorrow,
-                   'Last-Modified': self.yesterday,
-                   }),
+            (
+                200,
+                {
+                    "Date": self.yesterday,
+                    "Age": "86400",
+                    "Cache-Control": "public",
+                    "Expires": self.tomorrow,
+                    "Last-Modified": self.yesterday,
+                },
+            ),
             # Default missing Date header to right now
-            (200, {'Expires': self.tomorrow}),
+            (200, {"Expires": self.tomorrow}),
             # Firefox - Expires if age is greater than 10% of (Date - Last-Modified)
-            (200, {'Date': self.today, 'Last-Modified': self.yesterday, 'Age': str(86400 / 10 - 1)}),
+            (
+                200,
+                {
+                    "Date": self.today,
+                    "Last-Modified": self.yesterday,
+                    "Age": str(86400 / 10 - 1),
+                },
+            ),
             # Firefox - Set one year maxage to permanent redirects missing expiration info
-            (300, {}), (301, {}), (308, {}),
+            (300, {}),
+            (301, {}),
+            (308, {}),
         ]
         with self._middleware() as mw:
             for idx, (status, headers) in enumerate(sampledata):
-                req0 = Request(f'http://example-{idx}.com')
+                req0 = Request(f"http://example-{idx}.com")
                 res0 = Response(req0.url, status=status, headers=headers)
                 # cache fresh response
                 res1 = self._process_requestresponse(mw, req0, res0)
                 self.assertEqualResponse(res1, res0)
-                assert 'cached' not in res1.flags
+                assert "cached" not in res1.flags
                 # return fresh cached response without network interaction
                 res2 = self._process_requestresponse(mw, req0, None)
                 self.assertEqualResponse(res1, res2)
-                assert 'cached' in res2.flags
+                assert "cached" in res2.flags
                 # validate cached response if request max-age set as 0
-                req1 = req0.replace(headers={'Cache-Control': 'max-age=0'})
+                req1 = req0.replace(headers={"Cache-Control": "max-age=0"})
                 res304 = res0.replace(status=304)
                 assert mw.process_request(req1, self.spider) is None
                 res3 = self._process_requestresponse(mw, req1, res304)
                 self.assertEqualResponse(res1, res3)
-                assert 'cached' in res3.flags
+                assert "cached" in res3.flags
 
     def test_cached_and_stale(self):
         sampledata = [
-            (200, {'Date': self.today, 'Expires': self.yesterday}),
-            (200, {'Date': self.today, 'Expires': self.yesterday, 'Last-Modified': self.yesterday}),
-            (200, {'Expires': self.yesterday}),
-            (200, {'Expires': self.yesterday, 'ETag': 'foo'}),
-            (200, {'Expires': self.yesterday, 'Last-Modified': self.yesterday}),
-            (200, {'Expires': self.tomorrow, 'Age': '86405'}),
-            (200, {'Cache-Control': 'max-age=86400', 'Age': '86405'}),
+            (200, {"Date": self.today, "Expires": self.yesterday}),
+            (
+                200,
+                {
+                    "Date": self.today,
+                    "Expires": self.yesterday,
+                    "Last-Modified": self.yesterday,
+                },
+            ),
+            (200, {"Expires": self.yesterday}),
+            (200, {"Expires": self.yesterday, "ETag": "foo"}),
+            (200, {"Expires": self.yesterday, "Last-Modified": self.yesterday}),
+            (200, {"Expires": self.tomorrow, "Age": "86405"}),
+            (200, {"Cache-Control": "max-age=86400", "Age": "86405"}),
             # no-cache forces expiration, also revalidation if validators exists
-            (200, {'Cache-Control': 'no-cache'}),
-            (200, {'Cache-Control': 'no-cache', 'ETag': 'foo'}),
-            (200, {'Cache-Control': 'no-cache', 'Last-Modified': self.yesterday}),
-            (200, {'Cache-Control': 'no-cache,must-revalidate', 'Last-Modified': self.yesterday}),
-            (200, {'Cache-Control': 'must-revalidate', 'Expires': self.yesterday, 'Last-Modified': self.yesterday}),
-            (200, {'Cache-Control': 'max-age=86400,must-revalidate', 'Age': '86405'}),
+            (200, {"Cache-Control": "no-cache"}),
+            (200, {"Cache-Control": "no-cache", "ETag": "foo"}),
+            (200, {"Cache-Control": "no-cache", "Last-Modified": self.yesterday}),
+            (
+                200,
+                {
+                    "Cache-Control": "no-cache,must-revalidate",
+                    "Last-Modified": self.yesterday,
+                },
+            ),
+            (
+                200,
+                {
+                    "Cache-Control": "must-revalidate",
+                    "Expires": self.yesterday,
+                    "Last-Modified": self.yesterday,
+                },
+            ),
+            (200, {"Cache-Control": "max-age=86400,must-revalidate", "Age": "86405"}),
         ]
         with self._middleware() as mw:
             for idx, (status, headers) in enumerate(sampledata):
-                req0 = Request(f'http://example-{idx}.com')
+                req0 = Request(f"http://example-{idx}.com")
                 res0a = Response(req0.url, status=status, headers=headers)
                 # cache expired response
                 res1 = self._process_requestresponse(mw, req0, res0a)
                 self.assertEqualResponse(res1, res0a)
-                assert 'cached' not in res1.flags
+                assert "cached" not in res1.flags
                 # Same request but as cached response is stale a new response must
                 # be returned
-                res0b = res0a.replace(body=b'bar')
+                res0b = res0a.replace(body=b"bar")
                 res2 = self._process_requestresponse(mw, req0, res0b)
                 self.assertEqualResponse(res2, res0b)
-                assert 'cached' not in res2.flags
-                cc = headers.get('Cache-Control', '')
+                assert "cached" not in res2.flags
+                cc = headers.get("Cache-Control", "")
                 # Previous response expired too, subsequent request to same
                 # resource must revalidate and succeed on 304 if validators
                 # are present
-                if 'ETag' in headers or 'Last-Modified' in headers:
+                if "ETag" in headers or "Last-Modified" in headers:
                     res0c = res0b.replace(status=304)
                     res3 = self._process_requestresponse(mw, req0, res0c)
                     self.assertEqualResponse(res3, res0b)
-                    assert 'cached' in res3.flags
+                    assert "cached" in res3.flags
                     # get cached response on server errors unless must-revalidate
                     # in cached response
                     res0d = res0b.replace(status=500)
                     res4 = self._process_requestresponse(mw, req0, res0d)
-                    if 'must-revalidate' in cc:
-                        assert 'cached' not in res4.flags
+                    if "must-revalidate" in cc:
+                        assert "cached" not in res4.flags
                         self.assertEqualResponse(res4, res0d)
                     else:
-                        assert 'cached' in res4.flags
+                        assert "cached" in res4.flags
                         self.assertEqualResponse(res4, res0b)
                 # Requests with max-stale can fetch expired cached responses
                 # unless cached response has must-revalidate
-                req1 = req0.replace(headers={'Cache-Control': 'max-stale'})
+                req1 = req0.replace(headers={"Cache-Control": "max-stale"})
                 res5 = self._process_requestresponse(mw, req1, res0b)
                 self.assertEqualResponse(res5, res0b)
-                if 'no-cache' in cc or 'must-revalidate' in cc:
-                    assert 'cached' not in res5.flags
+                if "no-cache" in cc or "must-revalidate" in cc:
+                    assert "cached" not in res5.flags
                 else:
-                    assert 'cached' in res5.flags
+                    assert "cached" in res5.flags
 
     def test_process_exception(self):
         with self._middleware() as mw:
-            res0 = Response(self.request.url, headers={'Expires': self.yesterday})
+            res0 = Response(self.request.url, headers={"Expires": self.yesterday})
             req0 = Request(self.request.url)
             self._process_requestresponse(mw, req0, res0)
             for e in mw.DOWNLOAD_EXCEPTIONS:
                 # Simulate encountering an error on download attempts
                 assert mw.process_request(req0, self.spider) is None
-                res1 = mw.process_exception(req0, e('foo'), self.spider)
+                res1 = mw.process_exception(req0, e("foo"), self.spider)
                 # Use cached response as recovery
-                assert 'cached' in res1.flags
+                assert "cached" in res1.flags
                 self.assertEqualResponse(res0, res1)
             # Do not use cached response for unhandled exceptions
             mw.process_request(req0, self.spider)
-            assert mw.process_exception(req0, Exception('foo'), self.spider) is None
+            assert mw.process_exception(req0, Exception("foo"), self.spider) is None
 
     def test_ignore_response_cache_controls(self):
         sampledata = [
-            (200, {'Date': self.yesterday, 'Expires': self.tomorrow}),
-            (200, {'Date': self.yesterday, 'Cache-Control': 'no-store,max-age=86405'}),
-            (200, {'Age': '299', 'Cache-Control': 'max-age=300,no-cache'}),
-            (300, {'Cache-Control': 'no-cache'}),
-            (200, {'Expires': self.tomorrow, 'Cache-Control': 'no-store'}),
+            (200, {"Date": self.yesterday, "Expires": self.tomorrow}),
+            (200, {"Date": self.yesterday, "Cache-Control": "no-store,max-age=86405"}),
+            (200, {"Age": "299", "Cache-Control": "max-age=300,no-cache"}),
+            (300, {"Cache-Control": "no-cache"}),
+            (200, {"Expires": self.tomorrow, "Cache-Control": "no-store"}),
         ]
-        with self._middleware(HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=['no-cache', 'no-store']) as mw:
+        with self._middleware(
+            HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=["no-cache", "no-store"]
+        ) as mw:
             for idx, (status, headers) in enumerate(sampledata):
-                req0 = Request(f'http://example-{idx}.com')
+                req0 = Request(f"http://example-{idx}.com")
                 res0 = Response(req0.url, status=status, headers=headers)
                 # cache fresh response
                 res1 = self._process_requestresponse(mw, req0, res0)
                 self.assertEqualResponse(res1, res0)
-                assert 'cached' not in res1.flags
+                assert "cached" not in res1.flags
                 # return fresh cached response without network interaction
                 res2 = self._process_requestresponse(mw, req0, None)
                 self.assertEqualResponse(res1, res2)
-                assert 'cached' in res2.flags
+                assert "cached" in res2.flags
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_httpcompression.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_httpcompression.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,311 +1,332 @@
 from gzip import GzipFile
 from io import BytesIO
-from os.path import join
-from unittest import TestCase, SkipTest
+from pathlib import Path
+from unittest import SkipTest, TestCase
 from warnings import catch_warnings
 
-from scrapy.spiders import Spider
-from scrapy.http import Response, Request, HtmlResponse
-from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, ACCEPTED_ENCODINGS
+from w3lib.encoding import resolve_encoding
+
+from scrapy.downloadermiddlewares.httpcompression import (
+    ACCEPTED_ENCODINGS,
+    HttpCompressionMiddleware,
+)
 from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning
+from scrapy.http import HtmlResponse, Request, Response
 from scrapy.responsetypes import responsetypes
+from scrapy.spiders import Spider
 from scrapy.utils.gz import gunzip
 from scrapy.utils.test import get_crawler
 from tests import tests_datadir
-from w3lib.encoding import resolve_encoding
-
 
-SAMPLEDIR = join(tests_datadir, 'compressed')
+SAMPLEDIR = Path(tests_datadir, "compressed")
 
 FORMAT = {
-    'gzip': ('html-gzip.bin', 'gzip'),
-    'x-gzip': ('html-gzip.bin', 'gzip'),
-    'rawdeflate': ('html-rawdeflate.bin', 'deflate'),
-    'zlibdeflate': ('html-zlibdeflate.bin', 'deflate'),
-    'br': ('html-br.bin', 'br'),
+    "gzip": ("html-gzip.bin", "gzip"),
+    "x-gzip": ("html-gzip.bin", "gzip"),
+    "rawdeflate": ("html-rawdeflate.bin", "deflate"),
+    "zlibdeflate": ("html-zlibdeflate.bin", "deflate"),
+    "br": ("html-br.bin", "br"),
     # $ zstd raw.html --content-size -o html-zstd-static-content-size.bin
-    'zstd-static-content-size': ('html-zstd-static-content-size.bin', 'zstd'),
+    "zstd-static-content-size": ("html-zstd-static-content-size.bin", "zstd"),
     # $ zstd raw.html --no-content-size -o html-zstd-static-no-content-size.bin
-    'zstd-static-no-content-size': ('html-zstd-static-no-content-size.bin', 'zstd'),
+    "zstd-static-no-content-size": ("html-zstd-static-no-content-size.bin", "zstd"),
     # $ cat raw.html | zstd -o html-zstd-streaming-no-content-size.bin
-    'zstd-streaming-no-content-size': ('html-zstd-streaming-no-content-size.bin', 'zstd'),
+    "zstd-streaming-no-content-size": (
+        "html-zstd-streaming-no-content-size.bin",
+        "zstd",
+    ),
 }
 
 
 class HttpCompressionTest(TestCase):
-
     def setUp(self):
         self.crawler = get_crawler(Spider)
-        self.spider = self.crawler._create_spider('scrapytest.org')
+        self.spider = self.crawler._create_spider("scrapytest.org")
         self.mw = HttpCompressionMiddleware.from_crawler(self.crawler)
         self.crawler.stats.open_spider(self.spider)
 
     def _getresponse(self, coding):
         if coding not in FORMAT:
             raise ValueError()
 
         samplefile, contentencoding = FORMAT[coding]
 
-        with open(join(SAMPLEDIR, samplefile), 'rb') as sample:
-            body = sample.read()
+        body = (SAMPLEDIR / samplefile).read_bytes()
 
         headers = {
-            'Server': 'Yaws/1.49 Yet Another Web Server',
-            'Date': 'Sun, 08 Mar 2009 00:41:03 GMT',
-            'Content-Length': len(body),
-            'Content-Type': 'text/html',
-            'Content-Encoding': contentencoding,
+            "Server": "Yaws/1.49 Yet Another Web Server",
+            "Date": "Sun, 08 Mar 2009 00:41:03 GMT",
+            "Content-Length": len(body),
+            "Content-Type": "text/html",
+            "Content-Encoding": contentencoding,
         }
 
-        response = Response('http://scrapytest.org/', body=body, headers=headers)
-        response.request = Request('http://scrapytest.org', headers={'Accept-Encoding': 'gzip, deflate'})
+        response = Response("http://scrapytest.org/", body=body, headers=headers)
+        response.request = Request(
+            "http://scrapytest.org", headers={"Accept-Encoding": "gzip, deflate"}
+        )
         return response
 
     def assertStatsEqual(self, key, value):
         self.assertEqual(
             self.crawler.stats.get_value(key, spider=self.spider),
             value,
-            str(self.crawler.stats.get_stats(self.spider))
+            str(self.crawler.stats.get_stats(self.spider)),
         )
 
     def test_setting_false_compression_enabled(self):
         self.assertRaises(
             NotConfigured,
             HttpCompressionMiddleware.from_crawler,
-            get_crawler(settings_dict={'COMPRESSION_ENABLED': False})
+            get_crawler(settings_dict={"COMPRESSION_ENABLED": False}),
         )
 
     def test_setting_default_compression_enabled(self):
         self.assertIsInstance(
             HttpCompressionMiddleware.from_crawler(get_crawler()),
-            HttpCompressionMiddleware
+            HttpCompressionMiddleware,
         )
 
     def test_setting_true_compression_enabled(self):
         self.assertIsInstance(
             HttpCompressionMiddleware.from_crawler(
-                get_crawler(settings_dict={'COMPRESSION_ENABLED': True})
+                get_crawler(settings_dict={"COMPRESSION_ENABLED": True})
             ),
-            HttpCompressionMiddleware
+            HttpCompressionMiddleware,
         )
 
     def test_process_request(self):
-        request = Request('http://scrapytest.org')
-        assert 'Accept-Encoding' not in request.headers
+        request = Request("http://scrapytest.org")
+        assert "Accept-Encoding" not in request.headers
         self.mw.process_request(request, self.spider)
-        self.assertEqual(request.headers.get('Accept-Encoding'),
-                         b', '.join(ACCEPTED_ENCODINGS))
+        self.assertEqual(
+            request.headers.get("Accept-Encoding"), b", ".join(ACCEPTED_ENCODINGS)
+        )
 
     def test_process_response_gzip(self):
-        response = self._getresponse('gzip')
+        response = self._getresponse("gzip")
         request = response.request
 
-        self.assertEqual(response.headers['Content-Encoding'], b'gzip')
+        self.assertEqual(response.headers["Content-Encoding"], b"gzip")
         newresponse = self.mw.process_response(request, response, self.spider)
         assert newresponse is not response
-        assert newresponse.body.startswith(b'<!DOCTYPE')
-        assert 'Content-Encoding' not in newresponse.headers
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 74837)
+        assert newresponse.body.startswith(b"<!DOCTYPE")
+        assert "Content-Encoding" not in newresponse.headers
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 74837)
 
     def test_process_response_gzip_no_stats(self):
         mw = HttpCompressionMiddleware()
-        response = self._getresponse('gzip')
+        response = self._getresponse("gzip")
         request = response.request
 
-        self.assertEqual(response.headers['Content-Encoding'], b'gzip')
+        self.assertEqual(response.headers["Content-Encoding"], b"gzip")
         newresponse = mw.process_response(request, response, self.spider)
         self.assertEqual(mw.stats, None)
         assert newresponse is not response
-        assert newresponse.body.startswith(b'<!DOCTYPE')
-        assert 'Content-Encoding' not in newresponse.headers
+        assert newresponse.body.startswith(b"<!DOCTYPE")
+        assert "Content-Encoding" not in newresponse.headers
 
     def test_process_response_br(self):
         try:
             import brotli  # noqa: F401
         except ImportError:
             raise SkipTest("no brotli")
-        response = self._getresponse('br')
+        response = self._getresponse("br")
         request = response.request
-        self.assertEqual(response.headers['Content-Encoding'], b'br')
+        self.assertEqual(response.headers["Content-Encoding"], b"br")
         newresponse = self.mw.process_response(request, response, self.spider)
         assert newresponse is not response
         assert newresponse.body.startswith(b"<!DOCTYPE")
-        assert 'Content-Encoding' not in newresponse.headers
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 74837)
+        assert "Content-Encoding" not in newresponse.headers
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 74837)
 
     def test_process_response_zstd(self):
         try:
             import zstandard  # noqa: F401
         except ImportError:
             raise SkipTest("no zstd support (zstandard)")
         raw_content = None
         for check_key in FORMAT:
-            if not check_key.startswith('zstd-'):
+            if not check_key.startswith("zstd-"):
                 continue
             response = self._getresponse(check_key)
             request = response.request
-            self.assertEqual(response.headers['Content-Encoding'], b'zstd')
+            self.assertEqual(response.headers["Content-Encoding"], b"zstd")
             newresponse = self.mw.process_response(request, response, self.spider)
             if raw_content is None:
                 raw_content = newresponse.body
             else:
                 assert raw_content == newresponse.body
             assert newresponse is not response
             assert newresponse.body.startswith(b"<!DOCTYPE")
-            assert 'Content-Encoding' not in newresponse.headers
+            assert "Content-Encoding" not in newresponse.headers
 
     def test_process_response_rawdeflate(self):
-        response = self._getresponse('rawdeflate')
+        response = self._getresponse("rawdeflate")
         request = response.request
 
-        self.assertEqual(response.headers['Content-Encoding'], b'deflate')
+        self.assertEqual(response.headers["Content-Encoding"], b"deflate")
         newresponse = self.mw.process_response(request, response, self.spider)
         assert newresponse is not response
-        assert newresponse.body.startswith(b'<!DOCTYPE')
-        assert 'Content-Encoding' not in newresponse.headers
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 74840)
+        assert newresponse.body.startswith(b"<!DOCTYPE")
+        assert "Content-Encoding" not in newresponse.headers
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 74840)
 
     def test_process_response_zlibdelate(self):
-        response = self._getresponse('zlibdeflate')
+        response = self._getresponse("zlibdeflate")
         request = response.request
 
-        self.assertEqual(response.headers['Content-Encoding'], b'deflate')
+        self.assertEqual(response.headers["Content-Encoding"], b"deflate")
         newresponse = self.mw.process_response(request, response, self.spider)
         assert newresponse is not response
-        assert newresponse.body.startswith(b'<!DOCTYPE')
-        assert 'Content-Encoding' not in newresponse.headers
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 74840)
+        assert newresponse.body.startswith(b"<!DOCTYPE")
+        assert "Content-Encoding" not in newresponse.headers
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 74840)
 
     def test_process_response_plain(self):
-        response = Response('http://scrapytest.org', body=b'<!DOCTYPE...')
-        request = Request('http://scrapytest.org')
+        response = Response("http://scrapytest.org", body=b"<!DOCTYPE...")
+        request = Request("http://scrapytest.org")
 
-        assert not response.headers.get('Content-Encoding')
+        assert not response.headers.get("Content-Encoding")
         newresponse = self.mw.process_response(request, response, self.spider)
         assert newresponse is response
-        assert newresponse.body.startswith(b'<!DOCTYPE')
-        self.assertStatsEqual('httpcompression/response_count', None)
-        self.assertStatsEqual('httpcompression/response_bytes', None)
+        assert newresponse.body.startswith(b"<!DOCTYPE")
+        self.assertStatsEqual("httpcompression/response_count", None)
+        self.assertStatsEqual("httpcompression/response_bytes", None)
 
     def test_multipleencodings(self):
-        response = self._getresponse('gzip')
-        response.headers['Content-Encoding'] = ['uuencode', 'gzip']
+        response = self._getresponse("gzip")
+        response.headers["Content-Encoding"] = ["uuencode", "gzip"]
         request = response.request
         newresponse = self.mw.process_response(request, response, self.spider)
         assert newresponse is not response
-        self.assertEqual(newresponse.headers.getlist('Content-Encoding'), [b'uuencode'])
+        self.assertEqual(newresponse.headers.getlist("Content-Encoding"), [b"uuencode"])
 
     def test_process_response_encoding_inside_body(self):
         headers = {
-            'Content-Type': 'text/html',
-            'Content-Encoding': 'gzip',
+            "Content-Type": "text/html",
+            "Content-Encoding": "gzip",
         }
         f = BytesIO()
-        plainbody = (b'<html><head><title>Some page</title>'
-                     b'<meta http-equiv="Content-Type" content="text/html; charset=gb2312">')
-        zf = GzipFile(fileobj=f, mode='wb')
+        plainbody = (
+            b"<html><head><title>Some page</title>"
+            b'<meta http-equiv="Content-Type" content="text/html; charset=gb2312">'
+        )
+        zf = GzipFile(fileobj=f, mode="wb")
         zf.write(plainbody)
         zf.close()
-        response = Response("http;//www.example.com/", headers=headers, body=f.getvalue())
+        response = Response(
+            "http;//www.example.com/", headers=headers, body=f.getvalue()
+        )
         request = Request("http://www.example.com/")
 
         newresponse = self.mw.process_response(request, response, self.spider)
         assert isinstance(newresponse, HtmlResponse)
         self.assertEqual(newresponse.body, plainbody)
-        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 104)
+        self.assertEqual(newresponse.encoding, resolve_encoding("gb2312"))
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 104)
 
     def test_process_response_force_recalculate_encoding(self):
         headers = {
-            'Content-Type': 'text/html',
-            'Content-Encoding': 'gzip',
+            "Content-Type": "text/html",
+            "Content-Encoding": "gzip",
         }
         f = BytesIO()
-        plainbody = (b'<html><head><title>Some page</title>'
-                     b'<meta http-equiv="Content-Type" content="text/html; charset=gb2312">')
-        zf = GzipFile(fileobj=f, mode='wb')
+        plainbody = (
+            b"<html><head><title>Some page</title>"
+            b'<meta http-equiv="Content-Type" content="text/html; charset=gb2312">'
+        )
+        zf = GzipFile(fileobj=f, mode="wb")
         zf.write(plainbody)
         zf.close()
-        response = HtmlResponse("http;//www.example.com/page.html", headers=headers, body=f.getvalue())
+        response = HtmlResponse(
+            "http;//www.example.com/page.html", headers=headers, body=f.getvalue()
+        )
         request = Request("http://www.example.com/")
 
         newresponse = self.mw.process_response(request, response, self.spider)
         assert isinstance(newresponse, HtmlResponse)
         self.assertEqual(newresponse.body, plainbody)
-        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 104)
+        self.assertEqual(newresponse.encoding, resolve_encoding("gb2312"))
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 104)
 
     def test_process_response_no_content_type_header(self):
         headers = {
-            'Content-Encoding': 'identity',
+            "Content-Encoding": "identity",
         }
-        plainbody = (b'<html><head><title>Some page</title>'
-                     b'<meta http-equiv="Content-Type" content="text/html; charset=gb2312">')
-        respcls = responsetypes.from_args(url="http://www.example.com/index", headers=headers, body=plainbody)
-        response = respcls("http://www.example.com/index", headers=headers, body=plainbody)
+        plainbody = (
+            b"<html><head><title>Some page</title>"
+            b'<meta http-equiv="Content-Type" content="text/html; charset=gb2312">'
+        )
+        respcls = responsetypes.from_args(
+            url="http://www.example.com/index", headers=headers, body=plainbody
+        )
+        response = respcls(
+            "http://www.example.com/index", headers=headers, body=plainbody
+        )
         request = Request("http://www.example.com/index")
 
         newresponse = self.mw.process_response(request, response, self.spider)
         assert isinstance(newresponse, respcls)
         self.assertEqual(newresponse.body, plainbody)
-        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 104)
+        self.assertEqual(newresponse.encoding, resolve_encoding("gb2312"))
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 104)
 
     def test_process_response_gzipped_contenttype(self):
-        response = self._getresponse('gzip')
-        response.headers['Content-Type'] = 'application/gzip'
+        response = self._getresponse("gzip")
+        response.headers["Content-Type"] = "application/gzip"
         request = response.request
 
         newresponse = self.mw.process_response(request, response, self.spider)
         self.assertIsNot(newresponse, response)
-        self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))
-        self.assertNotIn('Content-Encoding', newresponse.headers)
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 74837)
+        self.assertTrue(newresponse.body.startswith(b"<!DOCTYPE"))
+        self.assertNotIn("Content-Encoding", newresponse.headers)
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 74837)
 
     def test_process_response_gzip_app_octetstream_contenttype(self):
-        response = self._getresponse('gzip')
-        response.headers['Content-Type'] = 'application/octet-stream'
+        response = self._getresponse("gzip")
+        response.headers["Content-Type"] = "application/octet-stream"
         request = response.request
 
         newresponse = self.mw.process_response(request, response, self.spider)
         self.assertIsNot(newresponse, response)
-        self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))
-        self.assertNotIn('Content-Encoding', newresponse.headers)
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 74837)
+        self.assertTrue(newresponse.body.startswith(b"<!DOCTYPE"))
+        self.assertNotIn("Content-Encoding", newresponse.headers)
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 74837)
 
     def test_process_response_gzip_binary_octetstream_contenttype(self):
-        response = self._getresponse('x-gzip')
-        response.headers['Content-Type'] = 'binary/octet-stream'
+        response = self._getresponse("x-gzip")
+        response.headers["Content-Type"] = "binary/octet-stream"
         request = response.request
 
         newresponse = self.mw.process_response(request, response, self.spider)
         self.assertIsNot(newresponse, response)
-        self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))
-        self.assertNotIn('Content-Encoding', newresponse.headers)
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 74837)
+        self.assertTrue(newresponse.body.startswith(b"<!DOCTYPE"))
+        self.assertNotIn("Content-Encoding", newresponse.headers)
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 74837)
 
     def test_process_response_gzipped_gzip_file(self):
         """Test that a gzip Content-Encoded .gz file is gunzipped
         only once by the middleware, leaving gunzipping of the file
         to upper layers.
         """
         headers = {
-            'Content-Type': 'application/gzip',
-            'Content-Encoding': 'gzip',
+            "Content-Type": "application/gzip",
+            "Content-Encoding": "gzip",
         }
         # build a gzipped file (here, a sitemap)
         f = BytesIO()
         plainbody = b"""<?xml version="1.0" encoding="UTF-8"?>
 <urlset xmlns="http://www.google.com/schemas/sitemap/0.84">
   <url>
     <loc>http://www.example.com/</loc>
@@ -316,63 +337,64 @@
   <url>
     <loc>http://www.example.com/Special-Offers.html</loc>
     <lastmod>2009-08-16</lastmod>
     <changefreq>weekly</changefreq>
     <priority>0.8</priority>
   </url>
 </urlset>"""
-        gz_file = GzipFile(fileobj=f, mode='wb')
+        gz_file = GzipFile(fileobj=f, mode="wb")
         gz_file.write(plainbody)
         gz_file.close()
 
         # build a gzipped response body containing this gzipped file
         r = BytesIO()
-        gz_resp = GzipFile(fileobj=r, mode='wb')
+        gz_resp = GzipFile(fileobj=r, mode="wb")
         gz_resp.write(f.getvalue())
         gz_resp.close()
 
-        response = Response("http;//www.example.com/", headers=headers, body=r.getvalue())
+        response = Response(
+            "http;//www.example.com/", headers=headers, body=r.getvalue()
+        )
         request = Request("http://www.example.com/")
 
         newresponse = self.mw.process_response(request, response, self.spider)
         self.assertEqual(gunzip(newresponse.body), plainbody)
-        self.assertStatsEqual('httpcompression/response_count', 1)
-        self.assertStatsEqual('httpcompression/response_bytes', 230)
+        self.assertStatsEqual("httpcompression/response_count", 1)
+        self.assertStatsEqual("httpcompression/response_bytes", 230)
 
     def test_process_response_head_request_no_decode_required(self):
-        response = self._getresponse('gzip')
-        response.headers['Content-Type'] = 'application/gzip'
+        response = self._getresponse("gzip")
+        response.headers["Content-Type"] = "application/gzip"
         request = response.request
-        request.method = 'HEAD'
+        request.method = "HEAD"
         response = response.replace(body=None)
         newresponse = self.mw.process_response(request, response, self.spider)
         self.assertIs(newresponse, response)
-        self.assertEqual(response.body, b'')
-        self.assertStatsEqual('httpcompression/response_count', None)
-        self.assertStatsEqual('httpcompression/response_bytes', None)
+        self.assertEqual(response.body, b"")
+        self.assertStatsEqual("httpcompression/response_count", None)
+        self.assertStatsEqual("httpcompression/response_bytes", None)
 
 
 class HttpCompressionSubclassTest(TestCase):
-
     def test_init_missing_stats(self):
         class HttpCompressionMiddlewareSubclass(HttpCompressionMiddleware):
-
             def __init__(self):
                 super().__init__()
 
         crawler = get_crawler(Spider)
         with catch_warnings(record=True) as caught_warnings:
             HttpCompressionMiddlewareSubclass.from_crawler(crawler)
         messages = tuple(
-            str(warning.message) for warning in caught_warnings
+            str(warning.message)
+            for warning in caught_warnings
             if warning.category is ScrapyDeprecationWarning
         )
         self.assertEqual(
             messages,
             (
                 (
                     "HttpCompressionMiddleware subclasses must either modify "
                     "their '__init__' method to support a 'stats' parameter "
                     "or reimplement the 'from_crawler' method."
                 ),
-            )
+            ),
         )
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_httpproxy.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_httpproxy.py`

 * *Files 18% similar despite different names*

```diff
@@ -5,459 +5,472 @@
 
 from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware
 from scrapy.exceptions import NotConfigured
 from scrapy.http import Request
 from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
 
-spider = Spider('foo')
+spider = Spider("foo")
 
 
 class TestHttpProxyMiddleware(TestCase):
 
     failureException = AssertionError
 
     def setUp(self):
         self._oldenv = os.environ.copy()
 
     def tearDown(self):
         os.environ = self._oldenv
 
     def test_not_enabled(self):
-        crawler = get_crawler(Spider, {'HTTPPROXY_ENABLED': False})
+        crawler = get_crawler(Spider, {"HTTPPROXY_ENABLED": False})
         with pytest.raises(NotConfigured):
             HttpProxyMiddleware.from_crawler(crawler)
 
     def test_no_environment_proxies(self):
-        os.environ = {'dummy_proxy': 'reset_env_and_do_not_raise'}
+        os.environ = {"dummy_proxy": "reset_env_and_do_not_raise"}
         mw = HttpProxyMiddleware()
 
-        for url in ('http://e.com', 'https://e.com', 'file:///tmp/a'):
+        for url in ("http://e.com", "https://e.com", "file:///tmp/a"):
             req = Request(url)
             assert mw.process_request(req, spider) is None
             self.assertEqual(req.url, url)
             self.assertEqual(req.meta, {})
 
     def test_environment_proxies(self):
-        os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'
-        os.environ['https_proxy'] = https_proxy = 'http://proxy.for.https:8080'
-        os.environ.pop('file_proxy', None)
+        os.environ["http_proxy"] = http_proxy = "https://proxy.for.http:3128"
+        os.environ["https_proxy"] = https_proxy = "http://proxy.for.https:8080"
+        os.environ.pop("file_proxy", None)
         mw = HttpProxyMiddleware()
 
         for url, proxy in [
-            ('http://e.com', http_proxy),
-            ('https://e.com', https_proxy),
-            ('file://tmp/a', None),
+            ("http://e.com", http_proxy),
+            ("https://e.com", https_proxy),
+            ("file://tmp/a", None),
         ]:
             req = Request(url)
             assert mw.process_request(req, spider) is None
             self.assertEqual(req.url, url)
-            self.assertEqual(req.meta.get('proxy'), proxy)
+            self.assertEqual(req.meta.get("proxy"), proxy)
 
     def test_proxy_precedence_meta(self):
-        os.environ['http_proxy'] = 'https://proxy.com'
+        os.environ["http_proxy"] = "https://proxy.com"
         mw = HttpProxyMiddleware()
-        req = Request('http://scrapytest.org', meta={'proxy': 'https://new.proxy:3128'})
+        req = Request("http://scrapytest.org", meta={"proxy": "https://new.proxy:3128"})
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta, {'proxy': 'https://new.proxy:3128'})
+        self.assertEqual(req.meta, {"proxy": "https://new.proxy:3128"})
 
     def test_proxy_auth(self):
-        os.environ['http_proxy'] = 'https://user:pass@proxy:3128'
+        os.environ["http_proxy"] = "https://user:pass@proxy:3128"
         mw = HttpProxyMiddleware()
-        req = Request('http://scrapytest.org')
+        req = Request("http://scrapytest.org")
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')
-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')
+        self.assertEqual(req.meta["proxy"], "https://proxy:3128")
+        self.assertEqual(req.headers.get("Proxy-Authorization"), b"Basic dXNlcjpwYXNz")
         # proxy from request.meta
-        req = Request('http://scrapytest.org', meta={'proxy': 'https://username:password@proxy:3128'})
+        req = Request(
+            "http://scrapytest.org",
+            meta={"proxy": "https://username:password@proxy:3128"},
+        )
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')
-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6cGFzc3dvcmQ=')
+        self.assertEqual(req.meta["proxy"], "https://proxy:3128")
+        self.assertEqual(
+            req.headers.get("Proxy-Authorization"), b"Basic dXNlcm5hbWU6cGFzc3dvcmQ="
+        )
 
     def test_proxy_auth_empty_passwd(self):
-        os.environ['http_proxy'] = 'https://user:@proxy:3128'
+        os.environ["http_proxy"] = "https://user:@proxy:3128"
         mw = HttpProxyMiddleware()
-        req = Request('http://scrapytest.org')
+        req = Request("http://scrapytest.org")
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')
-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')
+        self.assertEqual(req.meta["proxy"], "https://proxy:3128")
+        self.assertEqual(req.headers.get("Proxy-Authorization"), b"Basic dXNlcjo=")
         # proxy from request.meta
-        req = Request('http://scrapytest.org', meta={'proxy': 'https://username:@proxy:3128'})
+        req = Request(
+            "http://scrapytest.org", meta={"proxy": "https://username:@proxy:3128"}
+        )
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')
-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6')
+        self.assertEqual(req.meta["proxy"], "https://proxy:3128")
+        self.assertEqual(req.headers.get("Proxy-Authorization"), b"Basic dXNlcm5hbWU6")
 
     def test_proxy_auth_encoding(self):
         # utf-8 encoding
-        os.environ['http_proxy'] = 'https://m\u00E1n:pass@proxy:3128'
-        mw = HttpProxyMiddleware(auth_encoding='utf-8')
-        req = Request('http://scrapytest.org')
+        os.environ["http_proxy"] = "https://m\u00E1n:pass@proxy:3128"
+        mw = HttpProxyMiddleware(auth_encoding="utf-8")
+        req = Request("http://scrapytest.org")
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')
-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic bcOhbjpwYXNz')
+        self.assertEqual(req.meta["proxy"], "https://proxy:3128")
+        self.assertEqual(req.headers.get("Proxy-Authorization"), b"Basic bcOhbjpwYXNz")
 
         # proxy from request.meta
-        req = Request('http://scrapytest.org', meta={'proxy': 'https://\u00FCser:pass@proxy:3128'})
+        req = Request(
+            "http://scrapytest.org", meta={"proxy": "https://\u00FCser:pass@proxy:3128"}
+        )
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')
-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic w7xzZXI6cGFzcw==')
+        self.assertEqual(req.meta["proxy"], "https://proxy:3128")
+        self.assertEqual(
+            req.headers.get("Proxy-Authorization"), b"Basic w7xzZXI6cGFzcw=="
+        )
 
         # default latin-1 encoding
-        mw = HttpProxyMiddleware(auth_encoding='latin-1')
-        req = Request('http://scrapytest.org')
+        mw = HttpProxyMiddleware(auth_encoding="latin-1")
+        req = Request("http://scrapytest.org")
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')
-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic beFuOnBhc3M=')
+        self.assertEqual(req.meta["proxy"], "https://proxy:3128")
+        self.assertEqual(req.headers.get("Proxy-Authorization"), b"Basic beFuOnBhc3M=")
 
         # proxy from request.meta, latin-1 encoding
-        req = Request('http://scrapytest.org', meta={'proxy': 'https://\u00FCser:pass@proxy:3128'})
+        req = Request(
+            "http://scrapytest.org", meta={"proxy": "https://\u00FCser:pass@proxy:3128"}
+        )
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')
-        self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic /HNlcjpwYXNz')
+        self.assertEqual(req.meta["proxy"], "https://proxy:3128")
+        self.assertEqual(req.headers.get("Proxy-Authorization"), b"Basic /HNlcjpwYXNz")
 
     def test_proxy_already_seted(self):
-        os.environ['http_proxy'] = 'https://proxy.for.http:3128'
+        os.environ["http_proxy"] = "https://proxy.for.http:3128"
         mw = HttpProxyMiddleware()
-        req = Request('http://noproxy.com', meta={'proxy': None})
+        req = Request("http://noproxy.com", meta={"proxy": None})
         assert mw.process_request(req, spider) is None
-        assert 'proxy' in req.meta and req.meta['proxy'] is None
+        assert "proxy" in req.meta and req.meta["proxy"] is None
 
     def test_no_proxy(self):
-        os.environ['http_proxy'] = 'https://proxy.for.http:3128'
+        os.environ["http_proxy"] = "https://proxy.for.http:3128"
         mw = HttpProxyMiddleware()
 
-        os.environ['no_proxy'] = '*'
-        req = Request('http://noproxy.com')
+        os.environ["no_proxy"] = "*"
+        req = Request("http://noproxy.com")
         assert mw.process_request(req, spider) is None
-        assert 'proxy' not in req.meta
+        assert "proxy" not in req.meta
 
-        os.environ['no_proxy'] = 'other.com'
-        req = Request('http://noproxy.com')
+        os.environ["no_proxy"] = "other.com"
+        req = Request("http://noproxy.com")
         assert mw.process_request(req, spider) is None
-        assert 'proxy' in req.meta
+        assert "proxy" in req.meta
 
-        os.environ['no_proxy'] = 'other.com,noproxy.com'
-        req = Request('http://noproxy.com')
+        os.environ["no_proxy"] = "other.com,noproxy.com"
+        req = Request("http://noproxy.com")
         assert mw.process_request(req, spider) is None
-        assert 'proxy' not in req.meta
+        assert "proxy" not in req.meta
 
         # proxy from meta['proxy'] takes precedence
-        os.environ['no_proxy'] = '*'
-        req = Request('http://noproxy.com', meta={'proxy': 'http://proxy.com'})
+        os.environ["no_proxy"] = "*"
+        req = Request("http://noproxy.com", meta={"proxy": "http://proxy.com"})
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.meta, {'proxy': 'http://proxy.com'})
+        self.assertEqual(req.meta, {"proxy": "http://proxy.com"})
 
     def test_no_proxy_invalid_values(self):
-        os.environ['no_proxy'] = '/var/run/docker.sock'
+        os.environ["no_proxy"] = "/var/run/docker.sock"
         mw = HttpProxyMiddleware()
         # '/var/run/docker.sock' may be used by the user for
         # no_proxy value but is not parseable and should be skipped
-        assert 'no' not in mw.proxies
+        assert "no" not in mw.proxies
 
     def test_add_proxy_without_credentials(self):
         middleware = HttpProxyMiddleware()
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         assert middleware.process_request(request, spider) is None
-        request.meta['proxy'] = 'https://example.com'
+        request.meta["proxy"] = "https://example.com"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
-        self.assertNotIn(b'Proxy-Authorization', request.headers)
+        self.assertEqual(request.meta["proxy"], "https://example.com")
+        self.assertNotIn(b"Proxy-Authorization", request.headers)
 
     def test_add_proxy_with_credentials(self):
         middleware = HttpProxyMiddleware()
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         assert middleware.process_request(request, spider) is None
-        request.meta['proxy'] = 'https://user1:password1@example.com'
+        request.meta["proxy"] = "https://user1:password1@example.com"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
+        self.assertEqual(request.meta["proxy"], "https://example.com")
         encoded_credentials = middleware._basic_auth_header(
-            'user1',
-            'password1',
+            "user1",
+            "password1",
         )
         self.assertEqual(
-            request.headers['Proxy-Authorization'],
-            b'Basic ' + encoded_credentials,
+            request.headers["Proxy-Authorization"],
+            b"Basic " + encoded_credentials,
         )
 
     def test_remove_proxy_without_credentials(self):
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://example.com'},
+            "https://example.com",
+            meta={"proxy": "https://example.com"},
         )
         assert middleware.process_request(request, spider) is None
-        request.meta['proxy'] = None
+        request.meta["proxy"] = None
         assert middleware.process_request(request, spider) is None
-        self.assertIsNone(request.meta['proxy'])
-        self.assertNotIn(b'Proxy-Authorization', request.headers)
+        self.assertIsNone(request.meta["proxy"])
+        self.assertNotIn(b"Proxy-Authorization", request.headers)
 
     def test_remove_proxy_with_credentials(self):
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://user1:password1@example.com'},
+            "https://example.com",
+            meta={"proxy": "https://user1:password1@example.com"},
         )
         assert middleware.process_request(request, spider) is None
-        request.meta['proxy'] = None
+        request.meta["proxy"] = None
         assert middleware.process_request(request, spider) is None
-        self.assertIsNone(request.meta['proxy'])
-        self.assertNotIn(b'Proxy-Authorization', request.headers)
+        self.assertIsNone(request.meta["proxy"])
+        self.assertNotIn(b"Proxy-Authorization", request.headers)
 
     def test_add_credentials(self):
         """If the proxy request meta switches to a proxy URL with the same
         proxy and adds credentials (there were no credentials before), the new
         credentials must be used."""
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://example.com'},
+            "https://example.com",
+            meta={"proxy": "https://example.com"},
         )
         assert middleware.process_request(request, spider) is None
 
-        request.meta['proxy'] = 'https://user1:password1@example.com'
+        request.meta["proxy"] = "https://user1:password1@example.com"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
+        self.assertEqual(request.meta["proxy"], "https://example.com")
         encoded_credentials = middleware._basic_auth_header(
-            'user1',
-            'password1',
+            "user1",
+            "password1",
         )
         self.assertEqual(
-            request.headers['Proxy-Authorization'],
-            b'Basic ' + encoded_credentials,
+            request.headers["Proxy-Authorization"],
+            b"Basic " + encoded_credentials,
         )
 
     def test_change_credentials(self):
         """If the proxy request meta switches to a proxy URL with different
         credentials, those new credentials must be used."""
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://user1:password1@example.com'},
+            "https://example.com",
+            meta={"proxy": "https://user1:password1@example.com"},
         )
         assert middleware.process_request(request, spider) is None
-        request.meta['proxy'] = 'https://user2:password2@example.com'
+        request.meta["proxy"] = "https://user2:password2@example.com"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
+        self.assertEqual(request.meta["proxy"], "https://example.com")
         encoded_credentials = middleware._basic_auth_header(
-            'user2',
-            'password2',
+            "user2",
+            "password2",
         )
         self.assertEqual(
-            request.headers['Proxy-Authorization'],
-            b'Basic ' + encoded_credentials,
+            request.headers["Proxy-Authorization"],
+            b"Basic " + encoded_credentials,
         )
 
     def test_remove_credentials(self):
         """If the proxy request meta switches to a proxy URL with the same
         proxy but no credentials, the original credentials must be still
         used.
 
         To remove credentials while keeping the same proxy URL, users must
         delete the Proxy-Authorization header.
         """
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://user1:password1@example.com'},
+            "https://example.com",
+            meta={"proxy": "https://user1:password1@example.com"},
         )
         assert middleware.process_request(request, spider) is None
 
-        request.meta['proxy'] = 'https://example.com'
+        request.meta["proxy"] = "https://example.com"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
+        self.assertEqual(request.meta["proxy"], "https://example.com")
         encoded_credentials = middleware._basic_auth_header(
-            'user1',
-            'password1',
+            "user1",
+            "password1",
         )
         self.assertEqual(
-            request.headers['Proxy-Authorization'],
-            b'Basic ' + encoded_credentials,
+            request.headers["Proxy-Authorization"],
+            b"Basic " + encoded_credentials,
         )
 
-        request.meta['proxy'] = 'https://example.com'
-        del request.headers[b'Proxy-Authorization']
+        request.meta["proxy"] = "https://example.com"
+        del request.headers[b"Proxy-Authorization"]
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
-        self.assertNotIn(b'Proxy-Authorization', request.headers)
+        self.assertEqual(request.meta["proxy"], "https://example.com")
+        self.assertNotIn(b"Proxy-Authorization", request.headers)
 
     def test_change_proxy_add_credentials(self):
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://example.com'},
+            "https://example.com",
+            meta={"proxy": "https://example.com"},
         )
         assert middleware.process_request(request, spider) is None
 
-        request.meta['proxy'] = 'https://user1:password1@example.org'
+        request.meta["proxy"] = "https://user1:password1@example.org"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.org')
+        self.assertEqual(request.meta["proxy"], "https://example.org")
         encoded_credentials = middleware._basic_auth_header(
-            'user1',
-            'password1',
+            "user1",
+            "password1",
         )
         self.assertEqual(
-            request.headers['Proxy-Authorization'],
-            b'Basic ' + encoded_credentials,
+            request.headers["Proxy-Authorization"],
+            b"Basic " + encoded_credentials,
         )
 
     def test_change_proxy_keep_credentials(self):
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://user1:password1@example.com'},
+            "https://example.com",
+            meta={"proxy": "https://user1:password1@example.com"},
         )
         assert middleware.process_request(request, spider) is None
 
-        request.meta['proxy'] = 'https://user1:password1@example.org'
+        request.meta["proxy"] = "https://user1:password1@example.org"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.org')
+        self.assertEqual(request.meta["proxy"], "https://example.org")
         encoded_credentials = middleware._basic_auth_header(
-            'user1',
-            'password1',
+            "user1",
+            "password1",
         )
         self.assertEqual(
-            request.headers['Proxy-Authorization'],
-            b'Basic ' + encoded_credentials,
+            request.headers["Proxy-Authorization"],
+            b"Basic " + encoded_credentials,
         )
 
         # Make sure, indirectly, that _auth_proxy is updated.
-        request.meta['proxy'] = 'https://example.com'
+        request.meta["proxy"] = "https://example.com"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
-        self.assertNotIn(b'Proxy-Authorization', request.headers)
+        self.assertEqual(request.meta["proxy"], "https://example.com")
+        self.assertNotIn(b"Proxy-Authorization", request.headers)
 
     def test_change_proxy_change_credentials(self):
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://user1:password1@example.com'},
+            "https://example.com",
+            meta={"proxy": "https://user1:password1@example.com"},
         )
         assert middleware.process_request(request, spider) is None
 
-        request.meta['proxy'] = 'https://user2:password2@example.org'
+        request.meta["proxy"] = "https://user2:password2@example.org"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.org')
+        self.assertEqual(request.meta["proxy"], "https://example.org")
         encoded_credentials = middleware._basic_auth_header(
-            'user2',
-            'password2',
+            "user2",
+            "password2",
         )
         self.assertEqual(
-            request.headers['Proxy-Authorization'],
-            b'Basic ' + encoded_credentials,
+            request.headers["Proxy-Authorization"],
+            b"Basic " + encoded_credentials,
         )
 
     def test_change_proxy_remove_credentials(self):
         """If the proxy request meta switches to a proxy URL with a different
         proxy and no credentials, no credentials must be used."""
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://user1:password1@example.com'},
+            "https://example.com",
+            meta={"proxy": "https://user1:password1@example.com"},
         )
         assert middleware.process_request(request, spider) is None
-        request.meta['proxy'] = 'https://example.org'
+        request.meta["proxy"] = "https://example.org"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta, {'proxy': 'https://example.org'})
-        self.assertNotIn(b'Proxy-Authorization', request.headers)
+        self.assertEqual(request.meta, {"proxy": "https://example.org"})
+        self.assertNotIn(b"Proxy-Authorization", request.headers)
 
     def test_change_proxy_remove_credentials_preremoved_header(self):
         """Corner case of proxy switch with credentials removal where the
         credentials have been removed beforehand.
 
         It ensures that our implementation does not assume that the credentials
         header exists when trying to remove it.
         """
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            meta={'proxy': 'https://user1:password1@example.com'},
+            "https://example.com",
+            meta={"proxy": "https://user1:password1@example.com"},
         )
         assert middleware.process_request(request, spider) is None
-        request.meta['proxy'] = 'https://example.org'
-        del request.headers[b'Proxy-Authorization']
+        request.meta["proxy"] = "https://example.org"
+        del request.headers[b"Proxy-Authorization"]
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta, {'proxy': 'https://example.org'})
-        self.assertNotIn(b'Proxy-Authorization', request.headers)
+        self.assertEqual(request.meta, {"proxy": "https://example.org"})
+        self.assertNotIn(b"Proxy-Authorization", request.headers)
 
     def test_proxy_authentication_header_undefined_proxy(self):
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            headers={'Proxy-Authorization': 'Basic foo'},
+            "https://example.com",
+            headers={"Proxy-Authorization": "Basic foo"},
         )
         assert middleware.process_request(request, spider) is None
-        self.assertNotIn('proxy', request.meta)
-        self.assertNotIn(b'Proxy-Authorization', request.headers)
+        self.assertNotIn("proxy", request.meta)
+        self.assertNotIn(b"Proxy-Authorization", request.headers)
 
     def test_proxy_authentication_header_disabled_proxy(self):
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            headers={'Proxy-Authorization': 'Basic foo'},
-            meta={'proxy': None},
+            "https://example.com",
+            headers={"Proxy-Authorization": "Basic foo"},
+            meta={"proxy": None},
         )
         assert middleware.process_request(request, spider) is None
-        self.assertIsNone(request.meta['proxy'])
-        self.assertNotIn(b'Proxy-Authorization', request.headers)
+        self.assertIsNone(request.meta["proxy"])
+        self.assertNotIn(b"Proxy-Authorization", request.headers)
 
     def test_proxy_authentication_header_proxy_without_credentials(self):
         """As long as the proxy URL in request metadata remains the same, the
         Proxy-Authorization header is used and kept, and may even be
         changed."""
         middleware = HttpProxyMiddleware()
         request = Request(
-            'https://example.com',
-            headers={'Proxy-Authorization': 'Basic foo'},
-            meta={'proxy': 'https://example.com'},
+            "https://example.com",
+            headers={"Proxy-Authorization": "Basic foo"},
+            meta={"proxy": "https://example.com"},
         )
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
-        self.assertEqual(request.headers['Proxy-Authorization'], b'Basic foo')
+        self.assertEqual(request.meta["proxy"], "https://example.com")
+        self.assertEqual(request.headers["Proxy-Authorization"], b"Basic foo")
 
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
-        self.assertEqual(request.headers['Proxy-Authorization'], b'Basic foo')
+        self.assertEqual(request.meta["proxy"], "https://example.com")
+        self.assertEqual(request.headers["Proxy-Authorization"], b"Basic foo")
 
-        request.headers['Proxy-Authorization'] = b'Basic bar'
+        request.headers["Proxy-Authorization"] = b"Basic bar"
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
-        self.assertEqual(request.headers['Proxy-Authorization'], b'Basic bar')
+        self.assertEqual(request.meta["proxy"], "https://example.com")
+        self.assertEqual(request.headers["Proxy-Authorization"], b"Basic bar")
 
     def test_proxy_authentication_header_proxy_with_same_credentials(self):
         middleware = HttpProxyMiddleware()
         encoded_credentials = middleware._basic_auth_header(
-            'user1',
-            'password1',
+            "user1",
+            "password1",
         )
         request = Request(
-            'https://example.com',
-            headers={'Proxy-Authorization': b'Basic ' + encoded_credentials},
-            meta={'proxy': 'https://user1:password1@example.com'},
+            "https://example.com",
+            headers={"Proxy-Authorization": b"Basic " + encoded_credentials},
+            meta={"proxy": "https://user1:password1@example.com"},
         )
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
+        self.assertEqual(request.meta["proxy"], "https://example.com")
         self.assertEqual(
-            request.headers['Proxy-Authorization'],
-            b'Basic ' + encoded_credentials,
+            request.headers["Proxy-Authorization"],
+            b"Basic " + encoded_credentials,
         )
 
     def test_proxy_authentication_header_proxy_with_different_credentials(self):
         middleware = HttpProxyMiddleware()
         encoded_credentials1 = middleware._basic_auth_header(
-            'user1',
-            'password1',
+            "user1",
+            "password1",
         )
         request = Request(
-            'https://example.com',
-            headers={'Proxy-Authorization': b'Basic ' + encoded_credentials1},
-            meta={'proxy': 'https://user2:password2@example.com'},
+            "https://example.com",
+            headers={"Proxy-Authorization": b"Basic " + encoded_credentials1},
+            meta={"proxy": "https://user2:password2@example.com"},
         )
         assert middleware.process_request(request, spider) is None
-        self.assertEqual(request.meta['proxy'], 'https://example.com')
+        self.assertEqual(request.meta["proxy"], "https://example.com")
         encoded_credentials2 = middleware._basic_auth_header(
-            'user2',
-            'password2',
+            "user2",
+            "password2",
         )
         self.assertEqual(
-            request.headers['Proxy-Authorization'],
-            b'Basic ' + encoded_credentials2,
+            request.headers["Proxy-Authorization"],
+            b"Basic " + encoded_credentials2,
         )
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_redirect.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_redirect.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,317 +1,387 @@
 import unittest
 
-from scrapy.downloadermiddlewares.redirect import RedirectMiddleware, MetaRefreshMiddleware
-from scrapy.spiders import Spider
+from scrapy.downloadermiddlewares.redirect import (
+    MetaRefreshMiddleware,
+    RedirectMiddleware,
+)
 from scrapy.exceptions import IgnoreRequest
-from scrapy.http import Request, Response, HtmlResponse
+from scrapy.http import HtmlResponse, Request, Response
+from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
 
 
 class RedirectMiddlewareTest(unittest.TestCase):
-
     def setUp(self):
         self.crawler = get_crawler(Spider)
-        self.spider = self.crawler._create_spider('foo')
+        self.spider = self.crawler._create_spider("foo")
         self.mw = RedirectMiddleware.from_crawler(self.crawler)
 
     def test_priority_adjust(self):
-        req = Request('http://a.com')
-        rsp = Response('http://a.com', headers={'Location': 'http://a.com/redirected'}, status=301)
+        req = Request("http://a.com")
+        rsp = Response(
+            "http://a.com", headers={"Location": "http://a.com/redirected"}, status=301
+        )
         req2 = self.mw.process_response(req, rsp, self.spider)
         assert req2.priority > req.priority
 
     def test_redirect_3xx_permanent(self):
         def _test(method, status=301):
-            url = f'http://www.example.com/{status}'
-            url2 = 'http://www.example.com/redirected'
+            url = f"http://www.example.com/{status}"
+            url2 = "http://www.example.com/redirected"
             req = Request(url, method=method)
-            rsp = Response(url, headers={'Location': url2}, status=status)
+            rsp = Response(url, headers={"Location": url2}, status=status)
 
             req2 = self.mw.process_response(req, rsp, self.spider)
             assert isinstance(req2, Request)
             self.assertEqual(req2.url, url2)
             self.assertEqual(req2.method, method)
 
             # response without Location header but with status code is 3XX should be ignored
-            del rsp.headers['Location']
+            del rsp.headers["Location"]
             assert self.mw.process_response(req, rsp, self.spider) is rsp
 
-        _test('GET')
-        _test('POST')
-        _test('HEAD')
-
-        _test('GET', status=307)
-        _test('POST', status=307)
-        _test('HEAD', status=307)
-
-        _test('GET', status=308)
-        _test('POST', status=308)
-        _test('HEAD', status=308)
+        _test("GET")
+        _test("POST")
+        _test("HEAD")
+
+        _test("GET", status=307)
+        _test("POST", status=307)
+        _test("HEAD", status=307)
+
+        _test("GET", status=308)
+        _test("POST", status=308)
+        _test("HEAD", status=308)
 
     def test_dont_redirect(self):
-        url = 'http://www.example.com/301'
-        url2 = 'http://www.example.com/redirected'
-        req = Request(url, meta={'dont_redirect': True})
-        rsp = Response(url, headers={'Location': url2}, status=301)
+        url = "http://www.example.com/301"
+        url2 = "http://www.example.com/redirected"
+        req = Request(url, meta={"dont_redirect": True})
+        rsp = Response(url, headers={"Location": url2}, status=301)
 
         r = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(r, Response)
         assert r is rsp
 
         # Test that it redirects when dont_redirect is False
-        req = Request(url, meta={'dont_redirect': False})
+        req = Request(url, meta={"dont_redirect": False})
         rsp = Response(url2, status=200)
 
         r = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(r, Response)
         assert r is rsp
 
     def test_redirect_302(self):
-        url = 'http://www.example.com/302'
-        url2 = 'http://www.example.com/redirected2'
-        req = Request(url, method='POST', body='test',
-                      headers={'Content-Type': 'text/plain', 'Content-length': '4'})
-        rsp = Response(url, headers={'Location': url2}, status=302)
+        url = "http://www.example.com/302"
+        url2 = "http://www.example.com/redirected2"
+        req = Request(
+            url,
+            method="POST",
+            body="test",
+            headers={"Content-Type": "text/plain", "Content-length": "4"},
+        )
+        rsp = Response(url, headers={"Location": url2}, status=302)
 
         req2 = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req2, Request)
         self.assertEqual(req2.url, url2)
-        self.assertEqual(req2.method, 'GET')
-        assert 'Content-Type' not in req2.headers, "Content-Type header must not be present in redirected request"
-        assert 'Content-Length' not in req2.headers, "Content-Length header must not be present in redirected request"
+        self.assertEqual(req2.method, "GET")
+        assert (
+            "Content-Type" not in req2.headers
+        ), "Content-Type header must not be present in redirected request"
+        assert (
+            "Content-Length" not in req2.headers
+        ), "Content-Length header must not be present in redirected request"
         assert not req2.body, f"Redirected body must be empty, not '{req2.body}'"
 
         # response without Location header but with status code is 3XX should be ignored
-        del rsp.headers['Location']
+        del rsp.headers["Location"]
         assert self.mw.process_response(req, rsp, self.spider) is rsp
 
     def test_redirect_302_head(self):
-        url = 'http://www.example.com/302'
-        url2 = 'http://www.example.com/redirected2'
-        req = Request(url, method='HEAD')
-        rsp = Response(url, headers={'Location': url2}, status=302)
+        url = "http://www.example.com/302"
+        url2 = "http://www.example.com/redirected2"
+        req = Request(url, method="HEAD")
+        rsp = Response(url, headers={"Location": url2}, status=302)
 
         req2 = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req2, Request)
         self.assertEqual(req2.url, url2)
-        self.assertEqual(req2.method, 'HEAD')
+        self.assertEqual(req2.method, "HEAD")
 
         # response without Location header but with status code is 3XX should be ignored
-        del rsp.headers['Location']
+        del rsp.headers["Location"]
         assert self.mw.process_response(req, rsp, self.spider) is rsp
 
     def test_redirect_302_relative(self):
-        url = 'http://www.example.com/302'
-        url2 = '///i8n.example2.com/302'
-        url3 = 'http://i8n.example2.com/302'
-        req = Request(url, method='HEAD')
-        rsp = Response(url, headers={'Location': url2}, status=302)
+        url = "http://www.example.com/302"
+        url2 = "///i8n.example2.com/302"
+        url3 = "http://i8n.example2.com/302"
+        req = Request(url, method="HEAD")
+        rsp = Response(url, headers={"Location": url2}, status=302)
 
         req2 = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req2, Request)
         self.assertEqual(req2.url, url3)
-        self.assertEqual(req2.method, 'HEAD')
+        self.assertEqual(req2.method, "HEAD")
 
         # response without Location header but with status code is 3XX should be ignored
-        del rsp.headers['Location']
+        del rsp.headers["Location"]
         assert self.mw.process_response(req, rsp, self.spider) is rsp
 
     def test_max_redirect_times(self):
         self.mw.max_redirect_times = 1
-        req = Request('http://scrapytest.org/302')
-        rsp = Response('http://scrapytest.org/302', headers={'Location': '/redirected'}, status=302)
+        req = Request("http://scrapytest.org/302")
+        rsp = Response(
+            "http://scrapytest.org/302", headers={"Location": "/redirected"}, status=302
+        )
 
         req = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req, Request)
-        assert 'redirect_times' in req.meta
-        self.assertEqual(req.meta['redirect_times'], 1)
-        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)
+        assert "redirect_times" in req.meta
+        self.assertEqual(req.meta["redirect_times"], 1)
+        self.assertRaises(
+            IgnoreRequest, self.mw.process_response, req, rsp, self.spider
+        )
 
     def test_ttl(self):
         self.mw.max_redirect_times = 100
-        req = Request('http://scrapytest.org/302', meta={'redirect_ttl': 1})
-        rsp = Response('http://www.scrapytest.org/302', headers={'Location': '/redirected'}, status=302)
+        req = Request("http://scrapytest.org/302", meta={"redirect_ttl": 1})
+        rsp = Response(
+            "http://www.scrapytest.org/302",
+            headers={"Location": "/redirected"},
+            status=302,
+        )
 
         req = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req, Request)
-        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)
+        self.assertRaises(
+            IgnoreRequest, self.mw.process_response, req, rsp, self.spider
+        )
 
     def test_redirect_urls(self):
-        req1 = Request('http://scrapytest.org/first')
-        rsp1 = Response('http://scrapytest.org/first', headers={'Location': '/redirected'}, status=302)
+        req1 = Request("http://scrapytest.org/first")
+        rsp1 = Response(
+            "http://scrapytest.org/first",
+            headers={"Location": "/redirected"},
+            status=302,
+        )
         req2 = self.mw.process_response(req1, rsp1, self.spider)
-        rsp2 = Response('http://scrapytest.org/redirected', headers={'Location': '/redirected2'}, status=302)
+        rsp2 = Response(
+            "http://scrapytest.org/redirected",
+            headers={"Location": "/redirected2"},
+            status=302,
+        )
         req3 = self.mw.process_response(req2, rsp2, self.spider)
 
-        self.assertEqual(req2.url, 'http://scrapytest.org/redirected')
-        self.assertEqual(req2.meta['redirect_urls'], ['http://scrapytest.org/first'])
-        self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')
+        self.assertEqual(req2.url, "http://scrapytest.org/redirected")
+        self.assertEqual(req2.meta["redirect_urls"], ["http://scrapytest.org/first"])
+        self.assertEqual(req3.url, "http://scrapytest.org/redirected2")
         self.assertEqual(
-            req3.meta['redirect_urls'],
-            ['http://scrapytest.org/first', 'http://scrapytest.org/redirected']
+            req3.meta["redirect_urls"],
+            ["http://scrapytest.org/first", "http://scrapytest.org/redirected"],
         )
 
     def test_redirect_reasons(self):
-        req1 = Request('http://scrapytest.org/first')
-        rsp1 = Response('http://scrapytest.org/first', headers={'Location': '/redirected1'}, status=301)
+        req1 = Request("http://scrapytest.org/first")
+        rsp1 = Response(
+            "http://scrapytest.org/first",
+            headers={"Location": "/redirected1"},
+            status=301,
+        )
         req2 = self.mw.process_response(req1, rsp1, self.spider)
-        rsp2 = Response('http://scrapytest.org/redirected1', headers={'Location': '/redirected2'}, status=301)
+        rsp2 = Response(
+            "http://scrapytest.org/redirected1",
+            headers={"Location": "/redirected2"},
+            status=301,
+        )
         req3 = self.mw.process_response(req2, rsp2, self.spider)
 
-        self.assertEqual(req2.meta['redirect_reasons'], [301])
-        self.assertEqual(req3.meta['redirect_reasons'], [301, 301])
+        self.assertEqual(req2.meta["redirect_reasons"], [301])
+        self.assertEqual(req3.meta["redirect_reasons"], [301, 301])
 
     def test_spider_handling(self):
-        smartspider = self.crawler._create_spider('smarty')
+        smartspider = self.crawler._create_spider("smarty")
         smartspider.handle_httpstatus_list = [404, 301, 302]
-        url = 'http://www.example.com/301'
-        url2 = 'http://www.example.com/redirected'
+        url = "http://www.example.com/301"
+        url2 = "http://www.example.com/redirected"
         req = Request(url)
-        rsp = Response(url, headers={'Location': url2}, status=301)
+        rsp = Response(url, headers={"Location": url2}, status=301)
         r = self.mw.process_response(req, rsp, smartspider)
         self.assertIs(r, rsp)
 
     def test_request_meta_handling(self):
-        url = 'http://www.example.com/301'
-        url2 = 'http://www.example.com/redirected'
+        url = "http://www.example.com/301"
+        url2 = "http://www.example.com/redirected"
 
         def _test_passthrough(req):
-            rsp = Response(url, headers={'Location': url2}, status=301, request=req)
+            rsp = Response(url, headers={"Location": url2}, status=301, request=req)
             r = self.mw.process_response(req, rsp, self.spider)
             self.assertIs(r, rsp)
-        _test_passthrough(Request(url, meta={'handle_httpstatus_list': [404, 301, 302]}))
-        _test_passthrough(Request(url, meta={'handle_httpstatus_all': True}))
+
+        _test_passthrough(
+            Request(url, meta={"handle_httpstatus_list": [404, 301, 302]})
+        )
+        _test_passthrough(Request(url, meta={"handle_httpstatus_all": True}))
 
     def test_latin1_location(self):
-        req = Request('http://scrapytest.org/first')
-        latin1_location = '/ao'.encode('latin1')  # HTTP historically supports latin1
-        resp = Response('http://scrapytest.org/first', headers={'Location': latin1_location}, status=302)
+        req = Request("http://scrapytest.org/first")
+        latin1_location = "/ao".encode("latin1")  # HTTP historically supports latin1
+        resp = Response(
+            "http://scrapytest.org/first",
+            headers={"Location": latin1_location},
+            status=302,
+        )
         req_result = self.mw.process_response(req, resp, self.spider)
-        perc_encoded_utf8_url = 'http://scrapytest.org/a%E7%E3o'
+        perc_encoded_utf8_url = "http://scrapytest.org/a%E7%E3o"
         self.assertEqual(perc_encoded_utf8_url, req_result.url)
 
     def test_utf8_location(self):
-        req = Request('http://scrapytest.org/first')
-        utf8_location = '/ao'.encode('utf-8')  # header using UTF-8 encoding
-        resp = Response('http://scrapytest.org/first', headers={'Location': utf8_location}, status=302)
+        req = Request("http://scrapytest.org/first")
+        utf8_location = "/ao".encode("utf-8")  # header using UTF-8 encoding
+        resp = Response(
+            "http://scrapytest.org/first",
+            headers={"Location": utf8_location},
+            status=302,
+        )
         req_result = self.mw.process_response(req, resp, self.spider)
-        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'
+        perc_encoded_utf8_url = "http://scrapytest.org/a%C3%A7%C3%A3o"
         self.assertEqual(perc_encoded_utf8_url, req_result.url)
 
 
 class MetaRefreshMiddlewareTest(unittest.TestCase):
-
     def setUp(self):
         crawler = get_crawler(Spider)
-        self.spider = crawler._create_spider('foo')
+        self.spider = crawler._create_spider("foo")
         self.mw = MetaRefreshMiddleware.from_crawler(crawler)
 
-    def _body(self, interval=5, url='http://example.org/newpage'):
+    def _body(self, interval=5, url="http://example.org/newpage"):
         html = f"""<html><head><meta http-equiv="refresh" content="{interval};url={url}"/></head></html>"""
-        return html.encode('utf-8')
+        return html.encode("utf-8")
 
     def test_priority_adjust(self):
-        req = Request('http://a.com')
+        req = Request("http://a.com")
         rsp = HtmlResponse(req.url, body=self._body())
         req2 = self.mw.process_response(req, rsp, self.spider)
         assert req2.priority > req.priority
 
     def test_meta_refresh(self):
-        req = Request(url='http://example.org')
+        req = Request(url="http://example.org")
         rsp = HtmlResponse(req.url, body=self._body())
         req2 = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req2, Request)
-        self.assertEqual(req2.url, 'http://example.org/newpage')
+        self.assertEqual(req2.url, "http://example.org/newpage")
 
     def test_meta_refresh_with_high_interval(self):
         # meta-refresh with high intervals don't trigger redirects
-        req = Request(url='http://example.org')
-        rsp = HtmlResponse(url='http://example.org',
-                           body=self._body(interval=1000),
-                           encoding='utf-8')
+        req = Request(url="http://example.org")
+        rsp = HtmlResponse(
+            url="http://example.org", body=self._body(interval=1000), encoding="utf-8"
+        )
         rsp2 = self.mw.process_response(req, rsp, self.spider)
         assert rsp is rsp2
 
     def test_meta_refresh_trough_posted_request(self):
-        req = Request(url='http://example.org', method='POST', body='test',
-                      headers={'Content-Type': 'text/plain', 'Content-length': '4'})
+        req = Request(
+            url="http://example.org",
+            method="POST",
+            body="test",
+            headers={"Content-Type": "text/plain", "Content-length": "4"},
+        )
         rsp = HtmlResponse(req.url, body=self._body())
         req2 = self.mw.process_response(req, rsp, self.spider)
 
         assert isinstance(req2, Request)
-        self.assertEqual(req2.url, 'http://example.org/newpage')
-        self.assertEqual(req2.method, 'GET')
-        assert 'Content-Type' not in req2.headers, "Content-Type header must not be present in redirected request"
-        assert 'Content-Length' not in req2.headers, "Content-Length header must not be present in redirected request"
+        self.assertEqual(req2.url, "http://example.org/newpage")
+        self.assertEqual(req2.method, "GET")
+        assert (
+            "Content-Type" not in req2.headers
+        ), "Content-Type header must not be present in redirected request"
+        assert (
+            "Content-Length" not in req2.headers
+        ), "Content-Length header must not be present in redirected request"
         assert not req2.body, f"Redirected body must be empty, not '{req2.body}'"
 
     def test_max_redirect_times(self):
         self.mw.max_redirect_times = 1
-        req = Request('http://scrapytest.org/max')
+        req = Request("http://scrapytest.org/max")
         rsp = HtmlResponse(req.url, body=self._body())
 
         req = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req, Request)
-        assert 'redirect_times' in req.meta
-        self.assertEqual(req.meta['redirect_times'], 1)
-        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)
+        assert "redirect_times" in req.meta
+        self.assertEqual(req.meta["redirect_times"], 1)
+        self.assertRaises(
+            IgnoreRequest, self.mw.process_response, req, rsp, self.spider
+        )
 
     def test_ttl(self):
         self.mw.max_redirect_times = 100
-        req = Request('http://scrapytest.org/302', meta={'redirect_ttl': 1})
+        req = Request("http://scrapytest.org/302", meta={"redirect_ttl": 1})
         rsp = HtmlResponse(req.url, body=self._body())
 
         req = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req, Request)
-        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)
+        self.assertRaises(
+            IgnoreRequest, self.mw.process_response, req, rsp, self.spider
+        )
 
     def test_redirect_urls(self):
-        req1 = Request('http://scrapytest.org/first')
-        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'))
+        req1 = Request("http://scrapytest.org/first")
+        rsp1 = HtmlResponse(req1.url, body=self._body(url="/redirected"))
         req2 = self.mw.process_response(req1, rsp1, self.spider)
         assert isinstance(req2, Request), req2
-        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'))
+        rsp2 = HtmlResponse(req2.url, body=self._body(url="/redirected2"))
         req3 = self.mw.process_response(req2, rsp2, self.spider)
         assert isinstance(req3, Request), req3
-        self.assertEqual(req2.url, 'http://scrapytest.org/redirected')
-        self.assertEqual(req2.meta['redirect_urls'], ['http://scrapytest.org/first'])
-        self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')
+        self.assertEqual(req2.url, "http://scrapytest.org/redirected")
+        self.assertEqual(req2.meta["redirect_urls"], ["http://scrapytest.org/first"])
+        self.assertEqual(req3.url, "http://scrapytest.org/redirected2")
         self.assertEqual(
-            req3.meta['redirect_urls'],
-            ['http://scrapytest.org/first', 'http://scrapytest.org/redirected']
+            req3.meta["redirect_urls"],
+            ["http://scrapytest.org/first", "http://scrapytest.org/redirected"],
         )
 
     def test_redirect_reasons(self):
-        req1 = Request('http://scrapytest.org/first')
-        rsp1 = HtmlResponse('http://scrapytest.org/first', body=self._body(url='/redirected'))
+        req1 = Request("http://scrapytest.org/first")
+        rsp1 = HtmlResponse(
+            "http://scrapytest.org/first", body=self._body(url="/redirected")
+        )
         req2 = self.mw.process_response(req1, rsp1, self.spider)
-        rsp2 = HtmlResponse('http://scrapytest.org/redirected', body=self._body(url='/redirected1'))
+        rsp2 = HtmlResponse(
+            "http://scrapytest.org/redirected", body=self._body(url="/redirected1")
+        )
         req3 = self.mw.process_response(req2, rsp2, self.spider)
 
-        self.assertEqual(req2.meta['redirect_reasons'], ['meta refresh'])
-        self.assertEqual(req3.meta['redirect_reasons'], ['meta refresh', 'meta refresh'])
+        self.assertEqual(req2.meta["redirect_reasons"], ["meta refresh"])
+        self.assertEqual(
+            req3.meta["redirect_reasons"], ["meta refresh", "meta refresh"]
+        )
 
     def test_ignore_tags_default(self):
-        req = Request(url='http://example.org')
-        body = ('''<noscript><meta http-equiv="refresh" '''
-                '''content="0;URL='http://example.org/newpage'"></noscript>''')
+        req = Request(url="http://example.org")
+        body = (
+            """<noscript><meta http-equiv="refresh" """
+            """content="0;URL='http://example.org/newpage'"></noscript>"""
+        )
         rsp = HtmlResponse(req.url, body=body.encode())
         req2 = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req2, Request)
-        self.assertEqual(req2.url, 'http://example.org/newpage')
+        self.assertEqual(req2.url, "http://example.org/newpage")
 
     def test_ignore_tags_1_x_list(self):
         """Test that Scrapy 1.x behavior remains possible"""
-        settings = {'METAREFRESH_IGNORE_TAGS': ['script', 'noscript']}
+        settings = {"METAREFRESH_IGNORE_TAGS": ["script", "noscript"]}
         crawler = get_crawler(Spider, settings)
         mw = MetaRefreshMiddleware.from_crawler(crawler)
-        req = Request(url='http://example.org')
-        body = ('''<noscript><meta http-equiv="refresh" '''
-                '''content="0;URL='http://example.org/newpage'"></noscript>''')
+        req = Request(url="http://example.org")
+        body = (
+            """<noscript><meta http-equiv="refresh" """
+            """content="0;URL='http://example.org/newpage'"></noscript>"""
+        )
         rsp = HtmlResponse(req.url, body=body.encode())
         response = mw.process_response(req, rsp, self.spider)
         assert isinstance(response, Response)
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_retry.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_retry.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,83 +8,86 @@
     ConnectionDone,
     ConnectionLost,
     DNSLookupError,
     TCPTimedOutError,
 )
 from twisted.web.client import ResponseFailed
 
-from scrapy.downloadermiddlewares.retry import get_retry_request, RetryMiddleware
+from scrapy.downloadermiddlewares.retry import RetryMiddleware, get_retry_request
 from scrapy.exceptions import IgnoreRequest
 from scrapy.http import Request, Response
 from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
 
 
 class RetryTest(unittest.TestCase):
     def setUp(self):
         self.crawler = get_crawler(Spider)
-        self.spider = self.crawler._create_spider('foo')
+        self.spider = self.crawler._create_spider("foo")
         self.mw = RetryMiddleware.from_crawler(self.crawler)
         self.mw.max_retry_times = 2
 
     def test_priority_adjust(self):
-        req = Request('http://www.scrapytest.org/503')
-        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
+        req = Request("http://www.scrapytest.org/503")
+        rsp = Response("http://www.scrapytest.org/503", body=b"", status=503)
         req2 = self.mw.process_response(req, rsp, self.spider)
         assert req2.priority < req.priority
 
     def test_404(self):
-        req = Request('http://www.scrapytest.org/404')
-        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)
+        req = Request("http://www.scrapytest.org/404")
+        rsp = Response("http://www.scrapytest.org/404", body=b"", status=404)
 
         # dont retry 404s
         assert self.mw.process_response(req, rsp, self.spider) is rsp
 
     def test_dont_retry(self):
-        req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})
-        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
+        req = Request("http://www.scrapytest.org/503", meta={"dont_retry": True})
+        rsp = Response("http://www.scrapytest.org/503", body=b"", status=503)
 
         # first retry
         r = self.mw.process_response(req, rsp, self.spider)
         assert r is rsp
 
         # Test retry when dont_retry set to False
-        req = Request('http://www.scrapytest.org/503', meta={'dont_retry': False})
-        rsp = Response('http://www.scrapytest.org/503')
+        req = Request("http://www.scrapytest.org/503", meta={"dont_retry": False})
+        rsp = Response("http://www.scrapytest.org/503")
 
         # first retry
         r = self.mw.process_response(req, rsp, self.spider)
         assert r is rsp
 
     def test_dont_retry_exc(self):
-        req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})
+        req = Request("http://www.scrapytest.org/503", meta={"dont_retry": True})
 
         r = self.mw.process_exception(req, DNSLookupError(), self.spider)
         assert r is None
 
     def test_503(self):
-        req = Request('http://www.scrapytest.org/503')
-        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
+        req = Request("http://www.scrapytest.org/503")
+        rsp = Response("http://www.scrapytest.org/503", body=b"", status=503)
 
         # first retry
         req = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req, Request)
-        self.assertEqual(req.meta['retry_times'], 1)
+        self.assertEqual(req.meta["retry_times"], 1)
 
         # second retry
         req = self.mw.process_response(req, rsp, self.spider)
         assert isinstance(req, Request)
-        self.assertEqual(req.meta['retry_times'], 2)
+        self.assertEqual(req.meta["retry_times"], 2)
 
         # discard it
         assert self.mw.process_response(req, rsp, self.spider) is rsp
 
-        assert self.crawler.stats.get_value('retry/max_reached') == 1
-        assert self.crawler.stats.get_value('retry/reason_count/503 Service Unavailable') == 2
-        assert self.crawler.stats.get_value('retry/count') == 2
+        assert self.crawler.stats.get_value("retry/max_reached") == 1
+        assert (
+            self.crawler.stats.get_value("retry/reason_count/503 Service Unavailable")
+            == 2
+        )
+        assert self.crawler.stats.get_value("retry/count") == 2
 
     def test_twistederrors(self):
         exceptions = [
             ConnectError,
             ConnectionDone,
             ConnectionLost,
             ConnectionRefusedError,
@@ -92,148 +95,151 @@
             DNSLookupError,
             ResponseFailed,
             TCPTimedOutError,
             TimeoutError,
         ]
 
         for exc in exceptions:
-            req = Request(f'http://www.scrapytest.org/{exc.__name__}')
-            self._test_retry_exception(req, exc('foo'))
+            req = Request(f"http://www.scrapytest.org/{exc.__name__}")
+            self._test_retry_exception(req, exc("foo"))
 
         stats = self.crawler.stats
-        assert stats.get_value('retry/max_reached') == len(exceptions)
-        assert stats.get_value('retry/count') == len(exceptions) * 2
-        assert stats.get_value('retry/reason_count/twisted.internet.defer.TimeoutError') == 2
+        assert stats.get_value("retry/max_reached") == len(exceptions)
+        assert stats.get_value("retry/count") == len(exceptions) * 2
+        assert (
+            stats.get_value("retry/reason_count/twisted.internet.defer.TimeoutError")
+            == 2
+        )
 
     def _test_retry_exception(self, req, exception):
         # first retry
         req = self.mw.process_exception(req, exception, self.spider)
         assert isinstance(req, Request)
-        self.assertEqual(req.meta['retry_times'], 1)
+        self.assertEqual(req.meta["retry_times"], 1)
 
         # second retry
         req = self.mw.process_exception(req, exception, self.spider)
         assert isinstance(req, Request)
-        self.assertEqual(req.meta['retry_times'], 2)
+        self.assertEqual(req.meta["retry_times"], 2)
 
         # discard it
         req = self.mw.process_exception(req, exception, self.spider)
         self.assertEqual(req, None)
 
 
 class MaxRetryTimesTest(unittest.TestCase):
 
-    invalid_url = 'http://www.scrapytest.org/invalid_url'
+    invalid_url = "http://www.scrapytest.org/invalid_url"
 
     def get_spider_and_middleware(self, settings=None):
         crawler = get_crawler(Spider, settings or {})
-        spider = crawler._create_spider('foo')
+        spider = crawler._create_spider("foo")
         middleware = RetryMiddleware.from_crawler(crawler)
         return spider, middleware
 
     def test_with_settings_zero(self):
         max_retry_times = 0
-        settings = {'RETRY_TIMES': max_retry_times}
+        settings = {"RETRY_TIMES": max_retry_times}
         spider, middleware = self.get_spider_and_middleware(settings)
         req = Request(self.invalid_url)
         self._test_retry(
             req,
-            DNSLookupError('foo'),
+            DNSLookupError("foo"),
             max_retry_times,
             spider=spider,
             middleware=middleware,
         )
 
     def test_with_metakey_zero(self):
         max_retry_times = 0
         spider, middleware = self.get_spider_and_middleware()
-        meta = {'max_retry_times': max_retry_times}
+        meta = {"max_retry_times": max_retry_times}
         req = Request(self.invalid_url, meta=meta)
         self._test_retry(
             req,
-            DNSLookupError('foo'),
+            DNSLookupError("foo"),
             max_retry_times,
             spider=spider,
             middleware=middleware,
         )
 
     def test_without_metakey(self):
         max_retry_times = 5
-        settings = {'RETRY_TIMES': max_retry_times}
+        settings = {"RETRY_TIMES": max_retry_times}
         spider, middleware = self.get_spider_and_middleware(settings)
         req = Request(self.invalid_url)
         self._test_retry(
             req,
-            DNSLookupError('foo'),
+            DNSLookupError("foo"),
             max_retry_times,
             spider=spider,
             middleware=middleware,
         )
 
     def test_with_metakey_greater(self):
         meta_max_retry_times = 3
         middleware_max_retry_times = 2
 
-        req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})
+        req1 = Request(self.invalid_url, meta={"max_retry_times": meta_max_retry_times})
         req2 = Request(self.invalid_url)
 
-        settings = {'RETRY_TIMES': middleware_max_retry_times}
+        settings = {"RETRY_TIMES": middleware_max_retry_times}
         spider, middleware = self.get_spider_and_middleware(settings)
 
         self._test_retry(
             req1,
-            DNSLookupError('foo'),
+            DNSLookupError("foo"),
             meta_max_retry_times,
             spider=spider,
             middleware=middleware,
         )
         self._test_retry(
             req2,
-            DNSLookupError('foo'),
+            DNSLookupError("foo"),
             middleware_max_retry_times,
             spider=spider,
             middleware=middleware,
         )
 
     def test_with_metakey_lesser(self):
         meta_max_retry_times = 4
         middleware_max_retry_times = 5
 
-        req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})
+        req1 = Request(self.invalid_url, meta={"max_retry_times": meta_max_retry_times})
         req2 = Request(self.invalid_url)
 
-        settings = {'RETRY_TIMES': middleware_max_retry_times}
+        settings = {"RETRY_TIMES": middleware_max_retry_times}
         spider, middleware = self.get_spider_and_middleware(settings)
 
         self._test_retry(
             req1,
-            DNSLookupError('foo'),
+            DNSLookupError("foo"),
             meta_max_retry_times,
             spider=spider,
             middleware=middleware,
         )
         self._test_retry(
             req2,
-            DNSLookupError('foo'),
+            DNSLookupError("foo"),
             middleware_max_retry_times,
             spider=spider,
             middleware=middleware,
         )
 
     def test_with_dont_retry(self):
         max_retry_times = 4
         spider, middleware = self.get_spider_and_middleware()
         meta = {
-            'max_retry_times': max_retry_times,
-            'dont_retry': True,
+            "max_retry_times": max_retry_times,
+            "dont_retry": True,
         }
         req = Request(self.invalid_url, meta=meta)
         self._test_retry(
             req,
-            DNSLookupError('foo'),
+            DNSLookupError("foo"),
             0,
             spider=spider,
             middleware=middleware,
         )
 
     def _test_retry(
         self,
@@ -252,118 +258,114 @@
 
         # discard it
         req = middleware.process_exception(req, exception, spider)
         self.assertEqual(req, None)
 
 
 class GetRetryRequestTest(unittest.TestCase):
-
     def get_spider(self, settings=None):
         crawler = get_crawler(Spider, settings or {})
-        return crawler._create_spider('foo')
+        return crawler._create_spider("foo")
 
     def test_basic_usage(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
         with LogCapture() as log:
             new_request = get_retry_request(
                 request,
                 spider=spider,
             )
         self.assertIsInstance(new_request, Request)
         self.assertNotEqual(new_request, request)
         self.assertEqual(new_request.dont_filter, True)
         expected_retry_times = 1
-        self.assertEqual(new_request.meta['retry_times'], expected_retry_times)
+        self.assertEqual(new_request.meta["retry_times"], expected_retry_times)
         self.assertEqual(new_request.priority, -1)
         expected_reason = "unspecified"
-        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):
+        for stat in ("retry/count", f"retry/reason_count/{expected_reason}"):
             self.assertEqual(spider.crawler.stats.get_value(stat), 1)
         log.check_present(
             (
                 "scrapy.downloadermiddlewares.retry",
                 "DEBUG",
                 f"Retrying {request} (failed {expected_retry_times} times): "
                 f"{expected_reason}",
             )
         )
 
     def test_max_retries_reached(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
         max_retry_times = 0
         with LogCapture() as log:
             new_request = get_retry_request(
                 request,
                 spider=spider,
                 max_retry_times=max_retry_times,
             )
         self.assertEqual(new_request, None)
-        self.assertEqual(
-            spider.crawler.stats.get_value('retry/max_reached'),
-            1
-        )
+        self.assertEqual(spider.crawler.stats.get_value("retry/max_reached"), 1)
         failure_count = max_retry_times + 1
         expected_reason = "unspecified"
         log.check_present(
             (
                 "scrapy.downloadermiddlewares.retry",
                 "ERROR",
                 f"Gave up retrying {request} (failed {failure_count} times): "
                 f"{expected_reason}",
             )
         )
 
     def test_one_retry(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
         with LogCapture() as log:
             new_request = get_retry_request(
                 request,
                 spider=spider,
                 max_retry_times=1,
             )
         self.assertIsInstance(new_request, Request)
         self.assertNotEqual(new_request, request)
         self.assertEqual(new_request.dont_filter, True)
         expected_retry_times = 1
-        self.assertEqual(new_request.meta['retry_times'], expected_retry_times)
+        self.assertEqual(new_request.meta["retry_times"], expected_retry_times)
         self.assertEqual(new_request.priority, -1)
         expected_reason = "unspecified"
-        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):
+        for stat in ("retry/count", f"retry/reason_count/{expected_reason}"):
             self.assertEqual(spider.crawler.stats.get_value(stat), 1)
         log.check_present(
             (
                 "scrapy.downloadermiddlewares.retry",
                 "DEBUG",
                 f"Retrying {request} (failed {expected_retry_times} times): "
                 f"{expected_reason}",
             )
         )
 
     def test_two_retries(self):
         spider = self.get_spider()
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         new_request = request
         max_retry_times = 2
         for index in range(max_retry_times):
             with LogCapture() as log:
                 new_request = get_retry_request(
                     new_request,
                     spider=spider,
                     max_retry_times=max_retry_times,
                 )
             self.assertIsInstance(new_request, Request)
             self.assertNotEqual(new_request, request)
             self.assertEqual(new_request.dont_filter, True)
             expected_retry_times = index + 1
-            self.assertEqual(new_request.meta['retry_times'], expected_retry_times)
+            self.assertEqual(new_request.meta["retry_times"], expected_retry_times)
             self.assertEqual(new_request.priority, -expected_retry_times)
             expected_reason = "unspecified"
-            for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):
+            for stat in ("retry/count", f"retry/reason_count/{expected_reason}"):
                 value = spider.crawler.stats.get_value(stat)
                 self.assertEqual(value, expected_retry_times)
             log.check_present(
                 (
                     "scrapy.downloadermiddlewares.retry",
                     "DEBUG",
                     f"Retrying {request} (failed {expected_retry_times} times): "
@@ -374,220 +376,217 @@
         with LogCapture() as log:
             new_request = get_retry_request(
                 new_request,
                 spider=spider,
                 max_retry_times=max_retry_times,
             )
         self.assertEqual(new_request, None)
-        self.assertEqual(
-            spider.crawler.stats.get_value('retry/max_reached'),
-            1
-        )
+        self.assertEqual(spider.crawler.stats.get_value("retry/max_reached"), 1)
         failure_count = max_retry_times + 1
         expected_reason = "unspecified"
         log.check_present(
             (
                 "scrapy.downloadermiddlewares.retry",
                 "ERROR",
                 f"Gave up retrying {request} (failed {failure_count} times): "
                 f"{expected_reason}",
             )
         )
 
     def test_no_spider(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         with self.assertRaises(TypeError):
             get_retry_request(request)  # pylint: disable=missing-kwoa
 
     def test_max_retry_times_setting(self):
         max_retry_times = 0
-        spider = self.get_spider({'RETRY_TIMES': max_retry_times})
-        request = Request('https://example.com')
+        spider = self.get_spider({"RETRY_TIMES": max_retry_times})
+        request = Request("https://example.com")
         new_request = get_retry_request(
             request,
             spider=spider,
         )
         self.assertEqual(new_request, None)
 
     def test_max_retry_times_meta(self):
         max_retry_times = 0
-        spider = self.get_spider({'RETRY_TIMES': max_retry_times + 1})
-        meta = {'max_retry_times': max_retry_times}
-        request = Request('https://example.com', meta=meta)
+        spider = self.get_spider({"RETRY_TIMES": max_retry_times + 1})
+        meta = {"max_retry_times": max_retry_times}
+        request = Request("https://example.com", meta=meta)
         new_request = get_retry_request(
             request,
             spider=spider,
         )
         self.assertEqual(new_request, None)
 
     def test_max_retry_times_argument(self):
         max_retry_times = 0
-        spider = self.get_spider({'RETRY_TIMES': max_retry_times + 1})
-        meta = {'max_retry_times': max_retry_times + 1}
-        request = Request('https://example.com', meta=meta)
+        spider = self.get_spider({"RETRY_TIMES": max_retry_times + 1})
+        meta = {"max_retry_times": max_retry_times + 1}
+        request = Request("https://example.com", meta=meta)
         new_request = get_retry_request(
             request,
             spider=spider,
             max_retry_times=max_retry_times,
         )
         self.assertEqual(new_request, None)
 
     def test_priority_adjust_setting(self):
         priority_adjust = 1
-        spider = self.get_spider({'RETRY_PRIORITY_ADJUST': priority_adjust})
-        request = Request('https://example.com')
+        spider = self.get_spider({"RETRY_PRIORITY_ADJUST": priority_adjust})
+        request = Request("https://example.com")
         new_request = get_retry_request(
             request,
             spider=spider,
         )
         self.assertEqual(new_request.priority, priority_adjust)
 
     def test_priority_adjust_argument(self):
         priority_adjust = 1
-        spider = self.get_spider({'RETRY_PRIORITY_ADJUST': priority_adjust + 1})
-        request = Request('https://example.com')
+        spider = self.get_spider({"RETRY_PRIORITY_ADJUST": priority_adjust + 1})
+        request = Request("https://example.com")
         new_request = get_retry_request(
             request,
             spider=spider,
             priority_adjust=priority_adjust,
         )
         self.assertEqual(new_request.priority, priority_adjust)
 
     def test_log_extra_retry_success(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
-        with LogCapture(attributes=('spider',)) as log:
+        with LogCapture(attributes=("spider",)) as log:
             get_retry_request(
                 request,
                 spider=spider,
             )
         log.check_present(spider)
 
     def test_log_extra_retries_exceeded(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
-        with LogCapture(attributes=('spider',)) as log:
+        with LogCapture(attributes=("spider",)) as log:
             get_retry_request(
                 request,
                 spider=spider,
                 max_retry_times=0,
             )
         log.check_present(spider)
 
     def test_reason_string(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
-        expected_reason = 'because'
+        expected_reason = "because"
         with LogCapture() as log:
             get_retry_request(
                 request,
                 spider=spider,
                 reason=expected_reason,
             )
         expected_retry_times = 1
-        for stat in ('retry/count', f'retry/reason_count/{expected_reason}'):
+        for stat in ("retry/count", f"retry/reason_count/{expected_reason}"):
             self.assertEqual(spider.crawler.stats.get_value(stat), 1)
         log.check_present(
             (
                 "scrapy.downloadermiddlewares.retry",
                 "DEBUG",
                 f"Retrying {request} (failed {expected_retry_times} times): "
                 f"{expected_reason}",
             )
         )
 
     def test_reason_builtin_exception(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
         expected_reason = NotImplementedError()
-        expected_reason_string = 'builtins.NotImplementedError'
+        expected_reason_string = "builtins.NotImplementedError"
         with LogCapture() as log:
             get_retry_request(
                 request,
                 spider=spider,
                 reason=expected_reason,
             )
         expected_retry_times = 1
         stat = spider.crawler.stats.get_value(
-            f'retry/reason_count/{expected_reason_string}'
+            f"retry/reason_count/{expected_reason_string}"
         )
         self.assertEqual(stat, 1)
         log.check_present(
             (
                 "scrapy.downloadermiddlewares.retry",
                 "DEBUG",
                 f"Retrying {request} (failed {expected_retry_times} times): "
                 f"{expected_reason}",
             )
         )
 
     def test_reason_builtin_exception_class(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
         expected_reason = NotImplementedError
-        expected_reason_string = 'builtins.NotImplementedError'
+        expected_reason_string = "builtins.NotImplementedError"
         with LogCapture() as log:
             get_retry_request(
                 request,
                 spider=spider,
                 reason=expected_reason,
             )
         expected_retry_times = 1
         stat = spider.crawler.stats.get_value(
-            f'retry/reason_count/{expected_reason_string}'
+            f"retry/reason_count/{expected_reason_string}"
         )
         self.assertEqual(stat, 1)
         log.check_present(
             (
                 "scrapy.downloadermiddlewares.retry",
                 "DEBUG",
                 f"Retrying {request} (failed {expected_retry_times} times): "
                 f"{expected_reason}",
             )
         )
 
     def test_reason_custom_exception(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
         expected_reason = IgnoreRequest()
-        expected_reason_string = 'scrapy.exceptions.IgnoreRequest'
+        expected_reason_string = "scrapy.exceptions.IgnoreRequest"
         with LogCapture() as log:
             get_retry_request(
                 request,
                 spider=spider,
                 reason=expected_reason,
             )
         expected_retry_times = 1
         stat = spider.crawler.stats.get_value(
-            f'retry/reason_count/{expected_reason_string}'
+            f"retry/reason_count/{expected_reason_string}"
         )
         self.assertEqual(stat, 1)
         log.check_present(
             (
                 "scrapy.downloadermiddlewares.retry",
                 "DEBUG",
                 f"Retrying {request} (failed {expected_retry_times} times): "
                 f"{expected_reason}",
             )
         )
 
     def test_reason_custom_exception_class(self):
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         spider = self.get_spider()
         expected_reason = IgnoreRequest
-        expected_reason_string = 'scrapy.exceptions.IgnoreRequest'
+        expected_reason_string = "scrapy.exceptions.IgnoreRequest"
         with LogCapture() as log:
             get_retry_request(
                 request,
                 spider=spider,
                 reason=expected_reason,
             )
         expected_retry_times = 1
         stat = spider.crawler.stats.get_value(
-            f'retry/reason_count/{expected_reason_string}'
+            f"retry/reason_count/{expected_reason_string}"
         )
         self.assertEqual(stat, 1)
         log.check_present(
             (
                 "scrapy.downloadermiddlewares.retry",
                 "DEBUG",
                 f"Retrying {request} (failed {expected_retry_times} times): "
@@ -622,13 +621,16 @@
         stats_key = "custom_retry"
         get_retry_request(
             request,
             spider=spider,
             reason=expected_reason,
             stats_base_key=stats_key,
         )
-        for stat in (f"{stats_key}/count", f"{stats_key}/reason_count/{expected_reason}"):
+        for stat in (
+            f"{stats_key}/count",
+            f"{stats_key}/reason_count/{expected_reason}",
+        ):
             self.assertEqual(spider.crawler.stats.get_value(stat), 1)
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_robotstxt.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_robotstxt.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,202 +1,269 @@
 from unittest import mock
 
-from twisted.internet import reactor, error
+from twisted.internet import error, reactor
 from twisted.internet.defer import Deferred, DeferredList, maybeDeferred
 from twisted.python import failure
 from twisted.trial import unittest
-from scrapy.downloadermiddlewares.robotstxt import (RobotsTxtMiddleware,
-                                                    logger as mw_module_logger)
+
+from scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware
+from scrapy.downloadermiddlewares.robotstxt import logger as mw_module_logger
 from scrapy.exceptions import IgnoreRequest, NotConfigured
 from scrapy.http import Request, Response, TextResponse
+from scrapy.http.request import NO_CALLBACK
 from scrapy.settings import Settings
-from tests.test_robotstxt_interface import rerp_available, reppy_available
+from tests.test_robotstxt_interface import reppy_available, rerp_available
 
 
 class RobotsTxtMiddlewareTest(unittest.TestCase):
-
     def setUp(self):
         self.crawler = mock.MagicMock()
         self.crawler.settings = Settings()
         self.crawler.engine.download = mock.MagicMock()
 
     def tearDown(self):
         del self.crawler
 
     def test_robotstxt_settings(self):
         self.crawler.settings = Settings()
-        self.crawler.settings.set('USER_AGENT', 'CustomAgent')
+        self.crawler.settings.set("USER_AGENT", "CustomAgent")
         self.assertRaises(NotConfigured, RobotsTxtMiddleware, self.crawler)
 
     def _get_successful_crawler(self):
         crawler = self.crawler
-        crawler.settings.set('ROBOTSTXT_OBEY', True)
+        crawler.settings.set("ROBOTSTXT_OBEY", True)
         ROBOTS = """
 User-Agent: *
 Disallow: /admin/
 Disallow: /static/
 # taken from https://en.wikipedia.org/robots.txt
 Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:
 Disallow: /wiki/Kyttj:
 User-Agent: UnicdeBt
 Disallow: /some/randome/page.html
-""".encode('utf-8')
-        response = TextResponse('http://site.local/robots.txt', body=ROBOTS)
+""".encode(
+            "utf-8"
+        )
+        response = TextResponse("http://site.local/robots.txt", body=ROBOTS)
 
         def return_response(request):
             deferred = Deferred()
             reactor.callFromThread(deferred.callback, response)
             return deferred
+
         crawler.engine.download.side_effect = return_response
         return crawler
 
     def test_robotstxt(self):
         middleware = RobotsTxtMiddleware(self._get_successful_crawler())
-        return DeferredList([
-            self.assertNotIgnored(Request('http://site.local/allowed'), middleware),
-            self.assertIgnored(Request('http://site.local/admin/main'), middleware),
-            self.assertIgnored(Request('http://site.local/static/'), middleware),
-            self.assertIgnored(Request('http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:'), middleware),
-            self.assertIgnored(Request('http://site.local/wiki/Kyttj:'), middleware)
-        ], fireOnOneErrback=True)
+        return DeferredList(
+            [
+                self.assertNotIgnored(Request("http://site.local/allowed"), middleware),
+                maybeDeferred(self.assertRobotsTxtRequested, "http://site.local"),
+                self.assertIgnored(Request("http://site.local/admin/main"), middleware),
+                self.assertIgnored(Request("http://site.local/static/"), middleware),
+                self.assertIgnored(
+                    Request("http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:"),
+                    middleware,
+                ),
+                self.assertIgnored(
+                    Request("http://site.local/wiki/Kyttj:"), middleware
+                ),
+            ],
+            fireOnOneErrback=True,
+        )
 
     def test_robotstxt_ready_parser(self):
         middleware = RobotsTxtMiddleware(self._get_successful_crawler())
-        d = self.assertNotIgnored(Request('http://site.local/allowed'), middleware)
-        d.addCallback(lambda _: self.assertNotIgnored(Request('http://site.local/allowed'), middleware))
+        d = self.assertNotIgnored(Request("http://site.local/allowed"), middleware)
+        d.addCallback(
+            lambda _: self.assertNotIgnored(
+                Request("http://site.local/allowed"), middleware
+            )
+        )
         return d
 
     def test_robotstxt_meta(self):
         middleware = RobotsTxtMiddleware(self._get_successful_crawler())
-        meta = {'dont_obey_robotstxt': True}
-        return DeferredList([
-            self.assertNotIgnored(Request('http://site.local/allowed', meta=meta), middleware),
-            self.assertNotIgnored(Request('http://site.local/admin/main', meta=meta), middleware),
-            self.assertNotIgnored(Request('http://site.local/static/', meta=meta), middleware)
-        ], fireOnOneErrback=True)
+        meta = {"dont_obey_robotstxt": True}
+        return DeferredList(
+            [
+                self.assertNotIgnored(
+                    Request("http://site.local/allowed", meta=meta), middleware
+                ),
+                self.assertNotIgnored(
+                    Request("http://site.local/admin/main", meta=meta), middleware
+                ),
+                self.assertNotIgnored(
+                    Request("http://site.local/static/", meta=meta), middleware
+                ),
+            ],
+            fireOnOneErrback=True,
+        )
 
     def _get_garbage_crawler(self):
         crawler = self.crawler
-        crawler.settings.set('ROBOTSTXT_OBEY', True)
-        response = Response('http://site.local/robots.txt', body=b'GIF89a\xd3\x00\xfe\x00\xa2')
+        crawler.settings.set("ROBOTSTXT_OBEY", True)
+        response = Response(
+            "http://site.local/robots.txt", body=b"GIF89a\xd3\x00\xfe\x00\xa2"
+        )
 
         def return_response(request):
             deferred = Deferred()
             reactor.callFromThread(deferred.callback, response)
             return deferred
+
         crawler.engine.download.side_effect = return_response
         return crawler
 
     def test_robotstxt_garbage(self):
         # garbage response should be discarded, equal 'allow all'
         middleware = RobotsTxtMiddleware(self._get_garbage_crawler())
-        deferred = DeferredList([
-            self.assertNotIgnored(Request('http://site.local'), middleware),
-            self.assertNotIgnored(Request('http://site.local/allowed'), middleware),
-            self.assertNotIgnored(Request('http://site.local/admin/main'), middleware),
-            self.assertNotIgnored(Request('http://site.local/static/'), middleware)
-        ], fireOnOneErrback=True)
+        deferred = DeferredList(
+            [
+                self.assertNotIgnored(Request("http://site.local"), middleware),
+                self.assertNotIgnored(Request("http://site.local/allowed"), middleware),
+                self.assertNotIgnored(
+                    Request("http://site.local/admin/main"), middleware
+                ),
+                self.assertNotIgnored(Request("http://site.local/static/"), middleware),
+            ],
+            fireOnOneErrback=True,
+        )
         return deferred
 
     def _get_emptybody_crawler(self):
         crawler = self.crawler
-        crawler.settings.set('ROBOTSTXT_OBEY', True)
-        response = Response('http://site.local/robots.txt')
+        crawler.settings.set("ROBOTSTXT_OBEY", True)
+        response = Response("http://site.local/robots.txt")
 
         def return_response(request):
             deferred = Deferred()
             reactor.callFromThread(deferred.callback, response)
             return deferred
+
         crawler.engine.download.side_effect = return_response
         return crawler
 
     def test_robotstxt_empty_response(self):
         # empty response should equal 'allow all'
         middleware = RobotsTxtMiddleware(self._get_emptybody_crawler())
-        return DeferredList([
-            self.assertNotIgnored(Request('http://site.local/allowed'), middleware),
-            self.assertNotIgnored(Request('http://site.local/admin/main'), middleware),
-            self.assertNotIgnored(Request('http://site.local/static/'), middleware)
-        ], fireOnOneErrback=True)
+        return DeferredList(
+            [
+                self.assertNotIgnored(Request("http://site.local/allowed"), middleware),
+                self.assertNotIgnored(
+                    Request("http://site.local/admin/main"), middleware
+                ),
+                self.assertNotIgnored(Request("http://site.local/static/"), middleware),
+            ],
+            fireOnOneErrback=True,
+        )
 
     def test_robotstxt_error(self):
-        self.crawler.settings.set('ROBOTSTXT_OBEY', True)
-        err = error.DNSLookupError('Robotstxt address not found')
+        self.crawler.settings.set("ROBOTSTXT_OBEY", True)
+        err = error.DNSLookupError("Robotstxt address not found")
 
         def return_failure(request):
             deferred = Deferred()
             reactor.callFromThread(deferred.errback, failure.Failure(err))
             return deferred
+
         self.crawler.engine.download.side_effect = return_failure
 
         middleware = RobotsTxtMiddleware(self.crawler)
         middleware._logerror = mock.MagicMock(side_effect=middleware._logerror)
-        deferred = middleware.process_request(Request('http://site.local'), None)
+        deferred = middleware.process_request(Request("http://site.local"), None)
         deferred.addCallback(lambda _: self.assertTrue(middleware._logerror.called))
         return deferred
 
     def test_robotstxt_immediate_error(self):
-        self.crawler.settings.set('ROBOTSTXT_OBEY', True)
-        err = error.DNSLookupError('Robotstxt address not found')
+        self.crawler.settings.set("ROBOTSTXT_OBEY", True)
+        err = error.DNSLookupError("Robotstxt address not found")
 
         def immediate_failure(request):
             deferred = Deferred()
             deferred.errback(failure.Failure(err))
             return deferred
+
         self.crawler.engine.download.side_effect = immediate_failure
 
         middleware = RobotsTxtMiddleware(self.crawler)
-        return self.assertNotIgnored(Request('http://site.local'), middleware)
+        return self.assertNotIgnored(Request("http://site.local"), middleware)
 
     def test_ignore_robotstxt_request(self):
-        self.crawler.settings.set('ROBOTSTXT_OBEY', True)
+        self.crawler.settings.set("ROBOTSTXT_OBEY", True)
 
         def ignore_request(request):
             deferred = Deferred()
             reactor.callFromThread(deferred.errback, failure.Failure(IgnoreRequest()))
             return deferred
+
         self.crawler.engine.download.side_effect = ignore_request
 
         middleware = RobotsTxtMiddleware(self.crawler)
         mw_module_logger.error = mock.MagicMock()
 
-        d = self.assertNotIgnored(Request('http://site.local/allowed'), middleware)
+        d = self.assertNotIgnored(Request("http://site.local/allowed"), middleware)
         d.addCallback(lambda _: self.assertFalse(mw_module_logger.error.called))
         return d
 
     def test_robotstxt_user_agent_setting(self):
         crawler = self._get_successful_crawler()
-        crawler.settings.set('ROBOTSTXT_USER_AGENT', 'Examplebot')
-        crawler.settings.set('USER_AGENT', 'Mozilla/5.0 (X11; Linux x86_64)')
+        crawler.settings.set("ROBOTSTXT_USER_AGENT", "Examplebot")
+        crawler.settings.set("USER_AGENT", "Mozilla/5.0 (X11; Linux x86_64)")
         middleware = RobotsTxtMiddleware(crawler)
         rp = mock.MagicMock(return_value=True)
-        middleware.process_request_2(rp, Request('http://site.local/allowed'), None)
-        rp.allowed.assert_called_once_with('http://site.local/allowed', 'Examplebot')
+        middleware.process_request_2(rp, Request("http://site.local/allowed"), None)
+        rp.allowed.assert_called_once_with("http://site.local/allowed", "Examplebot")
+
+    def test_robotstxt_local_file(self):
+        middleware = RobotsTxtMiddleware(self._get_emptybody_crawler())
+        assert not middleware.process_request(
+            Request("data:text/plain,Hello World data"), None
+        )
+        assert not middleware.process_request(
+            Request("file:///tests/sample_data/test_site/nothinghere.html"), None
+        )
+        assert isinstance(
+            middleware.process_request(Request("http://site.local/allowed"), None),
+            Deferred,
+        )
 
     def assertNotIgnored(self, request, middleware):
         spider = None  # not actually used
         dfd = maybeDeferred(middleware.process_request, request, spider)
         dfd.addCallback(self.assertIsNone)
         return dfd
 
     def assertIgnored(self, request, middleware):
         spider = None  # not actually used
-        return self.assertFailure(maybeDeferred(middleware.process_request, request, spider),
-                                  IgnoreRequest)
+        return self.assertFailure(
+            maybeDeferred(middleware.process_request, request, spider), IgnoreRequest
+        )
+
+    def assertRobotsTxtRequested(self, base_url):
+        calls = self.crawler.engine.download.call_args_list
+        request = calls[0][0][0]
+        self.assertEqual(request.url, f"{base_url}/robots.txt")
+        self.assertEqual(request.callback, NO_CALLBACK)
 
 
 class RobotsTxtMiddlewareWithRerpTest(RobotsTxtMiddlewareTest):
     if not rerp_available():
         skip = "Rerp parser is not installed"
 
     def setUp(self):
         super().setUp()
-        self.crawler.settings.set('ROBOTSTXT_PARSER', 'scrapy.robotstxt.RerpRobotParser')
+        self.crawler.settings.set(
+            "ROBOTSTXT_PARSER", "scrapy.robotstxt.RerpRobotParser"
+        )
 
 
 class RobotsTxtMiddlewareWithReppyTest(RobotsTxtMiddlewareTest):
     if not reppy_available():
         skip = "Reppy parser is not installed"
 
     def setUp(self):
         super().setUp()
-        self.crawler.settings.set('ROBOTSTXT_PARSER', 'scrapy.robotstxt.ReppyRobotParser')
+        self.crawler.settings.set(
+            "ROBOTSTXT_PARSER", "scrapy.robotstxt.ReppyRobotParser"
+        )
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_stats.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_stats.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,63 +11,61 @@
 
 
 class MyException(Exception):
     pass
 
 
 class TestDownloaderStats(TestCase):
-
     def setUp(self):
         self.crawler = get_crawler(Spider)
-        self.spider = self.crawler._create_spider('scrapytest.org')
+        self.spider = self.crawler._create_spider("scrapytest.org")
         self.mw = DownloaderStats(self.crawler.stats)
 
         self.crawler.stats.open_spider(self.spider)
 
-        self.req = Request('http://scrapytest.org')
-        self.res = Response('scrapytest.org', status=400)
+        self.req = Request("http://scrapytest.org")
+        self.res = Response("scrapytest.org", status=400)
 
     def assertStatsEqual(self, key, value):
         self.assertEqual(
             self.crawler.stats.get_value(key, spider=self.spider),
             value,
-            str(self.crawler.stats.get_stats(self.spider))
+            str(self.crawler.stats.get_stats(self.spider)),
         )
 
     def test_process_request(self):
         self.mw.process_request(self.req, self.spider)
-        self.assertStatsEqual('downloader/request_count', 1)
+        self.assertStatsEqual("downloader/request_count", 1)
 
     def test_process_response(self):
         self.mw.process_response(self.req, self.res, self.spider)
-        self.assertStatsEqual('downloader/response_count', 1)
+        self.assertStatsEqual("downloader/response_count", 1)
 
     def test_response_len(self):
-        body = (b'', b'not_empty')  # empty/notempty body
-        headers = ({}, {'lang': 'en'}, {'lang': 'en', 'User-Agent': 'scrapy'})  # 0 headers, 1h and 2h
+        body = (b"", b"not_empty")  # empty/notempty body
+        headers = (
+            {},
+            {"lang": "en"},
+            {"lang": "en", "User-Agent": "scrapy"},
+        )  # 0 headers, 1h and 2h
         test_responses = [  # form test responses with all combinations of body/headers
-            Response(
-                url='scrapytest.org',
-                status=200,
-                body=r[0],
-                headers=r[1]
-            )
+            Response(url="scrapytest.org", status=200, body=r[0], headers=r[1])
             for r in product(body, headers)
         ]
         for test_response in test_responses:
-            self.crawler.stats.set_value('downloader/response_bytes', 0)
+            self.crawler.stats.set_value("downloader/response_bytes", 0)
             self.mw.process_response(self.req, test_response, self.spider)
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore", ScrapyDeprecationWarning)
                 resp_size = len(response_httprepr(test_response))
-            self.assertStatsEqual('downloader/response_bytes', resp_size)
+            self.assertStatsEqual("downloader/response_bytes", resp_size)
 
     def test_process_exception(self):
         self.mw.process_exception(self.req, MyException(), self.spider)
-        self.assertStatsEqual('downloader/exception_count', 1)
+        self.assertStatsEqual("downloader/exception_count", 1)
         self.assertStatsEqual(
-            'downloader/exception_type_count/tests.test_downloadermiddleware_stats.MyException',
-            1
+            "downloader/exception_type_count/tests.test_downloadermiddleware_stats.MyException",
+            1,
         )
 
     def tearDown(self):
-        self.crawler.stats.close_spider(self.spider, '')
+        self.crawler.stats.close_spider(self.spider, "")
```

### Comparing `Scrapy-2.7.1/tests/test_downloadermiddleware_useragent.py` & `Scrapy-2.8.0/tests/test_downloadermiddleware_useragent.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,54 +1,54 @@
 from unittest import TestCase
 
-from scrapy.spiders import Spider
-from scrapy.http import Request
 from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware
+from scrapy.http import Request
+from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
 
 
 class UserAgentMiddlewareTest(TestCase):
-
     def get_spider_and_mw(self, default_useragent):
-        crawler = get_crawler(Spider, {'USER_AGENT': default_useragent})
-        spider = crawler._create_spider('foo')
+        crawler = get_crawler(Spider, {"USER_AGENT": default_useragent})
+        spider = crawler._create_spider("foo")
         return spider, UserAgentMiddleware.from_crawler(crawler)
 
     def test_default_agent(self):
-        spider, mw = self.get_spider_and_mw('default_useragent')
-        req = Request('http://scrapytest.org/')
+        spider, mw = self.get_spider_and_mw("default_useragent")
+        req = Request("http://scrapytest.org/")
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.headers['User-Agent'], b'default_useragent')
+        self.assertEqual(req.headers["User-Agent"], b"default_useragent")
 
     def test_remove_agent(self):
-        # settings UESR_AGENT to None should remove the user agent
-        spider, mw = self.get_spider_and_mw('default_useragent')
+        # settings USER_AGENT to None should remove the user agent
+        spider, mw = self.get_spider_and_mw("default_useragent")
         spider.user_agent = None
         mw.spider_opened(spider)
-        req = Request('http://scrapytest.org/')
+        req = Request("http://scrapytest.org/")
         assert mw.process_request(req, spider) is None
-        assert req.headers.get('User-Agent') is None
+        assert req.headers.get("User-Agent") is None
 
     def test_spider_agent(self):
-        spider, mw = self.get_spider_and_mw('default_useragent')
-        spider.user_agent = 'spider_useragent'
+        spider, mw = self.get_spider_and_mw("default_useragent")
+        spider.user_agent = "spider_useragent"
         mw.spider_opened(spider)
-        req = Request('http://scrapytest.org/')
+        req = Request("http://scrapytest.org/")
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.headers['User-Agent'], b'spider_useragent')
+        self.assertEqual(req.headers["User-Agent"], b"spider_useragent")
 
     def test_header_agent(self):
-        spider, mw = self.get_spider_and_mw('default_useragent')
-        spider.user_agent = 'spider_useragent'
+        spider, mw = self.get_spider_and_mw("default_useragent")
+        spider.user_agent = "spider_useragent"
         mw.spider_opened(spider)
-        req = Request('http://scrapytest.org/',
-                      headers={'User-Agent': 'header_useragent'})
+        req = Request(
+            "http://scrapytest.org/", headers={"User-Agent": "header_useragent"}
+        )
         assert mw.process_request(req, spider) is None
-        self.assertEqual(req.headers['User-Agent'], b'header_useragent')
+        self.assertEqual(req.headers["User-Agent"], b"header_useragent")
 
     def test_no_agent(self):
         spider, mw = self.get_spider_and_mw(None)
         spider.user_agent = None
         mw.spider_opened(spider)
-        req = Request('http://scrapytest.org/')
+        req = Request("http://scrapytest.org/")
         assert mw.process_request(req, spider) is None
-        assert 'User-Agent' not in req.headers
+        assert "User-Agent" not in req.headers
```

### Comparing `Scrapy-2.7.1/tests/test_dupefilters.py` & `Scrapy-2.8.0/tests/test_dupefilters.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 import hashlib
-import tempfile
-import unittest
 import shutil
-import os
 import sys
+import tempfile
+import unittest
+from pathlib import Path
+
 from testfixtures import LogCapture
 
+from scrapy.core.scheduler import Scheduler
 from scrapy.dupefilters import RFPDupeFilter
 from scrapy.http import Request
-from scrapy.core.scheduler import Scheduler
 from scrapy.utils.python import to_bytes
 from scrapy.utils.test import get_crawler
 from tests.spiders import SimpleSpider
 
 
 def _get_dupefilter(*, crawler=None, settings=None, open=True):
     if crawler is None:
@@ -21,245 +22,257 @@
     dupefilter = scheduler.df
     if open:
         dupefilter.open()
     return dupefilter
 
 
 class FromCrawlerRFPDupeFilter(RFPDupeFilter):
-
     @classmethod
     def from_crawler(cls, crawler):
         df = super().from_crawler(crawler)
-        df.method = 'from_crawler'
+        df.method = "from_crawler"
         return df
 
 
 class FromSettingsRFPDupeFilter(RFPDupeFilter):
-
     @classmethod
     def from_settings(cls, settings, *, fingerprinter=None):
         df = super().from_settings(settings, fingerprinter=fingerprinter)
-        df.method = 'from_settings'
+        df.method = "from_settings"
         return df
 
 
 class DirectDupeFilter:
-    method = 'n/a'
+    method = "n/a"
 
 
 class RFPDupeFilterTest(unittest.TestCase):
-
     def test_df_from_crawler_scheduler(self):
-        settings = {'DUPEFILTER_DEBUG': True,
-                    'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,
-                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}
+        settings = {
+            "DUPEFILTER_DEBUG": True,
+            "DUPEFILTER_CLASS": FromCrawlerRFPDupeFilter,
+            "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+        }
         crawler = get_crawler(settings_dict=settings)
         scheduler = Scheduler.from_crawler(crawler)
         self.assertTrue(scheduler.df.debug)
-        self.assertEqual(scheduler.df.method, 'from_crawler')
+        self.assertEqual(scheduler.df.method, "from_crawler")
 
     def test_df_from_settings_scheduler(self):
-        settings = {'DUPEFILTER_DEBUG': True,
-                    'DUPEFILTER_CLASS': FromSettingsRFPDupeFilter,
-                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}
+        settings = {
+            "DUPEFILTER_DEBUG": True,
+            "DUPEFILTER_CLASS": FromSettingsRFPDupeFilter,
+            "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+        }
         crawler = get_crawler(settings_dict=settings)
         scheduler = Scheduler.from_crawler(crawler)
         self.assertTrue(scheduler.df.debug)
-        self.assertEqual(scheduler.df.method, 'from_settings')
+        self.assertEqual(scheduler.df.method, "from_settings")
 
     def test_df_direct_scheduler(self):
-        settings = {'DUPEFILTER_CLASS': DirectDupeFilter,
-                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}
+        settings = {
+            "DUPEFILTER_CLASS": DirectDupeFilter,
+            "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+        }
         crawler = get_crawler(settings_dict=settings)
         scheduler = Scheduler.from_crawler(crawler)
-        self.assertEqual(scheduler.df.method, 'n/a')
+        self.assertEqual(scheduler.df.method, "n/a")
 
     def test_filter(self):
         dupefilter = _get_dupefilter()
-        r1 = Request('http://scrapytest.org/1')
-        r2 = Request('http://scrapytest.org/2')
-        r3 = Request('http://scrapytest.org/2')
+        r1 = Request("http://scrapytest.org/1")
+        r2 = Request("http://scrapytest.org/2")
+        r3 = Request("http://scrapytest.org/2")
 
         assert not dupefilter.request_seen(r1)
         assert dupefilter.request_seen(r1)
 
         assert not dupefilter.request_seen(r2)
         assert dupefilter.request_seen(r3)
 
-        dupefilter.close('finished')
+        dupefilter.close("finished")
 
     def test_dupefilter_path(self):
-        r1 = Request('http://scrapytest.org/1')
-        r2 = Request('http://scrapytest.org/2')
+        r1 = Request("http://scrapytest.org/1")
+        r2 = Request("http://scrapytest.org/2")
 
         path = tempfile.mkdtemp()
         try:
-            df = _get_dupefilter(settings={'JOBDIR': path}, open=False)
+            df = _get_dupefilter(settings={"JOBDIR": path}, open=False)
             try:
                 df.open()
                 assert not df.request_seen(r1)
                 assert df.request_seen(r1)
             finally:
-                df.close('finished')
+                df.close("finished")
 
-            df2 = _get_dupefilter(settings={'JOBDIR': path}, open=False)
+            df2 = _get_dupefilter(settings={"JOBDIR": path}, open=False)
             assert df != df2
             try:
                 df2.open()
                 assert df2.request_seen(r1)
                 assert not df2.request_seen(r2)
                 assert df2.request_seen(r2)
             finally:
-                df2.close('finished')
+                df2.close("finished")
         finally:
             shutil.rmtree(path)
 
     def test_request_fingerprint(self):
         """Test if customization of request_fingerprint method will change
         output of request_seen.
 
         """
         dupefilter = _get_dupefilter()
-        r1 = Request('http://scrapytest.org/index.html')
-        r2 = Request('http://scrapytest.org/INDEX.html')
+        r1 = Request("http://scrapytest.org/index.html")
+        r2 = Request("http://scrapytest.org/INDEX.html")
 
         assert not dupefilter.request_seen(r1)
         assert not dupefilter.request_seen(r2)
 
-        dupefilter.close('finished')
+        dupefilter.close("finished")
 
         class RequestFingerprinter:
-
             def fingerprint(self, request):
                 fp = hashlib.sha1()
                 fp.update(to_bytes(request.url.lower()))
                 return fp.digest()
 
-        settings = {'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter}
+        settings = {"REQUEST_FINGERPRINTER_CLASS": RequestFingerprinter}
         case_insensitive_dupefilter = _get_dupefilter(settings=settings)
 
         assert not case_insensitive_dupefilter.request_seen(r1)
         assert case_insensitive_dupefilter.request_seen(r2)
 
-        case_insensitive_dupefilter.close('finished')
+        case_insensitive_dupefilter.close("finished")
 
     def test_seenreq_newlines(self):
-        """ Checks against adding duplicate \r to
-        line endings on Windows platforms. """
+        """Checks against adding duplicate \r to
+        line endings on Windows platforms."""
 
-        r1 = Request('http://scrapytest.org/1')
+        r1 = Request("http://scrapytest.org/1")
 
         path = tempfile.mkdtemp()
-        crawler = get_crawler(settings_dict={'JOBDIR': path})
+        crawler = get_crawler(settings_dict={"JOBDIR": path})
         try:
             scheduler = Scheduler.from_crawler(crawler)
             df = scheduler.df
             df.open()
             df.request_seen(r1)
-            df.close('finished')
+            df.close("finished")
 
-            with open(os.path.join(path, 'requests.seen'), 'rb') as seen_file:
+            with Path(path, "requests.seen").open("rb") as seen_file:
                 line = next(seen_file).decode()
-                assert not line.endswith('\r\r\n')
-                if sys.platform == 'win32':
-                    assert line.endswith('\r\n')
+                assert not line.endswith("\r\r\n")
+                if sys.platform == "win32":
+                    assert line.endswith("\r\n")
                 else:
-                    assert line.endswith('\n')
+                    assert line.endswith("\n")
 
         finally:
             shutil.rmtree(path)
 
     def test_log(self):
         with LogCapture() as log:
-            settings = {'DUPEFILTER_DEBUG': False,
-                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,
-                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}
+            settings = {
+                "DUPEFILTER_DEBUG": False,
+                "DUPEFILTER_CLASS": FromCrawlerRFPDupeFilter,
+                "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+            }
             crawler = get_crawler(SimpleSpider, settings_dict=settings)
             spider = SimpleSpider.from_crawler(crawler)
             dupefilter = _get_dupefilter(crawler=crawler)
 
-            r1 = Request('http://scrapytest.org/index.html')
-            r2 = Request('http://scrapytest.org/index.html')
+            r1 = Request("http://scrapytest.org/index.html")
+            r2 = Request("http://scrapytest.org/index.html")
 
             dupefilter.log(r1, spider)
             dupefilter.log(r2, spider)
 
-            assert crawler.stats.get_value('dupefilter/filtered') == 2
+            assert crawler.stats.get_value("dupefilter/filtered") == 2
             log.check_present(
                 (
-                    'scrapy.dupefilters',
-                    'DEBUG',
-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> - no more'
-                    ' duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)'
+                    "scrapy.dupefilters",
+                    "DEBUG",
+                    "Filtered duplicate request: <GET http://scrapytest.org/index.html> - no more"
+                    " duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)",
                 )
             )
 
-            dupefilter.close('finished')
+            dupefilter.close("finished")
 
     def test_log_debug(self):
         with LogCapture() as log:
-            settings = {'DUPEFILTER_DEBUG': True,
-                        'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,
-                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}
+            settings = {
+                "DUPEFILTER_DEBUG": True,
+                "DUPEFILTER_CLASS": FromCrawlerRFPDupeFilter,
+                "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+            }
             crawler = get_crawler(SimpleSpider, settings_dict=settings)
             spider = SimpleSpider.from_crawler(crawler)
             dupefilter = _get_dupefilter(crawler=crawler)
 
-            r1 = Request('http://scrapytest.org/index.html')
-            r2 = Request('http://scrapytest.org/index.html',
-                         headers={'Referer': 'http://scrapytest.org/INDEX.html'})
+            r1 = Request("http://scrapytest.org/index.html")
+            r2 = Request(
+                "http://scrapytest.org/index.html",
+                headers={"Referer": "http://scrapytest.org/INDEX.html"},
+            )
 
             dupefilter.log(r1, spider)
             dupefilter.log(r2, spider)
 
-            assert crawler.stats.get_value('dupefilter/filtered') == 2
+            assert crawler.stats.get_value("dupefilter/filtered") == 2
             log.check_present(
                 (
-                    'scrapy.dupefilters',
-                    'DEBUG',
-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)'
+                    "scrapy.dupefilters",
+                    "DEBUG",
+                    "Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)",
                 )
             )
             log.check_present(
                 (
-                    'scrapy.dupefilters',
-                    'DEBUG',
-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html>'
-                    ' (referer: http://scrapytest.org/INDEX.html)'
+                    "scrapy.dupefilters",
+                    "DEBUG",
+                    "Filtered duplicate request: <GET http://scrapytest.org/index.html>"
+                    " (referer: http://scrapytest.org/INDEX.html)",
                 )
             )
 
-            dupefilter.close('finished')
+            dupefilter.close("finished")
 
     def test_log_debug_default_dupefilter(self):
         with LogCapture() as log:
-            settings = {'DUPEFILTER_DEBUG': True,
-                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}
+            settings = {
+                "DUPEFILTER_DEBUG": True,
+                "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+            }
             crawler = get_crawler(SimpleSpider, settings_dict=settings)
             spider = SimpleSpider.from_crawler(crawler)
             dupefilter = _get_dupefilter(crawler=crawler)
 
-            r1 = Request('http://scrapytest.org/index.html')
-            r2 = Request('http://scrapytest.org/index.html',
-                         headers={'Referer': 'http://scrapytest.org/INDEX.html'})
+            r1 = Request("http://scrapytest.org/index.html")
+            r2 = Request(
+                "http://scrapytest.org/index.html",
+                headers={"Referer": "http://scrapytest.org/INDEX.html"},
+            )
 
             dupefilter.log(r1, spider)
             dupefilter.log(r2, spider)
 
-            assert crawler.stats.get_value('dupefilter/filtered') == 2
+            assert crawler.stats.get_value("dupefilter/filtered") == 2
             log.check_present(
                 (
-                    'scrapy.dupefilters',
-                    'DEBUG',
-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)'
+                    "scrapy.dupefilters",
+                    "DEBUG",
+                    "Filtered duplicate request: <GET http://scrapytest.org/index.html> (referer: None)",
                 )
             )
             log.check_present(
                 (
-                    'scrapy.dupefilters',
-                    'DEBUG',
-                    'Filtered duplicate request: <GET http://scrapytest.org/index.html>'
-                    ' (referer: http://scrapytest.org/INDEX.html)'
+                    "scrapy.dupefilters",
+                    "DEBUG",
+                    "Filtered duplicate request: <GET http://scrapytest.org/index.html>"
+                    " (referer: http://scrapytest.org/INDEX.html)",
                 )
             )
 
-            dupefilter.close('finished')
+            dupefilter.close("finished")
```

### Comparing `Scrapy-2.7.1/tests/test_engine.py` & `Scrapy-2.8.0/tests/test_engine.py`

 * *Files 13% similar despite different names*

```diff
@@ -6,41 +6,40 @@
 
 To view the testing web server in a browser you can start it by running this
 module with the ``runserver`` argument::
 
     python test_engine.py runserver
 """
 
-import os
 import re
 import subprocess
 import sys
 from collections import defaultdict
+from dataclasses import dataclass
+from pathlib import Path
 from threading import Timer
 from urllib.parse import urlparse
-from dataclasses import dataclass
 
-import pytest
 import attr
+import pytest
 from itemadapter import ItemAdapter
 from pydispatch import dispatcher
 from twisted.internet import defer, reactor
 from twisted.trial import unittest
 from twisted.web import server, static, util
 
 from scrapy import signals
 from scrapy.core.engine import ExecutionEngine
 from scrapy.exceptions import CloseSpider, ScrapyDeprecationWarning
 from scrapy.http import Request
-from scrapy.item import Item, Field
+from scrapy.item import Field, Item
 from scrapy.linkextractors import LinkExtractor
 from scrapy.spiders import Spider
 from scrapy.utils.signal import disconnect_all
 from scrapy.utils.test import get_crawler
-
 from tests import get_testdata, tests_datadir
 
 
 class TestItem(Item):
     name = Field()
     url = Field()
     price = Field()
@@ -77,19 +76,19 @@
             if itemre.search(link.url):
                 yield Request(url=link.url, callback=self.parse_item)
 
     def parse_item(self, response):
         adapter = ItemAdapter(self.item_cls())
         m = self.name_re.search(response.text)
         if m:
-            adapter['name'] = m.group(1)
-        adapter['url'] = response.url
+            adapter["name"] = m.group(1)
+        adapter["url"] = response.url
         m = self.price_re.search(response.text)
         if m:
-            adapter['price'] = m.group(1)
+            adapter["price"] = m.group(1)
         return adapter.item
 
 
 class TestDupeFilterSpider(TestSpider):
     def start_requests(self):
         return (Request(url) for url in self.start_urls)  # no dont_filter=True
 
@@ -105,15 +104,15 @@
 class DataClassItemsSpider(TestSpider):
     item_cls = DataClassItem
 
 
 class ItemZeroDivisionErrorSpider(TestSpider):
     custom_settings = {
         "ITEM_PIPELINES": {
-            "tests.pipelines.ProcessWithZeroDivisionErrorPipiline": 300,
+            "tests.pipelines.ProcessWithZeroDivisionErrorPipeline": 300,
         }
     }
 
 
 class ChangeCloseReasonSpider(TestSpider):
     @classmethod
     def from_crawler(cls, crawler, *args, **kwargs):
@@ -123,25 +122,27 @@
         return spider
 
     def spider_idle(self):
         raise CloseSpider(reason="custom_reason")
 
 
 def start_test_site(debug=False):
-    root_dir = os.path.join(tests_datadir, "test_site")
-    r = static.File(root_dir)
+    root_dir = Path(tests_datadir, "test_site")
+    r = static.File(str(root_dir))
     r.putChild(b"redirect", util.Redirect(b"/redirected"))
     r.putChild(b"redirected", static.Data(b"Redirected here", "text/plain"))
     numbers = [str(x).encode("utf8") for x in range(2**18)]
     r.putChild(b"numbers", static.Data(b"".join(numbers), "text/plain"))
 
     port = reactor.listenTCP(0, server.Site(r), interface="127.0.0.1")
     if debug:
-        print(f"Test server running at http://localhost:{port.getHost().port}/ "
-              "- hit Ctrl-C to finish.")
+        print(
+            f"Test server running at http://localhost:{port.getHost().port}/ "
+            "- hit Ctrl-C to finish."
+        )
     return port
 
 
 class CrawlerRun:
     """A class to run the crawler and keep track of events occurred"""
 
     def __init__(self, spider_class):
@@ -165,37 +166,41 @@
             self.geturl("/"),
             self.geturl("/redirect"),
             self.geturl("/redirect"),  # duplicate
             self.geturl("/numbers"),
         ]
 
         for name, signal in vars(signals).items():
-            if not name.startswith('_'):
+            if not name.startswith("_"):
                 dispatcher.connect(self.record_signal, signal)
 
         self.crawler = get_crawler(self.spider_class)
         self.crawler.signals.connect(self.item_scraped, signals.item_scraped)
         self.crawler.signals.connect(self.item_error, signals.item_error)
         self.crawler.signals.connect(self.headers_received, signals.headers_received)
         self.crawler.signals.connect(self.bytes_received, signals.bytes_received)
         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)
         self.crawler.signals.connect(self.request_dropped, signals.request_dropped)
-        self.crawler.signals.connect(self.request_reached, signals.request_reached_downloader)
-        self.crawler.signals.connect(self.response_downloaded, signals.response_downloaded)
+        self.crawler.signals.connect(
+            self.request_reached, signals.request_reached_downloader
+        )
+        self.crawler.signals.connect(
+            self.response_downloaded, signals.response_downloaded
+        )
         self.crawler.crawl(start_urls=start_urls)
         self.spider = self.crawler.spider
 
         self.deferred = defer.Deferred()
         dispatcher.connect(self.stop, signals.engine_stopped)
         return self.deferred
 
     def stop(self):
         self.port.stopListening()  # FIXME: wait for this Deferred
         for name, signal in vars(signals).items():
-            if not name.startswith('_'):
+            if not name.startswith("_"):
                 disconnect_all(signal)
         self.deferred.callback(None)
         return self.crawler.stop()
 
     def geturl(self, path):
         return f"http://localhost:{self.portno}{path}"
 
@@ -226,24 +231,29 @@
 
     def response_downloaded(self, response, spider):
         self.respplug.append((response, spider))
 
     def record_signal(self, *args, **kwargs):
         """Record a signal and its parameters"""
         signalargs = kwargs.copy()
-        sig = signalargs.pop('signal')
-        signalargs.pop('sender', None)
+        sig = signalargs.pop("signal")
+        signalargs.pop("sender", None)
         self.signals_caught[sig] = signalargs
 
 
 class EngineTest(unittest.TestCase):
     @defer.inlineCallbacks
     def test_crawler(self):
 
-        for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):
+        for spider in (
+            TestSpider,
+            DictItemsSpider,
+            AttrsItemsSpider,
+            DataClassItemsSpider,
+        ):
             run = CrawlerRun(spider)
             yield run.run()
             self._assert_visited_urls(run)
             self._assert_scheduled_requests(run, count=9)
             self._assert_downloaded_responses(run, count=9)
             self._assert_scraped_items(run)
             self._assert_signals_caught(run)
@@ -262,79 +272,89 @@
         yield run.run()
         self._assert_items_error(run)
 
     @defer.inlineCallbacks
     def test_crawler_change_close_reason_on_idle(self):
         run = CrawlerRun(ChangeCloseReasonSpider)
         yield run.run()
-        self.assertEqual({'spider': run.spider, 'reason': 'custom_reason'},
-                         run.signals_caught[signals.spider_closed])
+        self.assertEqual(
+            {"spider": run.spider, "reason": "custom_reason"},
+            run.signals_caught[signals.spider_closed],
+        )
 
     def _assert_visited_urls(self, run: CrawlerRun):
-        must_be_visited = ["/", "/redirect", "/redirected",
-                           "/item1.html", "/item2.html", "/item999.html"]
+        must_be_visited = [
+            "/",
+            "/redirect",
+            "/redirected",
+            "/item1.html",
+            "/item2.html",
+            "/item999.html",
+        ]
         urls_visited = {rp[0].url for rp in run.respplug}
         urls_expected = {run.geturl(p) for p in must_be_visited}
-        assert urls_expected <= urls_visited, f"URLs not visited: {list(urls_expected - urls_visited)}"
+        assert (
+            urls_expected <= urls_visited
+        ), f"URLs not visited: {list(urls_expected - urls_visited)}"
 
     def _assert_scheduled_requests(self, run: CrawlerRun, count=None):
         self.assertEqual(count, len(run.reqplug))
 
-        paths_expected = ['/item999.html', '/item2.html', '/item1.html']
+        paths_expected = ["/item999.html", "/item2.html", "/item1.html"]
 
         urls_requested = {rq[0].url for rq in run.reqplug}
         urls_expected = {run.geturl(p) for p in paths_expected}
         assert urls_expected <= urls_requested
         scheduled_requests_count = len(run.reqplug)
         dropped_requests_count = len(run.reqdropped)
         responses_count = len(run.respplug)
-        self.assertEqual(scheduled_requests_count,
-                         dropped_requests_count + responses_count)
-        self.assertEqual(len(run.reqreached),
-                         responses_count)
+        self.assertEqual(
+            scheduled_requests_count, dropped_requests_count + responses_count
+        )
+        self.assertEqual(len(run.reqreached), responses_count)
 
     def _assert_dropped_requests(self, run: CrawlerRun):
         self.assertEqual(len(run.reqdropped), 1)
 
     def _assert_downloaded_responses(self, run: CrawlerRun, count):
         # response tests
         self.assertEqual(count, len(run.respplug))
         self.assertEqual(count, len(run.reqreached))
 
         for response, _ in run.respplug:
-            if run.getpath(response.url) == '/item999.html':
+            if run.getpath(response.url) == "/item999.html":
                 self.assertEqual(404, response.status)
-            if run.getpath(response.url) == '/redirect':
+            if run.getpath(response.url) == "/redirect":
                 self.assertEqual(302, response.status)
 
     def _assert_items_error(self, run: CrawlerRun):
         self.assertEqual(2, len(run.itemerror))
         for item, response, spider, failure in run.itemerror:
             self.assertEqual(failure.value.__class__, ZeroDivisionError)
             self.assertEqual(spider, run.spider)
 
-            self.assertEqual(item['url'], response.url)
-            if 'item1.html' in item['url']:
-                self.assertEqual('Item 1 name', item['name'])
-                self.assertEqual('100', item['price'])
-            if 'item2.html' in item['url']:
-                self.assertEqual('Item 2 name', item['name'])
-                self.assertEqual('200', item['price'])
+            self.assertEqual(item["url"], response.url)
+            if "item1.html" in item["url"]:
+                self.assertEqual("Item 1 name", item["name"])
+                self.assertEqual("100", item["price"])
+            if "item2.html" in item["url"]:
+                self.assertEqual("Item 2 name", item["name"])
+                self.assertEqual("200", item["price"])
 
     def _assert_scraped_items(self, run: CrawlerRun):
         self.assertEqual(2, len(run.itemresp))
         for item, response in run.itemresp:
             item = ItemAdapter(item)
-            self.assertEqual(item['url'], response.url)
-            if 'item1.html' in item['url']:
-                self.assertEqual('Item 1 name', item['name'])
-                self.assertEqual('100', item['price'])
-            if 'item2.html' in item['url']:
-                self.assertEqual('Item 2 name', item['name'])
-                self.assertEqual('200', item['price'])
+            self.assertEqual(item["url"], response.url)
+            if "item1.html" in item["url"]:
+                self.assertEqual("Item 1 name", item["name"])
+                self.assertEqual("100", item["price"])
+            if "item2.html" in item["url"]:
+                self.assertEqual("Item 2 name", item["name"])
+                self.assertEqual("200", item["price"])
 
     def _assert_headers_received(self, run: CrawlerRun):
         for headers in run.headers.values():
             self.assertIn(b"Server", headers)
             self.assertIn(b"TwistedWeb", headers[b"Server"])
             self.assertIn(b"Date", headers)
             self.assertIn(b"Content-Type", headers)
@@ -347,36 +367,36 @@
                 self.assertEqual(joined_data, get_testdata("test_site", "index.html"))
             elif run.getpath(request.url) == "/item1.html":
                 self.assertEqual(joined_data, get_testdata("test_site", "item1.html"))
             elif run.getpath(request.url) == "/item2.html":
                 self.assertEqual(joined_data, get_testdata("test_site", "item2.html"))
             elif run.getpath(request.url) == "/redirected":
                 self.assertEqual(joined_data, b"Redirected here")
-            elif run.getpath(request.url) == '/redirect':
+            elif run.getpath(request.url) == "/redirect":
                 self.assertEqual(
                     joined_data,
                     b"\n<html>\n"
                     b"    <head>\n"
-                    b"        <meta http-equiv=\"refresh\" content=\"0;URL=/redirected\">\n"
+                    b'        <meta http-equiv="refresh" content="0;URL=/redirected">\n'
                     b"    </head>\n"
-                    b"    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n"
-                    b"    <a href=\"/redirected\">click here</a>\n"
+                    b'    <body bgcolor="#FFFFFF" text="#000000">\n'
+                    b'    <a href="/redirected">click here</a>\n'
                     b"    </body>\n"
-                    b"</html>\n"
+                    b"</html>\n",
                 )
             elif run.getpath(request.url) == "/tem999.html":
                 self.assertEqual(
                     joined_data,
                     b"\n<html>\n"
                     b"  <head><title>404 - No Such Resource</title></head>\n"
                     b"  <body>\n"
                     b"    <h1>No Such Resource</h1>\n"
                     b"    <p>File not found.</p>\n"
                     b"  </body>\n"
-                    b"</html>\n"
+                    b"</html>\n",
                 )
             elif run.getpath(request.url) == "/numbers":
                 # signal was fired multiple times
                 self.assertTrue(len(data) > 1)
                 # bytes were received in order
                 numbers = [str(x).encode("utf8") for x in range(2**18)]
                 self.assertEqual(joined_data, b"".join(numbers))
@@ -385,20 +405,24 @@
         assert signals.engine_started in run.signals_caught
         assert signals.engine_stopped in run.signals_caught
         assert signals.spider_opened in run.signals_caught
         assert signals.spider_idle in run.signals_caught
         assert signals.spider_closed in run.signals_caught
         assert signals.headers_received in run.signals_caught
 
-        self.assertEqual({'spider': run.spider},
-                         run.signals_caught[signals.spider_opened])
-        self.assertEqual({'spider': run.spider},
-                         run.signals_caught[signals.spider_idle])
-        self.assertEqual({'spider': run.spider, 'reason': 'finished'},
-                         run.signals_caught[signals.spider_closed])
+        self.assertEqual(
+            {"spider": run.spider}, run.signals_caught[signals.spider_opened]
+        )
+        self.assertEqual(
+            {"spider": run.spider}, run.signals_caught[signals.spider_idle]
+        )
+        self.assertEqual(
+            {"spider": run.spider, "reason": "finished"},
+            run.signals_caught[signals.spider_closed],
+        )
 
     @defer.inlineCallbacks
     def test_close_downloader(self):
         e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
         yield e.close()
 
     @defer.inlineCallbacks
@@ -411,114 +435,125 @@
                 lambda exc: self.assertEqual(str(exc), "Engine already running")
             )
         finally:
             yield e.stop()
 
     @defer.inlineCallbacks
     def test_close_spiders_downloader(self):
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="ExecutionEngine.open_spiders is deprecated, "
-                                "please use ExecutionEngine.spider instead"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="ExecutionEngine.open_spiders is deprecated, "
+            "please use ExecutionEngine.spider instead",
+        ):
             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
             yield e.open_spider(TestSpider(), [])
             self.assertEqual(len(e.open_spiders), 1)
             yield e.close()
             self.assertEqual(len(e.open_spiders), 0)
 
     @defer.inlineCallbacks
     def test_close_engine_spiders_downloader(self):
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="ExecutionEngine.open_spiders is deprecated, "
-                                "please use ExecutionEngine.spider instead"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="ExecutionEngine.open_spiders is deprecated, "
+            "please use ExecutionEngine.spider instead",
+        ):
             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
             yield e.open_spider(TestSpider(), [])
             e.start()
             self.assertTrue(e.running)
             yield e.close()
             self.assertFalse(e.running)
             self.assertEqual(len(e.open_spiders), 0)
 
     @defer.inlineCallbacks
     def test_crawl_deprecated_spider_arg(self):
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="Passing a 'spider' argument to "
-                                "ExecutionEngine.crawl is deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="Passing a 'spider' argument to "
+            "ExecutionEngine.crawl is deprecated",
+        ):
             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
             spider = TestSpider()
             yield e.open_spider(spider, [])
             e.start()
             e.crawl(Request("data:,"), spider)
             yield e.close()
 
     @defer.inlineCallbacks
     def test_download_deprecated_spider_arg(self):
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="Passing a 'spider' argument to "
-                                "ExecutionEngine.download is deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="Passing a 'spider' argument to "
+            "ExecutionEngine.download is deprecated",
+        ):
             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
             spider = TestSpider()
             yield e.open_spider(spider, [])
             e.start()
             e.download(Request("data:,"), spider)
             yield e.close()
 
     @defer.inlineCallbacks
     def test_deprecated_schedule(self):
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="ExecutionEngine.schedule is deprecated, please use "
-                                "ExecutionEngine.crawl or ExecutionEngine.download instead"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="ExecutionEngine.schedule is deprecated, please use "
+            "ExecutionEngine.crawl or ExecutionEngine.download instead",
+        ):
             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
             spider = TestSpider()
             yield e.open_spider(spider, [])
             e.start()
             e.schedule(Request("data:,"), spider)
             yield e.close()
 
     @defer.inlineCallbacks
     def test_deprecated_has_capacity(self):
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="ExecutionEngine.has_capacity is deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning, match="ExecutionEngine.has_capacity is deprecated"
+        ):
             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
             self.assertTrue(e.has_capacity())
             spider = TestSpider()
             yield e.open_spider(spider, [])
             self.assertFalse(e.has_capacity())
             e.start()
             yield e.close()
             self.assertTrue(e.has_capacity())
 
     def test_short_timeout(self):
         args = (
             sys.executable,
-            '-m',
-            'scrapy.cmdline',
-            'fetch',
-            '-s',
-            'CLOSESPIDER_TIMEOUT=0.001',
-            '-s',
-            'LOG_LEVEL=DEBUG',
-            'http://toscrape.com',
+            "-m",
+            "scrapy.cmdline",
+            "fetch",
+            "-s",
+            "CLOSESPIDER_TIMEOUT=0.001",
+            "-s",
+            "LOG_LEVEL=DEBUG",
+            "http://toscrape.com",
         )
         p = subprocess.Popen(
             args,
             stderr=subprocess.PIPE,
         )
 
         def kill_proc():
             p.kill()
             p.communicate()
-            assert False, 'Command took too much time to complete'
+            assert False, "Command took too much time to complete"
 
         timer = Timer(15, kill_proc)
         try:
             timer.start()
             _, stderr = p.communicate()
         finally:
             timer.cancel()
 
-        self.assertNotIn(b'Traceback', stderr)
+        self.assertNotIn(b"Traceback", stderr)
 
 
 if __name__ == "__main__":
-    if len(sys.argv) > 1 and sys.argv[1] == 'runserver':
+    if len(sys.argv) > 1 and sys.argv[1] == "runserver":
         start_test_site(debug=True)
         reactor.run()
```

### Comparing `Scrapy-2.7.1/tests/test_engine_stop_download_bytes.py` & `Scrapy-2.8.0/tests/test_engine_stop_download_bytes.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,47 +1,63 @@
 from testfixtures import LogCapture
 from twisted.internet import defer
 
 from scrapy.exceptions import StopDownload
-
 from tests.test_engine import (
     AttrsItemsSpider,
+    CrawlerRun,
     DataClassItemsSpider,
     DictItemsSpider,
-    TestSpider,
-    CrawlerRun,
     EngineTest,
+    TestSpider,
 )
 
 
 class BytesReceivedCrawlerRun(CrawlerRun):
     def bytes_received(self, data, request, spider):
         super().bytes_received(data, request, spider)
         raise StopDownload(fail=False)
 
 
 class BytesReceivedEngineTest(EngineTest):
     @defer.inlineCallbacks
     def test_crawler(self):
-        for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):
+        for spider in (
+            TestSpider,
+            DictItemsSpider,
+            AttrsItemsSpider,
+            DataClassItemsSpider,
+        ):
             run = BytesReceivedCrawlerRun(spider)
             with LogCapture() as log:
                 yield run.run()
-                log.check_present(("scrapy.core.downloader.handlers.http11",
-                                   "DEBUG",
-                                   f"Download stopped for <GET http://localhost:{run.portno}/redirected> "
-                                   "from signal handler BytesReceivedCrawlerRun.bytes_received"))
-                log.check_present(("scrapy.core.downloader.handlers.http11",
-                                   "DEBUG",
-                                   f"Download stopped for <GET http://localhost:{run.portno}/> "
-                                   "from signal handler BytesReceivedCrawlerRun.bytes_received"))
-                log.check_present(("scrapy.core.downloader.handlers.http11",
-                                   "DEBUG",
-                                   f"Download stopped for <GET http://localhost:{run.portno}/numbers> "
-                                   "from signal handler BytesReceivedCrawlerRun.bytes_received"))
+                log.check_present(
+                    (
+                        "scrapy.core.downloader.handlers.http11",
+                        "DEBUG",
+                        f"Download stopped for <GET http://localhost:{run.portno}/redirected> "
+                        "from signal handler BytesReceivedCrawlerRun.bytes_received",
+                    )
+                )
+                log.check_present(
+                    (
+                        "scrapy.core.downloader.handlers.http11",
+                        "DEBUG",
+                        f"Download stopped for <GET http://localhost:{run.portno}/> "
+                        "from signal handler BytesReceivedCrawlerRun.bytes_received",
+                    )
+                )
+                log.check_present(
+                    (
+                        "scrapy.core.downloader.handlers.http11",
+                        "DEBUG",
+                        f"Download stopped for <GET http://localhost:{run.portno}/numbers> "
+                        "from signal handler BytesReceivedCrawlerRun.bytes_received",
+                    )
+                )
             self._assert_visited_urls(run)
             self._assert_scheduled_requests(run, count=9)
             self._assert_downloaded_responses(run, count=9)
             self._assert_signals_caught(run)
             self._assert_headers_received(run)
             self._assert_bytes_received(run)
```

### Comparing `Scrapy-2.7.1/tests/test_engine_stop_download_headers.py` & `Scrapy-2.8.0/tests/test_engine_stop_download_headers.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,54 +1,72 @@
 from testfixtures import LogCapture
 from twisted.internet import defer
 
 from scrapy.exceptions import StopDownload
-
 from tests.test_engine import (
     AttrsItemsSpider,
+    CrawlerRun,
     DataClassItemsSpider,
     DictItemsSpider,
-    TestSpider,
-    CrawlerRun,
     EngineTest,
+    TestSpider,
 )
 
 
 class HeadersReceivedCrawlerRun(CrawlerRun):
     def headers_received(self, headers, body_length, request, spider):
         super().headers_received(headers, body_length, request, spider)
         raise StopDownload(fail=False)
 
 
 class HeadersReceivedEngineTest(EngineTest):
     @defer.inlineCallbacks
     def test_crawler(self):
-        for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):
+        for spider in (
+            TestSpider,
+            DictItemsSpider,
+            AttrsItemsSpider,
+            DataClassItemsSpider,
+        ):
             run = HeadersReceivedCrawlerRun(spider)
             with LogCapture() as log:
                 yield run.run()
-                log.check_present(("scrapy.core.downloader.handlers.http11",
-                                   "DEBUG",
-                                   f"Download stopped for <GET http://localhost:{run.portno}/redirected> from"
-                                   " signal handler HeadersReceivedCrawlerRun.headers_received"))
-                log.check_present(("scrapy.core.downloader.handlers.http11",
-                                   "DEBUG",
-                                   f"Download stopped for <GET http://localhost:{run.portno}/> from signal"
-                                   " handler HeadersReceivedCrawlerRun.headers_received"))
-                log.check_present(("scrapy.core.downloader.handlers.http11",
-                                   "DEBUG",
-                                   f"Download stopped for <GET http://localhost:{run.portno}/numbers> from"
-                                   " signal handler HeadersReceivedCrawlerRun.headers_received"))
+                log.check_present(
+                    (
+                        "scrapy.core.downloader.handlers.http11",
+                        "DEBUG",
+                        f"Download stopped for <GET http://localhost:{run.portno}/redirected> from"
+                        " signal handler HeadersReceivedCrawlerRun.headers_received",
+                    )
+                )
+                log.check_present(
+                    (
+                        "scrapy.core.downloader.handlers.http11",
+                        "DEBUG",
+                        f"Download stopped for <GET http://localhost:{run.portno}/> from signal"
+                        " handler HeadersReceivedCrawlerRun.headers_received",
+                    )
+                )
+                log.check_present(
+                    (
+                        "scrapy.core.downloader.handlers.http11",
+                        "DEBUG",
+                        f"Download stopped for <GET http://localhost:{run.portno}/numbers> from"
+                        " signal handler HeadersReceivedCrawlerRun.headers_received",
+                    )
+                )
             self._assert_visited_urls(run)
             self._assert_downloaded_responses(run, count=6)
             self._assert_signals_caught(run)
             self._assert_bytes_received(run)
             self._assert_headers_received(run)
 
     def _assert_bytes_received(self, run: CrawlerRun):
         self.assertEqual(0, len(run.bytes))
 
     def _assert_visited_urls(self, run: CrawlerRun):
         must_be_visited = ["/", "/redirect", "/redirected"]
         urls_visited = {rp[0].url for rp in run.respplug}
         urls_expected = {run.geturl(p) for p in must_be_visited}
-        assert urls_expected <= urls_visited, f"URLs not visited: {list(urls_expected - urls_visited)}"
+        assert (
+            urls_expected <= urls_visited
+        ), f"URLs not visited: {list(urls_expected - urls_visited)}"
```

### Comparing `Scrapy-2.7.1/tests/test_exporters.py` & `Scrapy-2.8.0/tests/test_exporters.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,29 +1,35 @@
-import re
+import dataclasses
 import json
 import marshal
 import pickle
+import re
 import tempfile
 import unittest
-import dataclasses
-from io import BytesIO
 from datetime import datetime
+from io import BytesIO
 from warnings import catch_warnings, filterwarnings
 
 import lxml.etree
 from itemadapter import ItemAdapter
 
-from scrapy.item import Item, Field
-from scrapy.utils.python import to_unicode
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.exporters import (
-    BaseItemExporter, PprintItemExporter, PickleItemExporter, CsvItemExporter,
-    XmlItemExporter, JsonLinesItemExporter, JsonItemExporter,
-    PythonItemExporter, MarshalItemExporter
+    BaseItemExporter,
+    CsvItemExporter,
+    JsonItemExporter,
+    JsonLinesItemExporter,
+    MarshalItemExporter,
+    PickleItemExporter,
+    PprintItemExporter,
+    PythonItemExporter,
+    XmlItemExporter,
 )
+from scrapy.item import Field, Item
+from scrapy.utils.python import to_unicode
 
 
 def custom_serializer(value):
     return str(int(value) + 2)
 
 
 class TestItem(Item):
@@ -50,15 +56,15 @@
 
 class BaseItemExporterTest(unittest.TestCase):
 
     item_class = TestItem
     custom_field_item_class = CustomFieldItem
 
     def setUp(self):
-        self.i = self.item_class(name='John\xa3', age='22')
+        self.i = self.item_class(name="John\xa3", age="22")
         self.output = BytesIO()
         self.ie = self._get_exporter()
 
     def _get_exporter(self, **kwargs):
         return BaseItemExporter(**kwargs)
 
     def _check_output(self):
@@ -67,18 +73,18 @@
     def _assert_expected_item(self, exported_dict):
         for k, v in exported_dict.items():
             exported_dict[k] = to_unicode(v)
         self.assertEqual(self.i, self.item_class(**exported_dict))
 
     def _get_nonstring_types_item(self):
         return {
-            'boolean': False,
-            'number': 22,
-            'time': datetime(2015, 1, 1, 1, 1, 1),
-            'float': 3.14,
+            "boolean": False,
+            "number": 22,
+            "time": datetime(2015, 1, 1, 1, 1, 1),
+            "float": 3.14,
         }
 
     def assertItemExportWorks(self, item):
         self.ie.start_exporting()
         try:
             self.ie.export_item(item)
         except NotImplementedError:
@@ -91,104 +97,114 @@
         self.assertItemExportWorks(self.i)
 
     def test_export_dict_item(self):
         self.assertItemExportWorks(ItemAdapter(self.i).asdict())
 
     def test_serialize_field(self):
         a = ItemAdapter(self.i)
-        res = self.ie.serialize_field(a.get_field_meta('name'), 'name', a['name'])
-        self.assertEqual(res, 'John\xa3')
+        res = self.ie.serialize_field(a.get_field_meta("name"), "name", a["name"])
+        self.assertEqual(res, "John\xa3")
 
-        res = self.ie.serialize_field(a.get_field_meta('age'), 'age', a['age'])
-        self.assertEqual(res, '22')
+        res = self.ie.serialize_field(a.get_field_meta("age"), "age", a["age"])
+        self.assertEqual(res, "22")
 
     def test_fields_to_export(self):
-        ie = self._get_exporter(fields_to_export=['name'])
-        self.assertEqual(list(ie._get_serialized_fields(self.i)), [('name', 'John\xa3')])
+        ie = self._get_exporter(fields_to_export=["name"])
+        self.assertEqual(
+            list(ie._get_serialized_fields(self.i)), [("name", "John\xa3")]
+        )
 
-        ie = self._get_exporter(fields_to_export=['name'], encoding='latin-1')
+        ie = self._get_exporter(fields_to_export=["name"], encoding="latin-1")
         _, name = list(ie._get_serialized_fields(self.i))[0]
         assert isinstance(name, str)
-        self.assertEqual(name, 'John\xa3')
+        self.assertEqual(name, "John\xa3")
 
-        ie = self._get_exporter(
-            fields_to_export={'name': ''}
-        )
-        self.assertEqual(
-            list(ie._get_serialized_fields(self.i)),
-            [('', 'John\xa3')]
-        )
+        ie = self._get_exporter(fields_to_export={"name": ""})
+        self.assertEqual(list(ie._get_serialized_fields(self.i)), [("", "John\xa3")])
 
     def test_field_custom_serializer(self):
-        i = self.custom_field_item_class(name='John\xa3', age='22')
+        i = self.custom_field_item_class(name="John\xa3", age="22")
         a = ItemAdapter(i)
         ie = self._get_exporter()
-        self.assertEqual(ie.serialize_field(a.get_field_meta('name'), 'name', a['name']), 'John\xa3')
-        self.assertEqual(ie.serialize_field(a.get_field_meta('age'), 'age', a['age']), '24')
+        self.assertEqual(
+            ie.serialize_field(a.get_field_meta("name"), "name", a["name"]), "John\xa3"
+        )
+        self.assertEqual(
+            ie.serialize_field(a.get_field_meta("age"), "age", a["age"]), "24"
+        )
 
 
 class BaseItemExporterDataclassTest(BaseItemExporterTest):
     item_class = TestDataClass
     custom_field_item_class = CustomFieldDataclass
 
 
 class PythonItemExporterTest(BaseItemExporterTest):
     def _get_exporter(self, **kwargs):
         return PythonItemExporter(binary=False, **kwargs)
 
     def test_invalid_option(self):
         with self.assertRaisesRegex(TypeError, "Unexpected options: invalid_option"):
-            PythonItemExporter(invalid_option='something')
+            PythonItemExporter(invalid_option="something")
 
     def test_nested_item(self):
-        i1 = self.item_class(name='Joseph', age='22')
-        i2 = dict(name='Maria', age=i1)
-        i3 = self.item_class(name='Jesus', age=i2)
+        i1 = self.item_class(name="Joseph", age="22")
+        i2 = dict(name="Maria", age=i1)
+        i3 = self.item_class(name="Jesus", age=i2)
         ie = self._get_exporter()
         exported = ie.export_item(i3)
         self.assertEqual(type(exported), dict)
         self.assertEqual(
             exported,
-            {'age': {'age': {'age': '22', 'name': 'Joseph'}, 'name': 'Maria'}, 'name': 'Jesus'}
+            {
+                "age": {"age": {"age": "22", "name": "Joseph"}, "name": "Maria"},
+                "name": "Jesus",
+            },
         )
-        self.assertEqual(type(exported['age']), dict)
-        self.assertEqual(type(exported['age']['age']), dict)
+        self.assertEqual(type(exported["age"]), dict)
+        self.assertEqual(type(exported["age"]["age"]), dict)
 
     def test_export_list(self):
-        i1 = self.item_class(name='Joseph', age='22')
-        i2 = self.item_class(name='Maria', age=[i1])
-        i3 = self.item_class(name='Jesus', age=[i2])
+        i1 = self.item_class(name="Joseph", age="22")
+        i2 = self.item_class(name="Maria", age=[i1])
+        i3 = self.item_class(name="Jesus", age=[i2])
         ie = self._get_exporter()
         exported = ie.export_item(i3)
         self.assertEqual(
             exported,
-            {'age': [{'age': [{'age': '22', 'name': 'Joseph'}], 'name': 'Maria'}], 'name': 'Jesus'}
+            {
+                "age": [{"age": [{"age": "22", "name": "Joseph"}], "name": "Maria"}],
+                "name": "Jesus",
+            },
         )
-        self.assertEqual(type(exported['age'][0]), dict)
-        self.assertEqual(type(exported['age'][0]['age'][0]), dict)
+        self.assertEqual(type(exported["age"][0]), dict)
+        self.assertEqual(type(exported["age"][0]["age"][0]), dict)
 
     def test_export_item_dict_list(self):
-        i1 = self.item_class(name='Joseph', age='22')
-        i2 = dict(name='Maria', age=[i1])
-        i3 = self.item_class(name='Jesus', age=[i2])
+        i1 = self.item_class(name="Joseph", age="22")
+        i2 = dict(name="Maria", age=[i1])
+        i3 = self.item_class(name="Jesus", age=[i2])
         ie = self._get_exporter()
         exported = ie.export_item(i3)
         self.assertEqual(
             exported,
-            {'age': [{'age': [{'age': '22', 'name': 'Joseph'}], 'name': 'Maria'}], 'name': 'Jesus'}
+            {
+                "age": [{"age": [{"age": "22", "name": "Joseph"}], "name": "Maria"}],
+                "name": "Jesus",
+            },
         )
-        self.assertEqual(type(exported['age'][0]), dict)
-        self.assertEqual(type(exported['age'][0]['age'][0]), dict)
+        self.assertEqual(type(exported["age"][0]), dict)
+        self.assertEqual(type(exported["age"][0]["age"][0]), dict)
 
     def test_export_binary(self):
         with catch_warnings():
-            filterwarnings('ignore', category=ScrapyDeprecationWarning)
+            filterwarnings("ignore", category=ScrapyDeprecationWarning)
             exporter = PythonItemExporter(binary=True)
-            value = self.item_class(name='John\xa3', age='22')
-            expected = {b'name': b'John\xc2\xa3', b'age': b'22'}
+            value = self.item_class(name="John\xa3", age="22")
+            expected = {b"name": b"John\xc2\xa3", b"age": b"22"}
             self.assertEqual(expected, exporter.export_item(value))
 
     def test_nonstring_types_item(self):
         item = self._get_nonstring_types_item()
         ie = self._get_exporter()
         exported = ie.export_item(item)
         self.assertEqual(exported, item)
@@ -196,38 +212,36 @@
 
 class PythonItemExporterDataclassTest(PythonItemExporterTest):
     item_class = TestDataClass
     custom_field_item_class = CustomFieldDataclass
 
 
 class PprintItemExporterTest(BaseItemExporterTest):
-
     def _get_exporter(self, **kwargs):
         return PprintItemExporter(self.output, **kwargs)
 
     def _check_output(self):
         self._assert_expected_item(eval(self.output.getvalue()))
 
 
 class PprintItemExporterDataclassTest(PprintItemExporterTest):
     item_class = TestDataClass
     custom_field_item_class = CustomFieldDataclass
 
 
 class PickleItemExporterTest(BaseItemExporterTest):
-
     def _get_exporter(self, **kwargs):
         return PickleItemExporter(self.output, **kwargs)
 
     def _check_output(self):
         self._assert_expected_item(pickle.loads(self.output.getvalue()))
 
     def test_export_multiple_items(self):
-        i1 = self.item_class(name='hello', age='world')
-        i2 = self.item_class(name='bye', age='world')
+        i1 = self.item_class(name="hello", age="world")
+        i2 = self.item_class(name="bye", age="world")
         f = BytesIO()
         ie = PickleItemExporter(f)
         ie.start_exporting()
         ie.export_item(i1)
         ie.export_item(i2)
         ie.finish_exporting()
         f.seek(0)
@@ -246,26 +260,25 @@
 
 class PickleItemExporterDataclassTest(PickleItemExporterTest):
     item_class = TestDataClass
     custom_field_item_class = CustomFieldDataclass
 
 
 class MarshalItemExporterTest(BaseItemExporterTest):
-
     def _get_exporter(self, **kwargs):
         self.output = tempfile.TemporaryFile()
         return MarshalItemExporter(self.output, **kwargs)
 
     def _check_output(self):
         self.output.seek(0)
         self._assert_expected_item(marshal.load(self.output))
 
     def test_nonstring_types_item(self):
         item = self._get_nonstring_types_item()
-        item.pop('time')  # datetime is not marshallable
+        item.pop("time")  # datetime is not marshallable
         fp = tempfile.TemporaryFile()
         ie = MarshalItemExporter(fp)
         ie.start_exporting()
         ie.export_item(item)
         ie.finish_exporting()
         fp.seek(0)
         self.assertEqual(marshal.load(fp), item)
@@ -283,167 +296,170 @@
 
     def assertCsvEqual(self, first, second, msg=None):
         def split_csv(csv):
             return [
                 sorted(re.split(r"(,|\s+)", line))
                 for line in to_unicode(csv).splitlines(True)
             ]
+
         return self.assertEqual(split_csv(first), split_csv(second), msg=msg)
 
     def _check_output(self):
         self.output.seek(0)
-        self.assertCsvEqual(to_unicode(self.output.read()), 'age,name\r\n22,John\xa3\r\n')
+        self.assertCsvEqual(
+            to_unicode(self.output.read()), "age,name\r\n22,John\xa3\r\n"
+        )
 
     def assertExportResult(self, item, expected, **kwargs):
         fp = BytesIO()
         ie = CsvItemExporter(fp, **kwargs)
         ie.start_exporting()
         ie.export_item(item)
         ie.finish_exporting()
         self.assertCsvEqual(fp.getvalue(), expected)
 
     def test_header_export_all(self):
         self.assertExportResult(
             item=self.i,
             fields_to_export=ItemAdapter(self.i).field_names(),
-            expected=b'age,name\r\n22,John\xc2\xa3\r\n',
+            expected=b"age,name\r\n22,John\xc2\xa3\r\n",
         )
 
     def test_header_export_all_dict(self):
         self.assertExportResult(
             item=ItemAdapter(self.i).asdict(),
-            expected=b'age,name\r\n22,John\xc2\xa3\r\n',
+            expected=b"age,name\r\n22,John\xc2\xa3\r\n",
         )
 
     def test_header_export_single_field(self):
         for item in [self.i, ItemAdapter(self.i).asdict()]:
             self.assertExportResult(
                 item=item,
-                fields_to_export=['age'],
-                expected=b'age\r\n22\r\n',
+                fields_to_export=["age"],
+                expected=b"age\r\n22\r\n",
             )
 
     def test_header_export_two_items(self):
         for item in [self.i, ItemAdapter(self.i).asdict()]:
             output = BytesIO()
             ie = CsvItemExporter(output)
             ie.start_exporting()
             ie.export_item(item)
             ie.export_item(item)
             ie.finish_exporting()
-            self.assertCsvEqual(output.getvalue(),
-                                b'age,name\r\n22,John\xc2\xa3\r\n22,John\xc2\xa3\r\n')
+            self.assertCsvEqual(
+                output.getvalue(), b"age,name\r\n22,John\xc2\xa3\r\n22,John\xc2\xa3\r\n"
+            )
 
     def test_header_no_header_line(self):
         for item in [self.i, ItemAdapter(self.i).asdict()]:
             self.assertExportResult(
                 item=item,
                 include_headers_line=False,
-                expected=b'22,John\xc2\xa3\r\n',
+                expected=b"22,John\xc2\xa3\r\n",
             )
 
     def test_join_multivalue(self):
         class TestItem2(Item):
             name = Field()
             friends = Field()
 
         for cls in TestItem2, dict:
             self.assertExportResult(
-                item=cls(name='John', friends=['Mary', 'Paul']),
+                item=cls(name="John", friends=["Mary", "Paul"]),
                 include_headers_line=False,
                 expected='"Mary,Paul",John\r\n',
             )
 
     def test_join_multivalue_not_strings(self):
         self.assertExportResult(
-            item=dict(name='John', friends=[4, 8]),
+            item=dict(name="John", friends=[4, 8]),
             include_headers_line=False,
             expected='"[4, 8]",John\r\n',
         )
 
     def test_nonstring_types_item(self):
         self.assertExportResult(
             item=self._get_nonstring_types_item(),
             include_headers_line=False,
-            expected='22,False,3.14,2015-01-01 01:01:01\r\n'
+            expected="22,False,3.14,2015-01-01 01:01:01\r\n",
         )
 
     def test_errors_default(self):
         with self.assertRaises(UnicodeEncodeError):
             self.assertExportResult(
-                item=dict(text='W\u0275\u200Brd'),
+                item=dict(text="W\u0275\u200Brd"),
                 expected=None,
-                encoding='windows-1251',
+                encoding="windows-1251",
             )
 
     def test_errors_xmlcharrefreplace(self):
         self.assertExportResult(
-            item=dict(text='W\u0275\u200Brd'),
+            item=dict(text="W\u0275\u200Brd"),
             include_headers_line=False,
-            expected='W&#629;&#8203;rd\r\n',
-            encoding='windows-1251',
-            errors='xmlcharrefreplace',
+            expected="W&#629;&#8203;rd\r\n",
+            encoding="windows-1251",
+            errors="xmlcharrefreplace",
         )
 
 
 class CsvItemExporterDataclassTest(CsvItemExporterTest):
     item_class = TestDataClass
     custom_field_item_class = CustomFieldDataclass
 
 
 class XmlItemExporterTest(BaseItemExporterTest):
-
     def _get_exporter(self, **kwargs):
         return XmlItemExporter(self.output, **kwargs)
 
     def assertXmlEquivalent(self, first, second, msg=None):
         def xmltuple(elem):
             children = list(elem.iterchildren())
             if children:
                 return [(child.tag, sorted(xmltuple(child))) for child in children]
-            else:
-                return [(elem.tag, [(elem.text, ())])]
+            return [(elem.tag, [(elem.text, ())])]
 
         def xmlsplit(xmlcontent):
             doc = lxml.etree.fromstring(xmlcontent)
             return xmltuple(doc)
+
         return self.assertEqual(xmlsplit(first), xmlsplit(second), msg)
 
     def assertExportResult(self, item, expected_value):
         fp = BytesIO()
         ie = XmlItemExporter(fp)
         ie.start_exporting()
         ie.export_item(item)
         ie.finish_exporting()
         self.assertXmlEquivalent(fp.getvalue(), expected_value)
 
     def _check_output(self):
         expected_value = (
             b'<?xml version="1.0" encoding="utf-8"?>\n'
-            b'<items><item><age>22</age><name>John\xc2\xa3</name></item></items>'
+            b"<items><item><age>22</age><name>John\xc2\xa3</name></item></items>"
         )
         self.assertXmlEquivalent(self.output.getvalue(), expected_value)
 
     def test_multivalued_fields(self):
         self.assertExportResult(
-            self.item_class(name=['John\xa3', 'Doe'], age=[1, 2, 3]),
+            self.item_class(name=["John\xa3", "Doe"], age=[1, 2, 3]),
             b"""<?xml version="1.0" encoding="utf-8"?>\n
             <items>
                 <item>
                     <name><value>John\xc2\xa3</value><value>Doe</value></name>
                     <age><value>1</value><value>2</value><value>3</value></age>
                 </item>
             </items>
-            """
+            """,
         )
 
     def test_nested_item(self):
-        i1 = dict(name='foo\xa3hoo', age='22')
-        i2 = dict(name='bar', age=i1)
-        i3 = self.item_class(name='buz', age=i2)
+        i1 = dict(name="foo\xa3hoo", age="22")
+        i2 = dict(name="bar", age=i1)
+        i3 = self.item_class(name="buz", age=i2)
 
         self.assertExportResult(
             i3,
             b"""<?xml version="1.0" encoding="utf-8"?>\n
                 <items>
                     <item>
                         <age>
@@ -452,35 +468,35 @@
                                 <name>foo\xc2\xa3hoo</name>
                             </age>
                             <name>bar</name>
                         </age>
                         <name>buz</name>
                     </item>
                 </items>
-            """
+            """,
         )
 
     def test_nested_list_item(self):
-        i1 = dict(name='foo')
-        i2 = dict(name='bar', v2={"egg": ["spam"]})
-        i3 = self.item_class(name='buz', age=[i1, i2])
+        i1 = dict(name="foo")
+        i2 = dict(name="bar", v2={"egg": ["spam"]})
+        i3 = self.item_class(name="buz", age=[i1, i2])
 
         self.assertExportResult(
             i3,
             b"""<?xml version="1.0" encoding="utf-8"?>\n
                 <items>
                     <item>
                         <age>
                             <value><name>foo</name></value>
                             <value><name>bar</name><v2><egg><value>spam</value></egg></v2></value>
                         </age>
                         <name>buz</name>
                     </item>
                 </items>
-            """
+            """,
         )
 
     def test_nonstring_types_item(self):
         item = self._get_nonstring_types_item()
         self.assertExportResult(
             item,
             b"""<?xml version="1.0" encoding="utf-8"?>\n
@@ -488,39 +504,42 @@
                    <item>
                        <float>3.14</float>
                        <boolean>False</boolean>
                        <number>22</number>
                        <time>2015-01-01 01:01:01</time>
                    </item>
                 </items>
-            """
+            """,
         )
 
 
 class XmlItemExporterDataclassTest(XmlItemExporterTest):
 
     item_class = TestDataClass
     custom_field_item_class = CustomFieldDataclass
 
 
 class JsonLinesItemExporterTest(BaseItemExporterTest):
 
-    _expected_nested = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': {'name': 'Joseph', 'age': '22'}}}
+    _expected_nested = {
+        "name": "Jesus",
+        "age": {"name": "Maria", "age": {"name": "Joseph", "age": "22"}},
+    }
 
     def _get_exporter(self, **kwargs):
         return JsonLinesItemExporter(self.output, **kwargs)
 
     def _check_output(self):
         exported = json.loads(to_unicode(self.output.getvalue().strip()))
         self.assertEqual(exported, ItemAdapter(self.i).asdict())
 
     def test_nested_item(self):
-        i1 = self.item_class(name='Joseph', age='22')
-        i2 = dict(name='Maria', age=i1)
-        i3 = self.item_class(name='Jesus', age=i2)
+        i1 = self.item_class(name="Joseph", age="22")
+        i2 = dict(name="Maria", age=i1)
+        i3 = self.item_class(name="Jesus", age=i2)
         self.ie.start_exporting()
         self.ie.export_item(i3)
         self.ie.finish_exporting()
         exported = json.loads(to_unicode(self.output.getvalue()))
         self.assertEqual(exported, self._expected_nested)
 
     def test_extra_keywords(self):
@@ -531,15 +550,15 @@
 
     def test_nonstring_types_item(self):
         item = self._get_nonstring_types_item()
         self.ie.start_exporting()
         self.ie.export_item(item)
         self.ie.finish_exporting()
         exported = json.loads(to_unicode(self.output.getvalue()))
-        item['time'] = str(item['time'])
+        item["time"] = str(item["time"])
         self.assertEqual(exported, item)
 
 
 class JsonLinesItemExporterDataclassTest(JsonLinesItemExporterTest):
 
     item_class = TestDataClass
     custom_field_item_class = CustomFieldDataclass
@@ -558,51 +577,56 @@
 
     def assertTwoItemsExported(self, item):
         self.ie.start_exporting()
         self.ie.export_item(item)
         self.ie.export_item(item)
         self.ie.finish_exporting()
         exported = json.loads(to_unicode(self.output.getvalue()))
-        self.assertEqual(exported, [ItemAdapter(item).asdict(), ItemAdapter(item).asdict()])
+        self.assertEqual(
+            exported, [ItemAdapter(item).asdict(), ItemAdapter(item).asdict()]
+        )
 
     def test_two_items(self):
         self.assertTwoItemsExported(self.i)
 
     def test_two_dict_items(self):
         self.assertTwoItemsExported(ItemAdapter(self.i).asdict())
 
     def test_nested_item(self):
-        i1 = self.item_class(name='Joseph\xa3', age='22')
-        i2 = self.item_class(name='Maria', age=i1)
-        i3 = self.item_class(name='Jesus', age=i2)
+        i1 = self.item_class(name="Joseph\xa3", age="22")
+        i2 = self.item_class(name="Maria", age=i1)
+        i3 = self.item_class(name="Jesus", age=i2)
         self.ie.start_exporting()
         self.ie.export_item(i3)
         self.ie.finish_exporting()
         exported = json.loads(to_unicode(self.output.getvalue()))
-        expected = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': ItemAdapter(i1).asdict()}}
+        expected = {
+            "name": "Jesus",
+            "age": {"name": "Maria", "age": ItemAdapter(i1).asdict()},
+        }
         self.assertEqual(exported, [expected])
 
     def test_nested_dict_item(self):
-        i1 = dict(name='Joseph\xa3', age='22')
-        i2 = self.item_class(name='Maria', age=i1)
-        i3 = dict(name='Jesus', age=i2)
+        i1 = dict(name="Joseph\xa3", age="22")
+        i2 = self.item_class(name="Maria", age=i1)
+        i3 = dict(name="Jesus", age=i2)
         self.ie.start_exporting()
         self.ie.export_item(i3)
         self.ie.finish_exporting()
         exported = json.loads(to_unicode(self.output.getvalue()))
-        expected = {'name': 'Jesus', 'age': {'name': 'Maria', 'age': i1}}
+        expected = {"name": "Jesus", "age": {"name": "Maria", "age": i1}}
         self.assertEqual(exported, [expected])
 
     def test_nonstring_types_item(self):
         item = self._get_nonstring_types_item()
         self.ie.start_exporting()
         self.ie.export_item(item)
         self.ie.finish_exporting()
         exported = json.loads(to_unicode(self.output.getvalue()))
-        item['time'] = str(item['time'])
+        item["time"] = str(item["time"])
         self.assertEqual(exported, [item])
 
 
 class JsonItemExporterDataclassTest(JsonItemExporterTest):
 
     item_class = TestDataClass
     custom_field_item_class = CustomFieldDataclass
@@ -615,31 +639,34 @@
     def setUp(self):
         if self.item_class is None:
             raise unittest.SkipTest("item class is None")
 
     def test_exporter_custom_serializer(self):
         class CustomItemExporter(BaseItemExporter):
             def serialize_field(self, field, name, value):
-                if name == 'age':
+                if name == "age":
                     return str(int(value) + 1)
-                else:
-                    return super().serialize_field(field, name, value)
+                return super().serialize_field(field, name, value)
 
-        i = self.item_class(name='John', age='22')
+        i = self.item_class(name="John", age="22")
         a = ItemAdapter(i)
         ie = CustomItemExporter()
 
-        self.assertEqual(ie.serialize_field(a.get_field_meta('name'), 'name', a['name']), 'John')
-        self.assertEqual(ie.serialize_field(a.get_field_meta('age'), 'age', a['age']), '23')
+        self.assertEqual(
+            ie.serialize_field(a.get_field_meta("name"), "name", a["name"]), "John"
+        )
+        self.assertEqual(
+            ie.serialize_field(a.get_field_meta("age"), "age", a["age"]), "23"
+        )
 
-        i2 = {'name': 'John', 'age': '22'}
-        self.assertEqual(ie.serialize_field({}, 'name', i2['name']), 'John')
-        self.assertEqual(ie.serialize_field({}, 'age', i2['age']), '23')
+        i2 = {"name": "John", "age": "22"}
+        self.assertEqual(ie.serialize_field({}, "name", i2["name"]), "John")
+        self.assertEqual(ie.serialize_field({}, "age", i2["age"]), "23")
 
 
 class CustomExporterDataclassTest(CustomExporterItemTest):
 
     item_class = TestDataClass
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_extension_telnet.py` & `Scrapy-2.8.0/tests/test_extension_telnet.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
-from twisted.trial import unittest
 from twisted.conch.telnet import ITelnetProtocol
 from twisted.cred import credentials
 from twisted.internet import defer
+from twisted.trial import unittest
 
 from scrapy.extensions.telnet import TelnetConsole
 from scrapy.utils.test import get_crawler
 
 
 class TelnetExtensionTest(unittest.TestCase):
     def _get_console_and_portal(self, settings=None):
@@ -20,34 +20,33 @@
         portal = protocol.protocolArgs[0]
 
         return console, portal
 
     @defer.inlineCallbacks
     def test_bad_credentials(self):
         console, portal = self._get_console_and_portal()
-        creds = credentials.UsernamePassword(b'username', b'password')
+        creds = credentials.UsernamePassword(b"username", b"password")
         d = portal.login(creds, None, ITelnetProtocol)
         yield self.assertFailure(d, ValueError)
         console.stop_listening()
 
     @defer.inlineCallbacks
     def test_good_credentials(self):
         console, portal = self._get_console_and_portal()
         creds = credentials.UsernamePassword(
-            console.username.encode('utf8'),
-            console.password.encode('utf8')
+            console.username.encode("utf8"), console.password.encode("utf8")
         )
         d = portal.login(creds, None, ITelnetProtocol)
         yield d
         console.stop_listening()
 
     @defer.inlineCallbacks
     def test_custom_credentials(self):
         settings = {
-            'TELNETCONSOLE_USERNAME': 'user',
-            'TELNETCONSOLE_PASSWORD': 'pass',
+            "TELNETCONSOLE_USERNAME": "user",
+            "TELNETCONSOLE_PASSWORD": "pass",
         }
         console, portal = self._get_console_and_portal(settings=settings)
-        creds = credentials.UsernamePassword(b'user', b'pass')
+        creds = credentials.UsernamePassword(b"user", b"pass")
         d = portal.login(creds, None, ITelnetProtocol)
         yield d
         console.stop_listening()
```

### Comparing `Scrapy-2.7.1/tests/test_feedexport.py` & `Scrapy-2.8.0/tests/test_feedexport.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,143 +1,136 @@
 import bz2
 import csv
 import gzip
 import json
 import lzma
-import os
 import random
 import shutil
 import string
 import sys
 import tempfile
 import warnings
 from abc import ABC, abstractmethod
 from collections import defaultdict
 from contextlib import ExitStack
 from io import BytesIO
 from logging import getLogger
+from os import PathLike
 from pathlib import Path
 from string import ascii_letters, digits
+from typing import Union
 from unittest import mock
-from urllib.parse import urljoin, quote
+from urllib.parse import quote, urljoin
 from urllib.request import pathname2url
 
 import lxml.etree
 import pytest
 from testfixtures import LogCapture
 from twisted.internet import defer
 from twisted.trial import unittest
 from w3lib.url import file_uri_to_path, path_to_file_uri
 from zope.interface import implementer
 from zope.interface.verify import verifyObject
 
 import scrapy
 from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning
-from scrapy.exporters import CsvItemExporter
+from scrapy.exporters import CsvItemExporter, JsonItemExporter
 from scrapy.extensions.feedexport import (
     BlockingFeedStorage,
     FeedExporter,
     FileFeedStorage,
     FTPFeedStorage,
     GCSFeedStorage,
     IFeedStorage,
     S3FeedStorage,
     StdoutFeedStorage,
+    _FeedSlot,
 )
 from scrapy.settings import Settings
 from scrapy.utils.python import to_unicode
-from scrapy.utils.test import (
-    get_crawler,
-    mock_google_cloud_storage,
-    skip_if_no_boto,
-)
-
+from scrapy.utils.test import get_crawler, mock_google_cloud_storage, skip_if_no_boto
 from tests.mockserver import MockFTPServer, MockServer
 from tests.spiders import ItemSpider
 
 
 def path_to_url(path):
-    return urljoin('file:', pathname2url(str(path)))
+    return urljoin("file:", pathname2url(str(path)))
 
 
 def printf_escape(string):
-    return string.replace('%', '%%')
+    return string.replace("%", "%%")
 
 
-def build_url(path):
-    if path[0] != '/':
-        path = '/' + path
-    return urljoin('file:', path)
+def build_url(path: Union[str, PathLike]) -> str:
+    path_str = str(path)
+    if path_str[0] != "/":
+        path_str = "/" + path_str
+    return urljoin("file:", path_str)
 
 
 class FileFeedStorageTest(unittest.TestCase):
-
     def test_store_file_uri(self):
-        path = os.path.abspath(self.mktemp())
-        uri = path_to_file_uri(path)
+        path = Path(self.mktemp()).resolve()
+        uri = path_to_file_uri(str(path))
         return self._assert_stores(FileFeedStorage(uri), path)
 
     def test_store_file_uri_makedirs(self):
-        path = os.path.abspath(self.mktemp())
-        path = os.path.join(path, 'more', 'paths', 'file.txt')
-        uri = path_to_file_uri(path)
+        path = Path(self.mktemp()).resolve() / "more" / "paths" / "file.txt"
+        uri = path_to_file_uri(str(path))
         return self._assert_stores(FileFeedStorage(uri), path)
 
     def test_store_direct_path(self):
-        path = os.path.abspath(self.mktemp())
-        return self._assert_stores(FileFeedStorage(path), path)
+        path = Path(self.mktemp()).resolve()
+        return self._assert_stores(FileFeedStorage(str(path)), path)
 
     def test_store_direct_path_relative(self):
-        path = self.mktemp()
-        return self._assert_stores(FileFeedStorage(path), path)
+        path = Path(self.mktemp())
+        return self._assert_stores(FileFeedStorage(str(path)), path)
 
     def test_interface(self):
         path = self.mktemp()
         st = FileFeedStorage(path)
         verifyObject(IFeedStorage, st)
 
-    def _store(self, feed_options=None):
-        path = os.path.abspath(self.mktemp())
-        storage = FileFeedStorage(path, feed_options=feed_options)
+    def _store(self, feed_options=None) -> Path:
+        path = Path(self.mktemp()).resolve()
+        storage = FileFeedStorage(str(path), feed_options=feed_options)
         spider = scrapy.Spider("default")
         file = storage.open(spider)
         file.write(b"content")
         storage.store(file)
         return path
 
     def test_append(self):
         path = self._store()
-        return self._assert_stores(FileFeedStorage(path), path, b"contentcontent")
+        return self._assert_stores(FileFeedStorage(str(path)), path, b"contentcontent")
 
     def test_overwrite(self):
         path = self._store({"overwrite": True})
         return self._assert_stores(
-            FileFeedStorage(path, feed_options={"overwrite": True}),
-            path
+            FileFeedStorage(str(path), feed_options={"overwrite": True}), path
         )
 
     @defer.inlineCallbacks
-    def _assert_stores(self, storage, path, expected_content=b"content"):
+    def _assert_stores(self, storage, path: Path, expected_content=b"content"):
         spider = scrapy.Spider("default")
         file = storage.open(spider)
         file.write(b"content")
         yield storage.store(file)
-        self.assertTrue(os.path.exists(path))
+        self.assertTrue(path.exists())
         try:
-            with open(path, 'rb') as fp:
-                self.assertEqual(fp.read(), expected_content)
+            self.assertEqual(path.read_bytes(), expected_content)
         finally:
-            os.unlink(path)
+            path.unlink()
 
 
 class FTPFeedStorageTest(unittest.TestCase):
-
     def get_test_spider(self, settings=None):
         class TestSpider(scrapy.Spider):
-            name = 'test_spider'
+            name = "test_spider"
 
         crawler = get_crawler(settings_dict=settings)
         spider = TestSpider.from_crawler(crawler)
         return spider
 
     def _store(self, uri, content, feed_options=None, settings=None):
         crawler = get_crawler(settings_dict=settings or {})
@@ -148,400 +141,389 @@
         )
         verifyObject(IFeedStorage, storage)
         spider = self.get_test_spider()
         file = storage.open(spider)
         file.write(content)
         return storage.store(file)
 
-    def _assert_stored(self, path, content):
+    def _assert_stored(self, path: Path, content):
         self.assertTrue(path.exists())
         try:
-            with path.open('rb') as fp:
-                self.assertEqual(fp.read(), content)
+            self.assertEqual(path.read_bytes(), content)
         finally:
-            os.unlink(str(path))
+            path.unlink()
 
     @defer.inlineCallbacks
     def test_append(self):
         with MockFTPServer() as ftp_server:
-            filename = 'file'
+            filename = "file"
             url = ftp_server.url(filename)
-            feed_options = {'overwrite': False}
+            feed_options = {"overwrite": False}
             yield self._store(url, b"foo", feed_options=feed_options)
             yield self._store(url, b"bar", feed_options=feed_options)
             self._assert_stored(ftp_server.path / filename, b"foobar")
 
     @defer.inlineCallbacks
     def test_overwrite(self):
         with MockFTPServer() as ftp_server:
-            filename = 'file'
+            filename = "file"
             url = ftp_server.url(filename)
             yield self._store(url, b"foo")
             yield self._store(url, b"bar")
             self._assert_stored(ftp_server.path / filename, b"bar")
 
     @defer.inlineCallbacks
     def test_append_active_mode(self):
         with MockFTPServer() as ftp_server:
-            settings = {'FEED_STORAGE_FTP_ACTIVE': True}
-            filename = 'file'
+            settings = {"FEED_STORAGE_FTP_ACTIVE": True}
+            filename = "file"
             url = ftp_server.url(filename)
-            feed_options = {'overwrite': False}
+            feed_options = {"overwrite": False}
             yield self._store(url, b"foo", feed_options=feed_options, settings=settings)
             yield self._store(url, b"bar", feed_options=feed_options, settings=settings)
             self._assert_stored(ftp_server.path / filename, b"foobar")
 
     @defer.inlineCallbacks
     def test_overwrite_active_mode(self):
         with MockFTPServer() as ftp_server:
-            settings = {'FEED_STORAGE_FTP_ACTIVE': True}
-            filename = 'file'
+            settings = {"FEED_STORAGE_FTP_ACTIVE": True}
+            filename = "file"
             url = ftp_server.url(filename)
             yield self._store(url, b"foo", settings=settings)
             yield self._store(url, b"bar", settings=settings)
             self._assert_stored(ftp_server.path / filename, b"bar")
 
     def test_uri_auth_quote(self):
         # RFC3986: 3.2.1. User Information
-        pw_quoted = quote(string.punctuation, safe='')
-        st = FTPFeedStorage(f'ftp://foo:{pw_quoted}@example.com/some_path', {})
+        pw_quoted = quote(string.punctuation, safe="")
+        st = FTPFeedStorage(f"ftp://foo:{pw_quoted}@example.com/some_path", {})
         self.assertEqual(st.password, string.punctuation)
 
 
 class BlockingFeedStorageTest(unittest.TestCase):
-
     def get_test_spider(self, settings=None):
         class TestSpider(scrapy.Spider):
-            name = 'test_spider'
+            name = "test_spider"
 
         crawler = get_crawler(settings_dict=settings)
         spider = TestSpider.from_crawler(crawler)
         return spider
 
     def test_default_temp_dir(self):
         b = BlockingFeedStorage()
 
         tmp = b.open(self.get_test_spider())
-        tmp_path = os.path.dirname(tmp.name)
-        self.assertEqual(tmp_path, tempfile.gettempdir())
+        tmp_path = Path(tmp.name).parent
+        self.assertEqual(str(tmp_path), tempfile.gettempdir())
 
     def test_temp_file(self):
         b = BlockingFeedStorage()
 
-        tests_path = os.path.dirname(os.path.abspath(__file__))
-        spider = self.get_test_spider({'FEED_TEMPDIR': tests_path})
+        tests_path = Path(__file__).resolve().parent
+        spider = self.get_test_spider({"FEED_TEMPDIR": str(tests_path)})
         tmp = b.open(spider)
-        tmp_path = os.path.dirname(tmp.name)
+        tmp_path = Path(tmp.name).parent
         self.assertEqual(tmp_path, tests_path)
 
     def test_invalid_folder(self):
         b = BlockingFeedStorage()
 
-        tests_path = os.path.dirname(os.path.abspath(__file__))
-        invalid_path = os.path.join(tests_path, 'invalid_path')
-        spider = self.get_test_spider({'FEED_TEMPDIR': invalid_path})
+        tests_path = Path(__file__).resolve().parent
+        invalid_path = tests_path / "invalid_path"
+        spider = self.get_test_spider({"FEED_TEMPDIR": str(invalid_path)})
 
         self.assertRaises(OSError, b.open, spider=spider)
 
 
 class S3FeedStorageTest(unittest.TestCase):
-
     def test_parse_credentials(self):
         skip_if_no_boto()
-        aws_credentials = {'AWS_ACCESS_KEY_ID': 'settings_key',
-                           'AWS_SECRET_ACCESS_KEY': 'settings_secret',
-                           'AWS_SESSION_TOKEN': 'settings_token'}
+        aws_credentials = {
+            "AWS_ACCESS_KEY_ID": "settings_key",
+            "AWS_SECRET_ACCESS_KEY": "settings_secret",
+            "AWS_SESSION_TOKEN": "settings_token",
+        }
         crawler = get_crawler(settings_dict=aws_credentials)
         # Instantiate with crawler
         storage = S3FeedStorage.from_crawler(
             crawler,
-            's3://mybucket/export.csv',
+            "s3://mybucket/export.csv",
         )
-        self.assertEqual(storage.access_key, 'settings_key')
-        self.assertEqual(storage.secret_key, 'settings_secret')
-        self.assertEqual(storage.session_token, 'settings_token')
+        self.assertEqual(storage.access_key, "settings_key")
+        self.assertEqual(storage.secret_key, "settings_secret")
+        self.assertEqual(storage.session_token, "settings_token")
         # Instantiate directly
-        storage = S3FeedStorage('s3://mybucket/export.csv',
-                                aws_credentials['AWS_ACCESS_KEY_ID'],
-                                aws_credentials['AWS_SECRET_ACCESS_KEY'],
-                                session_token=aws_credentials['AWS_SESSION_TOKEN'])
-        self.assertEqual(storage.access_key, 'settings_key')
-        self.assertEqual(storage.secret_key, 'settings_secret')
-        self.assertEqual(storage.session_token, 'settings_token')
+        storage = S3FeedStorage(
+            "s3://mybucket/export.csv",
+            aws_credentials["AWS_ACCESS_KEY_ID"],
+            aws_credentials["AWS_SECRET_ACCESS_KEY"],
+            session_token=aws_credentials["AWS_SESSION_TOKEN"],
+        )
+        self.assertEqual(storage.access_key, "settings_key")
+        self.assertEqual(storage.secret_key, "settings_secret")
+        self.assertEqual(storage.session_token, "settings_token")
         # URI priority > settings priority
-        storage = S3FeedStorage('s3://uri_key:uri_secret@mybucket/export.csv',
-                                aws_credentials['AWS_ACCESS_KEY_ID'],
-                                aws_credentials['AWS_SECRET_ACCESS_KEY'])
-        self.assertEqual(storage.access_key, 'uri_key')
-        self.assertEqual(storage.secret_key, 'uri_secret')
+        storage = S3FeedStorage(
+            "s3://uri_key:uri_secret@mybucket/export.csv",
+            aws_credentials["AWS_ACCESS_KEY_ID"],
+            aws_credentials["AWS_SECRET_ACCESS_KEY"],
+        )
+        self.assertEqual(storage.access_key, "uri_key")
+        self.assertEqual(storage.secret_key, "uri_secret")
 
     @defer.inlineCallbacks
     def test_store(self):
         skip_if_no_boto()
 
         settings = {
-            'AWS_ACCESS_KEY_ID': 'access_key',
-            'AWS_SECRET_ACCESS_KEY': 'secret_key',
+            "AWS_ACCESS_KEY_ID": "access_key",
+            "AWS_SECRET_ACCESS_KEY": "secret_key",
         }
         crawler = get_crawler(settings_dict=settings)
-        bucket = 'mybucket'
-        key = 'export.csv'
-        storage = S3FeedStorage.from_crawler(crawler, f's3://{bucket}/{key}')
+        bucket = "mybucket"
+        key = "export.csv"
+        storage = S3FeedStorage.from_crawler(crawler, f"s3://{bucket}/{key}")
         verifyObject(IFeedStorage, storage)
 
         file = mock.MagicMock()
         from botocore.stub import Stubber
+
         with Stubber(storage.s3_client) as stub:
             stub.add_response(
-                'put_object',
+                "put_object",
                 expected_params={
-                    'Body': file,
-                    'Bucket': bucket,
-                    'Key': key,
+                    "Body": file,
+                    "Bucket": bucket,
+                    "Key": key,
                 },
                 service_response={},
             )
 
             yield storage.store(file)
 
             stub.assert_no_pending_responses()
             self.assertEqual(
                 file.method_calls,
                 [
                     mock.call.seek(0),
                     # The call to read does not happen with Stubber
                     mock.call.close(),
-                ]
+                ],
             )
 
     def test_init_without_acl(self):
-        storage = S3FeedStorage(
-            's3://mybucket/export.csv',
-            'access_key',
-            'secret_key'
-        )
-        self.assertEqual(storage.access_key, 'access_key')
-        self.assertEqual(storage.secret_key, 'secret_key')
+        storage = S3FeedStorage("s3://mybucket/export.csv", "access_key", "secret_key")
+        self.assertEqual(storage.access_key, "access_key")
+        self.assertEqual(storage.secret_key, "secret_key")
         self.assertEqual(storage.acl, None)
 
     def test_init_with_acl(self):
         storage = S3FeedStorage(
-            's3://mybucket/export.csv',
-            'access_key',
-            'secret_key',
-            'custom-acl'
-        )
-        self.assertEqual(storage.access_key, 'access_key')
-        self.assertEqual(storage.secret_key, 'secret_key')
-        self.assertEqual(storage.acl, 'custom-acl')
+            "s3://mybucket/export.csv", "access_key", "secret_key", "custom-acl"
+        )
+        self.assertEqual(storage.access_key, "access_key")
+        self.assertEqual(storage.secret_key, "secret_key")
+        self.assertEqual(storage.acl, "custom-acl")
 
     def test_init_with_endpoint_url(self):
         storage = S3FeedStorage(
-            's3://mybucket/export.csv',
-            'access_key',
-            'secret_key',
-            endpoint_url='https://example.com'
-        )
-        self.assertEqual(storage.access_key, 'access_key')
-        self.assertEqual(storage.secret_key, 'secret_key')
-        self.assertEqual(storage.endpoint_url, 'https://example.com')
+            "s3://mybucket/export.csv",
+            "access_key",
+            "secret_key",
+            endpoint_url="https://example.com",
+        )
+        self.assertEqual(storage.access_key, "access_key")
+        self.assertEqual(storage.secret_key, "secret_key")
+        self.assertEqual(storage.endpoint_url, "https://example.com")
 
     def test_from_crawler_without_acl(self):
         settings = {
-            'AWS_ACCESS_KEY_ID': 'access_key',
-            'AWS_SECRET_ACCESS_KEY': 'secret_key',
+            "AWS_ACCESS_KEY_ID": "access_key",
+            "AWS_SECRET_ACCESS_KEY": "secret_key",
         }
         crawler = get_crawler(settings_dict=settings)
         storage = S3FeedStorage.from_crawler(
             crawler,
-            's3://mybucket/export.csv',
+            "s3://mybucket/export.csv",
         )
-        self.assertEqual(storage.access_key, 'access_key')
-        self.assertEqual(storage.secret_key, 'secret_key')
+        self.assertEqual(storage.access_key, "access_key")
+        self.assertEqual(storage.secret_key, "secret_key")
         self.assertEqual(storage.acl, None)
 
     def test_without_endpoint_url(self):
         settings = {
-            'AWS_ACCESS_KEY_ID': 'access_key',
-            'AWS_SECRET_ACCESS_KEY': 'secret_key',
+            "AWS_ACCESS_KEY_ID": "access_key",
+            "AWS_SECRET_ACCESS_KEY": "secret_key",
         }
         crawler = get_crawler(settings_dict=settings)
         storage = S3FeedStorage.from_crawler(
             crawler,
-            's3://mybucket/export.csv',
+            "s3://mybucket/export.csv",
         )
-        self.assertEqual(storage.access_key, 'access_key')
-        self.assertEqual(storage.secret_key, 'secret_key')
+        self.assertEqual(storage.access_key, "access_key")
+        self.assertEqual(storage.secret_key, "secret_key")
         self.assertEqual(storage.endpoint_url, None)
 
     def test_from_crawler_with_acl(self):
         settings = {
-            'AWS_ACCESS_KEY_ID': 'access_key',
-            'AWS_SECRET_ACCESS_KEY': 'secret_key',
-            'FEED_STORAGE_S3_ACL': 'custom-acl',
+            "AWS_ACCESS_KEY_ID": "access_key",
+            "AWS_SECRET_ACCESS_KEY": "secret_key",
+            "FEED_STORAGE_S3_ACL": "custom-acl",
         }
         crawler = get_crawler(settings_dict=settings)
         storage = S3FeedStorage.from_crawler(
             crawler,
-            's3://mybucket/export.csv',
+            "s3://mybucket/export.csv",
         )
-        self.assertEqual(storage.access_key, 'access_key')
-        self.assertEqual(storage.secret_key, 'secret_key')
-        self.assertEqual(storage.acl, 'custom-acl')
+        self.assertEqual(storage.access_key, "access_key")
+        self.assertEqual(storage.secret_key, "secret_key")
+        self.assertEqual(storage.acl, "custom-acl")
 
     def test_from_crawler_with_endpoint_url(self):
         settings = {
-            'AWS_ACCESS_KEY_ID': 'access_key',
-            'AWS_SECRET_ACCESS_KEY': 'secret_key',
-            'AWS_ENDPOINT_URL': 'https://example.com',
+            "AWS_ACCESS_KEY_ID": "access_key",
+            "AWS_SECRET_ACCESS_KEY": "secret_key",
+            "AWS_ENDPOINT_URL": "https://example.com",
         }
         crawler = get_crawler(settings_dict=settings)
-        storage = S3FeedStorage.from_crawler(
-            crawler,
-            's3://mybucket/export.csv'
-        )
-        self.assertEqual(storage.access_key, 'access_key')
-        self.assertEqual(storage.secret_key, 'secret_key')
-        self.assertEqual(storage.endpoint_url, 'https://example.com')
+        storage = S3FeedStorage.from_crawler(crawler, "s3://mybucket/export.csv")
+        self.assertEqual(storage.access_key, "access_key")
+        self.assertEqual(storage.secret_key, "secret_key")
+        self.assertEqual(storage.endpoint_url, "https://example.com")
 
     @defer.inlineCallbacks
     def test_store_botocore_without_acl(self):
         skip_if_no_boto()
         storage = S3FeedStorage(
-            's3://mybucket/export.csv',
-            'access_key',
-            'secret_key',
+            "s3://mybucket/export.csv",
+            "access_key",
+            "secret_key",
         )
-        self.assertEqual(storage.access_key, 'access_key')
-        self.assertEqual(storage.secret_key, 'secret_key')
+        self.assertEqual(storage.access_key, "access_key")
+        self.assertEqual(storage.secret_key, "secret_key")
         self.assertEqual(storage.acl, None)
 
         storage.s3_client = mock.MagicMock()
-        yield storage.store(BytesIO(b'test file'))
-        self.assertNotIn('ACL', storage.s3_client.put_object.call_args[1])
+        yield storage.store(BytesIO(b"test file"))
+        self.assertNotIn("ACL", storage.s3_client.put_object.call_args[1])
 
     @defer.inlineCallbacks
     def test_store_botocore_with_acl(self):
         skip_if_no_boto()
         storage = S3FeedStorage(
-            's3://mybucket/export.csv',
-            'access_key',
-            'secret_key',
-            'custom-acl'
-        )
-        self.assertEqual(storage.access_key, 'access_key')
-        self.assertEqual(storage.secret_key, 'secret_key')
-        self.assertEqual(storage.acl, 'custom-acl')
+            "s3://mybucket/export.csv", "access_key", "secret_key", "custom-acl"
+        )
+        self.assertEqual(storage.access_key, "access_key")
+        self.assertEqual(storage.secret_key, "secret_key")
+        self.assertEqual(storage.acl, "custom-acl")
 
         storage.s3_client = mock.MagicMock()
-        yield storage.store(BytesIO(b'test file'))
+        yield storage.store(BytesIO(b"test file"))
         self.assertEqual(
-            storage.s3_client.put_object.call_args[1].get('ACL'),
-            'custom-acl'
+            storage.s3_client.put_object.call_args[1].get("ACL"), "custom-acl"
         )
 
     def test_overwrite_default(self):
         with LogCapture() as log:
             S3FeedStorage(
-                's3://mybucket/export.csv',
-                'access_key',
-                'secret_key',
-                'custom-acl'
+                "s3://mybucket/export.csv", "access_key", "secret_key", "custom-acl"
             )
-        self.assertNotIn('S3 does not support appending to files', str(log))
+        self.assertNotIn("S3 does not support appending to files", str(log))
 
     def test_overwrite_false(self):
         with LogCapture() as log:
             S3FeedStorage(
-                's3://mybucket/export.csv',
-                'access_key',
-                'secret_key',
-                'custom-acl',
-                feed_options={'overwrite': False},
+                "s3://mybucket/export.csv",
+                "access_key",
+                "secret_key",
+                "custom-acl",
+                feed_options={"overwrite": False},
             )
-        self.assertIn('S3 does not support appending to files', str(log))
+        self.assertIn("S3 does not support appending to files", str(log))
 
 
 class GCSFeedStorageTest(unittest.TestCase):
-
     def test_parse_settings(self):
         try:
             from google.cloud.storage import Client  # noqa
         except ImportError:
             raise unittest.SkipTest("GCSFeedStorage requires google-cloud-storage")
 
-        settings = {'GCS_PROJECT_ID': '123', 'FEED_STORAGE_GCS_ACL': 'publicRead'}
+        settings = {"GCS_PROJECT_ID": "123", "FEED_STORAGE_GCS_ACL": "publicRead"}
         crawler = get_crawler(settings_dict=settings)
-        storage = GCSFeedStorage.from_crawler(crawler, 'gs://mybucket/export.csv')
-        assert storage.project_id == '123'
-        assert storage.acl == 'publicRead'
-        assert storage.bucket_name == 'mybucket'
-        assert storage.blob_name == 'export.csv'
+        storage = GCSFeedStorage.from_crawler(crawler, "gs://mybucket/export.csv")
+        assert storage.project_id == "123"
+        assert storage.acl == "publicRead"
+        assert storage.bucket_name == "mybucket"
+        assert storage.blob_name == "export.csv"
 
     def test_parse_empty_acl(self):
         try:
             from google.cloud.storage import Client  # noqa
         except ImportError:
             raise unittest.SkipTest("GCSFeedStorage requires google-cloud-storage")
 
-        settings = {'GCS_PROJECT_ID': '123', 'FEED_STORAGE_GCS_ACL': ''}
+        settings = {"GCS_PROJECT_ID": "123", "FEED_STORAGE_GCS_ACL": ""}
         crawler = get_crawler(settings_dict=settings)
-        storage = GCSFeedStorage.from_crawler(crawler, 'gs://mybucket/export.csv')
+        storage = GCSFeedStorage.from_crawler(crawler, "gs://mybucket/export.csv")
         assert storage.acl is None
 
-        settings = {'GCS_PROJECT_ID': '123', 'FEED_STORAGE_GCS_ACL': None}
+        settings = {"GCS_PROJECT_ID": "123", "FEED_STORAGE_GCS_ACL": None}
         crawler = get_crawler(settings_dict=settings)
-        storage = GCSFeedStorage.from_crawler(crawler, 'gs://mybucket/export.csv')
+        storage = GCSFeedStorage.from_crawler(crawler, "gs://mybucket/export.csv")
         assert storage.acl is None
 
     @defer.inlineCallbacks
     def test_store(self):
         try:
             from google.cloud.storage import Client  # noqa
         except ImportError:
             raise unittest.SkipTest("GCSFeedStorage requires google-cloud-storage")
 
-        uri = 'gs://mybucket/export.csv'
-        project_id = 'myproject-123'
-        acl = 'publicRead'
+        uri = "gs://mybucket/export.csv"
+        project_id = "myproject-123"
+        acl = "publicRead"
         (client_mock, bucket_mock, blob_mock) = mock_google_cloud_storage()
-        with mock.patch('google.cloud.storage.Client') as m:
+        with mock.patch("google.cloud.storage.Client") as m:
             m.return_value = client_mock
 
             f = mock.Mock()
             storage = GCSFeedStorage(uri, project_id, acl)
             yield storage.store(f)
 
             f.seek.assert_called_once_with(0)
             m.assert_called_once_with(project=project_id)
-            client_mock.get_bucket.assert_called_once_with('mybucket')
-            bucket_mock.blob.assert_called_once_with('export.csv')
+            client_mock.get_bucket.assert_called_once_with("mybucket")
+            bucket_mock.blob.assert_called_once_with("export.csv")
             blob_mock.upload_from_file.assert_called_once_with(f, predefined_acl=acl)
 
 
 class StdoutFeedStorageTest(unittest.TestCase):
-
     @defer.inlineCallbacks
     def test_store(self):
         out = BytesIO()
-        storage = StdoutFeedStorage('stdout:', _stdout=out)
+        storage = StdoutFeedStorage("stdout:", _stdout=out)
         file = storage.open(scrapy.Spider("default"))
         file.write(b"content")
         yield storage.store(file)
         self.assertEqual(out.getvalue(), b"content")
 
     def test_overwrite_default(self):
         with LogCapture() as log:
-            StdoutFeedStorage('stdout:')
-        self.assertNotIn('Standard output (stdout) storage does not support overwriting', str(log))
+            StdoutFeedStorage("stdout:")
+        self.assertNotIn(
+            "Standard output (stdout) storage does not support overwriting", str(log)
+        )
 
     def test_overwrite_true(self):
         with LogCapture() as log:
-            StdoutFeedStorage('stdout:', feed_options={'overwrite': True})
-        self.assertIn('Standard output (stdout) storage does not support overwriting', str(log))
+            StdoutFeedStorage("stdout:", feed_options={"overwrite": True})
+        self.assertIn(
+            "Standard output (stdout) storage does not support overwriting", str(log)
+        )
 
 
 class FromCrawlerMixin:
     init_with_crawler = False
 
     @classmethod
     def from_crawler(cls, crawler, *args, feed_options=None, **kwargs):
@@ -550,58 +532,55 @@
 
 
 class FromCrawlerCsvItemExporter(CsvItemExporter, FromCrawlerMixin):
     pass
 
 
 class FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):
-
     @classmethod
     def from_crawler(cls, crawler, *args, feed_options=None, **kwargs):
         cls.init_with_crawler = True
         return cls(*args, feed_options=feed_options, **kwargs)
 
 
 class DummyBlockingFeedStorage(BlockingFeedStorage):
-
     def __init__(self, uri, *args, feed_options=None):
-        self.path = file_uri_to_path(uri)
+        self.path = Path(file_uri_to_path(uri))
 
     def _store_in_thread(self, file):
-        dirname = os.path.dirname(self.path)
-        if dirname and not os.path.exists(dirname):
-            os.makedirs(dirname)
-        with open(self.path, 'ab') as output_file:
+        dirname = self.path.parent
+        if dirname and not dirname.exists():
+            dirname.mkdir(parents=True)
+        with self.path.open("ab") as output_file:
             output_file.write(file.read())
 
         file.close()
 
 
 class FailingBlockingFeedStorage(DummyBlockingFeedStorage):
-
     def _store_in_thread(self, file):
-        raise OSError('Cannot store')
+        raise OSError("Cannot store")
 
 
 @implementer(IFeedStorage)
 class LogOnStoreFileStorage:
     """
     This storage logs inside `store` method.
     It can be used to make sure `store` method is invoked.
     """
 
     def __init__(self, uri, feed_options=None):
         self.path = file_uri_to_path(uri)
         self.logger = getLogger()
 
     def open(self, spider):
-        return tempfile.NamedTemporaryFile(prefix='feed-')
+        return tempfile.NamedTemporaryFile(prefix="feed-")
 
     def store(self, file):
-        self.logger.info('Storage.store is called')
+        self.logger.info("Storage.store is called")
         file.close()
 
 
 class FeedExportTestBase(ABC, unittest.TestCase):
     __test__ = False
 
     class MyItem(scrapy.Item):
@@ -609,33 +588,33 @@
         egg = scrapy.Field()
         baz = scrapy.Field()
 
     class MyItem2(scrapy.Item):
         foo = scrapy.Field()
         hello = scrapy.Field()
 
-    def _random_temp_filename(self, inter_dir=''):
+    def _random_temp_filename(self, inter_dir="") -> Path:
         chars = [random.choice(ascii_letters + digits) for _ in range(15)]
-        filename = ''.join(chars)
-        return os.path.join(self.temp_dir, inter_dir, filename)
+        filename = "".join(chars)
+        return Path(self.temp_dir, inter_dir, filename)
 
     def setUp(self):
         self.temp_dir = tempfile.mkdtemp()
 
     def tearDown(self):
         shutil.rmtree(self.temp_dir, ignore_errors=True)
 
     @defer.inlineCallbacks
     def exported_data(self, items, settings):
         """
         Return exported data which a spider yielding ``items`` would return.
         """
 
         class TestSpider(scrapy.Spider):
-            name = 'testspider'
+            name = "testspider"
 
             def parse(self, response):
                 for item in items:
                     yield item
 
         data = yield self.run_and_export(TestSpider, settings)
         return data
@@ -643,15 +622,15 @@
     @defer.inlineCallbacks
     def exported_no_data(self, settings):
         """
         Return exported data which a spider yielding no ``items`` would return.
         """
 
         class TestSpider(scrapy.Spider):
-            name = 'testspider'
+            name = "testspider"
 
             def parse(self, response):
                 pass
 
         data = yield self.run_and_export(TestSpider, settings)
         return data
 
@@ -677,721 +656,867 @@
                 try:
                     result.append(load_func(temp))
                 except EOFError:
                     break
         return result
 
 
+class InstrumentedFeedSlot(_FeedSlot):
+    """Instrumented _FeedSlot subclass for keeping track of calls to
+    start_exporting and finish_exporting."""
+
+    def start_exporting(self):
+        self.update_listener("start")
+        super().start_exporting()
+
+    def finish_exporting(self):
+        self.update_listener("finish")
+        super().finish_exporting()
+
+    @classmethod
+    def subscribe__listener(cls, listener):
+        cls.update_listener = listener.update
+
+
+class IsExportingListener:
+    """When subscribed to InstrumentedFeedSlot, keeps track of when
+    a call to start_exporting has been made without a closing call to
+    finish_exporting and when a call to finish_exporting has been made
+    before a call to start_exporting."""
+
+    def __init__(self):
+        self.start_without_finish = False
+        self.finish_without_start = False
+
+    def update(self, method):
+        if method == "start":
+            self.start_without_finish = True
+        elif method == "finish":
+            if self.start_without_finish:
+                self.start_without_finish = False
+            else:
+                self.finish_before_start = True
+
+
+class ExceptionJsonItemExporter(JsonItemExporter):
+    """JsonItemExporter that throws an exception every time export_item is called."""
+
+    def export_item(self, _):
+        raise Exception("foo")
+
+
 class FeedExportTest(FeedExportTestBase):
     __test__ = True
 
     @defer.inlineCallbacks
     def run_and_export(self, spider_cls, settings):
-        """ Run spider with specified settings; return exported data. """
+        """Run spider with specified settings; return exported data."""
 
-        FEEDS = settings.get('FEEDS') or {}
-        settings['FEEDS'] = {
+        FEEDS = settings.get("FEEDS") or {}
+        settings["FEEDS"] = {
             printf_escape(path_to_url(file_path)): feed_options
             for file_path, feed_options in FEEDS.items()
         }
 
         content = {}
         try:
             with MockServer() as s:
-                spider_cls.start_urls = [s.url('/')]
+                spider_cls.start_urls = [s.url("/")]
                 crawler = get_crawler(spider_cls, settings)
                 yield crawler.crawl()
 
             for file_path, feed_options in FEEDS.items():
-                if not os.path.exists(str(file_path)):
+                if not Path(file_path).exists():
                     continue
 
-                with open(str(file_path), 'rb') as f:
-                    content[feed_options['format']] = f.read()
+                content[feed_options["format"]] = Path(file_path).read_bytes()
 
         finally:
             for file_path in FEEDS.keys():
-                if not os.path.exists(str(file_path)):
+                if not Path(file_path).exists():
                     continue
 
-                os.remove(str(file_path))
+                Path(file_path).unlink()
 
         return content
 
     @defer.inlineCallbacks
     def assertExportedCsv(self, items, header, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'csv'},
-            },
-        })
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": "csv"},
+                },
+            }
+        )
         data = yield self.exported_data(items, settings)
-        reader = csv.DictReader(to_unicode(data['csv']).splitlines())
+        reader = csv.DictReader(to_unicode(data["csv"]).splitlines())
         self.assertEqual(reader.fieldnames, list(header))
         self.assertEqual(rows, list(reader))
 
     @defer.inlineCallbacks
     def assertExportedJsonLines(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'jl'},
-            },
-        })
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": "jl"},
+                },
+            }
+        )
         data = yield self.exported_data(items, settings)
-        parsed = [json.loads(to_unicode(line)) for line in data['jl'].splitlines()]
+        parsed = [json.loads(to_unicode(line)) for line in data["jl"].splitlines()]
         rows = [{k: v for k, v in row.items() if v} for row in rows]
         self.assertEqual(rows, parsed)
 
     @defer.inlineCallbacks
     def assertExportedXml(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'xml'},
-            },
-        })
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": "xml"},
+                },
+            }
+        )
         data = yield self.exported_data(items, settings)
         rows = [{k: v for k, v in row.items() if v} for row in rows]
-        root = lxml.etree.fromstring(data['xml'])
-        got_rows = [{e.tag: e.text for e in it} for it in root.findall('item')]
+        root = lxml.etree.fromstring(data["xml"])
+        got_rows = [{e.tag: e.text for e in it} for it in root.findall("item")]
         self.assertEqual(rows, got_rows)
 
     @defer.inlineCallbacks
     def assertExportedMultiple(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'xml'},
-                self._random_temp_filename(): {'format': 'json'},
-            },
-        })
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": "xml"},
+                    self._random_temp_filename(): {"format": "json"},
+                },
+            }
+        )
         data = yield self.exported_data(items, settings)
         rows = [{k: v for k, v in row.items() if v} for row in rows]
         # XML
-        root = lxml.etree.fromstring(data['xml'])
-        xml_rows = [{e.tag: e.text for e in it} for it in root.findall('item')]
+        root = lxml.etree.fromstring(data["xml"])
+        xml_rows = [{e.tag: e.text for e in it} for it in root.findall("item")]
         self.assertEqual(rows, xml_rows)
         # JSON
-        json_rows = json.loads(to_unicode(data['json']))
+        json_rows = json.loads(to_unicode(data["json"]))
         self.assertEqual(rows, json_rows)
 
     @defer.inlineCallbacks
     def assertExportedPickle(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'pickle'},
-            },
-        })
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": "pickle"},
+                },
+            }
+        )
         data = yield self.exported_data(items, settings)
         expected = [{k: v for k, v in row.items() if v} for row in rows]
         import pickle
-        result = self._load_until_eof(data['pickle'], load_func=pickle.load)
+
+        result = self._load_until_eof(data["pickle"], load_func=pickle.load)
         self.assertEqual(expected, result)
 
     @defer.inlineCallbacks
     def assertExportedMarshal(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'marshal'},
-            },
-        })
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": "marshal"},
+                },
+            }
+        )
         data = yield self.exported_data(items, settings)
         expected = [{k: v for k, v in row.items() if v} for row in rows]
         import marshal
-        result = self._load_until_eof(data['marshal'], load_func=marshal.load)
+
+        result = self._load_until_eof(data["marshal"], load_func=marshal.load)
         self.assertEqual(expected, result)
 
     @defer.inlineCallbacks
     def test_stats_file_success(self):
         settings = {
             "FEEDS": {
-                printf_escape(path_to_url(self._random_temp_filename())): {
+                printf_escape(path_to_url(str(self._random_temp_filename()))): {
                     "format": "json",
                 }
             },
         }
         crawler = get_crawler(ItemSpider, settings)
         with MockServer() as mockserver:
             yield crawler.crawl(mockserver=mockserver)
-        self.assertIn("feedexport/success_count/FileFeedStorage", crawler.stats.get_stats())
-        self.assertEqual(crawler.stats.get_value("feedexport/success_count/FileFeedStorage"), 1)
+        self.assertIn(
+            "feedexport/success_count/FileFeedStorage", crawler.stats.get_stats()
+        )
+        self.assertEqual(
+            crawler.stats.get_value("feedexport/success_count/FileFeedStorage"), 1
+        )
 
     @defer.inlineCallbacks
     def test_stats_file_failed(self):
         settings = {
             "FEEDS": {
-                printf_escape(path_to_url(self._random_temp_filename())): {
+                printf_escape(path_to_url(str(self._random_temp_filename()))): {
                     "format": "json",
                 }
             },
         }
         crawler = get_crawler(ItemSpider, settings)
         with ExitStack() as stack:
             mockserver = stack.enter_context(MockServer())
             stack.enter_context(
                 mock.patch(
                     "scrapy.extensions.feedexport.FileFeedStorage.store",
-                    side_effect=KeyError("foo"))
+                    side_effect=KeyError("foo"),
+                )
             )
             yield crawler.crawl(mockserver=mockserver)
-        self.assertIn("feedexport/failed_count/FileFeedStorage", crawler.stats.get_stats())
-        self.assertEqual(crawler.stats.get_value("feedexport/failed_count/FileFeedStorage"), 1)
+        self.assertIn(
+            "feedexport/failed_count/FileFeedStorage", crawler.stats.get_stats()
+        )
+        self.assertEqual(
+            crawler.stats.get_value("feedexport/failed_count/FileFeedStorage"), 1
+        )
 
     @defer.inlineCallbacks
     def test_stats_multiple_file(self):
         settings = {
-            'AWS_ACCESS_KEY_ID': 'access_key',
-            'AWS_SECRET_ACCESS_KEY': 'secret_key',
+            "AWS_ACCESS_KEY_ID": "access_key",
+            "AWS_SECRET_ACCESS_KEY": "secret_key",
             "FEEDS": {
-                printf_escape(path_to_url(self._random_temp_filename())): {
+                printf_escape(path_to_url(str(self._random_temp_filename()))): {
                     "format": "json",
                 },
                 "s3://bucket/key/foo.csv": {
                     "format": "csv",
                 },
                 "stdout:": {
                     "format": "xml",
-                }
+                },
             },
         }
         crawler = get_crawler(ItemSpider, settings)
         with MockServer() as mockserver, mock.patch.object(S3FeedStorage, "store"):
             yield crawler.crawl(mockserver=mockserver)
-        self.assertIn("feedexport/success_count/FileFeedStorage", crawler.stats.get_stats())
-        self.assertIn("feedexport/success_count/S3FeedStorage", crawler.stats.get_stats())
-        self.assertIn("feedexport/success_count/StdoutFeedStorage", crawler.stats.get_stats())
-        self.assertEqual(crawler.stats.get_value("feedexport/success_count/FileFeedStorage"), 1)
-        self.assertEqual(crawler.stats.get_value("feedexport/success_count/S3FeedStorage"), 1)
-        self.assertEqual(crawler.stats.get_value("feedexport/success_count/StdoutFeedStorage"), 1)
+        self.assertIn(
+            "feedexport/success_count/FileFeedStorage", crawler.stats.get_stats()
+        )
+        self.assertIn(
+            "feedexport/success_count/S3FeedStorage", crawler.stats.get_stats()
+        )
+        self.assertIn(
+            "feedexport/success_count/StdoutFeedStorage", crawler.stats.get_stats()
+        )
+        self.assertEqual(
+            crawler.stats.get_value("feedexport/success_count/FileFeedStorage"), 1
+        )
+        self.assertEqual(
+            crawler.stats.get_value("feedexport/success_count/S3FeedStorage"), 1
+        )
+        self.assertEqual(
+            crawler.stats.get_value("feedexport/success_count/StdoutFeedStorage"), 1
+        )
 
     @defer.inlineCallbacks
     def test_export_items(self):
         # feed exporters use field names from Item
         items = [
-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),
-            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),
+            self.MyItem({"foo": "bar1", "egg": "spam1"}),
+            self.MyItem({"foo": "bar2", "egg": "spam2", "baz": "quux2"}),
         ]
         rows = [
-            {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},
-            {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}
+            {"egg": "spam1", "foo": "bar1", "baz": ""},
+            {"egg": "spam2", "foo": "bar2", "baz": "quux2"},
         ]
         header = self.MyItem.fields.keys()
         yield self.assertExported(items, header, rows)
 
     @defer.inlineCallbacks
     def test_export_no_items_not_store_empty(self):
-        for fmt in ('json', 'jsonlines', 'xml', 'csv'):
+        for fmt in ("json", "jsonlines", "xml", "csv"):
             settings = {
-                'FEEDS': {
-                    self._random_temp_filename(): {'format': fmt},
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": fmt},
                 },
             }
             data = yield self.exported_no_data(settings)
-            self.assertEqual(b'', data[fmt])
+            self.assertEqual(b"", data[fmt])
+
+    @defer.inlineCallbacks
+    def test_start_finish_exporting_items(self):
+        items = [
+            self.MyItem({"foo": "bar1", "egg": "spam1"}),
+        ]
+        settings = {
+            "FEEDS": {
+                self._random_temp_filename(): {"format": "json"},
+            },
+            "FEED_EXPORT_INDENT": None,
+        }
+
+        listener = IsExportingListener()
+        InstrumentedFeedSlot.subscribe__listener(listener)
+
+        with mock.patch("scrapy.extensions.feedexport._FeedSlot", InstrumentedFeedSlot):
+            _ = yield self.exported_data(items, settings)
+            self.assertFalse(listener.start_without_finish)
+            self.assertFalse(listener.finish_without_start)
+
+    @defer.inlineCallbacks
+    def test_start_finish_exporting_no_items(self):
+        items = []
+        settings = {
+            "FEEDS": {
+                self._random_temp_filename(): {"format": "json"},
+            },
+            "FEED_EXPORT_INDENT": None,
+        }
+
+        listener = IsExportingListener()
+        InstrumentedFeedSlot.subscribe__listener(listener)
+
+        with mock.patch("scrapy.extensions.feedexport._FeedSlot", InstrumentedFeedSlot):
+            _ = yield self.exported_data(items, settings)
+            self.assertFalse(listener.start_without_finish)
+            self.assertFalse(listener.finish_without_start)
+
+    @defer.inlineCallbacks
+    def test_start_finish_exporting_items_exception(self):
+        items = [
+            self.MyItem({"foo": "bar1", "egg": "spam1"}),
+        ]
+        settings = {
+            "FEEDS": {
+                self._random_temp_filename(): {"format": "json"},
+            },
+            "FEED_EXPORTERS": {"json": ExceptionJsonItemExporter},
+            "FEED_EXPORT_INDENT": None,
+        }
+
+        listener = IsExportingListener()
+        InstrumentedFeedSlot.subscribe__listener(listener)
+
+        with mock.patch("scrapy.extensions.feedexport._FeedSlot", InstrumentedFeedSlot):
+            _ = yield self.exported_data(items, settings)
+            self.assertFalse(listener.start_without_finish)
+            self.assertFalse(listener.finish_without_start)
+
+    @defer.inlineCallbacks
+    def test_start_finish_exporting_no_items_exception(self):
+        items = []
+        settings = {
+            "FEEDS": {
+                self._random_temp_filename(): {"format": "json"},
+            },
+            "FEED_EXPORTERS": {"json": ExceptionJsonItemExporter},
+            "FEED_EXPORT_INDENT": None,
+        }
+
+        listener = IsExportingListener()
+        InstrumentedFeedSlot.subscribe__listener(listener)
+
+        with mock.patch("scrapy.extensions.feedexport._FeedSlot", InstrumentedFeedSlot):
+            _ = yield self.exported_data(items, settings)
+            self.assertFalse(listener.start_without_finish)
+            self.assertFalse(listener.finish_without_start)
 
     @defer.inlineCallbacks
     def test_export_no_items_store_empty(self):
         formats = (
-            ('json', b'[]'),
-            ('jsonlines', b''),
-            ('xml', b'<?xml version="1.0" encoding="utf-8"?>\n<items></items>'),
-            ('csv', b''),
+            ("json", b"[]"),
+            ("jsonlines", b""),
+            ("xml", b'<?xml version="1.0" encoding="utf-8"?>\n<items></items>'),
+            ("csv", b""),
         )
 
         for fmt, expctd in formats:
             settings = {
-                'FEEDS': {
-                    self._random_temp_filename(): {'format': fmt},
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": fmt},
                 },
-                'FEED_STORE_EMPTY': True,
-                'FEED_EXPORT_INDENT': None,
+                "FEED_STORE_EMPTY": True,
+                "FEED_EXPORT_INDENT": None,
             }
             data = yield self.exported_no_data(settings)
             self.assertEqual(expctd, data[fmt])
 
     @defer.inlineCallbacks
     def test_export_no_items_multiple_feeds(self):
-        """ Make sure that `storage.store` is called for every feed. """
+        """Make sure that `storage.store` is called for every feed."""
         settings = {
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'json'},
-                self._random_temp_filename(): {'format': 'xml'},
-                self._random_temp_filename(): {'format': 'csv'},
+            "FEEDS": {
+                self._random_temp_filename(): {"format": "json"},
+                self._random_temp_filename(): {"format": "xml"},
+                self._random_temp_filename(): {"format": "csv"},
             },
-            'FEED_STORAGES': {'file': LogOnStoreFileStorage},
-            'FEED_STORE_EMPTY': False
+            "FEED_STORAGES": {"file": LogOnStoreFileStorage},
+            "FEED_STORE_EMPTY": False,
         }
 
         with LogCapture() as log:
             yield self.exported_no_data(settings)
 
         print(log)
-        self.assertEqual(str(log).count('Storage.store is called'), 3)
+        self.assertEqual(str(log).count("Storage.store is called"), 3)
 
     @defer.inlineCallbacks
     def test_export_multiple_item_classes(self):
 
         items = [
-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),
-            self.MyItem2({'hello': 'world2', 'foo': 'bar2'}),
-            self.MyItem({'foo': 'bar3', 'egg': 'spam3', 'baz': 'quux3'}),
-            {'hello': 'world4', 'egg': 'spam4'},
+            self.MyItem({"foo": "bar1", "egg": "spam1"}),
+            self.MyItem2({"hello": "world2", "foo": "bar2"}),
+            self.MyItem({"foo": "bar3", "egg": "spam3", "baz": "quux3"}),
+            {"hello": "world4", "egg": "spam4"},
         ]
 
         # by default, Scrapy uses fields of the first Item for CSV and
         # all fields for JSON Lines
         header = self.MyItem.fields.keys()
         rows_csv = [
-            {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},
-            {'egg': '', 'foo': 'bar2', 'baz': ''},
-            {'egg': 'spam3', 'foo': 'bar3', 'baz': 'quux3'},
-            {'egg': 'spam4', 'foo': '', 'baz': ''},
+            {"egg": "spam1", "foo": "bar1", "baz": ""},
+            {"egg": "", "foo": "bar2", "baz": ""},
+            {"egg": "spam3", "foo": "bar3", "baz": "quux3"},
+            {"egg": "spam4", "foo": "", "baz": ""},
         ]
         rows_jl = [dict(row) for row in items]
         yield self.assertExportedCsv(items, header, rows_csv)
         yield self.assertExportedJsonLines(items, rows_jl)
 
     @defer.inlineCallbacks
     def test_export_items_empty_field_list(self):
         # FEED_EXPORT_FIELDS==[] means the same as default None
-        items = [{'foo': 'bar'}]
+        items = [{"foo": "bar"}]
         header = ["foo"]
-        rows = [{'foo': 'bar'}]
-        settings = {'FEED_EXPORT_FIELDS': []}
+        rows = [{"foo": "bar"}]
+        settings = {"FEED_EXPORT_FIELDS": []}
         yield self.assertExportedCsv(items, header, rows)
         yield self.assertExportedJsonLines(items, rows, settings)
 
     @defer.inlineCallbacks
     def test_export_items_field_list(self):
-        items = [{'foo': 'bar'}]
+        items = [{"foo": "bar"}]
         header = ["foo", "baz"]
-        rows = [{'foo': 'bar', 'baz': ''}]
-        settings = {'FEED_EXPORT_FIELDS': header}
+        rows = [{"foo": "bar", "baz": ""}]
+        settings = {"FEED_EXPORT_FIELDS": header}
         yield self.assertExported(items, header, rows, settings=settings)
 
     @defer.inlineCallbacks
     def test_export_items_comma_separated_field_list(self):
-        items = [{'foo': 'bar'}]
+        items = [{"foo": "bar"}]
         header = ["foo", "baz"]
-        rows = [{'foo': 'bar', 'baz': ''}]
-        settings = {'FEED_EXPORT_FIELDS': ",".join(header)}
+        rows = [{"foo": "bar", "baz": ""}]
+        settings = {"FEED_EXPORT_FIELDS": ",".join(header)}
         yield self.assertExported(items, header, rows, settings=settings)
 
     @defer.inlineCallbacks
     def test_export_items_json_field_list(self):
-        items = [{'foo': 'bar'}]
+        items = [{"foo": "bar"}]
         header = ["foo", "baz"]
-        rows = [{'foo': 'bar', 'baz': ''}]
-        settings = {'FEED_EXPORT_FIELDS': json.dumps(header)}
+        rows = [{"foo": "bar", "baz": ""}]
+        settings = {"FEED_EXPORT_FIELDS": json.dumps(header)}
         yield self.assertExported(items, header, rows, settings=settings)
 
     @defer.inlineCallbacks
     def test_export_items_field_names(self):
-        items = [{'foo': 'bar'}]
-        header = {'foo': 'Foo'}
-        rows = [{'Foo': 'bar'}]
-        settings = {'FEED_EXPORT_FIELDS': header}
-        yield self.assertExported(items, list(header.values()), rows,
-                                  settings=settings)
+        items = [{"foo": "bar"}]
+        header = {"foo": "Foo"}
+        rows = [{"Foo": "bar"}]
+        settings = {"FEED_EXPORT_FIELDS": header}
+        yield self.assertExported(items, list(header.values()), rows, settings=settings)
 
     @defer.inlineCallbacks
     def test_export_items_dict_field_names(self):
-        items = [{'foo': 'bar'}]
+        items = [{"foo": "bar"}]
         header = {
-            'baz': 'Baz',
-            'foo': 'Foo',
+            "baz": "Baz",
+            "foo": "Foo",
         }
-        rows = [{'Baz': '', 'Foo': 'bar'}]
-        settings = {'FEED_EXPORT_FIELDS': header}
-        yield self.assertExported(items, ['Baz', 'Foo'], rows,
-                                  settings=settings)
+        rows = [{"Baz": "", "Foo": "bar"}]
+        settings = {"FEED_EXPORT_FIELDS": header}
+        yield self.assertExported(items, ["Baz", "Foo"], rows, settings=settings)
 
     @defer.inlineCallbacks
     def test_export_items_json_field_names(self):
-        items = [{'foo': 'bar'}]
-        header = {'foo': 'Foo'}
-        rows = [{'Foo': 'bar'}]
-        settings = {'FEED_EXPORT_FIELDS': json.dumps(header)}
-        yield self.assertExported(items, list(header.values()), rows,
-                                  settings=settings)
+        items = [{"foo": "bar"}]
+        header = {"foo": "Foo"}
+        rows = [{"Foo": "bar"}]
+        settings = {"FEED_EXPORT_FIELDS": json.dumps(header)}
+        yield self.assertExported(items, list(header.values()), rows, settings=settings)
 
     @defer.inlineCallbacks
     def test_export_based_on_item_classes(self):
         items = [
-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),
-            self.MyItem2({'hello': 'world2', 'foo': 'bar2'}),
-            {'hello': 'world3', 'egg': 'spam3'},
+            self.MyItem({"foo": "bar1", "egg": "spam1"}),
+            self.MyItem2({"hello": "world2", "foo": "bar2"}),
+            {"hello": "world3", "egg": "spam3"},
         ]
 
         formats = {
-            'csv': b'baz,egg,foo\r\n,spam1,bar1\r\n',
-            'json': b'[\n{"hello": "world2", "foo": "bar2"}\n]',
-            'jsonlines': (
+            "csv": b"baz,egg,foo\r\n,spam1,bar1\r\n",
+            "json": b'[\n{"hello": "world2", "foo": "bar2"}\n]',
+            "jsonlines": (
                 b'{"foo": "bar1", "egg": "spam1"}\n'
                 b'{"hello": "world2", "foo": "bar2"}\n'
             ),
-            'xml': (
+            "xml": (
                 b'<?xml version="1.0" encoding="utf-8"?>\n<items>\n<item>'
-                b'<foo>bar1</foo><egg>spam1</egg></item>\n<item><hello>'
-                b'world2</hello><foo>bar2</foo></item>\n<item><hello>world3'
-                b'</hello><egg>spam3</egg></item>\n</items>'
+                b"<foo>bar1</foo><egg>spam1</egg></item>\n<item><hello>"
+                b"world2</hello><foo>bar2</foo></item>\n<item><hello>world3"
+                b"</hello><egg>spam3</egg></item>\n</items>"
             ),
         }
 
         settings = {
-            'FEEDS': {
+            "FEEDS": {
                 self._random_temp_filename(): {
-                    'format': 'csv',
-                    'item_classes': [self.MyItem],
+                    "format": "csv",
+                    "item_classes": [self.MyItem],
                 },
                 self._random_temp_filename(): {
-                    'format': 'json',
-                    'item_classes': [self.MyItem2],
+                    "format": "json",
+                    "item_classes": [self.MyItem2],
                 },
                 self._random_temp_filename(): {
-                    'format': 'jsonlines',
-                    'item_classes': [self.MyItem, self.MyItem2],
+                    "format": "jsonlines",
+                    "item_classes": [self.MyItem, self.MyItem2],
                 },
                 self._random_temp_filename(): {
-                    'format': 'xml',
+                    "format": "xml",
                 },
             },
         }
 
         data = yield self.exported_data(items, settings)
         for fmt, expected in formats.items():
             self.assertEqual(expected, data[fmt])
 
     @defer.inlineCallbacks
     def test_export_based_on_custom_filters(self):
         items = [
-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),
-            self.MyItem2({'hello': 'world2', 'foo': 'bar2'}),
-            {'hello': 'world3', 'egg': 'spam3'},
+            self.MyItem({"foo": "bar1", "egg": "spam1"}),
+            self.MyItem2({"hello": "world2", "foo": "bar2"}),
+            {"hello": "world3", "egg": "spam3"},
         ]
 
         MyItem = self.MyItem
 
         class CustomFilter1:
             def __init__(self, feed_options):
                 pass
 
             def accepts(self, item):
                 return isinstance(item, MyItem)
 
         class CustomFilter2(scrapy.extensions.feedexport.ItemFilter):
             def accepts(self, item):
-                if 'foo' not in item.fields:
+                if "foo" not in item.fields:
                     return False
                 return True
 
         class CustomFilter3(scrapy.extensions.feedexport.ItemFilter):
             def accepts(self, item):
-                if isinstance(item, tuple(self.item_classes)) and item['foo'] == "bar1":
+                if isinstance(item, tuple(self.item_classes)) and item["foo"] == "bar1":
                     return True
                 return False
 
         formats = {
-            'json': b'[\n{"foo": "bar1", "egg": "spam1"}\n]',
-            'xml': (
+            "json": b'[\n{"foo": "bar1", "egg": "spam1"}\n]',
+            "xml": (
                 b'<?xml version="1.0" encoding="utf-8"?>\n<items>\n<item>'
-                b'<foo>bar1</foo><egg>spam1</egg></item>\n<item><hello>'
-                b'world2</hello><foo>bar2</foo></item>\n</items>'
+                b"<foo>bar1</foo><egg>spam1</egg></item>\n<item><hello>"
+                b"world2</hello><foo>bar2</foo></item>\n</items>"
             ),
-            'jsonlines': b'{"foo": "bar1", "egg": "spam1"}\n',
+            "jsonlines": b'{"foo": "bar1", "egg": "spam1"}\n',
         }
 
         settings = {
-            'FEEDS': {
+            "FEEDS": {
                 self._random_temp_filename(): {
-                    'format': 'json',
-                    'item_filter': CustomFilter1,
+                    "format": "json",
+                    "item_filter": CustomFilter1,
                 },
                 self._random_temp_filename(): {
-                    'format': 'xml',
-                    'item_filter': CustomFilter2,
+                    "format": "xml",
+                    "item_filter": CustomFilter2,
                 },
                 self._random_temp_filename(): {
-                    'format': 'jsonlines',
-                    'item_classes': [self.MyItem, self.MyItem2],
-                    'item_filter': CustomFilter3,
+                    "format": "jsonlines",
+                    "item_classes": [self.MyItem, self.MyItem2],
+                    "item_filter": CustomFilter3,
                 },
             },
         }
 
         data = yield self.exported_data(items, settings)
         for fmt, expected in formats.items():
             self.assertEqual(expected, data[fmt])
 
     @defer.inlineCallbacks
     def test_export_dicts(self):
         # When dicts are used, only keys from the first row are used as
         # a header for CSV, and all fields are used for JSON Lines.
         items = [
-            {'foo': 'bar', 'egg': 'spam'},
-            {'foo': 'bar', 'egg': 'spam', 'baz': 'quux'},
-        ]
-        rows_csv = [
-            {'egg': 'spam', 'foo': 'bar'},
-            {'egg': 'spam', 'foo': 'bar'}
+            {"foo": "bar", "egg": "spam"},
+            {"foo": "bar", "egg": "spam", "baz": "quux"},
         ]
+        rows_csv = [{"egg": "spam", "foo": "bar"}, {"egg": "spam", "foo": "bar"}]
         rows_jl = items
-        yield self.assertExportedCsv(items, ['foo', 'egg'], rows_csv)
+        yield self.assertExportedCsv(items, ["foo", "egg"], rows_csv)
         yield self.assertExportedJsonLines(items, rows_jl)
 
     @defer.inlineCallbacks
     def test_export_feed_export_fields(self):
         # FEED_EXPORT_FIELDS option allows to order export fields
         # and to select a subset of fields to export, both for Items and dicts.
 
         for item_cls in [self.MyItem, dict]:
             items = [
-                item_cls({'foo': 'bar1', 'egg': 'spam1'}),
-                item_cls({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),
+                item_cls({"foo": "bar1", "egg": "spam1"}),
+                item_cls({"foo": "bar2", "egg": "spam2", "baz": "quux2"}),
             ]
 
             # export all columns
-            settings = {'FEED_EXPORT_FIELDS': 'foo,baz,egg'}
+            settings = {"FEED_EXPORT_FIELDS": "foo,baz,egg"}
             rows = [
-                {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},
-                {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}
+                {"egg": "spam1", "foo": "bar1", "baz": ""},
+                {"egg": "spam2", "foo": "bar2", "baz": "quux2"},
             ]
-            yield self.assertExported(items, ['foo', 'baz', 'egg'], rows,
-                                      settings=settings)
+            yield self.assertExported(
+                items, ["foo", "baz", "egg"], rows, settings=settings
+            )
 
             # export a subset of columns
-            settings = {'FEED_EXPORT_FIELDS': 'egg,baz'}
-            rows = [
-                {'egg': 'spam1', 'baz': ''},
-                {'egg': 'spam2', 'baz': 'quux2'}
-            ]
-            yield self.assertExported(items, ['egg', 'baz'], rows,
-                                      settings=settings)
+            settings = {"FEED_EXPORT_FIELDS": "egg,baz"}
+            rows = [{"egg": "spam1", "baz": ""}, {"egg": "spam2", "baz": "quux2"}]
+            yield self.assertExported(items, ["egg", "baz"], rows, settings=settings)
 
     @defer.inlineCallbacks
     def test_export_encoding(self):
-        items = [dict({'foo': 'Test\xd6'})]
+        items = [dict({"foo": "Test\xd6"})]
 
         formats = {
-            'json': '[{"foo": "Test\\u00d6"}]'.encode('utf-8'),
-            'jsonlines': '{"foo": "Test\\u00d6"}\n'.encode('utf-8'),
-            'xml': (
+            "json": '[{"foo": "Test\\u00d6"}]'.encode("utf-8"),
+            "jsonlines": '{"foo": "Test\\u00d6"}\n'.encode("utf-8"),
+            "xml": (
                 '<?xml version="1.0" encoding="utf-8"?>\n'
-                '<items><item><foo>Test\xd6</foo></item></items>'
-            ).encode('utf-8'),
-            'csv': 'foo\r\nTest\xd6\r\n'.encode('utf-8'),
+                "<items><item><foo>Test\xd6</foo></item></items>"
+            ).encode("utf-8"),
+            "csv": "foo\r\nTest\xd6\r\n".encode("utf-8"),
         }
 
         for fmt, expected in formats.items():
             settings = {
-                'FEEDS': {
-                    self._random_temp_filename(): {'format': fmt},
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": fmt},
                 },
-                'FEED_EXPORT_INDENT': None,
+                "FEED_EXPORT_INDENT": None,
             }
             data = yield self.exported_data(items, settings)
             self.assertEqual(expected, data[fmt])
 
         formats = {
-            'json': '[{"foo": "Test\xd6"}]'.encode('latin-1'),
-            'jsonlines': '{"foo": "Test\xd6"}\n'.encode('latin-1'),
-            'xml': (
+            "json": '[{"foo": "Test\xd6"}]'.encode("latin-1"),
+            "jsonlines": '{"foo": "Test\xd6"}\n'.encode("latin-1"),
+            "xml": (
                 '<?xml version="1.0" encoding="latin-1"?>\n'
-                '<items><item><foo>Test\xd6</foo></item></items>'
-            ).encode('latin-1'),
-            'csv': 'foo\r\nTest\xd6\r\n'.encode('latin-1'),
+                "<items><item><foo>Test\xd6</foo></item></items>"
+            ).encode("latin-1"),
+            "csv": "foo\r\nTest\xd6\r\n".encode("latin-1"),
         }
 
         for fmt, expected in formats.items():
             settings = {
-                'FEEDS': {
-                    self._random_temp_filename(): {'format': fmt},
+                "FEEDS": {
+                    self._random_temp_filename(): {"format": fmt},
                 },
-                'FEED_EXPORT_INDENT': None,
-                'FEED_EXPORT_ENCODING': 'latin-1',
+                "FEED_EXPORT_INDENT": None,
+                "FEED_EXPORT_ENCODING": "latin-1",
             }
             data = yield self.exported_data(items, settings)
             self.assertEqual(expected, data[fmt])
 
     @defer.inlineCallbacks
     def test_export_multiple_configs(self):
-        items = [dict({'foo': 'FOO', 'bar': 'BAR'})]
+        items = [dict({"foo": "FOO", "bar": "BAR"})]
 
         formats = {
-            'json': '[\n{"bar": "BAR"}\n]'.encode('utf-8'),
-            'xml': (
+            "json": '[\n{"bar": "BAR"}\n]'.encode("utf-8"),
+            "xml": (
                 '<?xml version="1.0" encoding="latin-1"?>\n'
-                '<items>\n  <item>\n    <foo>FOO</foo>\n  </item>\n</items>'
-            ).encode('latin-1'),
-            'csv': 'bar,foo\r\nBAR,FOO\r\n'.encode('utf-8'),
+                "<items>\n  <item>\n    <foo>FOO</foo>\n  </item>\n</items>"
+            ).encode("latin-1"),
+            "csv": "bar,foo\r\nBAR,FOO\r\n".encode("utf-8"),
         }
 
         settings = {
-            'FEEDS': {
+            "FEEDS": {
                 self._random_temp_filename(): {
-                    'format': 'json',
-                    'indent': 0,
-                    'fields': ['bar'],
-                    'encoding': 'utf-8',
+                    "format": "json",
+                    "indent": 0,
+                    "fields": ["bar"],
+                    "encoding": "utf-8",
                 },
                 self._random_temp_filename(): {
-                    'format': 'xml',
-                    'indent': 2,
-                    'fields': ['foo'],
-                    'encoding': 'latin-1',
+                    "format": "xml",
+                    "indent": 2,
+                    "fields": ["foo"],
+                    "encoding": "latin-1",
                 },
                 self._random_temp_filename(): {
-                    'format': 'csv',
-                    'indent': None,
-                    'fields': ['bar', 'foo'],
-                    'encoding': 'utf-8',
+                    "format": "csv",
+                    "indent": None,
+                    "fields": ["bar", "foo"],
+                    "encoding": "utf-8",
                 },
             },
         }
 
         data = yield self.exported_data(items, settings)
         for fmt, expected in formats.items():
             self.assertEqual(expected, data[fmt])
 
     @defer.inlineCallbacks
     def test_export_indentation(self):
         items = [
-            {'foo': ['bar']},
-            {'key': 'value'},
+            {"foo": ["bar"]},
+            {"key": "value"},
         ]
 
         test_cases = [
             # JSON
             {
-                'format': 'json',
-                'indent': None,
-                'expected': b'[{"foo": ["bar"]},{"key": "value"}]',
+                "format": "json",
+                "indent": None,
+                "expected": b'[{"foo": ["bar"]},{"key": "value"}]',
             },
             {
-                'format': 'json',
-                'indent': -1,
-                'expected': b"""[
+                "format": "json",
+                "indent": -1,
+                "expected": b"""[
 {"foo": ["bar"]},
 {"key": "value"}
 ]""",
             },
             {
-                'format': 'json',
-                'indent': 0,
-                'expected': b"""[
+                "format": "json",
+                "indent": 0,
+                "expected": b"""[
 {"foo": ["bar"]},
 {"key": "value"}
 ]""",
             },
             {
-                'format': 'json',
-                'indent': 2,
-                'expected': b"""[
+                "format": "json",
+                "indent": 2,
+                "expected": b"""[
 {
   "foo": [
     "bar"
   ]
 },
 {
   "key": "value"
 }
 ]""",
             },
             {
-                'format': 'json',
-                'indent': 4,
-                'expected': b"""[
+                "format": "json",
+                "indent": 4,
+                "expected": b"""[
 {
     "foo": [
         "bar"
     ]
 },
 {
     "key": "value"
 }
 ]""",
             },
             {
-                'format': 'json',
-                'indent': 5,
-                'expected': b"""[
+                "format": "json",
+                "indent": 5,
+                "expected": b"""[
 {
      "foo": [
           "bar"
      ]
 },
 {
      "key": "value"
 }
 ]""",
             },
-
             # XML
             {
-                'format': 'xml',
-                'indent': None,
-                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
+                "format": "xml",
+                "indent": None,
+                "expected": b"""<?xml version="1.0" encoding="utf-8"?>
 <items><item><foo><value>bar</value></foo></item><item><key>value</key></item></items>""",
             },
             {
-                'format': 'xml',
-                'indent': -1,
-                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
+                "format": "xml",
+                "indent": -1,
+                "expected": b"""<?xml version="1.0" encoding="utf-8"?>
 <items>
 <item><foo><value>bar</value></foo></item>
 <item><key>value</key></item>
 </items>""",
             },
             {
-                'format': 'xml',
-                'indent': 0,
-                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
+                "format": "xml",
+                "indent": 0,
+                "expected": b"""<?xml version="1.0" encoding="utf-8"?>
 <items>
 <item><foo><value>bar</value></foo></item>
 <item><key>value</key></item>
 </items>""",
             },
             {
-                'format': 'xml',
-                'indent': 2,
-                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
+                "format": "xml",
+                "indent": 2,
+                "expected": b"""<?xml version="1.0" encoding="utf-8"?>
 <items>
   <item>
     <foo>
       <value>bar</value>
     </foo>
   </item>
   <item>
     <key>value</key>
   </item>
 </items>""",
             },
             {
-                'format': 'xml',
-                'indent': 4,
-                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
+                "format": "xml",
+                "indent": 4,
+                "expected": b"""<?xml version="1.0" encoding="utf-8"?>
 <items>
     <item>
         <foo>
             <value>bar</value>
         </foo>
     </item>
     <item>
         <key>value</key>
     </item>
 </items>""",
             },
             {
-                'format': 'xml',
-                'indent': 5,
-                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
+                "format": "xml",
+                "indent": 5,
+                "expected": b"""<?xml version="1.0" encoding="utf-8"?>
 <items>
      <item>
           <foo>
                <value>bar</value>
           </foo>
      </item>
      <item>
@@ -1399,349 +1524,366 @@
      </item>
 </items>""",
             },
         ]
 
         for row in test_cases:
             settings = {
-                'FEEDS': {
+                "FEEDS": {
                     self._random_temp_filename(): {
-                        'format': row['format'],
-                        'indent': row['indent'],
+                        "format": row["format"],
+                        "indent": row["indent"],
                     },
                 },
             }
             data = yield self.exported_data(items, settings)
-            self.assertEqual(row['expected'], data[row['format']])
+            self.assertEqual(row["expected"], data[row["format"]])
 
     @defer.inlineCallbacks
     def test_init_exporters_storages_with_crawler(self):
         settings = {
-            'FEED_EXPORTERS': {'csv': FromCrawlerCsvItemExporter},
-            'FEED_STORAGES': {'file': FromCrawlerFileFeedStorage},
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'csv'},
+            "FEED_EXPORTERS": {"csv": FromCrawlerCsvItemExporter},
+            "FEED_STORAGES": {"file": FromCrawlerFileFeedStorage},
+            "FEEDS": {
+                self._random_temp_filename(): {"format": "csv"},
             },
         }
         yield self.exported_data(items=[], settings=settings)
         self.assertTrue(FromCrawlerCsvItemExporter.init_with_crawler)
         self.assertTrue(FromCrawlerFileFeedStorage.init_with_crawler)
 
     @defer.inlineCallbacks
-    def test_pathlib_uri(self):
-        feed_path = Path(self._random_temp_filename())
+    def test_str_uri(self):
         settings = {
-            'FEED_STORE_EMPTY': True,
-            'FEEDS': {
-                feed_path: {'format': 'csv'}
-            },
+            "FEED_STORE_EMPTY": True,
+            "FEEDS": {str(self._random_temp_filename()): {"format": "csv"}},
         }
         data = yield self.exported_no_data(settings)
-        self.assertEqual(data['csv'], b'')
+        self.assertEqual(data["csv"], b"")
 
     @defer.inlineCallbacks
     def test_multiple_feeds_success_logs_blocking_feed_storage(self):
         settings = {
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'json'},
-                self._random_temp_filename(): {'format': 'xml'},
-                self._random_temp_filename(): {'format': 'csv'},
+            "FEEDS": {
+                self._random_temp_filename(): {"format": "json"},
+                self._random_temp_filename(): {"format": "xml"},
+                self._random_temp_filename(): {"format": "csv"},
             },
-            'FEED_STORAGES': {'file': DummyBlockingFeedStorage},
+            "FEED_STORAGES": {"file": DummyBlockingFeedStorage},
         }
         items = [
-            {'foo': 'bar1', 'baz': ''},
-            {'foo': 'bar2', 'baz': 'quux'},
+            {"foo": "bar1", "baz": ""},
+            {"foo": "bar2", "baz": "quux"},
         ]
         with LogCapture() as log:
             yield self.exported_data(items, settings)
 
         print(log)
-        for fmt in ['json', 'xml', 'csv']:
-            self.assertIn(f'Stored {fmt} feed (2 items)', str(log))
+        for fmt in ["json", "xml", "csv"]:
+            self.assertIn(f"Stored {fmt} feed (2 items)", str(log))
 
     @defer.inlineCallbacks
     def test_multiple_feeds_failing_logs_blocking_feed_storage(self):
         settings = {
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'json'},
-                self._random_temp_filename(): {'format': 'xml'},
-                self._random_temp_filename(): {'format': 'csv'},
+            "FEEDS": {
+                self._random_temp_filename(): {"format": "json"},
+                self._random_temp_filename(): {"format": "xml"},
+                self._random_temp_filename(): {"format": "csv"},
             },
-            'FEED_STORAGES': {'file': FailingBlockingFeedStorage},
+            "FEED_STORAGES": {"file": FailingBlockingFeedStorage},
         }
         items = [
-            {'foo': 'bar1', 'baz': ''},
-            {'foo': 'bar2', 'baz': 'quux'},
+            {"foo": "bar1", "baz": ""},
+            {"foo": "bar2", "baz": "quux"},
         ]
         with LogCapture() as log:
             yield self.exported_data(items, settings)
 
         print(log)
-        for fmt in ['json', 'xml', 'csv']:
-            self.assertIn(f'Error storing {fmt} feed (2 items)', str(log))
+        for fmt in ["json", "xml", "csv"]:
+            self.assertIn(f"Error storing {fmt} feed (2 items)", str(log))
 
     @defer.inlineCallbacks
     def test_extend_kwargs(self):
-        items = [{'foo': 'FOO', 'bar': 'BAR'}]
+        items = [{"foo": "FOO", "bar": "BAR"}]
 
-        expected_with_title_csv = 'foo,bar\r\nFOO,BAR\r\n'.encode('utf-8')
-        expected_without_title_csv = 'FOO,BAR\r\n'.encode('utf-8')
+        expected_with_title_csv = "foo,bar\r\nFOO,BAR\r\n".encode("utf-8")
+        expected_without_title_csv = "FOO,BAR\r\n".encode("utf-8")
         test_cases = [
             # with title
             {
-                'options': {
-                    'format': 'csv',
-                    'item_export_kwargs': {'include_headers_line': True},
+                "options": {
+                    "format": "csv",
+                    "item_export_kwargs": {"include_headers_line": True},
                 },
-                'expected': expected_with_title_csv,
+                "expected": expected_with_title_csv,
             },
             # without title
             {
-                'options': {
-                    'format': 'csv',
-                    'item_export_kwargs': {'include_headers_line': False},
+                "options": {
+                    "format": "csv",
+                    "item_export_kwargs": {"include_headers_line": False},
                 },
-                'expected': expected_without_title_csv,
+                "expected": expected_without_title_csv,
             },
         ]
 
         for row in test_cases:
-            feed_options = row['options']
+            feed_options = row["options"]
             settings = {
-                'FEEDS': {
+                "FEEDS": {
                     self._random_temp_filename(): feed_options,
                 },
-                'FEED_EXPORT_INDENT': None,
+                "FEED_EXPORT_INDENT": None,
             }
 
             data = yield self.exported_data(items, settings)
-            self.assertEqual(row['expected'], data[feed_options['format']])
+            self.assertEqual(row["expected"], data[feed_options["format"]])
 
 
 class FeedPostProcessedExportsTest(FeedExportTestBase):
     __test__ = True
 
-    items = [{'foo': 'bar'}]
-    expected = b'foo\r\nbar\r\n'
+    items = [{"foo": "bar"}]
+    expected = b"foo\r\nbar\r\n"
 
     class MyPlugin1:
         def __init__(self, file, feed_options):
             self.file = file
             self.feed_options = feed_options
-            self.char = self.feed_options.get('plugin1_char', b'')
+            self.char = self.feed_options.get("plugin1_char", b"")
 
         def write(self, data):
             written_count = self.file.write(data)
             written_count += self.file.write(self.char)
             return written_count
 
         def close(self):
             self.file.close()
 
-    def _named_tempfile(self, name):
-        return os.path.join(self.temp_dir, name)
+    def _named_tempfile(self, name) -> str:
+        return str(Path(self.temp_dir, name))
 
     @defer.inlineCallbacks
     def run_and_export(self, spider_cls, settings):
-        """ Run spider with specified settings; return exported data with filename. """
+        """Run spider with specified settings; return exported data with filename."""
 
-        FEEDS = settings.get('FEEDS') or {}
-        settings['FEEDS'] = {
+        FEEDS = settings.get("FEEDS") or {}
+        settings["FEEDS"] = {
             printf_escape(path_to_url(file_path)): feed_options
             for file_path, feed_options in FEEDS.items()
         }
 
         content = {}
         try:
             with MockServer() as s:
-                spider_cls.start_urls = [s.url('/')]
+                spider_cls.start_urls = [s.url("/")]
                 crawler = get_crawler(spider_cls, settings)
                 yield crawler.crawl()
 
             for file_path, feed_options in FEEDS.items():
-                if not os.path.exists(str(file_path)):
+                if not Path(file_path).exists():
                     continue
 
-                with open(str(file_path), 'rb') as f:
-                    content[str(file_path)] = f.read()
+                content[str(file_path)] = Path(file_path).read_bytes()
 
         finally:
             for file_path in FEEDS.keys():
-                if not os.path.exists(str(file_path)):
+                if not Path(file_path).exists():
                     continue
 
-                os.remove(str(file_path))
+                Path(file_path).unlink()
 
         return content
 
-    def get_gzip_compressed(self, data, compresslevel=9, mtime=0, filename=''):
+    def get_gzip_compressed(self, data, compresslevel=9, mtime=0, filename=""):
         data_stream = BytesIO()
-        gzipf = gzip.GzipFile(fileobj=data_stream, filename=filename, mtime=mtime,
-                              compresslevel=compresslevel, mode="wb")
+        gzipf = gzip.GzipFile(
+            fileobj=data_stream,
+            filename=filename,
+            mtime=mtime,
+            compresslevel=compresslevel,
+            mode="wb",
+        )
         gzipf.write(data)
         gzipf.close()
         data_stream.seek(0)
         return data_stream.read()
 
     @defer.inlineCallbacks
     def test_gzip_plugin(self):
 
-        filename = self._named_tempfile('gzip_file')
+        filename = self._named_tempfile("gzip_file")
 
         settings = {
-            'FEEDS': {
+            "FEEDS": {
                 filename: {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.GzipPlugin"],
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
         try:
             gzip.decompress(data[filename])
         except OSError:
             self.fail("Received invalid gzip data.")
 
     @defer.inlineCallbacks
     def test_gzip_plugin_compresslevel(self):
 
         filename_to_compressed = {
-            self._named_tempfile('compresslevel_0'): self.get_gzip_compressed(self.expected, compresslevel=0),
-            self._named_tempfile('compresslevel_9'): self.get_gzip_compressed(self.expected, compresslevel=9),
+            self._named_tempfile("compresslevel_0"): self.get_gzip_compressed(
+                self.expected, compresslevel=0
+            ),
+            self._named_tempfile("compresslevel_9"): self.get_gzip_compressed(
+                self.expected, compresslevel=9
+            ),
         }
 
         settings = {
-            'FEEDS': {
-                self._named_tempfile('compresslevel_0'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],
-                    'gzip_compresslevel': 0,
-                    'gzip_mtime': 0,
-                    'gzip_filename': "",
-                },
-                self._named_tempfile('compresslevel_9'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],
-                    'gzip_compresslevel': 9,
-                    'gzip_mtime': 0,
-                    'gzip_filename': "",
+            "FEEDS": {
+                self._named_tempfile("compresslevel_0"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.GzipPlugin"],
+                    "gzip_compresslevel": 0,
+                    "gzip_mtime": 0,
+                    "gzip_filename": "",
+                },
+                self._named_tempfile("compresslevel_9"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.GzipPlugin"],
+                    "gzip_compresslevel": 9,
+                    "gzip_mtime": 0,
+                    "gzip_filename": "",
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
 
         for filename, compressed in filename_to_compressed.items():
             result = gzip.decompress(data[filename])
             self.assertEqual(compressed, data[filename])
             self.assertEqual(self.expected, result)
 
     @defer.inlineCallbacks
     def test_gzip_plugin_mtime(self):
         filename_to_compressed = {
-            self._named_tempfile('mtime_123'): self.get_gzip_compressed(self.expected, mtime=123),
-            self._named_tempfile('mtime_123456789'): self.get_gzip_compressed(self.expected, mtime=123456789),
+            self._named_tempfile("mtime_123"): self.get_gzip_compressed(
+                self.expected, mtime=123
+            ),
+            self._named_tempfile("mtime_123456789"): self.get_gzip_compressed(
+                self.expected, mtime=123456789
+            ),
         }
 
         settings = {
-            'FEEDS': {
-                self._named_tempfile('mtime_123'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],
-                    'gzip_mtime': 123,
-                    'gzip_filename': "",
-                },
-                self._named_tempfile('mtime_123456789'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],
-                    'gzip_mtime': 123456789,
-                    'gzip_filename': "",
+            "FEEDS": {
+                self._named_tempfile("mtime_123"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.GzipPlugin"],
+                    "gzip_mtime": 123,
+                    "gzip_filename": "",
+                },
+                self._named_tempfile("mtime_123456789"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.GzipPlugin"],
+                    "gzip_mtime": 123456789,
+                    "gzip_filename": "",
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
 
         for filename, compressed in filename_to_compressed.items():
             result = gzip.decompress(data[filename])
             self.assertEqual(compressed, data[filename])
             self.assertEqual(self.expected, result)
 
     @defer.inlineCallbacks
     def test_gzip_plugin_filename(self):
         filename_to_compressed = {
-            self._named_tempfile('filename_FILE1'): self.get_gzip_compressed(self.expected, filename="FILE1"),
-            self._named_tempfile('filename_FILE2'): self.get_gzip_compressed(self.expected, filename="FILE2"),
+            self._named_tempfile("filename_FILE1"): self.get_gzip_compressed(
+                self.expected, filename="FILE1"
+            ),
+            self._named_tempfile("filename_FILE2"): self.get_gzip_compressed(
+                self.expected, filename="FILE2"
+            ),
         }
 
         settings = {
-            'FEEDS': {
-                self._named_tempfile('filename_FILE1'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],
-                    'gzip_mtime': 0,
-                    'gzip_filename': "FILE1",
-                },
-                self._named_tempfile('filename_FILE2'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],
-                    'gzip_mtime': 0,
-                    'gzip_filename': "FILE2",
+            "FEEDS": {
+                self._named_tempfile("filename_FILE1"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.GzipPlugin"],
+                    "gzip_mtime": 0,
+                    "gzip_filename": "FILE1",
+                },
+                self._named_tempfile("filename_FILE2"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.GzipPlugin"],
+                    "gzip_mtime": 0,
+                    "gzip_filename": "FILE2",
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
 
         for filename, compressed in filename_to_compressed.items():
             result = gzip.decompress(data[filename])
             self.assertEqual(compressed, data[filename])
             self.assertEqual(self.expected, result)
 
     @defer.inlineCallbacks
     def test_lzma_plugin(self):
 
-        filename = self._named_tempfile('lzma_file')
+        filename = self._named_tempfile("lzma_file")
 
         settings = {
-            'FEEDS': {
+            "FEEDS": {
                 filename: {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.LZMAPlugin"],
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
         try:
             lzma.decompress(data[filename])
         except lzma.LZMAError:
             self.fail("Received invalid lzma data.")
 
     @defer.inlineCallbacks
     def test_lzma_plugin_format(self):
 
         filename_to_compressed = {
-            self._named_tempfile('format_FORMAT_XZ'): lzma.compress(self.expected, format=lzma.FORMAT_XZ),
-            self._named_tempfile('format_FORMAT_ALONE'): lzma.compress(self.expected, format=lzma.FORMAT_ALONE),
+            self._named_tempfile("format_FORMAT_XZ"): lzma.compress(
+                self.expected, format=lzma.FORMAT_XZ
+            ),
+            self._named_tempfile("format_FORMAT_ALONE"): lzma.compress(
+                self.expected, format=lzma.FORMAT_ALONE
+            ),
         }
 
         settings = {
-            'FEEDS': {
-                self._named_tempfile('format_FORMAT_XZ'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],
-                    'lzma_format': lzma.FORMAT_XZ,
+            "FEEDS": {
+                self._named_tempfile("format_FORMAT_XZ"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.LZMAPlugin"],
+                    "lzma_format": lzma.FORMAT_XZ,
                 },
-                self._named_tempfile('format_FORMAT_ALONE'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],
-                    'lzma_format': lzma.FORMAT_ALONE,
+                self._named_tempfile("format_FORMAT_ALONE"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.LZMAPlugin"],
+                    "lzma_format": lzma.FORMAT_ALONE,
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
 
         for filename, compressed in filename_to_compressed.items():
@@ -1749,29 +1891,33 @@
             self.assertEqual(compressed, data[filename])
             self.assertEqual(self.expected, result)
 
     @defer.inlineCallbacks
     def test_lzma_plugin_check(self):
 
         filename_to_compressed = {
-            self._named_tempfile('check_CHECK_NONE'): lzma.compress(self.expected, check=lzma.CHECK_NONE),
-            self._named_tempfile('check_CHECK_CRC256'): lzma.compress(self.expected, check=lzma.CHECK_SHA256),
+            self._named_tempfile("check_CHECK_NONE"): lzma.compress(
+                self.expected, check=lzma.CHECK_NONE
+            ),
+            self._named_tempfile("check_CHECK_CRC256"): lzma.compress(
+                self.expected, check=lzma.CHECK_SHA256
+            ),
         }
 
         settings = {
-            'FEEDS': {
-                self._named_tempfile('check_CHECK_NONE'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],
-                    'lzma_check': lzma.CHECK_NONE,
+            "FEEDS": {
+                self._named_tempfile("check_CHECK_NONE"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.LZMAPlugin"],
+                    "lzma_check": lzma.CHECK_NONE,
                 },
-                self._named_tempfile('check_CHECK_CRC256'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],
-                    'lzma_check': lzma.CHECK_SHA256,
+                self._named_tempfile("check_CHECK_CRC256"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.LZMAPlugin"],
+                    "lzma_check": lzma.CHECK_SHA256,
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
 
         for filename, compressed in filename_to_compressed.items():
@@ -1779,29 +1925,33 @@
             self.assertEqual(compressed, data[filename])
             self.assertEqual(self.expected, result)
 
     @defer.inlineCallbacks
     def test_lzma_plugin_preset(self):
 
         filename_to_compressed = {
-            self._named_tempfile('preset_PRESET_0'): lzma.compress(self.expected, preset=0),
-            self._named_tempfile('preset_PRESET_9'): lzma.compress(self.expected, preset=9),
+            self._named_tempfile("preset_PRESET_0"): lzma.compress(
+                self.expected, preset=0
+            ),
+            self._named_tempfile("preset_PRESET_9"): lzma.compress(
+                self.expected, preset=9
+            ),
         }
 
         settings = {
-            'FEEDS': {
-                self._named_tempfile('preset_PRESET_0'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],
-                    'lzma_preset': 0,
+            "FEEDS": {
+                self._named_tempfile("preset_PRESET_0"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.LZMAPlugin"],
+                    "lzma_preset": 0,
                 },
-                self._named_tempfile('preset_PRESET_9'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],
-                    'lzma_preset': 9,
+                self._named_tempfile("preset_PRESET_9"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.LZMAPlugin"],
+                    "lzma_preset": 9,
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
 
         for filename, compressed in filename_to_compressed.items():
@@ -1811,864 +1961,936 @@
 
     @defer.inlineCallbacks
     def test_lzma_plugin_filters(self):
         if "PyPy" in sys.version:
             # https://foss.heptapod.net/pypy/pypy/-/issues/3527
             raise unittest.SkipTest("lzma filters doesn't work in PyPy")
 
-        filters = [{'id': lzma.FILTER_LZMA2}]
+        filters = [{"id": lzma.FILTER_LZMA2}]
         compressed = lzma.compress(self.expected, filters=filters)
-        filename = self._named_tempfile('filters')
+        filename = self._named_tempfile("filters")
 
         settings = {
-            'FEEDS': {
+            "FEEDS": {
                 filename: {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],
-                    'lzma_filters': filters,
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.LZMAPlugin"],
+                    "lzma_filters": filters,
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
         self.assertEqual(compressed, data[filename])
         result = lzma.decompress(data[filename])
         self.assertEqual(self.expected, result)
 
     @defer.inlineCallbacks
     def test_bz2_plugin(self):
 
-        filename = self._named_tempfile('bz2_file')
+        filename = self._named_tempfile("bz2_file")
 
         settings = {
-            'FEEDS': {
+            "FEEDS": {
                 filename: {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.Bz2Plugin'],
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.Bz2Plugin"],
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
         try:
             bz2.decompress(data[filename])
         except OSError:
             self.fail("Received invalid bz2 data.")
 
     @defer.inlineCallbacks
     def test_bz2_plugin_compresslevel(self):
 
         filename_to_compressed = {
-            self._named_tempfile('compresslevel_1'): bz2.compress(self.expected, compresslevel=1),
-            self._named_tempfile('compresslevel_9'): bz2.compress(self.expected, compresslevel=9),
+            self._named_tempfile("compresslevel_1"): bz2.compress(
+                self.expected, compresslevel=1
+            ),
+            self._named_tempfile("compresslevel_9"): bz2.compress(
+                self.expected, compresslevel=9
+            ),
         }
 
         settings = {
-            'FEEDS': {
-                self._named_tempfile('compresslevel_1'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.Bz2Plugin'],
-                    'bz2_compresslevel': 1,
+            "FEEDS": {
+                self._named_tempfile("compresslevel_1"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.Bz2Plugin"],
+                    "bz2_compresslevel": 1,
                 },
-                self._named_tempfile('compresslevel_9'): {
-                    'format': 'csv',
-                    'postprocessing': ['scrapy.extensions.postprocessing.Bz2Plugin'],
-                    'bz2_compresslevel': 9,
+                self._named_tempfile("compresslevel_9"): {
+                    "format": "csv",
+                    "postprocessing": ["scrapy.extensions.postprocessing.Bz2Plugin"],
+                    "bz2_compresslevel": 9,
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
 
         for filename, compressed in filename_to_compressed.items():
             result = bz2.decompress(data[filename])
             self.assertEqual(compressed, data[filename])
             self.assertEqual(self.expected, result)
 
     @defer.inlineCallbacks
     def test_custom_plugin(self):
-        filename = self._named_tempfile('csv_file')
+        filename = self._named_tempfile("csv_file")
 
         settings = {
-            'FEEDS': {
+            "FEEDS": {
                 filename: {
-                    'format': 'csv',
-                    'postprocessing': [self.MyPlugin1],
+                    "format": "csv",
+                    "postprocessing": [self.MyPlugin1],
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
         self.assertEqual(self.expected, data[filename])
 
     @defer.inlineCallbacks
     def test_custom_plugin_with_parameter(self):
 
-        expected = b'foo\r\n\nbar\r\n\n'
-        filename = self._named_tempfile('newline')
+        expected = b"foo\r\n\nbar\r\n\n"
+        filename = self._named_tempfile("newline")
 
         settings = {
-            'FEEDS': {
+            "FEEDS": {
                 filename: {
-                    'format': 'csv',
-                    'postprocessing': [self.MyPlugin1],
-                    'plugin1_char': b'\n'
+                    "format": "csv",
+                    "postprocessing": [self.MyPlugin1],
+                    "plugin1_char": b"\n",
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
         self.assertEqual(expected, data[filename])
 
     @defer.inlineCallbacks
     def test_custom_plugin_with_compression(self):
 
-        expected = b'foo\r\n\nbar\r\n\n'
+        expected = b"foo\r\n\nbar\r\n\n"
 
         filename_to_decompressor = {
-            self._named_tempfile('bz2'): bz2.decompress,
-            self._named_tempfile('lzma'): lzma.decompress,
-            self._named_tempfile('gzip'): gzip.decompress,
+            self._named_tempfile("bz2"): bz2.decompress,
+            self._named_tempfile("lzma"): lzma.decompress,
+            self._named_tempfile("gzip"): gzip.decompress,
         }
 
         settings = {
-            'FEEDS': {
-                self._named_tempfile('bz2'): {
-                    'format': 'csv',
-                    'postprocessing': [self.MyPlugin1, 'scrapy.extensions.postprocessing.Bz2Plugin'],
-                    'plugin1_char': b'\n',
-                },
-                self._named_tempfile('lzma'): {
-                    'format': 'csv',
-                    'postprocessing': [self.MyPlugin1, 'scrapy.extensions.postprocessing.LZMAPlugin'],
-                    'plugin1_char': b'\n',
-                },
-                self._named_tempfile('gzip'): {
-                    'format': 'csv',
-                    'postprocessing': [self.MyPlugin1, 'scrapy.extensions.postprocessing.GzipPlugin'],
-                    'plugin1_char': b'\n',
+            "FEEDS": {
+                self._named_tempfile("bz2"): {
+                    "format": "csv",
+                    "postprocessing": [
+                        self.MyPlugin1,
+                        "scrapy.extensions.postprocessing.Bz2Plugin",
+                    ],
+                    "plugin1_char": b"\n",
+                },
+                self._named_tempfile("lzma"): {
+                    "format": "csv",
+                    "postprocessing": [
+                        self.MyPlugin1,
+                        "scrapy.extensions.postprocessing.LZMAPlugin",
+                    ],
+                    "plugin1_char": b"\n",
+                },
+                self._named_tempfile("gzip"): {
+                    "format": "csv",
+                    "postprocessing": [
+                        self.MyPlugin1,
+                        "scrapy.extensions.postprocessing.GzipPlugin",
+                    ],
+                    "plugin1_char": b"\n",
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
 
         for filename, decompressor in filename_to_decompressor.items():
             result = decompressor(data[filename])
             self.assertEqual(expected, result)
 
     @defer.inlineCallbacks
     def test_exports_compatibility_with_postproc(self):
         import marshal
         import pickle
+
         filename_to_expected = {
-            self._named_tempfile('csv'): b'foo\r\nbar\r\n',
-            self._named_tempfile('json'): b'[\n{"foo": "bar"}\n]',
-            self._named_tempfile('jsonlines'): b'{"foo": "bar"}\n',
-            self._named_tempfile('xml'): b'<?xml version="1.0" encoding="utf-8"?>\n'
-                                         b'<items>\n<item><foo>bar</foo></item>\n</items>',
+            self._named_tempfile("csv"): b"foo\r\nbar\r\n",
+            self._named_tempfile("json"): b'[\n{"foo": "bar"}\n]',
+            self._named_tempfile("jsonlines"): b'{"foo": "bar"}\n',
+            self._named_tempfile("xml"): b'<?xml version="1.0" encoding="utf-8"?>\n'
+            b"<items>\n<item><foo>bar</foo></item>\n</items>",
         }
 
         settings = {
-            'FEEDS': {
-                self._named_tempfile('csv'): {
-                    'format': 'csv',
-                    'postprocessing': [self.MyPlugin1],
+            "FEEDS": {
+                self._named_tempfile("csv"): {
+                    "format": "csv",
+                    "postprocessing": [self.MyPlugin1],
                     # empty plugin to activate postprocessing.PostProcessingManager
                 },
-                self._named_tempfile('json'): {
-                    'format': 'json',
-                    'postprocessing': [self.MyPlugin1],
-                },
-                self._named_tempfile('jsonlines'): {
-                    'format': 'jsonlines',
-                    'postprocessing': [self.MyPlugin1],
-                },
-                self._named_tempfile('xml'): {
-                    'format': 'xml',
-                    'postprocessing': [self.MyPlugin1],
-                },
-                self._named_tempfile('marshal'): {
-                    'format': 'marshal',
-                    'postprocessing': [self.MyPlugin1],
-                },
-                self._named_tempfile('pickle'): {
-                    'format': 'pickle',
-                    'postprocessing': [self.MyPlugin1],
+                self._named_tempfile("json"): {
+                    "format": "json",
+                    "postprocessing": [self.MyPlugin1],
+                },
+                self._named_tempfile("jsonlines"): {
+                    "format": "jsonlines",
+                    "postprocessing": [self.MyPlugin1],
+                },
+                self._named_tempfile("xml"): {
+                    "format": "xml",
+                    "postprocessing": [self.MyPlugin1],
+                },
+                self._named_tempfile("marshal"): {
+                    "format": "marshal",
+                    "postprocessing": [self.MyPlugin1],
+                },
+                self._named_tempfile("pickle"): {
+                    "format": "pickle",
+                    "postprocessing": [self.MyPlugin1],
                 },
             },
         }
 
         data = yield self.exported_data(self.items, settings)
 
         for filename, result in data.items():
-            if 'pickle' in filename:
+            if "pickle" in filename:
                 expected, result = self.items[0], pickle.loads(result)
-            elif 'marshal' in filename:
+            elif "marshal" in filename:
                 expected, result = self.items[0], marshal.loads(result)
             else:
                 expected = filename_to_expected[filename]
             self.assertEqual(expected, result)
 
 
 class BatchDeliveriesTest(FeedExportTestBase):
     __test__ = True
-    _file_mark = '_%(batch_time)s_#%(batch_id)02d_'
+    _file_mark = "_%(batch_time)s_#%(batch_id)02d_"
 
     @defer.inlineCallbacks
     def run_and_export(self, spider_cls, settings):
-        """ Run spider with specified settings; return exported data. """
+        """Run spider with specified settings; return exported data."""
 
-        FEEDS = settings.get('FEEDS') or {}
-        settings['FEEDS'] = {
-            build_url(file_path): feed
-            for file_path, feed in FEEDS.items()
+        FEEDS = settings.get("FEEDS") or {}
+        settings["FEEDS"] = {
+            build_url(file_path): feed for file_path, feed in FEEDS.items()
         }
         content = defaultdict(list)
         try:
             with MockServer() as s:
-                spider_cls.start_urls = [s.url('/')]
+                spider_cls.start_urls = [s.url("/")]
                 crawler = get_crawler(spider_cls, settings)
                 yield crawler.crawl()
 
             for path, feed in FEEDS.items():
-                dir_name = os.path.dirname(path)
-                for file in sorted(os.listdir(dir_name)):
-                    with open(os.path.join(dir_name, file), 'rb') as f:
-                        data = f.read()
-                        content[feed['format']].append(data)
+                dir_name = Path(path).parent
+                for file in sorted(dir_name.iterdir()):
+                    content[feed["format"]].append(file.read_bytes())
         finally:
             self.tearDown()
         defer.returnValue(content)
 
     @defer.inlineCallbacks
     def assertExportedJsonLines(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                os.path.join(self._random_temp_filename(), 'jl', self._file_mark): {'format': 'jl'},
-            },
-        })
-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename()
+                    / "jl"
+                    / self._file_mark: {"format": "jl"},
+                },
+            }
+        )
+        batch_size = Settings(settings).getint("FEED_EXPORT_BATCH_ITEM_COUNT")
         rows = [{k: v for k, v in row.items() if v} for row in rows]
         data = yield self.exported_data(items, settings)
-        for batch in data['jl']:
-            got_batch = [json.loads(to_unicode(batch_item)) for batch_item in batch.splitlines()]
+        for batch in data["jl"]:
+            got_batch = [
+                json.loads(to_unicode(batch_item)) for batch_item in batch.splitlines()
+            ]
             expected_batch, rows = rows[:batch_size], rows[batch_size:]
             self.assertEqual(expected_batch, got_batch)
 
     @defer.inlineCallbacks
     def assertExportedCsv(self, items, header, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {'format': 'csv'},
-            },
-        })
-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename()
+                    / "csv"
+                    / self._file_mark: {"format": "csv"},
+                },
+            }
+        )
+        batch_size = Settings(settings).getint("FEED_EXPORT_BATCH_ITEM_COUNT")
         data = yield self.exported_data(items, settings)
-        for batch in data['csv']:
+        for batch in data["csv"]:
             got_batch = csv.DictReader(to_unicode(batch).splitlines())
             self.assertEqual(list(header), got_batch.fieldnames)
             expected_batch, rows = rows[:batch_size], rows[batch_size:]
             self.assertEqual(expected_batch, list(got_batch))
 
     @defer.inlineCallbacks
     def assertExportedXml(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},
-            },
-        })
-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename()
+                    / "xml"
+                    / self._file_mark: {"format": "xml"},
+                },
+            }
+        )
+        batch_size = Settings(settings).getint("FEED_EXPORT_BATCH_ITEM_COUNT")
         rows = [{k: v for k, v in row.items() if v} for row in rows]
         data = yield self.exported_data(items, settings)
-        for batch in data['xml']:
+        for batch in data["xml"]:
             root = lxml.etree.fromstring(batch)
-            got_batch = [{e.tag: e.text for e in it} for it in root.findall('item')]
+            got_batch = [{e.tag: e.text for e in it} for it in root.findall("item")]
             expected_batch, rows = rows[:batch_size], rows[batch_size:]
             self.assertEqual(expected_batch, got_batch)
 
     @defer.inlineCallbacks
     def assertExportedMultiple(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},
-                os.path.join(self._random_temp_filename(), 'json', self._file_mark): {'format': 'json'},
-            },
-        })
-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename()
+                    / "xml"
+                    / self._file_mark: {"format": "xml"},
+                    self._random_temp_filename()
+                    / "json"
+                    / self._file_mark: {"format": "json"},
+                },
+            }
+        )
+        batch_size = Settings(settings).getint("FEED_EXPORT_BATCH_ITEM_COUNT")
         rows = [{k: v for k, v in row.items() if v} for row in rows]
         data = yield self.exported_data(items, settings)
         # XML
         xml_rows = rows.copy()
-        for batch in data['xml']:
+        for batch in data["xml"]:
             root = lxml.etree.fromstring(batch)
-            got_batch = [{e.tag: e.text for e in it} for it in root.findall('item')]
+            got_batch = [{e.tag: e.text for e in it} for it in root.findall("item")]
             expected_batch, xml_rows = xml_rows[:batch_size], xml_rows[batch_size:]
             self.assertEqual(expected_batch, got_batch)
         # JSON
         json_rows = rows.copy()
-        for batch in data['json']:
-            got_batch = json.loads(batch.decode('utf-8'))
+        for batch in data["json"]:
+            got_batch = json.loads(batch.decode("utf-8"))
             expected_batch, json_rows = json_rows[:batch_size], json_rows[batch_size:]
             self.assertEqual(expected_batch, got_batch)
 
     @defer.inlineCallbacks
     def assertExportedPickle(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                os.path.join(self._random_temp_filename(), 'pickle', self._file_mark): {'format': 'pickle'},
-            },
-        })
-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename()
+                    / "pickle"
+                    / self._file_mark: {"format": "pickle"},
+                },
+            }
+        )
+        batch_size = Settings(settings).getint("FEED_EXPORT_BATCH_ITEM_COUNT")
         rows = [{k: v for k, v in row.items() if v} for row in rows]
         data = yield self.exported_data(items, settings)
         import pickle
-        for batch in data['pickle']:
+
+        for batch in data["pickle"]:
             got_batch = self._load_until_eof(batch, load_func=pickle.load)
             expected_batch, rows = rows[:batch_size], rows[batch_size:]
             self.assertEqual(expected_batch, got_batch)
 
     @defer.inlineCallbacks
     def assertExportedMarshal(self, items, rows, settings=None):
         settings = settings or {}
-        settings.update({
-            'FEEDS': {
-                os.path.join(self._random_temp_filename(), 'marshal', self._file_mark): {'format': 'marshal'},
-            },
-        })
-        batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')
+        settings.update(
+            {
+                "FEEDS": {
+                    self._random_temp_filename()
+                    / "marshal"
+                    / self._file_mark: {"format": "marshal"},
+                },
+            }
+        )
+        batch_size = Settings(settings).getint("FEED_EXPORT_BATCH_ITEM_COUNT")
         rows = [{k: v for k, v in row.items() if v} for row in rows]
         data = yield self.exported_data(items, settings)
         import marshal
-        for batch in data['marshal']:
+
+        for batch in data["marshal"]:
             got_batch = self._load_until_eof(batch, load_func=marshal.load)
             expected_batch, rows = rows[:batch_size], rows[batch_size:]
             self.assertEqual(expected_batch, got_batch)
 
     @defer.inlineCallbacks
     def test_export_items(self):
-        """ Test partial deliveries in all supported formats """
+        """Test partial deliveries in all supported formats"""
         items = [
-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),
-            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),
-            self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),
+            self.MyItem({"foo": "bar1", "egg": "spam1"}),
+            self.MyItem({"foo": "bar2", "egg": "spam2", "baz": "quux2"}),
+            self.MyItem({"foo": "bar3", "baz": "quux3"}),
         ]
         rows = [
-            {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},
-            {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'},
-            {'foo': 'bar3', 'baz': 'quux3', 'egg': ''}
+            {"egg": "spam1", "foo": "bar1", "baz": ""},
+            {"egg": "spam2", "foo": "bar2", "baz": "quux2"},
+            {"foo": "bar3", "baz": "quux3", "egg": ""},
         ]
-        settings = {
-            'FEED_EXPORT_BATCH_ITEM_COUNT': 2
-        }
+        settings = {"FEED_EXPORT_BATCH_ITEM_COUNT": 2}
         header = self.MyItem.fields.keys()
         yield self.assertExported(items, header, rows, settings=settings)
 
     def test_wrong_path(self):
-        """ If path is without %(batch_time)s and %(batch_id) an exception must be raised """
+        """If path is without %(batch_time)s and %(batch_id) an exception must be raised"""
         settings = {
-            'FEEDS': {
-                self._random_temp_filename(): {'format': 'xml'},
+            "FEEDS": {
+                self._random_temp_filename(): {"format": "xml"},
             },
-            'FEED_EXPORT_BATCH_ITEM_COUNT': 1
+            "FEED_EXPORT_BATCH_ITEM_COUNT": 1,
         }
         crawler = get_crawler(settings_dict=settings)
         self.assertRaises(NotConfigured, FeedExporter, crawler)
 
     @defer.inlineCallbacks
     def test_export_no_items_not_store_empty(self):
-        for fmt in ('json', 'jsonlines', 'xml', 'csv'):
+        for fmt in ("json", "jsonlines", "xml", "csv"):
             settings = {
-                'FEEDS': {
-                    os.path.join(self._random_temp_filename(), fmt, self._file_mark): {'format': fmt},
+                "FEEDS": {
+                    self._random_temp_filename()
+                    / fmt
+                    / self._file_mark: {"format": fmt},
                 },
-                'FEED_EXPORT_BATCH_ITEM_COUNT': 1
+                "FEED_EXPORT_BATCH_ITEM_COUNT": 1,
             }
             data = yield self.exported_no_data(settings)
             data = dict(data)
-            self.assertEqual(b'', data[fmt][0])
+            self.assertEqual(b"", data[fmt][0])
 
     @defer.inlineCallbacks
     def test_export_no_items_store_empty(self):
         formats = (
-            ('json', b'[]'),
-            ('jsonlines', b''),
-            ('xml', b'<?xml version="1.0" encoding="utf-8"?>\n<items></items>'),
-            ('csv', b''),
+            ("json", b"[]"),
+            ("jsonlines", b""),
+            ("xml", b'<?xml version="1.0" encoding="utf-8"?>\n<items></items>'),
+            ("csv", b""),
         )
 
         for fmt, expctd in formats:
             settings = {
-                'FEEDS': {
-                    os.path.join(self._random_temp_filename(), fmt, self._file_mark): {'format': fmt},
-                },
-                'FEED_STORE_EMPTY': True,
-                'FEED_EXPORT_INDENT': None,
-                'FEED_EXPORT_BATCH_ITEM_COUNT': 1,
+                "FEEDS": {
+                    self._random_temp_filename()
+                    / fmt
+                    / self._file_mark: {"format": fmt},
+                },
+                "FEED_STORE_EMPTY": True,
+                "FEED_EXPORT_INDENT": None,
+                "FEED_EXPORT_BATCH_ITEM_COUNT": 1,
             }
             data = yield self.exported_no_data(settings)
             data = dict(data)
             self.assertEqual(expctd, data[fmt][0])
 
     @defer.inlineCallbacks
     def test_export_multiple_configs(self):
-        items = [dict({'foo': 'FOO', 'bar': 'BAR'}), dict({'foo': 'FOO1', 'bar': 'BAR1'})]
+        items = [
+            dict({"foo": "FOO", "bar": "BAR"}),
+            dict({"foo": "FOO1", "bar": "BAR1"}),
+        ]
 
         formats = {
-            'json': ['[\n{"bar": "BAR"}\n]'.encode('utf-8'),
-                     '[\n{"bar": "BAR1"}\n]'.encode('utf-8')],
-            'xml': [
+            "json": [
+                '[\n{"bar": "BAR"}\n]'.encode("utf-8"),
+                '[\n{"bar": "BAR1"}\n]'.encode("utf-8"),
+            ],
+            "xml": [
                 (
                     '<?xml version="1.0" encoding="latin-1"?>\n'
-                    '<items>\n  <item>\n    <foo>FOO</foo>\n  </item>\n</items>'
-                ).encode('latin-1'),
+                    "<items>\n  <item>\n    <foo>FOO</foo>\n  </item>\n</items>"
+                ).encode("latin-1"),
                 (
                     '<?xml version="1.0" encoding="latin-1"?>\n'
-                    '<items>\n  <item>\n    <foo>FOO1</foo>\n  </item>\n</items>'
-                ).encode('latin-1')
+                    "<items>\n  <item>\n    <foo>FOO1</foo>\n  </item>\n</items>"
+                ).encode("latin-1"),
+            ],
+            "csv": [
+                "foo,bar\r\nFOO,BAR\r\n".encode("utf-8"),
+                "foo,bar\r\nFOO1,BAR1\r\n".encode("utf-8"),
             ],
-            'csv': ['foo,bar\r\nFOO,BAR\r\n'.encode('utf-8'),
-                    'foo,bar\r\nFOO1,BAR1\r\n'.encode('utf-8')],
         }
 
         settings = {
-            'FEEDS': {
-                os.path.join(self._random_temp_filename(), 'json', self._file_mark): {
-                    'format': 'json',
-                    'indent': 0,
-                    'fields': ['bar'],
-                    'encoding': 'utf-8',
-                },
-                os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {
-                    'format': 'xml',
-                    'indent': 2,
-                    'fields': ['foo'],
-                    'encoding': 'latin-1',
-                },
-                os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {
-                    'format': 'csv',
-                    'indent': None,
-                    'fields': ['foo', 'bar'],
-                    'encoding': 'utf-8',
+            "FEEDS": {
+                self._random_temp_filename()
+                / "json"
+                / self._file_mark: {
+                    "format": "json",
+                    "indent": 0,
+                    "fields": ["bar"],
+                    "encoding": "utf-8",
+                },
+                self._random_temp_filename()
+                / "xml"
+                / self._file_mark: {
+                    "format": "xml",
+                    "indent": 2,
+                    "fields": ["foo"],
+                    "encoding": "latin-1",
+                },
+                self._random_temp_filename()
+                / "csv"
+                / self._file_mark: {
+                    "format": "csv",
+                    "indent": None,
+                    "fields": ["foo", "bar"],
+                    "encoding": "utf-8",
                 },
             },
-            'FEED_EXPORT_BATCH_ITEM_COUNT': 1,
+            "FEED_EXPORT_BATCH_ITEM_COUNT": 1,
         }
         data = yield self.exported_data(items, settings)
         for fmt, expected in formats.items():
             for expected_batch, got_batch in zip(expected, data[fmt]):
                 self.assertEqual(expected_batch, got_batch)
 
     @defer.inlineCallbacks
     def test_batch_item_count_feeds_setting(self):
-        items = [dict({'foo': 'FOO'}), dict({'foo': 'FOO1'})]
+        items = [dict({"foo": "FOO"}), dict({"foo": "FOO1"})]
         formats = {
-            'json': ['[{"foo": "FOO"}]'.encode('utf-8'),
-                     '[{"foo": "FOO1"}]'.encode('utf-8')],
+            "json": [
+                '[{"foo": "FOO"}]'.encode("utf-8"),
+                '[{"foo": "FOO1"}]'.encode("utf-8"),
+            ],
         }
         settings = {
-            'FEEDS': {
-                os.path.join(self._random_temp_filename(), 'json', self._file_mark): {
-                    'format': 'json',
-                    'indent': None,
-                    'encoding': 'utf-8',
-                    'batch_item_count': 1,
+            "FEEDS": {
+                self._random_temp_filename()
+                / "json"
+                / self._file_mark: {
+                    "format": "json",
+                    "indent": None,
+                    "encoding": "utf-8",
+                    "batch_item_count": 1,
                 },
             },
         }
         data = yield self.exported_data(items, settings)
         for fmt, expected in formats.items():
             for expected_batch, got_batch in zip(expected, data[fmt]):
                 self.assertEqual(expected_batch, got_batch)
 
-    @pytest.mark.skipif(sys.platform == 'win32', reason='Odd behaviour on file creation/output')
+    @pytest.mark.skipif(
+        sys.platform == "win32", reason="Odd behaviour on file creation/output"
+    )
     @defer.inlineCallbacks
     def test_batch_path_differ(self):
         """
         Test that the name of all batch files differ from each other.
         So %(batch_time)s replaced with the current date.
         """
         items = [
-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),
-            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),
-            self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),
+            self.MyItem({"foo": "bar1", "egg": "spam1"}),
+            self.MyItem({"foo": "bar2", "egg": "spam2", "baz": "quux2"}),
+            self.MyItem({"foo": "bar3", "baz": "quux3"}),
         ]
         settings = {
-            'FEEDS': {
-                os.path.join(self._random_temp_filename(), '%(batch_time)s'): {
-                    'format': 'json',
+            "FEEDS": {
+                self._random_temp_filename()
+                / "%(batch_time)s": {
+                    "format": "json",
                 },
             },
-            'FEED_EXPORT_BATCH_ITEM_COUNT': 1,
+            "FEED_EXPORT_BATCH_ITEM_COUNT": 1,
         }
         data = yield self.exported_data(items, settings)
-        self.assertEqual(len(items), len([_ for _ in data['json'] if _]))
+        self.assertEqual(len(items), len([_ for _ in data["json"] if _]))
 
     @defer.inlineCallbacks
     def test_stats_batch_file_success(self):
         settings = {
             "FEEDS": {
-                build_url(os.path.join(self._random_temp_filename(), "json", self._file_mark)): {
+                build_url(
+                    str(self._random_temp_filename() / "json" / self._file_mark)
+                ): {
                     "format": "json",
                 }
             },
             "FEED_EXPORT_BATCH_ITEM_COUNT": 1,
         }
         crawler = get_crawler(ItemSpider, settings)
         with MockServer() as mockserver:
             yield crawler.crawl(total=2, mockserver=mockserver)
-        self.assertIn("feedexport/success_count/FileFeedStorage", crawler.stats.get_stats())
-        self.assertEqual(crawler.stats.get_value("feedexport/success_count/FileFeedStorage"), 12)
+        self.assertIn(
+            "feedexport/success_count/FileFeedStorage", crawler.stats.get_stats()
+        )
+        self.assertEqual(
+            crawler.stats.get_value("feedexport/success_count/FileFeedStorage"), 12
+        )
 
     @defer.inlineCallbacks
     def test_s3_export(self):
         skip_if_no_boto()
 
-        bucket = 'mybucket'
+        bucket = "mybucket"
         items = [
-            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),
-            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),
-            self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),
+            self.MyItem({"foo": "bar1", "egg": "spam1"}),
+            self.MyItem({"foo": "bar2", "egg": "spam2", "baz": "quux2"}),
+            self.MyItem({"foo": "bar3", "baz": "quux3"}),
         ]
 
         class CustomS3FeedStorage(S3FeedStorage):
 
             stubs = []
 
             def open(self, *args, **kwargs):
                 from botocore.stub import ANY, Stubber
+
                 stub = Stubber(self.s3_client)
                 stub.activate()
                 CustomS3FeedStorage.stubs.append(stub)
                 stub.add_response(
-                    'put_object',
+                    "put_object",
                     expected_params={
-                        'Body': ANY,
-                        'Bucket': bucket,
-                        'Key': ANY,
+                        "Body": ANY,
+                        "Bucket": bucket,
+                        "Key": ANY,
                     },
                     service_response={},
                 )
                 return super().open(*args, **kwargs)
 
-        key = 'export.csv'
-        uri = f's3://{bucket}/{key}/%(batch_time)s.json'
+        key = "export.csv"
+        uri = f"s3://{bucket}/{key}/%(batch_time)s.json"
         batch_item_count = 1
         settings = {
-            'AWS_ACCESS_KEY_ID': 'access_key',
-            'AWS_SECRET_ACCESS_KEY': 'secret_key',
-            'FEED_EXPORT_BATCH_ITEM_COUNT': batch_item_count,
-            'FEED_STORAGES': {
-                's3': CustomS3FeedStorage,
+            "AWS_ACCESS_KEY_ID": "access_key",
+            "AWS_SECRET_ACCESS_KEY": "secret_key",
+            "FEED_EXPORT_BATCH_ITEM_COUNT": batch_item_count,
+            "FEED_STORAGES": {
+                "s3": CustomS3FeedStorage,
             },
-            'FEEDS': {
+            "FEEDS": {
                 uri: {
-                    'format': 'json',
+                    "format": "json",
                 },
             },
         }
         crawler = get_crawler(settings_dict=settings)
         storage = S3FeedStorage.from_crawler(crawler, uri)
         verifyObject(IFeedStorage, storage)
 
         class TestSpider(scrapy.Spider):
-            name = 'testspider'
+            name = "testspider"
 
             def parse(self, response):
                 for item in items:
                     yield item
 
         with MockServer() as server:
-            TestSpider.start_urls = [server.url('/')]
+            TestSpider.start_urls = [server.url("/")]
             crawler = get_crawler(TestSpider, settings)
             yield crawler.crawl()
 
         self.assertEqual(len(CustomS3FeedStorage.stubs), len(items) + 1)
         for stub in CustomS3FeedStorage.stubs[:-1]:
             stub.assert_no_pending_responses()
 
 
 class FeedExportInitTest(unittest.TestCase):
-
     def test_unsupported_storage(self):
         settings = {
-            'FEEDS': {
-                'unsupported://uri': {},
+            "FEEDS": {
+                "unsupported://uri": {},
             },
         }
         crawler = get_crawler(settings_dict=settings)
         with self.assertRaises(NotConfigured):
             FeedExporter.from_crawler(crawler)
 
     def test_unsupported_format(self):
         settings = {
-            'FEEDS': {
-                'file://path': {
-                    'format': 'unsupported_format',
+            "FEEDS": {
+                "file://path": {
+                    "format": "unsupported_format",
                 },
             },
         }
         crawler = get_crawler(settings_dict=settings)
         with self.assertRaises(NotConfigured):
             FeedExporter.from_crawler(crawler)
 
 
 class StdoutFeedStorageWithoutFeedOptions(StdoutFeedStorage):
-
     def __init__(self, uri):
         super().__init__(uri)
 
 
 class StdoutFeedStoragePreFeedOptionsTest(unittest.TestCase):
     """Make sure that any feed exporter created by users before the
     introduction of the ``feed_options`` parameter continues to work as
     expected, and simply issues a warning."""
 
     def test_init(self):
         settings_dict = {
-            'FEED_URI': 'file:///tmp/foobar',
-            'FEED_STORAGES': {
-                'file': StdoutFeedStorageWithoutFeedOptions
-            },
+            "FEED_URI": "file:///tmp/foobar",
+            "FEED_STORAGES": {"file": StdoutFeedStorageWithoutFeedOptions},
         }
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated",
+        ):
             crawler = get_crawler(settings_dict=settings_dict)
             feed_exporter = FeedExporter.from_crawler(crawler)
 
         spider = scrapy.Spider("default")
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="StdoutFeedStorageWithoutFeedOptions does not support "
-                                "the 'feed_options' keyword argument."):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="StdoutFeedStorageWithoutFeedOptions does not support "
+            "the 'feed_options' keyword argument.",
+        ):
             feed_exporter.open_spider(spider)
 
 
 class FileFeedStorageWithoutFeedOptions(FileFeedStorage):
-
     def __init__(self, uri):
         super().__init__(uri)
 
 
 class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):
     """Make sure that any feed exporter created by users before the
     introduction of the ``feed_options`` parameter continues to work as
     expected, and simply issues a warning."""
 
     maxDiff = None
 
     def test_init(self):
         with tempfile.NamedTemporaryFile() as temp:
             settings_dict = {
-                'FEED_URI': f'file:///{temp.name}',
-                'FEED_STORAGES': {
-                    'file': FileFeedStorageWithoutFeedOptions
-                },
+                "FEED_URI": f"file:///{temp.name}",
+                "FEED_STORAGES": {"file": FileFeedStorageWithoutFeedOptions},
             }
-            with pytest.warns(ScrapyDeprecationWarning,
-                              match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated"):
+            with pytest.warns(
+                ScrapyDeprecationWarning,
+                match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated",
+            ):
                 crawler = get_crawler(settings_dict=settings_dict)
                 feed_exporter = FeedExporter.from_crawler(crawler)
         spider = scrapy.Spider("default")
 
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="FileFeedStorageWithoutFeedOptions does not support "
-                                "the 'feed_options' keyword argument."):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="FileFeedStorageWithoutFeedOptions does not support "
+            "the 'feed_options' keyword argument.",
+        ):
             feed_exporter.open_spider(spider)
 
 
 class S3FeedStorageWithoutFeedOptions(S3FeedStorage):
-
     def __init__(self, uri, access_key, secret_key, acl, endpoint_url, **kwargs):
         super().__init__(uri, access_key, secret_key, acl, endpoint_url, **kwargs)
 
 
 class S3FeedStorageWithoutFeedOptionsWithFromCrawler(S3FeedStorage):
-
     @classmethod
     def from_crawler(cls, crawler, uri):
         return super().from_crawler(crawler, uri)
 
 
 class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):
     """Make sure that any feed exporter created by users before the
     introduction of the ``feed_options`` parameter continues to work as
     expected, and simply issues a warning."""
 
     maxDiff = None
 
     def test_init(self):
         settings_dict = {
-            'FEED_URI': 'file:///tmp/foobar',
-            'FEED_STORAGES': {
-                'file': S3FeedStorageWithoutFeedOptions
-            },
+            "FEED_URI": "file:///tmp/foobar",
+            "FEED_STORAGES": {"file": S3FeedStorageWithoutFeedOptions},
         }
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated",
+        ):
             crawler = get_crawler(settings_dict=settings_dict)
             feed_exporter = FeedExporter.from_crawler(crawler)
 
         spider = scrapy.Spider("default")
         spider.crawler = crawler
 
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="S3FeedStorageWithoutFeedOptions does not support "
-                                "the 'feed_options' keyword argument."):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="S3FeedStorageWithoutFeedOptions does not support "
+            "the 'feed_options' keyword argument.",
+        ):
             feed_exporter.open_spider(spider)
 
     def test_from_crawler(self):
         settings_dict = {
-            'FEED_URI': 'file:///tmp/foobar',
-            'FEED_STORAGES': {
-                'file': S3FeedStorageWithoutFeedOptionsWithFromCrawler
-            },
+            "FEED_URI": "file:///tmp/foobar",
+            "FEED_STORAGES": {"file": S3FeedStorageWithoutFeedOptionsWithFromCrawler},
         }
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated",
+        ):
             crawler = get_crawler(settings_dict=settings_dict)
             feed_exporter = FeedExporter.from_crawler(crawler)
 
         spider = scrapy.Spider("default")
         spider.crawler = crawler
 
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="S3FeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler does not support "
-                                "the 'feed_options' keyword argument."):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="S3FeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler does not support "
+            "the 'feed_options' keyword argument.",
+        ):
             feed_exporter.open_spider(spider)
 
 
 class FTPFeedStorageWithoutFeedOptions(FTPFeedStorage):
-
     def __init__(self, uri, use_active_mode=False):
         super().__init__(uri)
 
 
 class FTPFeedStorageWithoutFeedOptionsWithFromCrawler(FTPFeedStorage):
-
     @classmethod
     def from_crawler(cls, crawler, uri):
         return super().from_crawler(crawler, uri)
 
 
 class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):
     """Make sure that any feed exporter created by users before the
     introduction of the ``feed_options`` parameter continues to work as
     expected, and simply issues a warning."""
 
     maxDiff = None
 
     def test_init(self):
         settings_dict = {
-            'FEED_URI': 'file:///tmp/foobar',
-            'FEED_STORAGES': {
-                'file': FTPFeedStorageWithoutFeedOptions
-            },
+            "FEED_URI": "file:///tmp/foobar",
+            "FEED_STORAGES": {"file": FTPFeedStorageWithoutFeedOptions},
         }
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated",
+        ):
             crawler = get_crawler(settings_dict=settings_dict)
             feed_exporter = FeedExporter.from_crawler(crawler)
 
         spider = scrapy.Spider("default")
         spider.crawler = crawler
 
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="FTPFeedStorageWithoutFeedOptions does not support "
-                                "the 'feed_options' keyword argument."):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="FTPFeedStorageWithoutFeedOptions does not support "
+            "the 'feed_options' keyword argument.",
+        ):
             feed_exporter.open_spider(spider)
 
     def test_from_crawler(self):
         settings_dict = {
-            'FEED_URI': 'file:///tmp/foobar',
-            'FEED_STORAGES': {
-                'file': FTPFeedStorageWithoutFeedOptionsWithFromCrawler
-            },
+            "FEED_URI": "file:///tmp/foobar",
+            "FEED_STORAGES": {"file": FTPFeedStorageWithoutFeedOptionsWithFromCrawler},
         }
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated"):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated",
+        ):
             crawler = get_crawler(settings_dict=settings_dict)
             feed_exporter = FeedExporter.from_crawler(crawler)
 
         spider = scrapy.Spider("default")
         spider.crawler = crawler
 
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="FTPFeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler does not support "
-                                "the 'feed_options' keyword argument."):
+        with pytest.warns(
+            ScrapyDeprecationWarning,
+            match="FTPFeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler does not support "
+            "the 'feed_options' keyword argument.",
+        ):
             feed_exporter.open_spider(spider)
 
 
 class URIParamsTest:
 
     spider_name = "uri_params_spider"
     deprecated_options = False
 
-    def build_settings(self, uri='file:///tmp/foobar', uri_params=None):
+    def build_settings(self, uri="file:///tmp/foobar", uri_params=None):
         raise NotImplementedError
 
     def _crawler_feed_exporter(self, settings):
         if self.deprecated_options:
-            with pytest.warns(ScrapyDeprecationWarning,
-                              match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated"):
+            with pytest.warns(
+                ScrapyDeprecationWarning,
+                match="The `FEED_URI` and `FEED_FORMAT` settings have been deprecated",
+            ):
                 crawler = get_crawler(settings_dict=settings)
                 feed_exporter = FeedExporter.from_crawler(crawler)
         else:
             crawler = get_crawler(settings_dict=settings)
             feed_exporter = FeedExporter.from_crawler(crawler)
         return crawler, feed_exporter
 
     def test_default(self):
         settings = self.build_settings(
-            uri='file:///tmp/%(name)s',
+            uri="file:///tmp/%(name)s",
         )
         crawler, feed_exporter = self._crawler_feed_exporter(settings)
         spider = scrapy.Spider(self.spider_name)
         spider.crawler = crawler
 
         with warnings.catch_warnings():
             warnings.simplefilter("error", ScrapyDeprecationWarning)
             feed_exporter.open_spider(spider)
 
-        self.assertEqual(
-            feed_exporter.slots[0].uri,
-            f'file:///tmp/{self.spider_name}'
-        )
+        self.assertEqual(feed_exporter.slots[0].uri, f"file:///tmp/{self.spider_name}")
 
     def test_none(self):
         def uri_params(params, spider):
             pass
 
         settings = self.build_settings(
-            uri='file:///tmp/%(name)s',
+            uri="file:///tmp/%(name)s",
             uri_params=uri_params,
         )
         crawler, feed_exporter = self._crawler_feed_exporter(settings)
         spider = scrapy.Spider(self.spider_name)
         spider.crawler = crawler
 
-        with pytest.warns(ScrapyDeprecationWarning,
-                          match="Modifying the params dictionary in-place"):
+        with pytest.warns(
+            ScrapyDeprecationWarning, match="Modifying the params dictionary in-place"
+        ):
             feed_exporter.open_spider(spider)
 
-        self.assertEqual(
-            feed_exporter.slots[0].uri,
-            f'file:///tmp/{self.spider_name}'
-        )
+        self.assertEqual(feed_exporter.slots[0].uri, f"file:///tmp/{self.spider_name}")
 
     def test_empty_dict(self):
         def uri_params(params, spider):
             return {}
 
         settings = self.build_settings(
-            uri='file:///tmp/%(name)s',
+            uri="file:///tmp/%(name)s",
             uri_params=uri_params,
         )
         crawler, feed_exporter = self._crawler_feed_exporter(settings)
         spider = scrapy.Spider(self.spider_name)
         spider.crawler = crawler
 
         with warnings.catch_warnings():
@@ -2677,70 +2899,64 @@
                 feed_exporter.open_spider(spider)
 
     def test_params_as_is(self):
         def uri_params(params, spider):
             return params
 
         settings = self.build_settings(
-            uri='file:///tmp/%(name)s',
+            uri="file:///tmp/%(name)s",
             uri_params=uri_params,
         )
         crawler, feed_exporter = self._crawler_feed_exporter(settings)
         spider = scrapy.Spider(self.spider_name)
         spider.crawler = crawler
         with warnings.catch_warnings():
             warnings.simplefilter("error", ScrapyDeprecationWarning)
             feed_exporter.open_spider(spider)
 
-        self.assertEqual(
-            feed_exporter.slots[0].uri,
-            f'file:///tmp/{self.spider_name}'
-        )
+        self.assertEqual(feed_exporter.slots[0].uri, f"file:///tmp/{self.spider_name}")
 
     def test_custom_param(self):
         def uri_params(params, spider):
-            return {**params, 'foo': self.spider_name}
+            return {**params, "foo": self.spider_name}
 
         settings = self.build_settings(
-            uri='file:///tmp/%(foo)s',
+            uri="file:///tmp/%(foo)s",
             uri_params=uri_params,
         )
         crawler, feed_exporter = self._crawler_feed_exporter(settings)
         spider = scrapy.Spider(self.spider_name)
         spider.crawler = crawler
         with warnings.catch_warnings():
             warnings.simplefilter("error", ScrapyDeprecationWarning)
             feed_exporter.open_spider(spider)
 
-        self.assertEqual(
-            feed_exporter.slots[0].uri,
-            f'file:///tmp/{self.spider_name}'
-        )
+        self.assertEqual(feed_exporter.slots[0].uri, f"file:///tmp/{self.spider_name}")
 
 
 class URIParamsSettingTest(URIParamsTest, unittest.TestCase):
     deprecated_options = True
 
-    def build_settings(self, uri='file:///tmp/foobar', uri_params=None):
+    def build_settings(self, uri="file:///tmp/foobar", uri_params=None):
         extra_settings = {}
         if uri_params:
-            extra_settings['FEED_URI_PARAMS'] = uri_params
+            extra_settings["FEED_URI_PARAMS"] = uri_params
         return {
-            'FEED_URI': uri,
+            "FEED_URI": uri,
             **extra_settings,
         }
 
 
 class URIParamsFeedOptionTest(URIParamsTest, unittest.TestCase):
     deprecated_options = False
 
-    def build_settings(self, uri='file:///tmp/foobar', uri_params=None):
+    def build_settings(self, uri="file:///tmp/foobar", uri_params=None):
         options = {
-            'format': 'jl',
+            "format": "jl",
         }
         if uri_params:
-            options['uri_params'] = uri_params
+            options["uri_params"] = uri_params
         return {
-            'FEEDS': {
+            "FEEDS": {
                 uri: options,
             },
         }
```

### Comparing `Scrapy-2.7.1/tests/test_http2_client_protocol.py` & `Scrapy-2.8.0/tests/test_http2_client_protocol.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,133 +1,135 @@
 import json
-import os
 import random
 import re
 import shutil
 import string
 from ipaddress import IPv4Address
+from pathlib import Path
 from unittest import mock, skipIf
 from urllib.parse import urlencode
 
 from twisted.internet import reactor
-from twisted.internet.defer import CancelledError, Deferred, DeferredList, inlineCallbacks
+from twisted.internet.defer import (
+    CancelledError,
+    Deferred,
+    DeferredList,
+    inlineCallbacks,
+)
 from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint
 from twisted.internet.error import TimeoutError
-from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate, Certificate
+from twisted.internet.ssl import Certificate, PrivateCertificate, optionsForClientTLS
 from twisted.python.failure import Failure
 from twisted.trial.unittest import TestCase
-from twisted.web.client import ResponseFailed, URI
-from twisted.web.http import H2_ENABLED, Request as TxRequest
-from twisted.web.server import Site, NOT_DONE_YET
+from twisted.web.client import URI, ResponseFailed
+from twisted.web.http import H2_ENABLED
+from twisted.web.http import Request as TxRequest
+from twisted.web.server import NOT_DONE_YET, Site
 from twisted.web.static import File
 
-from scrapy.http import Request, Response, JsonRequest
+from scrapy.http import JsonRequest, Request, Response
 from scrapy.settings import Settings
 from scrapy.spiders import Spider
-from tests.mockserver import ssl_context_factory, LeafResource, Status
+from tests.mockserver import LeafResource, Status, ssl_context_factory
 
 
 def generate_random_string(size):
-    return ''.join(random.choices(
-        string.ascii_uppercase + string.digits,
-        k=size
-    ))
+    return "".join(random.choices(string.ascii_uppercase + string.digits, k=size))
 
 
 def make_html_body(val):
-    response = f'''<html>
+    response = f"""<html>
 <h1>Hello from HTTP2<h1>
 <p>{val}</p>
-</html>'''
-    return bytes(response, 'utf-8')
+</html>"""
+    return bytes(response, "utf-8")
 
 
 class DummySpider(Spider):
-    name = 'dummy'
+    name = "dummy"
     start_urls: list = []
 
     def parse(self, response):
         print(response)
 
 
 class Data:
     SMALL_SIZE = 1024  # 1 KB
-    LARGE_SIZE = 1024 ** 2  # 1 MB
+    LARGE_SIZE = 1024**2  # 1 MB
 
     STR_SMALL = generate_random_string(SMALL_SIZE)
     STR_LARGE = generate_random_string(LARGE_SIZE)
 
     EXTRA_SMALL = generate_random_string(1024 * 15)
-    EXTRA_LARGE = generate_random_string((1024 ** 2) * 15)
+    EXTRA_LARGE = generate_random_string((1024**2) * 15)
 
     HTML_SMALL = make_html_body(STR_SMALL)
     HTML_LARGE = make_html_body(STR_LARGE)
 
-    JSON_SMALL = {'data': STR_SMALL}
-    JSON_LARGE = {'data': STR_LARGE}
+    JSON_SMALL = {"data": STR_SMALL}
+    JSON_LARGE = {"data": STR_LARGE}
 
-    DATALOSS = b'Dataloss Content'
-    NO_CONTENT_LENGTH = b'This response do not have any content-length header'
+    DATALOSS = b"Dataloss Content"
+    NO_CONTENT_LENGTH = b"This response do not have any content-length header"
 
 
 class GetDataHtmlSmall(LeafResource):
     def render_GET(self, request: TxRequest):
-        request.setHeader('Content-Type', 'text/html; charset=UTF-8')
+        request.setHeader("Content-Type", "text/html; charset=UTF-8")
         return Data.HTML_SMALL
 
 
 class GetDataHtmlLarge(LeafResource):
     def render_GET(self, request: TxRequest):
-        request.setHeader('Content-Type', 'text/html; charset=UTF-8')
+        request.setHeader("Content-Type", "text/html; charset=UTF-8")
         return Data.HTML_LARGE
 
 
 class PostDataJsonMixin:
     @staticmethod
     def make_response(request: TxRequest, extra_data: str):
         response = {
-            'request-headers': {},
-            'request-body': json.loads(request.content.read()),
-            'extra-data': extra_data
+            "request-headers": {},
+            "request-body": json.loads(request.content.read()),
+            "extra-data": extra_data,
         }
         for k, v in request.requestHeaders.getAllRawHeaders():
-            response['request-headers'][str(k, 'utf-8')] = str(v[0], 'utf-8')
+            response["request-headers"][str(k, "utf-8")] = str(v[0], "utf-8")
 
-        response_bytes = bytes(json.dumps(response), 'utf-8')
-        request.setHeader('Content-Type', 'application/json; charset=UTF-8')
-        request.setHeader('Content-Encoding', 'UTF-8')
+        response_bytes = bytes(json.dumps(response), "utf-8")
+        request.setHeader("Content-Type", "application/json; charset=UTF-8")
+        request.setHeader("Content-Encoding", "UTF-8")
         return response_bytes
 
 
 class PostDataJsonSmall(LeafResource, PostDataJsonMixin):
     def render_POST(self, request: TxRequest):
         return self.make_response(request, Data.EXTRA_SMALL)
 
 
 class PostDataJsonLarge(LeafResource, PostDataJsonMixin):
     def render_POST(self, request: TxRequest):
         return self.make_response(request, Data.EXTRA_LARGE)
 
 
 class Dataloss(LeafResource):
-
     def render_GET(self, request: TxRequest):
         request.setHeader(b"Content-Length", b"1024")
         self.deferRequest(request, 0, self._delayed_render, request)
         return NOT_DONE_YET
 
     @staticmethod
     def _delayed_render(request: TxRequest):
         request.write(Data.DATALOSS)
         request.finish()
 
 
 class NoContentLengthHeader(LeafResource):
     def render_GET(self, request: TxRequest):
-        request.requestHeaders.removeHeader('Content-Length')
+        request.requestHeaders.removeHeader("Content-Length")
         self.deferRequest(request, 0, self._delayed_render, request)
         return NOT_DONE_YET
 
     @staticmethod
     def _delayed_render(request: TxRequest):
         request.write(Data.NO_CONTENT_LENGTH)
         request.finish()
@@ -136,95 +138,107 @@
 class TimeoutResponse(LeafResource):
     def render_GET(self, request: TxRequest):
         return NOT_DONE_YET
 
 
 class QueryParams(LeafResource):
     def render_GET(self, request: TxRequest):
-        request.setHeader('Content-Type', 'application/json; charset=UTF-8')
-        request.setHeader('Content-Encoding', 'UTF-8')
+        request.setHeader("Content-Type", "application/json; charset=UTF-8")
+        request.setHeader("Content-Encoding", "UTF-8")
 
         query_params = {}
         for k, v in request.args.items():
-            query_params[str(k, 'utf-8')] = str(v[0], 'utf-8')
+            query_params[str(k, "utf-8")] = str(v[0], "utf-8")
 
-        return bytes(json.dumps(query_params), 'utf-8')
+        return bytes(json.dumps(query_params), "utf-8")
 
 
 class RequestHeaders(LeafResource):
     """Sends all the headers received as a response"""
 
     def render_GET(self, request: TxRequest):
-        request.setHeader('Content-Type', 'application/json; charset=UTF-8')
-        request.setHeader('Content-Encoding', 'UTF-8')
+        request.setHeader("Content-Type", "application/json; charset=UTF-8")
+        request.setHeader("Content-Encoding", "UTF-8")
         headers = {}
         for k, v in request.requestHeaders.getAllRawHeaders():
-            headers[str(k, 'utf-8')] = str(v[0], 'utf-8')
+            headers[str(k, "utf-8")] = str(v[0], "utf-8")
 
-        return bytes(json.dumps(headers), 'utf-8')
+        return bytes(json.dumps(headers), "utf-8")
 
 
-def get_client_certificate(key_file, certificate_file) -> PrivateCertificate:
-    with open(key_file, 'r') as key, open(certificate_file, 'r') as certificate:
-        pem = ''.join(key.readlines()) + ''.join(certificate.readlines())
+def get_client_certificate(
+    key_file: Path, certificate_file: Path
+) -> PrivateCertificate:
+    pem = key_file.read_text(encoding="utf-8") + certificate_file.read_text(
+        encoding="utf-8"
+    )
 
     return PrivateCertificate.loadPEM(pem)
 
 
 @skipIf(not H2_ENABLED, "HTTP/2 support in Twisted is not enabled")
 class Https2ClientProtocolTestCase(TestCase):
-    scheme = 'https'
-    key_file = os.path.join(os.path.dirname(__file__), 'keys', 'localhost.key')
-    certificate_file = os.path.join(os.path.dirname(__file__), 'keys', 'localhost.crt')
+    scheme = "https"
+    key_file = Path(__file__).parent / "keys" / "localhost.key"
+    certificate_file = Path(__file__).parent / "keys" / "localhost.crt"
 
     def _init_resource(self):
         self.temp_directory = self.mktemp()
-        os.mkdir(self.temp_directory)
+        Path(self.temp_directory).mkdir()
         r = File(self.temp_directory)
-        r.putChild(b'get-data-html-small', GetDataHtmlSmall())
-        r.putChild(b'get-data-html-large', GetDataHtmlLarge())
+        r.putChild(b"get-data-html-small", GetDataHtmlSmall())
+        r.putChild(b"get-data-html-large", GetDataHtmlLarge())
 
-        r.putChild(b'post-data-json-small', PostDataJsonSmall())
-        r.putChild(b'post-data-json-large', PostDataJsonLarge())
+        r.putChild(b"post-data-json-small", PostDataJsonSmall())
+        r.putChild(b"post-data-json-large", PostDataJsonLarge())
 
-        r.putChild(b'dataloss', Dataloss())
-        r.putChild(b'no-content-length-header', NoContentLengthHeader())
-        r.putChild(b'status', Status())
-        r.putChild(b'query-params', QueryParams())
-        r.putChild(b'timeout', TimeoutResponse())
-        r.putChild(b'request-headers', RequestHeaders())
+        r.putChild(b"dataloss", Dataloss())
+        r.putChild(b"no-content-length-header", NoContentLengthHeader())
+        r.putChild(b"status", Status())
+        r.putChild(b"query-params", QueryParams())
+        r.putChild(b"timeout", TimeoutResponse())
+        r.putChild(b"request-headers", RequestHeaders())
         return r
 
     @inlineCallbacks
     def setUp(self):
         # Initialize resource tree
         root = self._init_resource()
         self.site = Site(root, timeout=None)
 
         # Start server for testing
-        self.hostname = 'localhost'
-        context_factory = ssl_context_factory(self.key_file, self.certificate_file)
+        self.hostname = "localhost"
+        context_factory = ssl_context_factory(
+            str(self.key_file), str(self.certificate_file)
+        )
 
-        server_endpoint = SSL4ServerEndpoint(reactor, 0, context_factory, interface=self.hostname)
+        server_endpoint = SSL4ServerEndpoint(
+            reactor, 0, context_factory, interface=self.hostname
+        )
         self.server = yield server_endpoint.listen(self.site)
         self.port_number = self.server.getHost().port
 
         # Connect H2 client with server
-        self.client_certificate = get_client_certificate(self.key_file, self.certificate_file)
+        self.client_certificate = get_client_certificate(
+            self.key_file, self.certificate_file
+        )
         client_options = optionsForClientTLS(
             hostname=self.hostname,
             trustRoot=self.client_certificate,
-            acceptableProtocols=[b'h2']
+            acceptableProtocols=[b"h2"],
         )
-        uri = URI.fromBytes(bytes(self.get_url('/'), 'utf-8'))
+        uri = URI.fromBytes(bytes(self.get_url("/"), "utf-8"))
 
         self.conn_closed_deferred = Deferred()
         from scrapy.core.http2.protocol import H2ClientFactory
+
         h2_client_factory = H2ClientFactory(uri, Settings(), self.conn_closed_deferred)
-        client_endpoint = SSL4ClientEndpoint(reactor, self.hostname, self.port_number, client_options)
+        client_endpoint = SSL4ClientEndpoint(
+            reactor, self.hostname, self.port_number, client_options
+        )
         self.client = yield client_endpoint.connect(h2_client_factory)
 
     @inlineCallbacks
     def tearDown(self):
         if self.client.connected:
             yield self.client.transport.loseConnection()
             yield self.client.transport.abortConnection()
@@ -233,328 +247,323 @@
         self.conn_closed_deferred = None
 
     def get_url(self, path):
         """
         :param path: Should have / at the starting compulsorily if not empty
         :return: Complete url
         """
-        assert len(path) > 0 and (path[0] == '/' or path[0] == '&')
-        return f'{self.scheme}://{self.hostname}:{self.port_number}{path}'
+        assert len(path) > 0 and (path[0] == "/" or path[0] == "&")
+        return f"{self.scheme}://{self.hostname}:{self.port_number}{path}"
 
     def make_request(self, request: Request) -> Deferred:
         return self.client.request(request, DummySpider())
 
     @staticmethod
     def _check_repeat(get_deferred, count):
         d_list = []
         for _ in range(count):
             d = get_deferred()
             d_list.append(d)
 
         return DeferredList(d_list, fireOnOneErrback=True)
 
-    def _check_GET(
-        self,
-        request: Request,
-        expected_body,
-        expected_status
-    ):
+    def _check_GET(self, request: Request, expected_body, expected_status):
         def check_response(response: Response):
             self.assertEqual(response.status, expected_status)
             self.assertEqual(response.body, expected_body)
             self.assertEqual(response.request, request)
 
-            content_length = int(response.headers.get('Content-Length'))
+            content_length = int(response.headers.get("Content-Length"))
             self.assertEqual(len(response.body), content_length)
 
         d = self.make_request(request)
         d.addCallback(check_response)
         d.addErrback(self.fail)
         return d
 
     def test_GET_small_body(self):
-        request = Request(self.get_url('/get-data-html-small'))
+        request = Request(self.get_url("/get-data-html-small"))
         return self._check_GET(request, Data.HTML_SMALL, 200)
 
     def test_GET_large_body(self):
-        request = Request(self.get_url('/get-data-html-large'))
+        request = Request(self.get_url("/get-data-html-large"))
         return self._check_GET(request, Data.HTML_LARGE, 200)
 
     def _check_GET_x10(self, *args, **kwargs):
         def get_deferred():
             return self._check_GET(*args, **kwargs)
 
         return self._check_repeat(get_deferred, 10)
 
     def test_GET_small_body_x10(self):
         return self._check_GET_x10(
-            Request(self.get_url('/get-data-html-small')),
-            Data.HTML_SMALL,
-            200
+            Request(self.get_url("/get-data-html-small")), Data.HTML_SMALL, 200
         )
 
     def test_GET_large_body_x10(self):
         return self._check_GET_x10(
-            Request(self.get_url('/get-data-html-large')),
-            Data.HTML_LARGE,
-            200
+            Request(self.get_url("/get-data-html-large")), Data.HTML_LARGE, 200
         )
 
     def _check_POST_json(
         self,
         request: Request,
         expected_request_body,
         expected_extra_data,
-        expected_status: int
+        expected_status: int,
     ):
         d = self.make_request(request)
 
         def assert_response(response: Response):
             self.assertEqual(response.status, expected_status)
             self.assertEqual(response.request, request)
 
-            content_length = int(response.headers.get('Content-Length'))
+            content_length = int(response.headers.get("Content-Length"))
             self.assertEqual(len(response.body), content_length)
 
             # Parse the body
-            content_encoding = str(response.headers[b'Content-Encoding'], 'utf-8')
+            content_encoding = str(response.headers[b"Content-Encoding"], "utf-8")
             body = json.loads(str(response.body, content_encoding))
-            self.assertIn('request-body', body)
-            self.assertIn('extra-data', body)
-            self.assertIn('request-headers', body)
+            self.assertIn("request-body", body)
+            self.assertIn("extra-data", body)
+            self.assertIn("request-headers", body)
 
-            request_body = body['request-body']
+            request_body = body["request-body"]
             self.assertEqual(request_body, expected_request_body)
 
-            extra_data = body['extra-data']
+            extra_data = body["extra-data"]
             self.assertEqual(extra_data, expected_extra_data)
 
             # Check if headers were sent successfully
-            request_headers = body['request-headers']
+            request_headers = body["request-headers"]
             for k, v in request.headers.items():
-                k_str = str(k, 'utf-8')
+                k_str = str(k, "utf-8")
                 self.assertIn(k_str, request_headers)
-                self.assertEqual(request_headers[k_str], str(v[0], 'utf-8'))
+                self.assertEqual(request_headers[k_str], str(v[0], "utf-8"))
 
         d.addCallback(assert_response)
         d.addErrback(self.fail)
         return d
 
     def test_POST_small_json(self):
-        request = JsonRequest(url=self.get_url('/post-data-json-small'), method='POST', data=Data.JSON_SMALL)
-        return self._check_POST_json(
-            request,
-            Data.JSON_SMALL,
-            Data.EXTRA_SMALL,
-            200
+        request = JsonRequest(
+            url=self.get_url("/post-data-json-small"),
+            method="POST",
+            data=Data.JSON_SMALL,
         )
+        return self._check_POST_json(request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200)
 
     def test_POST_large_json(self):
-        request = JsonRequest(url=self.get_url('/post-data-json-large'), method='POST', data=Data.JSON_LARGE)
-        return self._check_POST_json(
-            request,
-            Data.JSON_LARGE,
-            Data.EXTRA_LARGE,
-            200
+        request = JsonRequest(
+            url=self.get_url("/post-data-json-large"),
+            method="POST",
+            data=Data.JSON_LARGE,
         )
+        return self._check_POST_json(request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200)
 
     def _check_POST_json_x10(self, *args, **kwargs):
         def get_deferred():
             return self._check_POST_json(*args, **kwargs)
 
         return self._check_repeat(get_deferred, 10)
 
     def test_POST_small_json_x10(self):
-        request = JsonRequest(url=self.get_url('/post-data-json-small'), method='POST', data=Data.JSON_SMALL)
+        request = JsonRequest(
+            url=self.get_url("/post-data-json-small"),
+            method="POST",
+            data=Data.JSON_SMALL,
+        )
         return self._check_POST_json_x10(
-            request,
-            Data.JSON_SMALL,
-            Data.EXTRA_SMALL,
-            200
+            request, Data.JSON_SMALL, Data.EXTRA_SMALL, 200
         )
 
     def test_POST_large_json_x10(self):
-        request = JsonRequest(url=self.get_url('/post-data-json-large'), method='POST', data=Data.JSON_LARGE)
+        request = JsonRequest(
+            url=self.get_url("/post-data-json-large"),
+            method="POST",
+            data=Data.JSON_LARGE,
+        )
         return self._check_POST_json_x10(
-            request,
-            Data.JSON_LARGE,
-            Data.EXTRA_LARGE,
-            200
+            request, Data.JSON_LARGE, Data.EXTRA_LARGE, 200
         )
 
     @inlineCallbacks
     def test_invalid_negotiated_protocol(self):
-        with mock.patch("scrapy.core.http2.protocol.PROTOCOL_NAME", return_value=b"not-h2"):
-            request = Request(url=self.get_url('/status?n=200'))
+        with mock.patch(
+            "scrapy.core.http2.protocol.PROTOCOL_NAME", return_value=b"not-h2"
+        ):
+            request = Request(url=self.get_url("/status?n=200"))
             with self.assertRaises(ResponseFailed):
                 yield self.make_request(request)
 
     def test_cancel_request(self):
-        request = Request(url=self.get_url('/get-data-html-large'))
+        request = Request(url=self.get_url("/get-data-html-large"))
 
         def assert_response(response: Response):
             self.assertEqual(response.status, 499)
             self.assertEqual(response.request, request)
 
         d = self.make_request(request)
         d.addCallback(assert_response)
         d.addErrback(self.fail)
         d.cancel()
 
         return d
 
     def test_download_maxsize_exceeded(self):
-        request = Request(url=self.get_url('/get-data-html-large'), meta={'download_maxsize': 1000})
+        request = Request(
+            url=self.get_url("/get-data-html-large"), meta={"download_maxsize": 1000}
+        )
 
         def assert_cancelled_error(failure):
             self.assertIsInstance(failure.value, CancelledError)
             error_pattern = re.compile(
-                rf'Cancelling download of {request.url}: received response '
-                rf'size \(\d*\) larger than download max size \(1000\)'
+                rf"Cancelling download of {request.url}: received response "
+                rf"size \(\d*\) larger than download max size \(1000\)"
             )
             self.assertEqual(len(re.findall(error_pattern, str(failure.value))), 1)
 
         d = self.make_request(request)
         d.addCallback(self.fail)
         d.addErrback(assert_cancelled_error)
         return d
 
     def test_received_dataloss_response(self):
         """In case when value of Header Content-Length != len(Received Data)
         ProtocolError is raised"""
-        request = Request(url=self.get_url('/dataloss'))
+        request = Request(url=self.get_url("/dataloss"))
 
         def assert_failure(failure: Failure):
             self.assertTrue(len(failure.value.reasons) > 0)
             from h2.exceptions import InvalidBodyLengthError
-            self.assertTrue(any(
-                isinstance(error, InvalidBodyLengthError)
-                for error in failure.value.reasons
-            ))
+
+            self.assertTrue(
+                any(
+                    isinstance(error, InvalidBodyLengthError)
+                    for error in failure.value.reasons
+                )
+            )
 
         d = self.make_request(request)
         d.addCallback(self.fail)
         d.addErrback(assert_failure)
         return d
 
     def test_missing_content_length_header(self):
-        request = Request(url=self.get_url('/no-content-length-header'))
+        request = Request(url=self.get_url("/no-content-length-header"))
 
         def assert_content_length(response: Response):
             self.assertEqual(response.status, 200)
             self.assertEqual(response.body, Data.NO_CONTENT_LENGTH)
             self.assertEqual(response.request, request)
-            self.assertNotIn('Content-Length', response.headers)
+            self.assertNotIn("Content-Length", response.headers)
 
         d = self.make_request(request)
         d.addCallback(assert_content_length)
         d.addErrback(self.fail)
         return d
 
     @inlineCallbacks
-    def _check_log_warnsize(
-        self,
-        request,
-        warn_pattern,
-        expected_body
-    ):
-        with self.assertLogs('scrapy.core.http2.stream', level='WARNING') as cm:
+    def _check_log_warnsize(self, request, warn_pattern, expected_body):
+        with self.assertLogs("scrapy.core.http2.stream", level="WARNING") as cm:
             response = yield self.make_request(request)
             self.assertEqual(response.status, 200)
             self.assertEqual(response.request, request)
             self.assertEqual(response.body, expected_body)
 
             # Check the warning is raised only once for this request
-            self.assertEqual(sum(
-                len(re.findall(warn_pattern, log))
-                for log in cm.output
-            ), 1)
+            self.assertEqual(
+                sum(len(re.findall(warn_pattern, log)) for log in cm.output), 1
+            )
 
     @inlineCallbacks
     def test_log_expected_warnsize(self):
-        request = Request(url=self.get_url('/get-data-html-large'), meta={'download_warnsize': 1000})
+        request = Request(
+            url=self.get_url("/get-data-html-large"), meta={"download_warnsize": 1000}
+        )
         warn_pattern = re.compile(
-            rf'Expected response size \(\d*\) larger than '
-            rf'download warn size \(1000\) in request {request}'
+            rf"Expected response size \(\d*\) larger than "
+            rf"download warn size \(1000\) in request {request}"
         )
 
         yield self._check_log_warnsize(request, warn_pattern, Data.HTML_LARGE)
 
     @inlineCallbacks
     def test_log_received_warnsize(self):
-        request = Request(url=self.get_url('/no-content-length-header'), meta={'download_warnsize': 10})
+        request = Request(
+            url=self.get_url("/no-content-length-header"),
+            meta={"download_warnsize": 10},
+        )
         warn_pattern = re.compile(
-            rf'Received more \(\d*\) bytes than download '
-            rf'warn size \(10\) in request {request}'
+            rf"Received more \(\d*\) bytes than download "
+            rf"warn size \(10\) in request {request}"
         )
 
         yield self._check_log_warnsize(request, warn_pattern, Data.NO_CONTENT_LENGTH)
 
     def test_max_concurrent_streams(self):
         """Send 500 requests at one to check if we can handle
         very large number of request.
         """
 
         def get_deferred():
             return self._check_GET(
-                Request(self.get_url('/get-data-html-small')),
-                Data.HTML_SMALL,
-                200
+                Request(self.get_url("/get-data-html-small")), Data.HTML_SMALL, 200
             )
 
         return self._check_repeat(get_deferred, 500)
 
     def test_inactive_stream(self):
         """Here we send 110 requests considering the MAX_CONCURRENT_STREAMS
         by default is 100. After sending the first 100 requests we close the
         connection."""
         d_list = []
 
         def assert_inactive_stream(failure):
             self.assertIsNotNone(failure.check(ResponseFailed))
             from scrapy.core.http2.stream import InactiveStreamClosed
-            self.assertTrue(any(
-                isinstance(e, InactiveStreamClosed)
-                for e in failure.value.reasons
-            ))
+
+            self.assertTrue(
+                any(isinstance(e, InactiveStreamClosed) for e in failure.value.reasons)
+            )
 
         # Send 100 request (we do not check the result)
         for _ in range(100):
-            d = self.make_request(Request(self.get_url('/get-data-html-small')))
+            d = self.make_request(Request(self.get_url("/get-data-html-small")))
             d.addBoth(lambda _: None)
             d_list.append(d)
 
         # Now send 10 extra request and save the response deferred in a list
         for _ in range(10):
-            d = self.make_request(Request(self.get_url('/get-data-html-small')))
+            d = self.make_request(Request(self.get_url("/get-data-html-small")))
             d.addCallback(self.fail)
             d.addErrback(assert_inactive_stream)
             d_list.append(d)
 
         # Close the connection now to fire all the extra 10 requests errback
         # with InactiveStreamClosed
         self.client.transport.loseConnection()
 
         return DeferredList(d_list, consumeErrors=True, fireOnOneErrback=True)
 
     def test_invalid_request_type(self):
         with self.assertRaises(TypeError):
-            self.make_request('https://InvalidDataTypePassed.com')
+            self.make_request("https://InvalidDataTypePassed.com")
 
     def test_query_parameters(self):
         params = {
-            'a': generate_random_string(20),
-            'b': generate_random_string(20),
-            'c': generate_random_string(20),
-            'd': generate_random_string(20)
+            "a": generate_random_string(20),
+            "b": generate_random_string(20),
+            "c": generate_random_string(20),
+            "d": generate_random_string(20),
         }
-        request = Request(self.get_url(f'/query-params?{urlencode(params)}'))
+        request = Request(self.get_url(f"/query-params?{urlencode(params)}"))
 
         def assert_query_params(response: Response):
-            content_encoding = str(response.headers[b'Content-Encoding'], 'utf-8')
+            content_encoding = str(response.headers[b"Content-Encoding"], "utf-8")
             data = json.loads(str(response.body, content_encoding))
             self.assertEqual(data, params)
 
         d = self.make_request(request)
         d.addCallback(assert_query_params)
         d.addErrback(self.fail)
 
@@ -562,108 +571,119 @@
 
     def test_status_codes(self):
         def assert_response_status(response: Response, expected_status: int):
             self.assertEqual(response.status, expected_status)
 
         d_list = []
         for status in [200, 404]:
-            request = Request(self.get_url(f'/status?n={status}'))
+            request = Request(self.get_url(f"/status?n={status}"))
             d = self.make_request(request)
             d.addCallback(assert_response_status, status)
             d.addErrback(self.fail)
             d_list.append(d)
 
         return DeferredList(d_list, fireOnOneErrback=True)
 
     def test_response_has_correct_certificate_ip_address(self):
-        request = Request(self.get_url('/status?n=200'))
+        request = Request(self.get_url("/status?n=200"))
 
         def assert_metadata(response: Response):
             self.assertEqual(response.request, request)
             self.assertIsInstance(response.certificate, Certificate)
             self.assertIsNotNone(response.certificate.original)
-            self.assertEqual(response.certificate.getIssuer(), self.client_certificate.getIssuer())
-            self.assertTrue(response.certificate.getPublicKey().matches(self.client_certificate.getPublicKey()))
+            self.assertEqual(
+                response.certificate.getIssuer(), self.client_certificate.getIssuer()
+            )
+            self.assertTrue(
+                response.certificate.getPublicKey().matches(
+                    self.client_certificate.getPublicKey()
+                )
+            )
 
             self.assertIsInstance(response.ip_address, IPv4Address)
-            self.assertEqual(str(response.ip_address), '127.0.0.1')
+            self.assertEqual(str(response.ip_address), "127.0.0.1")
 
         d = self.make_request(request)
         d.addCallback(assert_metadata)
         d.addErrback(self.fail)
 
         return d
 
     def _check_invalid_netloc(self, url):
         request = Request(url)
 
         def assert_invalid_hostname(failure: Failure):
             from scrapy.core.http2.stream import InvalidHostname
+
             self.assertIsNotNone(failure.check(InvalidHostname))
             error_msg = str(failure.value)
-            self.assertIn('localhost', error_msg)
-            self.assertIn('127.0.0.1', error_msg)
+            self.assertIn("localhost", error_msg)
+            self.assertIn("127.0.0.1", error_msg)
             self.assertIn(str(request), error_msg)
 
         d = self.make_request(request)
         d.addCallback(self.fail)
         d.addErrback(assert_invalid_hostname)
         return d
 
     def test_invalid_hostname(self):
-        return self._check_invalid_netloc('https://notlocalhost.notlocalhostdomain')
+        return self._check_invalid_netloc("https://notlocalhost.notlocalhostdomain")
 
     def test_invalid_host_port(self):
         port = self.port_number + 1
-        return self._check_invalid_netloc(f'https://127.0.0.1:{port}')
+        return self._check_invalid_netloc(f"https://127.0.0.1:{port}")
 
     def test_connection_stays_with_invalid_requests(self):
         d_list = [
             self.test_invalid_hostname(),
             self.test_invalid_host_port(),
             self.test_GET_small_body(),
-            self.test_POST_small_json()
+            self.test_POST_small_json(),
         ]
 
         return DeferredList(d_list, fireOnOneErrback=True)
 
     def test_connection_timeout(self):
-        request = Request(self.get_url('/timeout'))
+        request = Request(self.get_url("/timeout"))
         d = self.make_request(request)
 
         # Update the timer to 1s to test connection timeout
         self.client.setTimeout(1)
 
         def assert_timeout_error(failure: Failure):
             for err in failure.value.reasons:
                 from scrapy.core.http2.protocol import H2ClientProtocol
+
                 if isinstance(err, TimeoutError):
-                    self.assertIn(f"Connection was IDLE for more than {H2ClientProtocol.IDLE_TIMEOUT}s", str(err))
+                    self.assertIn(
+                        f"Connection was IDLE for more than {H2ClientProtocol.IDLE_TIMEOUT}s",
+                        str(err),
+                    )
                     break
             else:
                 self.fail()
 
         d.addCallback(self.fail)
         d.addErrback(assert_timeout_error)
         return d
 
     def test_request_headers_received(self):
-        request = Request(self.get_url('/request-headers'), headers={
-            'header-1': 'header value 1',
-            'header-2': 'header value 2'
-        })
+        request = Request(
+            self.get_url("/request-headers"),
+            headers={"header-1": "header value 1", "header-2": "header value 2"},
+        )
         d = self.make_request(request)
 
         def assert_request_headers(response: Response):
             self.assertEqual(response.status, 200)
             self.assertEqual(response.request, request)
 
-            response_headers = json.loads(str(response.body, 'utf-8'))
+            response_headers = json.loads(str(response.body, "utf-8"))
             self.assertIsInstance(response_headers, dict)
             for k, v in request.headers.items():
-                k, v = str(k, 'utf-8'), str(v[0], 'utf-8')
+                k, v = str(k, "utf-8"), str(v[0], "utf-8")
                 self.assertIn(k, response_headers)
                 self.assertEqual(v, response_headers[k])
 
         d.addErrback(self.fail)
         d.addCallback(assert_request_headers)
         return d
```

### Comparing `Scrapy-2.7.1/tests/test_http_cookies.py` & `Scrapy-2.8.0/tests/test_http_cookies.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-from urllib.parse import urlparse
 from unittest import TestCase
+from urllib.parse import urlparse
 
 from scrapy.http import Request, Response
 from scrapy.http.cookies import WrappedRequest, WrappedResponse
 
 
 class WrappedRequestTest(TestCase):
-
     def setUp(self):
-        self.request = Request("http://www.example.com/page.html",
-                               headers={"Content-Type": "text/html"})
+        self.request = Request(
+            "http://www.example.com/page.html", headers={"Content-Type": "text/html"}
+        )
         self.wrapped = WrappedRequest(self.request)
 
     def test_get_full_url(self):
         self.assertEqual(self.wrapped.get_full_url(), self.request.url)
         self.assertEqual(self.wrapped.full_url, self.request.url)
 
     def test_get_host(self):
@@ -25,44 +25,43 @@
         self.assertEqual(self.wrapped.type, urlparse(self.request.url).scheme)
 
     def test_is_unverifiable(self):
         self.assertFalse(self.wrapped.is_unverifiable())
         self.assertFalse(self.wrapped.unverifiable)
 
     def test_is_unverifiable2(self):
-        self.request.meta['is_unverifiable'] = True
+        self.request.meta["is_unverifiable"] = True
         self.assertTrue(self.wrapped.is_unverifiable())
         self.assertTrue(self.wrapped.unverifiable)
 
     def test_get_origin_req_host(self):
-        self.assertEqual(self.wrapped.origin_req_host, 'www.example.com')
+        self.assertEqual(self.wrapped.origin_req_host, "www.example.com")
 
     def test_has_header(self):
-        self.assertTrue(self.wrapped.has_header('content-type'))
-        self.assertFalse(self.wrapped.has_header('xxxxx'))
+        self.assertTrue(self.wrapped.has_header("content-type"))
+        self.assertFalse(self.wrapped.has_header("xxxxx"))
 
     def test_get_header(self):
-        self.assertEqual(self.wrapped.get_header('content-type'), 'text/html')
-        self.assertEqual(self.wrapped.get_header('xxxxx', 'def'), 'def')
+        self.assertEqual(self.wrapped.get_header("content-type"), "text/html")
+        self.assertEqual(self.wrapped.get_header("xxxxx", "def"), "def")
 
     def test_header_items(self):
-        self.assertEqual(self.wrapped.header_items(),
-                         [('Content-Type', ['text/html'])])
+        self.assertEqual(self.wrapped.header_items(), [("Content-Type", ["text/html"])])
 
     def test_add_unredirected_header(self):
-        self.wrapped.add_unredirected_header('hello', 'world')
-        self.assertEqual(self.request.headers['hello'], b'world')
+        self.wrapped.add_unredirected_header("hello", "world")
+        self.assertEqual(self.request.headers["hello"], b"world")
 
 
 class WrappedResponseTest(TestCase):
-
     def setUp(self):
-        self.response = Response("http://www.example.com/page.html",
-                                 headers={"Content-TYpe": "text/html"})
+        self.response = Response(
+            "http://www.example.com/page.html", headers={"Content-TYpe": "text/html"}
+        )
         self.wrapped = WrappedResponse(self.response)
 
     def test_info(self):
         self.assertIs(self.wrapped.info(), self.wrapped)
 
     def test_get_all(self):
         # get_all result must be native string
-        self.assertEqual(self.wrapped.get_all('content-type'), ['text/html'])
+        self.assertEqual(self.wrapped.get_all("content-type"), ["text/html"])
```

### Comparing `Scrapy-2.7.1/tests/test_http_headers.py` & `Scrapy-2.8.0/tests/test_http_headers.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,161 +1,166 @@
-import unittest
 import copy
+import unittest
 
 from scrapy.http import Headers
 
 
 class HeadersTest(unittest.TestCase):
-
     def assertSortedEqual(self, first, second, msg=None):
         return self.assertEqual(sorted(first), sorted(second), msg)
 
     def test_basics(self):
-        h = Headers({'Content-Type': 'text/html', 'Content-Length': 1234})
-        assert h['Content-Type']
-        assert h['Content-Length']
-
-        self.assertRaises(KeyError, h.__getitem__, 'Accept')
-        self.assertEqual(h.get('Accept'), None)
-        self.assertEqual(h.getlist('Accept'), [])
-
-        self.assertEqual(h.get('Accept', '*/*'), b'*/*')
-        self.assertEqual(h.getlist('Accept', '*/*'), [b'*/*'])
-        self.assertEqual(h.getlist('Accept', ['text/html', 'images/jpeg']),
-                         [b'text/html', b'images/jpeg'])
+        h = Headers({"Content-Type": "text/html", "Content-Length": 1234})
+        assert h["Content-Type"]
+        assert h["Content-Length"]
+
+        self.assertRaises(KeyError, h.__getitem__, "Accept")
+        self.assertEqual(h.get("Accept"), None)
+        self.assertEqual(h.getlist("Accept"), [])
+
+        self.assertEqual(h.get("Accept", "*/*"), b"*/*")
+        self.assertEqual(h.getlist("Accept", "*/*"), [b"*/*"])
+        self.assertEqual(
+            h.getlist("Accept", ["text/html", "images/jpeg"]),
+            [b"text/html", b"images/jpeg"],
+        )
 
     def test_single_value(self):
         h = Headers()
-        h['Content-Type'] = 'text/html'
-        self.assertEqual(h['Content-Type'], b'text/html')
-        self.assertEqual(h.get('Content-Type'), b'text/html')
-        self.assertEqual(h.getlist('Content-Type'), [b'text/html'])
+        h["Content-Type"] = "text/html"
+        self.assertEqual(h["Content-Type"], b"text/html")
+        self.assertEqual(h.get("Content-Type"), b"text/html")
+        self.assertEqual(h.getlist("Content-Type"), [b"text/html"])
 
     def test_multivalue(self):
         h = Headers()
-        h['X-Forwarded-For'] = hlist = ['ip1', 'ip2']
-        self.assertEqual(h['X-Forwarded-For'], b'ip2')
-        self.assertEqual(h.get('X-Forwarded-For'), b'ip2')
-        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1', b'ip2'])
-        assert h.getlist('X-Forwarded-For') is not hlist
+        h["X-Forwarded-For"] = hlist = ["ip1", "ip2"]
+        self.assertEqual(h["X-Forwarded-For"], b"ip2")
+        self.assertEqual(h.get("X-Forwarded-For"), b"ip2")
+        self.assertEqual(h.getlist("X-Forwarded-For"), [b"ip1", b"ip2"])
+        assert h.getlist("X-Forwarded-For") is not hlist
 
     def test_multivalue_for_one_header(self):
         h = Headers((("a", "b"), ("a", "c")))
         self.assertEqual(h["a"], b"c")
         self.assertEqual(h.get("a"), b"c")
         self.assertEqual(h.getlist("a"), [b"b", b"c"])
 
     def test_encode_utf8(self):
-        h = Headers({'key': '\xa3'}, encoding='utf-8')
+        h = Headers({"key": "\xa3"}, encoding="utf-8")
         key, val = dict(h).popitem()
         assert isinstance(key, bytes), key
         assert isinstance(val[0], bytes), val[0]
-        self.assertEqual(val[0], b'\xc2\xa3')
+        self.assertEqual(val[0], b"\xc2\xa3")
 
     def test_encode_latin1(self):
-        h = Headers({'key': '\xa3'}, encoding='latin1')
+        h = Headers({"key": "\xa3"}, encoding="latin1")
         key, val = dict(h).popitem()
-        self.assertEqual(val[0], b'\xa3')
+        self.assertEqual(val[0], b"\xa3")
 
     def test_encode_multiple(self):
-        h = Headers({'key': ['\xa3']}, encoding='utf-8')
+        h = Headers({"key": ["\xa3"]}, encoding="utf-8")
         key, val = dict(h).popitem()
-        self.assertEqual(val[0], b'\xc2\xa3')
+        self.assertEqual(val[0], b"\xc2\xa3")
 
     def test_delete_and_contains(self):
         h = Headers()
-        h['Content-Type'] = 'text/html'
-        assert 'Content-Type' in h
-        del h['Content-Type']
-        assert 'Content-Type' not in h
+        h["Content-Type"] = "text/html"
+        assert "Content-Type" in h
+        del h["Content-Type"]
+        assert "Content-Type" not in h
 
     def test_setdefault(self):
         h = Headers()
-        hlist = ['ip1', 'ip2']
-        olist = h.setdefault('X-Forwarded-For', hlist)
-        assert h.getlist('X-Forwarded-For') is not hlist
-        assert h.getlist('X-Forwarded-For') is olist
+        hlist = ["ip1", "ip2"]
+        olist = h.setdefault("X-Forwarded-For", hlist)
+        assert h.getlist("X-Forwarded-For") is not hlist
+        assert h.getlist("X-Forwarded-For") is olist
 
         h = Headers()
-        olist = h.setdefault('X-Forwarded-For', 'ip1')
-        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1'])
-        assert h.getlist('X-Forwarded-For') is olist
+        olist = h.setdefault("X-Forwarded-For", "ip1")
+        self.assertEqual(h.getlist("X-Forwarded-For"), [b"ip1"])
+        assert h.getlist("X-Forwarded-For") is olist
 
     def test_iterables(self):
-        idict = {'Content-Type': 'text/html', 'X-Forwarded-For': ['ip1', 'ip2']}
+        idict = {"Content-Type": "text/html", "X-Forwarded-For": ["ip1", "ip2"]}
 
         h = Headers(idict)
-        self.assertDictEqual(dict(h),
-                             {b'Content-Type': [b'text/html'],
-                              b'X-Forwarded-For': [b'ip1', b'ip2']})
-        self.assertSortedEqual(h.keys(),
-                               [b'X-Forwarded-For', b'Content-Type'])
-        self.assertSortedEqual(h.items(),
-                               [(b'X-Forwarded-For', [b'ip1', b'ip2']),
-                                (b'Content-Type', [b'text/html'])])
-        self.assertSortedEqual(h.values(), [b'ip2', b'text/html'])
+        self.assertDictEqual(
+            dict(h),
+            {b"Content-Type": [b"text/html"], b"X-Forwarded-For": [b"ip1", b"ip2"]},
+        )
+        self.assertSortedEqual(h.keys(), [b"X-Forwarded-For", b"Content-Type"])
+        self.assertSortedEqual(
+            h.items(),
+            [(b"X-Forwarded-For", [b"ip1", b"ip2"]), (b"Content-Type", [b"text/html"])],
+        )
+        self.assertSortedEqual(h.values(), [b"ip2", b"text/html"])
 
     def test_update(self):
         h = Headers()
-        h.update({'Content-Type': 'text/html',
-                  'X-Forwarded-For': ['ip1', 'ip2']})
-        self.assertEqual(h.getlist('Content-Type'), [b'text/html'])
-        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1', b'ip2'])
+        h.update({"Content-Type": "text/html", "X-Forwarded-For": ["ip1", "ip2"]})
+        self.assertEqual(h.getlist("Content-Type"), [b"text/html"])
+        self.assertEqual(h.getlist("X-Forwarded-For"), [b"ip1", b"ip2"])
 
     def test_copy(self):
-        h1 = Headers({'header1': ['value1', 'value2']})
+        h1 = Headers({"header1": ["value1", "value2"]})
         h2 = copy.copy(h1)
         self.assertEqual(h1, h2)
-        self.assertEqual(h1.getlist('header1'), h2.getlist('header1'))
-        assert h1.getlist('header1') is not h2.getlist('header1')
+        self.assertEqual(h1.getlist("header1"), h2.getlist("header1"))
+        assert h1.getlist("header1") is not h2.getlist("header1")
         assert isinstance(h2, Headers)
 
     def test_appendlist(self):
-        h1 = Headers({'header1': 'value1'})
-        h1.appendlist('header1', 'value3')
-        self.assertEqual(h1.getlist('header1'), [b'value1', b'value3'])
+        h1 = Headers({"header1": "value1"})
+        h1.appendlist("header1", "value3")
+        self.assertEqual(h1.getlist("header1"), [b"value1", b"value3"])
 
         h1 = Headers()
-        h1.appendlist('header1', 'value1')
-        h1.appendlist('header1', 'value3')
-        self.assertEqual(h1.getlist('header1'), [b'value1', b'value3'])
+        h1.appendlist("header1", "value1")
+        h1.appendlist("header1", "value3")
+        self.assertEqual(h1.getlist("header1"), [b"value1", b"value3"])
 
     def test_setlist(self):
-        h1 = Headers({'header1': 'value1'})
-        self.assertEqual(h1.getlist('header1'), [b'value1'])
-        h1.setlist('header1', [b'value2', b'value3'])
-        self.assertEqual(h1.getlist('header1'), [b'value2', b'value3'])
+        h1 = Headers({"header1": "value1"})
+        self.assertEqual(h1.getlist("header1"), [b"value1"])
+        h1.setlist("header1", [b"value2", b"value3"])
+        self.assertEqual(h1.getlist("header1"), [b"value2", b"value3"])
 
     def test_setlistdefault(self):
-        h1 = Headers({'header1': 'value1'})
-        h1.setlistdefault('header1', ['value2', 'value3'])
-        h1.setlistdefault('header2', ['value2', 'value3'])
-        self.assertEqual(h1.getlist('header1'), [b'value1'])
-        self.assertEqual(h1.getlist('header2'), [b'value2', b'value3'])
+        h1 = Headers({"header1": "value1"})
+        h1.setlistdefault("header1", ["value2", "value3"])
+        h1.setlistdefault("header2", ["value2", "value3"])
+        self.assertEqual(h1.getlist("header1"), [b"value1"])
+        self.assertEqual(h1.getlist("header2"), [b"value2", b"value3"])
 
     def test_none_value(self):
         h1 = Headers()
-        h1['foo'] = 'bar'
-        h1['foo'] = None
-        h1.setdefault('foo', 'bar')
-        self.assertEqual(h1.get('foo'), None)
-        self.assertEqual(h1.getlist('foo'), [])
+        h1["foo"] = "bar"
+        h1["foo"] = None
+        h1.setdefault("foo", "bar")
+        self.assertEqual(h1.get("foo"), None)
+        self.assertEqual(h1.getlist("foo"), [])
 
     def test_int_value(self):
-        h1 = Headers({'hey': 5})
-        h1['foo'] = 1
-        h1.setdefault('bar', 2)
-        h1.setlist('buz', [1, 'dos', 3])
-        self.assertEqual(h1.getlist('foo'), [b'1'])
-        self.assertEqual(h1.getlist('bar'), [b'2'])
-        self.assertEqual(h1.getlist('buz'), [b'1', b'dos', b'3'])
-        self.assertEqual(h1.getlist('hey'), [b'5'])
+        h1 = Headers({"hey": 5})
+        h1["foo"] = 1
+        h1.setdefault("bar", 2)
+        h1.setlist("buz", [1, "dos", 3])
+        self.assertEqual(h1.getlist("foo"), [b"1"])
+        self.assertEqual(h1.getlist("bar"), [b"2"])
+        self.assertEqual(h1.getlist("buz"), [b"1", b"dos", b"3"])
+        self.assertEqual(h1.getlist("hey"), [b"5"])
 
     def test_invalid_value(self):
-        self.assertRaisesRegex(TypeError, 'Unsupported value type',
-                               Headers, {'foo': object()})
-        self.assertRaisesRegex(TypeError, 'Unsupported value type',
-                               Headers().__setitem__, 'foo', object())
-        self.assertRaisesRegex(TypeError, 'Unsupported value type',
-                               Headers().setdefault, 'foo', object())
-        self.assertRaisesRegex(TypeError, 'Unsupported value type',
-                               Headers().setlist, 'foo', [object()])
+        self.assertRaisesRegex(
+            TypeError, "Unsupported value type", Headers, {"foo": object()}
+        )
+        self.assertRaisesRegex(
+            TypeError, "Unsupported value type", Headers().__setitem__, "foo", object()
+        )
+        self.assertRaisesRegex(
+            TypeError, "Unsupported value type", Headers().setdefault, "foo", object()
+        )
+        self.assertRaisesRegex(
+            TypeError, "Unsupported value type", Headers().setlist, "foo", [object()]
+        )
```

### Comparing `Scrapy-2.7.1/tests/test_http_request.py` & `Scrapy-2.8.0/tests/test_http_request.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,87 +1,97 @@
-import unittest
-import re
 import json
-import xmlrpc.client
+import re
+import unittest
 import warnings
+import xmlrpc.client
 from unittest import mock
 from urllib.parse import parse_qs, unquote_to_bytes, urlparse
 
-from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse
+from scrapy.http import (
+    FormRequest,
+    Headers,
+    HtmlResponse,
+    JsonRequest,
+    Request,
+    XmlRpcRequest,
+)
+from scrapy.http.request import NO_CALLBACK
 from scrapy.utils.python import to_bytes, to_unicode
 
 
 class RequestTest(unittest.TestCase):
 
     request_class = Request
-    default_method = 'GET'
+    default_method = "GET"
     default_headers = {}
     default_meta = {}
 
     def test_init(self):
         # Request requires url in the __init__ method
         self.assertRaises(Exception, self.request_class)
 
         # url argument must be basestring
         self.assertRaises(TypeError, self.request_class, 123)
-        r = self.request_class('http://www.example.com')
+        r = self.request_class("http://www.example.com")
 
         r = self.request_class("http://www.example.com")
         assert isinstance(r.url, str)
         self.assertEqual(r.url, "http://www.example.com")
         self.assertEqual(r.method, self.default_method)
 
         assert isinstance(r.headers, Headers)
         self.assertEqual(r.headers, self.default_headers)
         self.assertEqual(r.meta, self.default_meta)
 
         meta = {"lala": "lolo"}
         headers = {b"caca": b"coco"}
-        r = self.request_class("http://www.example.com", meta=meta, headers=headers, body="a body")
+        r = self.request_class(
+            "http://www.example.com", meta=meta, headers=headers, body="a body"
+        )
 
         assert r.meta is not meta
         self.assertEqual(r.meta, meta)
         assert r.headers is not headers
         self.assertEqual(r.headers[b"caca"], b"coco")
 
     def test_url_scheme(self):
         # This test passes by not raising any (ValueError) exception
-        self.request_class('http://example.org')
-        self.request_class('https://example.org')
-        self.request_class('s3://example.org')
-        self.request_class('ftp://example.org')
-        self.request_class('about:config')
-        self.request_class('data:,Hello%2C%20World!')
+        self.request_class("http://example.org")
+        self.request_class("https://example.org")
+        self.request_class("s3://example.org")
+        self.request_class("ftp://example.org")
+        self.request_class("about:config")
+        self.request_class("data:,Hello%2C%20World!")
 
     def test_url_no_scheme(self):
-        self.assertRaises(ValueError, self.request_class, 'foo')
-        self.assertRaises(ValueError, self.request_class, '/foo/')
-        self.assertRaises(ValueError, self.request_class, '/foo:bar')
+        self.assertRaises(ValueError, self.request_class, "foo")
+        self.assertRaises(ValueError, self.request_class, "/foo/")
+        self.assertRaises(ValueError, self.request_class, "/foo:bar")
 
     def test_headers(self):
         # Different ways of setting headers attribute
-        url = 'http://www.scrapy.org'
-        headers = {b'Accept': 'gzip', b'Custom-Header': 'nothing to tell you'}
+        url = "http://www.scrapy.org"
+        headers = {b"Accept": "gzip", b"Custom-Header": "nothing to tell you"}
         r = self.request_class(url=url, headers=headers)
         p = self.request_class(url=url, headers=r.headers)
 
         self.assertEqual(r.headers, p.headers)
         self.assertFalse(r.headers is headers)
         self.assertFalse(p.headers is r.headers)
 
         # headers must not be unicode
-        h = Headers({'key1': 'val1', 'key2': 'val2'})
-        h['newkey'] = 'newval'
+        h = Headers({"key1": "val1", "key2": "val2"})
+        h["newkey"] = "newval"
         for k, v in h.items():
             self.assertIsInstance(k, bytes)
             for s in v:
                 self.assertIsInstance(s, bytes)
 
     def test_eq(self):
-        url = 'http://www.scrapy.org'
+        url = "http://www.scrapy.org"
         r1 = self.request_class(url=url)
         r2 = self.request_class(url=url)
         self.assertNotEqual(r1, r2)
 
         set_ = set()
         set_.add(r1)
         set_.add(r2)
@@ -111,21 +121,25 @@
         self.assertEqual(r.url, "http://www.scrapy.org/price/%C2%A3")
 
     def test_url_encoding_query(self):
         r1 = self.request_class(url="http://www.scrapy.org/price/?unit=")
         self.assertEqual(r1.url, "http://www.scrapy.org/price/%C2%A3?unit=%C2%B5")
 
         # should be same as above
-        r2 = self.request_class(url="http://www.scrapy.org/price/?unit=", encoding="utf-8")
+        r2 = self.request_class(
+            url="http://www.scrapy.org/price/?unit=", encoding="utf-8"
+        )
         self.assertEqual(r2.url, "http://www.scrapy.org/price/%C2%A3?unit=%C2%B5")
 
     def test_url_encoding_query_latin1(self):
         # encoding is used for encoding query-string before percent-escaping;
         # path is still UTF-8 encoded before percent-escaping
-        r3 = self.request_class(url="http://www.scrapy.org/price/?currency=", encoding="latin1")
+        r3 = self.request_class(
+            url="http://www.scrapy.org/price/?currency=", encoding="latin1"
+        )
         self.assertEqual(r3.url, "http://www.scrapy.org/price/%C2%B5?currency=%A3")
 
     def test_url_encoding_nonutf8_untouched(self):
         # percent-escaping sequences that do not match valid UTF-8 sequences
         # should be kept untouched (just upper-cased perhaps)
         #
         # See https://tools.ietf.org/html/rfc3987#section-3.2
@@ -150,149 +164,181 @@
         self.assertEqual(r3.url, "http://www.scrapy.org/r%C3%A9sum%C3%A9/%a3")
 
         r4 = self.request_class(url="http://www.example.org/r%E9sum%E9.html")
         self.assertEqual(r4.url, "http://www.example.org/r%E9sum%E9.html")
 
     def test_body(self):
         r1 = self.request_class(url="http://www.example.com/")
-        assert r1.body == b''
+        assert r1.body == b""
 
         r2 = self.request_class(url="http://www.example.com/", body=b"")
         assert isinstance(r2.body, bytes)
-        self.assertEqual(r2.encoding, 'utf-8')  # default encoding
+        self.assertEqual(r2.encoding, "utf-8")  # default encoding
 
-        r3 = self.request_class(url="http://www.example.com/", body="Price: \xa3100", encoding='utf-8')
+        r3 = self.request_class(
+            url="http://www.example.com/", body="Price: \xa3100", encoding="utf-8"
+        )
         assert isinstance(r3.body, bytes)
         self.assertEqual(r3.body, b"Price: \xc2\xa3100")
 
-        r4 = self.request_class(url="http://www.example.com/", body="Price: \xa3100", encoding='latin1')
+        r4 = self.request_class(
+            url="http://www.example.com/", body="Price: \xa3100", encoding="latin1"
+        )
         assert isinstance(r4.body, bytes)
         self.assertEqual(r4.body, b"Price: \xa3100")
 
     def test_ajax_url(self):
         # ascii url
         r = self.request_class(url="http://www.example.com/ajax.html#!key=value")
-        self.assertEqual(r.url, "http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue")
+        self.assertEqual(
+            r.url, "http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue"
+        )
         # unicode url
         r = self.request_class(url="http://www.example.com/ajax.html#!key=value")
-        self.assertEqual(r.url, "http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue")
+        self.assertEqual(
+            r.url, "http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue"
+        )
 
     def test_copy(self):
         """Test Request copy"""
 
         def somecallback():
             pass
 
-        r1 = self.request_class("http://www.example.com", flags=['f1', 'f2'],
-                                callback=somecallback, errback=somecallback)
-        r1.meta['foo'] = 'bar'
-        r1.cb_kwargs['key'] = 'value'
+        r1 = self.request_class(
+            "http://www.example.com",
+            flags=["f1", "f2"],
+            callback=somecallback,
+            errback=somecallback,
+        )
+        r1.meta["foo"] = "bar"
+        r1.cb_kwargs["key"] = "value"
         r2 = r1.copy()
 
         # make sure copy does not propagate callbacks
         assert r1.callback is somecallback
         assert r1.errback is somecallback
         assert r2.callback is r1.callback
         assert r2.errback is r2.errback
 
         # make sure flags list is shallow copied
         assert r1.flags is not r2.flags, "flags must be a shallow copy, not identical"
         self.assertEqual(r1.flags, r2.flags)
 
         # make sure cb_kwargs dict is shallow copied
-        assert r1.cb_kwargs is not r2.cb_kwargs, "cb_kwargs must be a shallow copy, not identical"
+        assert (
+            r1.cb_kwargs is not r2.cb_kwargs
+        ), "cb_kwargs must be a shallow copy, not identical"
         self.assertEqual(r1.cb_kwargs, r2.cb_kwargs)
 
         # make sure meta dict is shallow copied
         assert r1.meta is not r2.meta, "meta must be a shallow copy, not identical"
         self.assertEqual(r1.meta, r2.meta)
 
         # make sure headers attribute is shallow copied
-        assert r1.headers is not r2.headers, "headers must be a shallow copy, not identical"
+        assert (
+            r1.headers is not r2.headers
+        ), "headers must be a shallow copy, not identical"
         self.assertEqual(r1.headers, r2.headers)
         self.assertEqual(r1.encoding, r2.encoding)
         self.assertEqual(r1.dont_filter, r2.dont_filter)
 
         # Request.body can be identical since it's an immutable object (str)
 
     def test_copy_inherited_classes(self):
         """Test Request children copies preserve their class"""
 
         class CustomRequest(self.request_class):
             pass
 
-        r1 = CustomRequest('http://www.example.com')
+        r1 = CustomRequest("http://www.example.com")
         r2 = r1.copy()
 
-        assert type(r2) is CustomRequest
+        assert isinstance(r2, CustomRequest)
 
     def test_replace(self):
         """Test Request.replace() method"""
-        r1 = self.request_class("http://www.example.com", method='GET')
+        r1 = self.request_class("http://www.example.com", method="GET")
         hdrs = Headers(r1.headers)
-        hdrs[b'key'] = b'value'
+        hdrs[b"key"] = b"value"
         r2 = r1.replace(method="POST", body="New body", headers=hdrs)
         self.assertEqual(r1.url, r2.url)
         self.assertEqual((r1.method, r2.method), ("GET", "POST"))
-        self.assertEqual((r1.body, r2.body), (b'', b"New body"))
+        self.assertEqual((r1.body, r2.body), (b"", b"New body"))
         self.assertEqual((r1.headers, r2.headers), (self.default_headers, hdrs))
 
         # Empty attributes (which may fail if not compared properly)
-        r3 = self.request_class("http://www.example.com", meta={'a': 1}, dont_filter=True)
-        r4 = r3.replace(url="http://www.example.com/2", body=b'', meta={}, dont_filter=False)
+        r3 = self.request_class(
+            "http://www.example.com", meta={"a": 1}, dont_filter=True
+        )
+        r4 = r3.replace(
+            url="http://www.example.com/2", body=b"", meta={}, dont_filter=False
+        )
         self.assertEqual(r4.url, "http://www.example.com/2")
-        self.assertEqual(r4.body, b'')
+        self.assertEqual(r4.body, b"")
         self.assertEqual(r4.meta, {})
         assert r4.dont_filter is False
 
     def test_method_always_str(self):
         r = self.request_class("http://www.example.com", method="POST")
         assert isinstance(r.method, str)
 
     def test_immutable_attributes(self):
         r = self.request_class("http://example.com")
-        self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')
-        self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')
+        self.assertRaises(AttributeError, setattr, r, "url", "http://example2.com")
+        self.assertRaises(AttributeError, setattr, r, "body", "xxx")
 
     def test_callback_and_errback(self):
         def a_function():
             pass
 
-        r1 = self.request_class('http://example.com')
+        r1 = self.request_class("http://example.com")
         self.assertIsNone(r1.callback)
         self.assertIsNone(r1.errback)
 
-        r2 = self.request_class('http://example.com', callback=a_function)
+        r2 = self.request_class("http://example.com", callback=a_function)
         self.assertIs(r2.callback, a_function)
         self.assertIsNone(r2.errback)
 
-        r3 = self.request_class('http://example.com', errback=a_function)
+        r3 = self.request_class("http://example.com", errback=a_function)
         self.assertIsNone(r3.callback)
         self.assertIs(r3.errback, a_function)
 
         r4 = self.request_class(
-            url='http://example.com',
+            url="http://example.com",
             callback=a_function,
             errback=a_function,
         )
         self.assertIs(r4.callback, a_function)
         self.assertIs(r4.errback, a_function)
 
+        r5 = self.request_class(
+            url="http://example.com",
+            callback=NO_CALLBACK,
+            errback=NO_CALLBACK,
+        )
+        self.assertIs(r5.callback, NO_CALLBACK)
+        self.assertIs(r5.errback, NO_CALLBACK)
+
     def test_callback_and_errback_type(self):
         with self.assertRaises(TypeError):
-            self.request_class('http://example.com', callback='a_function')
+            self.request_class("http://example.com", callback="a_function")
         with self.assertRaises(TypeError):
-            self.request_class('http://example.com', errback='a_function')
+            self.request_class("http://example.com", errback="a_function")
         with self.assertRaises(TypeError):
             self.request_class(
-                url='http://example.com',
-                callback='a_function',
-                errback='a_function',
+                url="http://example.com",
+                callback="a_function",
+                errback="a_function",
             )
 
+    def test_no_callback(self):
+        with self.assertRaises(RuntimeError):
+            NO_CALLBACK()
+
     def test_from_curl(self):
         # Note: more curated tests regarding curl conversion are in
         # `test_utils_curl.py`
         curl_command = (
             "curl 'http://httpbin.org/post' -X POST -H 'Cookie: _gauges_unique"
             "_year=1; _gauges_unique=1; _gauges_unique_month=1; _gauges_unique"
             "_hour=1; _gauges_unique_day=1' -H 'Origin: http://httpbin.org' -H"
@@ -307,59 +353,70 @@
             "p-alive' --data 'custname=John+Smith&custtel=500&custemail=jsmith"
             "%40example.org&size=small&topping=cheese&topping=onion&delivery=1"
             "2%3A15&comments=' --compressed"
         )
         r = self.request_class.from_curl(curl_command)
         self.assertEqual(r.method, "POST")
         self.assertEqual(r.url, "http://httpbin.org/post")
-        self.assertEqual(r.body,
-                         b"custname=John+Smith&custtel=500&custemail=jsmith%40"
-                         b"example.org&size=small&topping=cheese&topping=onion"
-                         b"&delivery=12%3A15&comments=")
-        self.assertEqual(r.cookies, {
-            '_gauges_unique_year': '1',
-            '_gauges_unique': '1',
-            '_gauges_unique_month': '1',
-            '_gauges_unique_hour': '1',
-            '_gauges_unique_day': '1'
-        })
-        self.assertEqual(r.headers, {
-            b'Origin': [b'http://httpbin.org'],
-            b'Accept-Encoding': [b'gzip, deflate'],
-            b'Accept-Language': [b'en-US,en;q=0.9,ru;q=0.8,es;q=0.7'],
-            b'Upgrade-Insecure-Requests': [b'1'],
-            b'User-Agent': [b'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.'
-                            b'36 (KHTML, like Gecko) Ubuntu Chromium/62.0.3202'
-                            b'.75 Chrome/62.0.3202.75 Safari/537.36'],
-            b'Content-Type': [b'application /x-www-form-urlencoded'],
-            b'Accept': [b'text/html,application/xhtml+xml,application/xml;q=0.'
-                        b'9,image/webp,image/apng,*/*;q=0.8'],
-            b'Cache-Control': [b'max-age=0'],
-            b'Referer': [b'http://httpbin.org/forms/post'],
-            b'Connection': [b'keep-alive']})
+        self.assertEqual(
+            r.body,
+            b"custname=John+Smith&custtel=500&custemail=jsmith%40"
+            b"example.org&size=small&topping=cheese&topping=onion"
+            b"&delivery=12%3A15&comments=",
+        )
+        self.assertEqual(
+            r.cookies,
+            {
+                "_gauges_unique_year": "1",
+                "_gauges_unique": "1",
+                "_gauges_unique_month": "1",
+                "_gauges_unique_hour": "1",
+                "_gauges_unique_day": "1",
+            },
+        )
+        self.assertEqual(
+            r.headers,
+            {
+                b"Origin": [b"http://httpbin.org"],
+                b"Accept-Encoding": [b"gzip, deflate"],
+                b"Accept-Language": [b"en-US,en;q=0.9,ru;q=0.8,es;q=0.7"],
+                b"Upgrade-Insecure-Requests": [b"1"],
+                b"User-Agent": [
+                    b"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537."
+                    b"36 (KHTML, like Gecko) Ubuntu Chromium/62.0.3202"
+                    b".75 Chrome/62.0.3202.75 Safari/537.36"
+                ],
+                b"Content-Type": [b"application /x-www-form-urlencoded"],
+                b"Accept": [
+                    b"text/html,application/xhtml+xml,application/xml;q=0."
+                    b"9,image/webp,image/apng,*/*;q=0.8"
+                ],
+                b"Cache-Control": [b"max-age=0"],
+                b"Referer": [b"http://httpbin.org/forms/post"],
+                b"Connection": [b"keep-alive"],
+            },
+        )
 
     def test_from_curl_with_kwargs(self):
         r = self.request_class.from_curl(
-            'curl -X PATCH "http://example.org"',
-            method="POST",
-            meta={'key': 'value'}
+            'curl -X PATCH "http://example.org"', method="POST", meta={"key": "value"}
         )
         self.assertEqual(r.method, "POST")
         self.assertEqual(r.meta, {"key": "value"})
 
     def test_from_curl_ignore_unknown_options(self):
         # By default: it works and ignores the unknown options: --foo and -z
         with warnings.catch_warnings():  # avoid warning when executing tests
-            warnings.simplefilter('ignore')
+            warnings.simplefilter("ignore")
             r = self.request_class.from_curl(
                 'curl -X DELETE "http://example.org" --foo -z',
             )
             self.assertEqual(r.method, "DELETE")
 
-        # If `ignore_unknon_options` is set to `False` it raises an error with
+        # If `ignore_unknown_options` is set to `False` it raises an error with
         # the unknown options: --foo and -z
         self.assertRaises(
             ValueError,
             lambda: self.request_class.from_curl(
                 'curl -X PATCH "http://example.org" --foo -z',
                 ignore_unknown_options=False,
             ),
@@ -373,591 +430,689 @@
     def assertQueryEqual(self, first, second, msg=None):
         first = to_unicode(first).split("&")
         second = to_unicode(second).split("&")
         return self.assertEqual(sorted(first), sorted(second), msg)
 
     def test_empty_formdata(self):
         r1 = self.request_class("http://www.example.com", formdata={})
-        self.assertEqual(r1.body, b'')
+        self.assertEqual(r1.body, b"")
 
     def test_formdata_overrides_querystring(self):
-        data = (('a', 'one'), ('a', 'two'), ('b', '2'))
-        url = self.request_class('http://www.example.com/?a=0&b=1&c=3#fragment',
-                                 method='GET', formdata=data).url.split('#')[0]
-        fs = _qs(self.request_class(url, method='GET', formdata=data))
-        self.assertEqual(set(fs[b'a']), {b'one', b'two'})
-        self.assertEqual(fs[b'b'], [b'2'])
-        self.assertIsNone(fs.get(b'c'))
-
-        data = {'a': '1', 'b': '2'}
-        fs = _qs(self.request_class('http://www.example.com/', method='GET', formdata=data))
-        self.assertEqual(fs[b'a'], [b'1'])
-        self.assertEqual(fs[b'b'], [b'2'])
+        data = (("a", "one"), ("a", "two"), ("b", "2"))
+        url = self.request_class(
+            "http://www.example.com/?a=0&b=1&c=3#fragment", method="GET", formdata=data
+        ).url.split("#")[0]
+        fs = _qs(self.request_class(url, method="GET", formdata=data))
+        self.assertEqual(set(fs[b"a"]), {b"one", b"two"})
+        self.assertEqual(fs[b"b"], [b"2"])
+        self.assertIsNone(fs.get(b"c"))
+
+        data = {"a": "1", "b": "2"}
+        fs = _qs(
+            self.request_class("http://www.example.com/", method="GET", formdata=data)
+        )
+        self.assertEqual(fs[b"a"], [b"1"])
+        self.assertEqual(fs[b"b"], [b"2"])
 
     def test_default_encoding_bytes(self):
         # using default encoding (utf-8)
-        data = {b'one': b'two', b'price': b'\xc2\xa3 100'}
+        data = {b"one": b"two", b"price": b"\xc2\xa3 100"}
         r2 = self.request_class("http://www.example.com", formdata=data)
-        self.assertEqual(r2.method, 'POST')
-        self.assertEqual(r2.encoding, 'utf-8')
-        self.assertQueryEqual(r2.body, b'price=%C2%A3+100&one=two')
-        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(r2.method, "POST")
+        self.assertEqual(r2.encoding, "utf-8")
+        self.assertQueryEqual(r2.body, b"price=%C2%A3+100&one=two")
+        self.assertEqual(
+            r2.headers[b"Content-Type"], b"application/x-www-form-urlencoded"
+        )
 
     def test_default_encoding_textual_data(self):
         # using default encoding (utf-8)
-        data = {' one': 'two', 'price': ' 100'}
+        data = {" one": "two", "price": " 100"}
         r2 = self.request_class("http://www.example.com", formdata=data)
-        self.assertEqual(r2.method, 'POST')
-        self.assertEqual(r2.encoding, 'utf-8')
-        self.assertQueryEqual(r2.body, b'price=%C2%A3+100&%C2%B5+one=two')
-        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(r2.method, "POST")
+        self.assertEqual(r2.encoding, "utf-8")
+        self.assertQueryEqual(r2.body, b"price=%C2%A3+100&%C2%B5+one=two")
+        self.assertEqual(
+            r2.headers[b"Content-Type"], b"application/x-www-form-urlencoded"
+        )
 
     def test_default_encoding_mixed_data(self):
         # using default encoding (utf-8)
-        data = {'\u00b5one': b'two', b'price\xc2\xa3': '\u00a3 100'}
+        data = {"\u00b5one": b"two", b"price\xc2\xa3": "\u00a3 100"}
         r2 = self.request_class("http://www.example.com", formdata=data)
-        self.assertEqual(r2.method, 'POST')
-        self.assertEqual(r2.encoding, 'utf-8')
-        self.assertQueryEqual(r2.body, b'%C2%B5one=two&price%C2%A3=%C2%A3+100')
-        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(r2.method, "POST")
+        self.assertEqual(r2.encoding, "utf-8")
+        self.assertQueryEqual(r2.body, b"%C2%B5one=two&price%C2%A3=%C2%A3+100")
+        self.assertEqual(
+            r2.headers[b"Content-Type"], b"application/x-www-form-urlencoded"
+        )
 
     def test_custom_encoding_bytes(self):
-        data = {b'\xb5 one': b'two', b'price': b'\xa3 100'}
-        r2 = self.request_class("http://www.example.com", formdata=data, encoding='latin1')
-        self.assertEqual(r2.method, 'POST')
-        self.assertEqual(r2.encoding, 'latin1')
-        self.assertQueryEqual(r2.body, b'price=%A3+100&%B5+one=two')
-        self.assertEqual(r2.headers[b'Content-Type'], b'application/x-www-form-urlencoded')
+        data = {b"\xb5 one": b"two", b"price": b"\xa3 100"}
+        r2 = self.request_class(
+            "http://www.example.com", formdata=data, encoding="latin1"
+        )
+        self.assertEqual(r2.method, "POST")
+        self.assertEqual(r2.encoding, "latin1")
+        self.assertQueryEqual(r2.body, b"price=%A3+100&%B5+one=two")
+        self.assertEqual(
+            r2.headers[b"Content-Type"], b"application/x-www-form-urlencoded"
+        )
 
     def test_custom_encoding_textual_data(self):
-        data = {'price': ' 100'}
-        r3 = self.request_class("http://www.example.com", formdata=data, encoding='latin1')
-        self.assertEqual(r3.encoding, 'latin1')
-        self.assertEqual(r3.body, b'price=%A3+100')
+        data = {"price": " 100"}
+        r3 = self.request_class(
+            "http://www.example.com", formdata=data, encoding="latin1"
+        )
+        self.assertEqual(r3.encoding, "latin1")
+        self.assertEqual(r3.body, b"price=%A3+100")
 
     def test_multi_key_values(self):
         # using multiples values for a single key
-        data = {'price': '\xa3 100', 'colours': ['red', 'blue', 'green']}
+        data = {"price": "\xa3 100", "colours": ["red", "blue", "green"]}
         r3 = self.request_class("http://www.example.com", formdata=data)
-        self.assertQueryEqual(r3.body, b'colours=red&colours=blue&colours=green&price=%C2%A3+100')
+        self.assertQueryEqual(
+            r3.body, b"colours=red&colours=blue&colours=green&price=%C2%A3+100"
+        )
 
     def test_from_response_post(self):
         response = _buildresponse(
             b"""<form action="post.php" method="POST">
             <input type="hidden" name="test" value="val1">
             <input type="hidden" name="test" value="val2">
             <input type="hidden" name="test2" value="xxx">
             </form>""",
-            url="http://www.example.com/this/list.html")
-        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})
+            url="http://www.example.com/this/list.html",
+        )
+        req = self.request_class.from_response(
+            response, formdata={"one": ["two", "three"], "six": "seven"}
+        )
 
-        self.assertEqual(req.method, 'POST')
-        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(req.method, "POST")
+        self.assertEqual(
+            req.headers[b"Content-type"], b"application/x-www-form-urlencoded"
+        )
         self.assertEqual(req.url, "http://www.example.com/this/post.php")
         fs = _qs(req)
-        self.assertEqual(set(fs[b'test']), {b'val1', b'val2'})
-        self.assertEqual(set(fs[b'one']), {b'two', b'three'})
-        self.assertEqual(fs[b'test2'], [b'xxx'])
-        self.assertEqual(fs[b'six'], [b'seven'])
+        self.assertEqual(set(fs[b"test"]), {b"val1", b"val2"})
+        self.assertEqual(set(fs[b"one"]), {b"two", b"three"})
+        self.assertEqual(fs[b"test2"], [b"xxx"])
+        self.assertEqual(fs[b"six"], [b"seven"])
 
     def test_from_response_post_nonascii_bytes_utf8(self):
         response = _buildresponse(
             b"""<form action="post.php" method="POST">
             <input type="hidden" name="test \xc2\xa3" value="val1">
             <input type="hidden" name="test \xc2\xa3" value="val2">
             <input type="hidden" name="test2" value="xxx \xc2\xb5">
             </form>""",
-            url="http://www.example.com/this/list.html")
-        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})
+            url="http://www.example.com/this/list.html",
+        )
+        req = self.request_class.from_response(
+            response, formdata={"one": ["two", "three"], "six": "seven"}
+        )
 
-        self.assertEqual(req.method, 'POST')
-        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(req.method, "POST")
+        self.assertEqual(
+            req.headers[b"Content-type"], b"application/x-www-form-urlencoded"
+        )
         self.assertEqual(req.url, "http://www.example.com/this/post.php")
         fs = _qs(req, to_unicode=True)
-        self.assertEqual(set(fs['test ']), {'val1', 'val2'})
-        self.assertEqual(set(fs['one']), {'two', 'three'})
-        self.assertEqual(fs['test2'], ['xxx '])
-        self.assertEqual(fs['six'], ['seven'])
+        self.assertEqual(set(fs["test "]), {"val1", "val2"})
+        self.assertEqual(set(fs["one"]), {"two", "three"})
+        self.assertEqual(fs["test2"], ["xxx "])
+        self.assertEqual(fs["six"], ["seven"])
 
     def test_from_response_post_nonascii_bytes_latin1(self):
         response = _buildresponse(
             b"""<form action="post.php" method="POST">
             <input type="hidden" name="test \xa3" value="val1">
             <input type="hidden" name="test \xa3" value="val2">
             <input type="hidden" name="test2" value="xxx \xb5">
             </form>""",
             url="http://www.example.com/this/list.html",
-            encoding='latin1',
+            encoding="latin1",
+        )
+        req = self.request_class.from_response(
+            response, formdata={"one": ["two", "three"], "six": "seven"}
         )
-        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})
 
-        self.assertEqual(req.method, 'POST')
-        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(req.method, "POST")
+        self.assertEqual(
+            req.headers[b"Content-type"], b"application/x-www-form-urlencoded"
+        )
         self.assertEqual(req.url, "http://www.example.com/this/post.php")
-        fs = _qs(req, to_unicode=True, encoding='latin1')
-        self.assertEqual(set(fs['test ']), {'val1', 'val2'})
-        self.assertEqual(set(fs['one']), {'two', 'three'})
-        self.assertEqual(fs['test2'], ['xxx '])
-        self.assertEqual(fs['six'], ['seven'])
+        fs = _qs(req, to_unicode=True, encoding="latin1")
+        self.assertEqual(set(fs["test "]), {"val1", "val2"})
+        self.assertEqual(set(fs["one"]), {"two", "three"})
+        self.assertEqual(fs["test2"], ["xxx "])
+        self.assertEqual(fs["six"], ["seven"])
 
     def test_from_response_post_nonascii_unicode(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="test " value="val1">
             <input type="hidden" name="test " value="val2">
             <input type="hidden" name="test2" value="xxx ">
             </form>""",
-            url="http://www.example.com/this/list.html")
-        req = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})
+            url="http://www.example.com/this/list.html",
+        )
+        req = self.request_class.from_response(
+            response, formdata={"one": ["two", "three"], "six": "seven"}
+        )
 
-        self.assertEqual(req.method, 'POST')
-        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(req.method, "POST")
+        self.assertEqual(
+            req.headers[b"Content-type"], b"application/x-www-form-urlencoded"
+        )
         self.assertEqual(req.url, "http://www.example.com/this/post.php")
         fs = _qs(req, to_unicode=True)
-        self.assertEqual(set(fs['test ']), {'val1', 'val2'})
-        self.assertEqual(set(fs['one']), {'two', 'three'})
-        self.assertEqual(fs['test2'], ['xxx '])
-        self.assertEqual(fs['six'], ['seven'])
+        self.assertEqual(set(fs["test "]), {"val1", "val2"})
+        self.assertEqual(set(fs["one"]), {"two", "three"})
+        self.assertEqual(fs["test2"], ["xxx "])
+        self.assertEqual(fs["six"], ["seven"])
 
     def test_from_response_duplicate_form_key(self):
-        response = _buildresponse(
-            '<form></form>',
-            url='http://www.example.com')
+        response = _buildresponse("<form></form>", url="http://www.example.com")
         req = self.request_class.from_response(
             response=response,
-            method='GET',
-            formdata=(('foo', 'bar'), ('foo', 'baz')),
+            method="GET",
+            formdata=(("foo", "bar"), ("foo", "baz")),
         )
-        self.assertEqual(urlparse(req.url).hostname, 'www.example.com')
-        self.assertEqual(urlparse(req.url).query, 'foo=bar&foo=baz')
+        self.assertEqual(urlparse(req.url).hostname, "www.example.com")
+        self.assertEqual(urlparse(req.url).query, "foo=bar&foo=baz")
 
     def test_from_response_override_duplicate_form_key(self):
         response = _buildresponse(
             """<form action="get.php" method="POST">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="3">
-            </form>""")
+            </form>"""
+        )
         req = self.request_class.from_response(
-            response,
-            formdata=(('two', '2'), ('two', '4')))
+            response, formdata=(("two", "2"), ("two", "4"))
+        )
         fs = _qs(req)
-        self.assertEqual(fs[b'one'], [b'1'])
-        self.assertEqual(fs[b'two'], [b'2', b'4'])
+        self.assertEqual(fs[b"one"], [b"1"])
+        self.assertEqual(fs[b"two"], [b"2", b"4"])
 
     def test_from_response_extra_headers(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="test" value="val1">
             <input type="hidden" name="test" value="val2">
             <input type="hidden" name="test2" value="xxx">
-            </form>""")
+            </form>"""
+        )
         req = self.request_class.from_response(
             response=response,
-            formdata={'one': ['two', 'three'], 'six': 'seven'},
+            formdata={"one": ["two", "three"], "six": "seven"},
             headers={"Accept-Encoding": "gzip,deflate"},
         )
-        self.assertEqual(req.method, 'POST')
-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')
-        self.assertEqual(req.headers['Accept-Encoding'], b'gzip,deflate')
+        self.assertEqual(req.method, "POST")
+        self.assertEqual(
+            req.headers["Content-type"], b"application/x-www-form-urlencoded"
+        )
+        self.assertEqual(req.headers["Accept-Encoding"], b"gzip,deflate")
 
     def test_from_response_get(self):
         response = _buildresponse(
             """<form action="get.php" method="GET">
             <input type="hidden" name="test" value="val1">
             <input type="hidden" name="test" value="val2">
             <input type="hidden" name="test2" value="xxx">
             </form>""",
-            url="http://www.example.com/this/list.html")
-        r1 = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})
-        self.assertEqual(r1.method, 'GET')
+            url="http://www.example.com/this/list.html",
+        )
+        r1 = self.request_class.from_response(
+            response, formdata={"one": ["two", "three"], "six": "seven"}
+        )
+        self.assertEqual(r1.method, "GET")
         self.assertEqual(urlparse(r1.url).hostname, "www.example.com")
         self.assertEqual(urlparse(r1.url).path, "/this/get.php")
         fs = _qs(r1)
-        self.assertEqual(set(fs[b'test']), {b'val1', b'val2'})
-        self.assertEqual(set(fs[b'one']), {b'two', b'three'})
-        self.assertEqual(fs[b'test2'], [b'xxx'])
-        self.assertEqual(fs[b'six'], [b'seven'])
+        self.assertEqual(set(fs[b"test"]), {b"val1", b"val2"})
+        self.assertEqual(set(fs[b"one"]), {b"two", b"three"})
+        self.assertEqual(fs[b"test2"], [b"xxx"])
+        self.assertEqual(fs[b"six"], [b"seven"])
 
     def test_from_response_override_params(self):
         response = _buildresponse(
             """<form action="get.php" method="POST">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="3">
-            </form>""")
-        req = self.request_class.from_response(response, formdata={'two': '2'})
+            </form>"""
+        )
+        req = self.request_class.from_response(response, formdata={"two": "2"})
         fs = _qs(req)
-        self.assertEqual(fs[b'one'], [b'1'])
-        self.assertEqual(fs[b'two'], [b'2'])
+        self.assertEqual(fs[b"one"], [b"1"])
+        self.assertEqual(fs[b"two"], [b"2"])
 
     def test_from_response_drop_params(self):
         response = _buildresponse(
             """<form action="get.php" method="POST">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="3">
-            </form>""")
-        req = self.request_class.from_response(response, formdata={'two': None})
+            </form>"""
+        )
+        req = self.request_class.from_response(response, formdata={"two": None})
         fs = _qs(req)
-        self.assertEqual(fs[b'one'], [b'1'])
-        self.assertNotIn(b'two', fs)
+        self.assertEqual(fs[b"one"], [b"1"])
+        self.assertNotIn(b"two", fs)
 
     def test_from_response_override_method(self):
         response = _buildresponse(
-            '''<html><body>
+            """<html><body>
             <form action="/app"></form>
-            </body></html>''')
+            </body></html>"""
+        )
         request = FormRequest.from_response(response)
-        self.assertEqual(request.method, 'GET')
-        request = FormRequest.from_response(response, method='POST')
-        self.assertEqual(request.method, 'POST')
+        self.assertEqual(request.method, "GET")
+        request = FormRequest.from_response(response, method="POST")
+        self.assertEqual(request.method, "POST")
 
     def test_from_response_override_url(self):
         response = _buildresponse(
-            '''<html><body>
+            """<html><body>
             <form action="/app"></form>
-            </body></html>''')
+            </body></html>"""
+        )
         request = FormRequest.from_response(response)
-        self.assertEqual(request.url, 'http://example.com/app')
-        request = FormRequest.from_response(response, url='http://foo.bar/absolute')
-        self.assertEqual(request.url, 'http://foo.bar/absolute')
-        request = FormRequest.from_response(response, url='/relative')
-        self.assertEqual(request.url, 'http://example.com/relative')
+        self.assertEqual(request.url, "http://example.com/app")
+        request = FormRequest.from_response(response, url="http://foo.bar/absolute")
+        self.assertEqual(request.url, "http://foo.bar/absolute")
+        request = FormRequest.from_response(response, url="/relative")
+        self.assertEqual(request.url, "http://example.com/relative")
 
     def test_from_response_case_insensitive(self):
         response = _buildresponse(
             """<form action="get.php" method="GET">
             <input type="SuBmIt" name="clickable1" value="clicked1">
             <input type="iMaGe" name="i1" src="http://my.image.org/1.jpg">
             <input type="submit" name="clickable2" value="clicked2">
-            </form>""")
+            </form>"""
+        )
         req = self.request_class.from_response(response)
         fs = _qs(req)
-        self.assertEqual(fs[b'clickable1'], [b'clicked1'])
-        self.assertFalse(b'i1' in fs, fs)  # xpath in _get_inputs()
-        self.assertFalse(b'clickable2' in fs, fs)  # xpath in _get_clickable()
+        self.assertEqual(fs[b"clickable1"], [b"clicked1"])
+        self.assertFalse(b"i1" in fs, fs)  # xpath in _get_inputs()
+        self.assertFalse(b"clickable2" in fs, fs)  # xpath in _get_clickable()
 
     def test_from_response_submit_first_clickable(self):
         response = _buildresponse(
             """<form action="get.php" method="GET">
             <input type="submit" name="clickable1" value="clicked1">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="3">
             <input type="submit" name="clickable2" value="clicked2">
-            </form>""")
-        req = self.request_class.from_response(response, formdata={'two': '2'})
+            </form>"""
+        )
+        req = self.request_class.from_response(response, formdata={"two": "2"})
         fs = _qs(req)
-        self.assertEqual(fs[b'clickable1'], [b'clicked1'])
-        self.assertFalse(b'clickable2' in fs, fs)
-        self.assertEqual(fs[b'one'], [b'1'])
-        self.assertEqual(fs[b'two'], [b'2'])
+        self.assertEqual(fs[b"clickable1"], [b"clicked1"])
+        self.assertFalse(b"clickable2" in fs, fs)
+        self.assertEqual(fs[b"one"], [b"1"])
+        self.assertEqual(fs[b"two"], [b"2"])
 
     def test_from_response_submit_not_first_clickable(self):
         response = _buildresponse(
             """<form action="get.php" method="GET">
             <input type="submit" name="clickable1" value="clicked1">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="3">
             <input type="submit" name="clickable2" value="clicked2">
-            </form>""")
+            </form>"""
+        )
         req = self.request_class.from_response(
-            response, formdata={'two': '2'}, clickdata={'name': 'clickable2'}
+            response, formdata={"two": "2"}, clickdata={"name": "clickable2"}
         )
         fs = _qs(req)
-        self.assertEqual(fs[b'clickable2'], [b'clicked2'])
-        self.assertFalse(b'clickable1' in fs, fs)
-        self.assertEqual(fs[b'one'], [b'1'])
-        self.assertEqual(fs[b'two'], [b'2'])
+        self.assertEqual(fs[b"clickable2"], [b"clicked2"])
+        self.assertFalse(b"clickable1" in fs, fs)
+        self.assertEqual(fs[b"one"], [b"1"])
+        self.assertEqual(fs[b"two"], [b"2"])
 
     def test_from_response_dont_submit_image_as_input(self):
         response = _buildresponse(
             """<form>
             <input type="hidden" name="i1" value="i1v">
             <input type="image" name="i2" src="http://my.image.org/1.jpg">
             <input type="submit" name="i3" value="i3v">
-            </form>""")
+            </form>"""
+        )
         req = self.request_class.from_response(response, dont_click=True)
         fs = _qs(req)
-        self.assertEqual(fs, {b'i1': [b'i1v']})
+        self.assertEqual(fs, {b"i1": [b"i1v"]})
 
     def test_from_response_dont_submit_reset_as_input(self):
         response = _buildresponse(
             """<form>
             <input type="hidden" name="i1" value="i1v">
             <input type="text" name="i2" value="i2v">
             <input type="reset" name="resetme">
             <input type="submit" name="i3" value="i3v">
-            </form>""")
+            </form>"""
+        )
         req = self.request_class.from_response(response, dont_click=True)
         fs = _qs(req)
-        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b'i2v']})
+        self.assertEqual(fs, {b"i1": [b"i1v"], b"i2": [b"i2v"]})
 
     def test_from_response_clickdata_does_not_ignore_image(self):
         response = _buildresponse(
             """<form>
             <input type="text" name="i1" value="i1v">
             <input id="image" name="i2" type="image" value="i2v" alt="Login" src="http://my.image.org/1.jpg">
-            </form>""")
+            </form>"""
+        )
         req = self.request_class.from_response(response)
         fs = _qs(req)
-        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b'i2v']})
+        self.assertEqual(fs, {b"i1": [b"i1v"], b"i2": [b"i2v"]})
 
     def test_from_response_multiple_clickdata(self):
         response = _buildresponse(
             """<form action="get.php" method="GET">
             <input type="submit" name="clickable" value="clicked1">
             <input type="submit" name="clickable" value="clicked2">
             <input type="hidden" name="one" value="clicked1">
             <input type="hidden" name="two" value="clicked2">
-            </form>""")
+            </form>"""
+        )
         req = self.request_class.from_response(
-            response, clickdata={'name': 'clickable', 'value': 'clicked2'}
+            response, clickdata={"name": "clickable", "value": "clicked2"}
         )
         fs = _qs(req)
-        self.assertEqual(fs[b'clickable'], [b'clicked2'])
-        self.assertEqual(fs[b'one'], [b'clicked1'])
-        self.assertEqual(fs[b'two'], [b'clicked2'])
+        self.assertEqual(fs[b"clickable"], [b"clicked2"])
+        self.assertEqual(fs[b"one"], [b"clicked1"])
+        self.assertEqual(fs[b"two"], [b"clicked2"])
 
     def test_from_response_unicode_clickdata(self):
         response = _buildresponse(
             """<form action="get.php" method="GET">
             <input type="submit" name="price in \u00a3" value="\u00a3 1000">
             <input type="submit" name="price in \u20ac" value="\u20ac 2000">
             <input type="hidden" name="poundsign" value="\u00a3">
             <input type="hidden" name="eurosign" value="\u20ac">
-            </form>""")
+            </form>"""
+        )
         req = self.request_class.from_response(
-            response, clickdata={'name': 'price in \u00a3'}
+            response, clickdata={"name": "price in \u00a3"}
         )
         fs = _qs(req, to_unicode=True)
-        self.assertTrue(fs['price in \u00a3'])
+        self.assertTrue(fs["price in \u00a3"])
 
     def test_from_response_unicode_clickdata_latin1(self):
         response = _buildresponse(
             """<form action="get.php" method="GET">
             <input type="submit" name="price in \u00a3" value="\u00a3 1000">
             <input type="submit" name="price in \u00a5" value="\u00a5 2000">
             <input type="hidden" name="poundsign" value="\u00a3">
             <input type="hidden" name="yensign" value="\u00a5">
             </form>""",
-            encoding='latin1')
+            encoding="latin1",
+        )
         req = self.request_class.from_response(
-            response, clickdata={'name': 'price in \u00a5'}
+            response, clickdata={"name": "price in \u00a5"}
         )
-        fs = _qs(req, to_unicode=True, encoding='latin1')
-        self.assertTrue(fs['price in \u00a5'])
+        fs = _qs(req, to_unicode=True, encoding="latin1")
+        self.assertTrue(fs["price in \u00a5"])
 
     def test_from_response_multiple_forms_clickdata(self):
         response = _buildresponse(
             """<form name="form1">
             <input type="submit" name="clickable" value="clicked1">
             <input type="hidden" name="field1" value="value1">
             </form>
             <form name="form2">
             <input type="submit" name="clickable" value="clicked2">
             <input type="hidden" name="field2" value="value2">
             </form>
-            """)
+            """
+        )
         req = self.request_class.from_response(
-            response, formname='form2', clickdata={'name': 'clickable'}
+            response, formname="form2", clickdata={"name": "clickable"}
         )
         fs = _qs(req)
-        self.assertEqual(fs[b'clickable'], [b'clicked2'])
-        self.assertEqual(fs[b'field2'], [b'value2'])
-        self.assertFalse(b'field1' in fs, fs)
+        self.assertEqual(fs[b"clickable"], [b"clicked2"])
+        self.assertEqual(fs[b"field2"], [b"value2"])
+        self.assertFalse(b"field1" in fs, fs)
 
     def test_from_response_override_clickable(self):
-        response = _buildresponse('''<form><input type="submit" name="clickme" value="one"> </form>''')
+        response = _buildresponse(
+            """<form><input type="submit" name="clickme" value="one"> </form>"""
+        )
         req = self.request_class.from_response(
-            response, formdata={'clickme': 'two'}, clickdata={'name': 'clickme'}
+            response, formdata={"clickme": "two"}, clickdata={"name": "clickme"}
         )
         fs = _qs(req)
-        self.assertEqual(fs[b'clickme'], [b'two'])
+        self.assertEqual(fs[b"clickme"], [b"two"])
 
     def test_from_response_dont_click(self):
         response = _buildresponse(
             """<form action="get.php" method="GET">
             <input type="submit" name="clickable1" value="clicked1">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="3">
             <input type="submit" name="clickable2" value="clicked2">
-            </form>""")
+            </form>"""
+        )
         r1 = self.request_class.from_response(response, dont_click=True)
         fs = _qs(r1)
-        self.assertFalse(b'clickable1' in fs, fs)
-        self.assertFalse(b'clickable2' in fs, fs)
+        self.assertFalse(b"clickable1" in fs, fs)
+        self.assertFalse(b"clickable2" in fs, fs)
 
     def test_from_response_ambiguous_clickdata(self):
         response = _buildresponse(
             """
             <form action="get.php" method="GET">
             <input type="submit" name="clickable1" value="clicked1">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="3">
             <input type="submit" name="clickable2" value="clicked2">
-            </form>""")
-        self.assertRaises(ValueError, self.request_class.from_response,
-                          response, clickdata={'type': 'submit'})
+            </form>"""
+        )
+        self.assertRaises(
+            ValueError,
+            self.request_class.from_response,
+            response,
+            clickdata={"type": "submit"},
+        )
 
     def test_from_response_non_matching_clickdata(self):
         response = _buildresponse(
             """<form>
             <input type="submit" name="clickable" value="clicked">
-            </form>""")
-        self.assertRaises(ValueError, self.request_class.from_response,
-                          response, clickdata={'nonexistent': 'notme'})
+            </form>"""
+        )
+        self.assertRaises(
+            ValueError,
+            self.request_class.from_response,
+            response,
+            clickdata={"nonexistent": "notme"},
+        )
 
     def test_from_response_nr_index_clickdata(self):
         response = _buildresponse(
             """<form>
             <input type="submit" name="clickable1" value="clicked1">
             <input type="submit" name="clickable2" value="clicked2">
             </form>
-            """)
-        req = self.request_class.from_response(response, clickdata={'nr': 1})
+            """
+        )
+        req = self.request_class.from_response(response, clickdata={"nr": 1})
         fs = _qs(req)
-        self.assertIn(b'clickable2', fs)
-        self.assertNotIn(b'clickable1', fs)
+        self.assertIn(b"clickable2", fs)
+        self.assertNotIn(b"clickable1", fs)
 
     def test_from_response_invalid_nr_index_clickdata(self):
         response = _buildresponse(
             """<form>
             <input type="submit" name="clickable" value="clicked">
             </form>
-            """)
-        self.assertRaises(ValueError, self.request_class.from_response,
-                          response, clickdata={'nr': 1})
+            """
+        )
+        self.assertRaises(
+            ValueError, self.request_class.from_response, response, clickdata={"nr": 1}
+        )
 
     def test_from_response_errors_noform(self):
         response = _buildresponse("""<html></html>""")
         self.assertRaises(ValueError, self.request_class.from_response, response)
 
     def test_from_response_invalid_html5(self):
-        response = _buildresponse("""<!DOCTYPE html><body></html><form>"""
-                                  """<input type="text" name="foo" value="xxx">"""
-                                  """</form></body></html>""")
-        req = self.request_class.from_response(response, formdata={'bar': 'buz'})
+        response = _buildresponse(
+            """<!DOCTYPE html><body></html><form>"""
+            """<input type="text" name="foo" value="xxx">"""
+            """</form></body></html>"""
+        )
+        req = self.request_class.from_response(response, formdata={"bar": "buz"})
         fs = _qs(req)
-        self.assertEqual(fs, {b'foo': [b'xxx'], b'bar': [b'buz']})
+        self.assertEqual(fs, {b"foo": [b"xxx"], b"bar": [b"buz"]})
 
     def test_from_response_errors_formnumber(self):
         response = _buildresponse(
             """<form action="get.php" method="GET">
             <input type="hidden" name="test" value="val1">
             <input type="hidden" name="test" value="val2">
             <input type="hidden" name="test2" value="xxx">
-            </form>""")
-        self.assertRaises(IndexError, self.request_class.from_response, response, formnumber=1)
+            </form>"""
+        )
+        self.assertRaises(
+            IndexError, self.request_class.from_response, response, formnumber=1
+        )
 
     def test_from_response_noformname(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="2">
-            </form>""")
-        r1 = self.request_class.from_response(response, formdata={'two': '3'})
-        self.assertEqual(r1.method, 'POST')
-        self.assertEqual(r1.headers['Content-type'], b'application/x-www-form-urlencoded')
+            </form>"""
+        )
+        r1 = self.request_class.from_response(response, formdata={"two": "3"})
+        self.assertEqual(r1.method, "POST")
+        self.assertEqual(
+            r1.headers["Content-type"], b"application/x-www-form-urlencoded"
+        )
         fs = _qs(r1)
-        self.assertEqual(fs, {b'one': [b'1'], b'two': [b'3']})
+        self.assertEqual(fs, {b"one": [b"1"], b"two": [b"3"]})
 
     def test_from_response_formname_exists(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="2">
             </form>
             <form name="form2" action="post.php" method="POST">
             <input type="hidden" name="three" value="3">
             <input type="hidden" name="four" value="4">
-            </form>""")
+            </form>"""
+        )
         r1 = self.request_class.from_response(response, formname="form2")
-        self.assertEqual(r1.method, 'POST')
+        self.assertEqual(r1.method, "POST")
         fs = _qs(r1)
-        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})
+        self.assertEqual(fs, {b"four": [b"4"], b"three": [b"3"]})
 
-    def test_from_response_formname_notexist(self):
+    def test_from_response_formname_nonexistent(self):
         response = _buildresponse(
             """<form name="form1" action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             </form>
             <form name="form2" action="post.php" method="POST">
             <input type="hidden" name="two" value="2">
-            </form>""")
+            </form>"""
+        )
         r1 = self.request_class.from_response(response, formname="form3")
-        self.assertEqual(r1.method, 'POST')
+        self.assertEqual(r1.method, "POST")
         fs = _qs(r1)
-        self.assertEqual(fs, {b'one': [b'1']})
+        self.assertEqual(fs, {b"one": [b"1"]})
 
     def test_from_response_formname_errors_formnumber(self):
         response = _buildresponse(
             """<form name="form1" action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             </form>
             <form name="form2" action="post.php" method="POST">
             <input type="hidden" name="two" value="2">
-            </form>""")
-        self.assertRaises(IndexError, self.request_class.from_response,
-                          response, formname="form3", formnumber=2)
+            </form>"""
+        )
+        self.assertRaises(
+            IndexError,
+            self.request_class.from_response,
+            response,
+            formname="form3",
+            formnumber=2,
+        )
 
     def test_from_response_formid_exists(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="2">
             </form>
             <form id="form2" action="post.php" method="POST">
             <input type="hidden" name="three" value="3">
             <input type="hidden" name="four" value="4">
-            </form>""")
+            </form>"""
+        )
         r1 = self.request_class.from_response(response, formid="form2")
-        self.assertEqual(r1.method, 'POST')
+        self.assertEqual(r1.method, "POST")
         fs = _qs(r1)
-        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})
+        self.assertEqual(fs, {b"four": [b"4"], b"three": [b"3"]})
 
-    def test_from_response_formname_notexists_fallback_formid(self):
+    def test_from_response_formname_nonexistent_fallback_formid(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="2">
             </form>
             <form id="form2" name="form2" action="post.php" method="POST">
             <input type="hidden" name="three" value="3">
             <input type="hidden" name="four" value="4">
-            </form>""")
-        r1 = self.request_class.from_response(response, formname="form3", formid="form2")
-        self.assertEqual(r1.method, 'POST')
+            </form>"""
+        )
+        r1 = self.request_class.from_response(
+            response, formname="form3", formid="form2"
+        )
+        self.assertEqual(r1.method, "POST")
         fs = _qs(r1)
-        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})
+        self.assertEqual(fs, {b"four": [b"4"], b"three": [b"3"]})
 
-    def test_from_response_formid_notexist(self):
+    def test_from_response_formid_nonexistent(self):
         response = _buildresponse(
             """<form id="form1" action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             </form>
             <form id="form2" action="post.php" method="POST">
             <input type="hidden" name="two" value="2">
-            </form>""")
+            </form>"""
+        )
         r1 = self.request_class.from_response(response, formid="form3")
-        self.assertEqual(r1.method, 'POST')
+        self.assertEqual(r1.method, "POST")
         fs = _qs(r1)
-        self.assertEqual(fs, {b'one': [b'1']})
+        self.assertEqual(fs, {b"one": [b"1"]})
 
     def test_from_response_formid_errors_formnumber(self):
         response = _buildresponse(
             """<form id="form1" action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             </form>
             <form id="form2" name="form2" action="post.php" method="POST">
             <input type="hidden" name="two" value="2">
-            </form>""")
-        self.assertRaises(IndexError, self.request_class.from_response,
-                          response, formid="form3", formnumber=2)
+            </form>"""
+        )
+        self.assertRaises(
+            IndexError,
+            self.request_class.from_response,
+            response,
+            formid="form3",
+            formnumber=2,
+        )
 
     def test_from_response_select(self):
         res = _buildresponse(
-            '''<form>
+            """<form>
             <select name="i1">
                 <option value="i1v1">option 1</option>
                 <option value="i1v2" selected>option 2</option>
             </select>
             <select name="i2">
                 <option value="i2v1">option 1</option>
                 <option value="i2v2">option 2</option>
@@ -973,475 +1128,525 @@
             </select>
             <select name="i5" multiple>
                 <option value="i5v1">option 1</option>
                 <option value="i5v2">option 2</option>
             </select>
             <select name="i6"></select>
             <select name="i7"/>
-            </form>''')
+            </form>"""
+        )
         req = self.request_class.from_response(res)
         fs = _qs(req, to_unicode=True)
-        self.assertEqual(fs, {'i1': ['i1v2'], 'i2': ['i2v1'], 'i4': ['i4v2', 'i4v3']})
+        self.assertEqual(fs, {"i1": ["i1v2"], "i2": ["i2v1"], "i4": ["i4v2", "i4v3"]})
 
     def test_from_response_radio(self):
         res = _buildresponse(
-            '''<form>
+            """<form>
             <input type="radio" name="i1" value="i1v1">
             <input type="radio" name="i1" value="iv2" checked>
             <input type="radio" name="i2" checked>
             <input type="radio" name="i2">
             <input type="radio" name="i3" value="i3v1">
             <input type="radio" name="i3">
             <input type="radio" value="i4v1">
             <input type="radio">
-            </form>''')
+            </form>"""
+        )
         req = self.request_class.from_response(res)
         fs = _qs(req)
-        self.assertEqual(fs, {b'i1': [b'iv2'], b'i2': [b'on']})
+        self.assertEqual(fs, {b"i1": [b"iv2"], b"i2": [b"on"]})
 
     def test_from_response_checkbox(self):
         res = _buildresponse(
-            '''<form>
+            """<form>
             <input type="checkbox" name="i1" value="i1v1">
             <input type="checkbox" name="i1" value="iv2" checked>
             <input type="checkbox" name="i2" checked>
             <input type="checkbox" name="i2">
             <input type="checkbox" name="i3" value="i3v1">
             <input type="checkbox" name="i3">
             <input type="checkbox" value="i4v1">
             <input type="checkbox">
-            </form>''')
+            </form>"""
+        )
         req = self.request_class.from_response(res)
         fs = _qs(req)
-        self.assertEqual(fs, {b'i1': [b'iv2'], b'i2': [b'on']})
+        self.assertEqual(fs, {b"i1": [b"iv2"], b"i2": [b"on"]})
 
     def test_from_response_input_text(self):
         res = _buildresponse(
-            '''<form>
+            """<form>
             <input type="text" name="i1" value="i1v1">
             <input type="text" name="i2">
             <input type="text" value="i3v1">
             <input type="text">
             <input name="i4" value="i4v1">
-            </form>''')
+            </form>"""
+        )
         req = self.request_class.from_response(res)
         fs = _qs(req)
-        self.assertEqual(fs, {b'i1': [b'i1v1'], b'i2': [b''], b'i4': [b'i4v1']})
+        self.assertEqual(fs, {b"i1": [b"i1v1"], b"i2": [b""], b"i4": [b"i4v1"]})
 
     def test_from_response_input_hidden(self):
         res = _buildresponse(
-            '''<form>
+            """<form>
             <input type="hidden" name="i1" value="i1v1">
             <input type="hidden" name="i2">
             <input type="hidden" value="i3v1">
             <input type="hidden">
-            </form>''')
+            </form>"""
+        )
         req = self.request_class.from_response(res)
         fs = _qs(req)
-        self.assertEqual(fs, {b'i1': [b'i1v1'], b'i2': [b'']})
+        self.assertEqual(fs, {b"i1": [b"i1v1"], b"i2": [b""]})
 
     def test_from_response_input_textarea(self):
         res = _buildresponse(
-            '''<form>
+            """<form>
             <textarea name="i1">i1v</textarea>
             <textarea name="i2"></textarea>
             <textarea name="i3"/>
             <textarea>i4v</textarea>
-            </form>''')
+            </form>"""
+        )
         req = self.request_class.from_response(res)
         fs = _qs(req)
-        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b''], b'i3': [b'']})
+        self.assertEqual(fs, {b"i1": [b"i1v"], b"i2": [b""], b"i3": [b""]})
 
     def test_from_response_descendants(self):
         res = _buildresponse(
-            '''<form>
+            """<form>
             <div>
               <fieldset>
                 <input type="text" name="i1">
                 <select name="i2">
                     <option value="v1" selected>
                 </select>
               </fieldset>
               <input type="radio" name="i3" value="i3v2" checked>
               <input type="checkbox" name="i4" value="i4v2" checked>
               <textarea name="i5"></textarea>
               <input type="hidden" name="h1" value="h1v">
               </div>
             <input type="hidden" name="h2" value="h2v">
-            </form>''')
+            </form>"""
+        )
         req = self.request_class.from_response(res)
         fs = _qs(req)
-        self.assertEqual(set(fs), {b'h2', b'i2', b'i1', b'i3', b'h1', b'i5', b'i4'})
+        self.assertEqual(set(fs), {b"h2", b"i2", b"i1", b"i3", b"h1", b"i5", b"i4"})
 
     def test_from_response_xpath(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="2">
             </form>
             <form action="post2.php" method="POST">
             <input type="hidden" name="three" value="3">
             <input type="hidden" name="four" value="4">
-            </form>""")
-        r1 = self.request_class.from_response(response, formxpath="//form[@action='post.php']")
+            </form>"""
+        )
+        r1 = self.request_class.from_response(
+            response, formxpath="//form[@action='post.php']"
+        )
         fs = _qs(r1)
-        self.assertEqual(fs[b'one'], [b'1'])
+        self.assertEqual(fs[b"one"], [b"1"])
 
-        r1 = self.request_class.from_response(response, formxpath="//form/input[@name='four']")
+        r1 = self.request_class.from_response(
+            response, formxpath="//form/input[@name='four']"
+        )
         fs = _qs(r1)
-        self.assertEqual(fs[b'three'], [b'3'])
+        self.assertEqual(fs[b"three"], [b"3"])
 
-        self.assertRaises(ValueError, self.request_class.from_response,
-                          response, formxpath="//form/input[@name='abc']")
+        self.assertRaises(
+            ValueError,
+            self.request_class.from_response,
+            response,
+            formxpath="//form/input[@name='abc']",
+        )
 
     def test_from_response_unicode_xpath(self):
         response = _buildresponse(b'<form name="\xd1\x8a"></form>')
-        r = self.request_class.from_response(response, formxpath="//form[@name='\u044a']")
+        r = self.request_class.from_response(
+            response, formxpath="//form[@name='\u044a']"
+        )
         fs = _qs(r)
         self.assertEqual(fs, {})
 
         xpath = "//form[@name='\u03b1']"
-        self.assertRaisesRegex(ValueError, re.escape(xpath),
-                               self.request_class.from_response,
-                               response, formxpath=xpath)
+        self.assertRaisesRegex(
+            ValueError,
+            re.escape(xpath),
+            self.request_class.from_response,
+            response,
+            formxpath=xpath,
+        )
 
     def test_from_response_button_submit(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="test1" value="val1">
             <input type="hidden" name="test2" value="val2">
             <button type="submit" name="button1" value="submit1">Submit</button>
             </form>""",
-            url="http://www.example.com/this/list.html")
+            url="http://www.example.com/this/list.html",
+        )
         req = self.request_class.from_response(response)
-        self.assertEqual(req.method, 'POST')
-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(req.method, "POST")
+        self.assertEqual(
+            req.headers["Content-type"], b"application/x-www-form-urlencoded"
+        )
         self.assertEqual(req.url, "http://www.example.com/this/post.php")
         fs = _qs(req)
-        self.assertEqual(fs[b'test1'], [b'val1'])
-        self.assertEqual(fs[b'test2'], [b'val2'])
-        self.assertEqual(fs[b'button1'], [b'submit1'])
+        self.assertEqual(fs[b"test1"], [b"val1"])
+        self.assertEqual(fs[b"test2"], [b"val2"])
+        self.assertEqual(fs[b"button1"], [b"submit1"])
 
     def test_from_response_button_notype(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="test1" value="val1">
             <input type="hidden" name="test2" value="val2">
             <button name="button1" value="submit1">Submit</button>
             </form>""",
-            url="http://www.example.com/this/list.html")
+            url="http://www.example.com/this/list.html",
+        )
         req = self.request_class.from_response(response)
-        self.assertEqual(req.method, 'POST')
-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(req.method, "POST")
+        self.assertEqual(
+            req.headers["Content-type"], b"application/x-www-form-urlencoded"
+        )
         self.assertEqual(req.url, "http://www.example.com/this/post.php")
         fs = _qs(req)
-        self.assertEqual(fs[b'test1'], [b'val1'])
-        self.assertEqual(fs[b'test2'], [b'val2'])
-        self.assertEqual(fs[b'button1'], [b'submit1'])
+        self.assertEqual(fs[b"test1"], [b"val1"])
+        self.assertEqual(fs[b"test2"], [b"val2"])
+        self.assertEqual(fs[b"button1"], [b"submit1"])
 
     def test_from_response_submit_novalue(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="test1" value="val1">
             <input type="hidden" name="test2" value="val2">
             <input type="submit" name="button1">Submit</button>
             </form>""",
-            url="http://www.example.com/this/list.html")
+            url="http://www.example.com/this/list.html",
+        )
         req = self.request_class.from_response(response)
-        self.assertEqual(req.method, 'POST')
-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(req.method, "POST")
+        self.assertEqual(
+            req.headers["Content-type"], b"application/x-www-form-urlencoded"
+        )
         self.assertEqual(req.url, "http://www.example.com/this/post.php")
         fs = _qs(req)
-        self.assertEqual(fs[b'test1'], [b'val1'])
-        self.assertEqual(fs[b'test2'], [b'val2'])
-        self.assertEqual(fs[b'button1'], [b''])
+        self.assertEqual(fs[b"test1"], [b"val1"])
+        self.assertEqual(fs[b"test2"], [b"val2"])
+        self.assertEqual(fs[b"button1"], [b""])
 
     def test_from_response_button_novalue(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="test1" value="val1">
             <input type="hidden" name="test2" value="val2">
             <button type="submit" name="button1">Submit</button>
             </form>""",
-            url="http://www.example.com/this/list.html")
+            url="http://www.example.com/this/list.html",
+        )
         req = self.request_class.from_response(response)
-        self.assertEqual(req.method, 'POST')
-        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')
+        self.assertEqual(req.method, "POST")
+        self.assertEqual(
+            req.headers["Content-type"], b"application/x-www-form-urlencoded"
+        )
         self.assertEqual(req.url, "http://www.example.com/this/post.php")
         fs = _qs(req)
-        self.assertEqual(fs[b'test1'], [b'val1'])
-        self.assertEqual(fs[b'test2'], [b'val2'])
-        self.assertEqual(fs[b'button1'], [b''])
+        self.assertEqual(fs[b"test1"], [b"val1"])
+        self.assertEqual(fs[b"test2"], [b"val2"])
+        self.assertEqual(fs[b"button1"], [b""])
 
     def test_html_base_form_action(self):
         response = _buildresponse(
             """
             <html>
                 <head>
                     <base href=" http://b.com/">
                 </head>
                 <body>
                     <form action="test_form">
                     </form>
                 </body>
             </html>
             """,
-            url='http://a.com/'
+            url="http://a.com/",
         )
         req = self.request_class.from_response(response)
-        self.assertEqual(req.url, 'http://b.com/test_form')
+        self.assertEqual(req.url, "http://b.com/test_form")
 
     def test_spaces_in_action(self):
         resp = _buildresponse('<body><form action=" path\n"></form></body>')
         req = self.request_class.from_response(resp)
-        self.assertEqual(req.url, 'http://example.com/path')
+        self.assertEqual(req.url, "http://example.com/path")
 
     def test_from_response_css(self):
         response = _buildresponse(
             """<form action="post.php" method="POST">
             <input type="hidden" name="one" value="1">
             <input type="hidden" name="two" value="2">
             </form>
             <form action="post2.php" method="POST">
             <input type="hidden" name="three" value="3">
             <input type="hidden" name="four" value="4">
-            </form>""")
-        r1 = self.request_class.from_response(response, formcss="form[action='post.php']")
+            </form>"""
+        )
+        r1 = self.request_class.from_response(
+            response, formcss="form[action='post.php']"
+        )
         fs = _qs(r1)
-        self.assertEqual(fs[b'one'], [b'1'])
+        self.assertEqual(fs[b"one"], [b"1"])
 
         r1 = self.request_class.from_response(response, formcss="input[name='four']")
         fs = _qs(r1)
-        self.assertEqual(fs[b'three'], [b'3'])
+        self.assertEqual(fs[b"three"], [b"3"])
 
-        self.assertRaises(ValueError, self.request_class.from_response,
-                          response, formcss="input[name='abc']")
+        self.assertRaises(
+            ValueError,
+            self.request_class.from_response,
+            response,
+            formcss="input[name='abc']",
+        )
 
     def test_from_response_valid_form_methods(self):
-        form_methods = [[method, method] for method in self.request_class.valid_form_methods]
-        form_methods.append(['UNKNOWN', 'GET'])
+        form_methods = [
+            [method, method] for method in self.request_class.valid_form_methods
+        ]
+        form_methods.append(["UNKNOWN", "GET"])
 
         for method, expected in form_methods:
             response = _buildresponse(
                 f'<form action="post.php" method="{method}">'
                 '<input type="hidden" name="one" value="1">'
-                '</form>'
+                "</form>"
             )
             r = self.request_class.from_response(response)
             self.assertEqual(r.method, expected)
 
 
 def _buildresponse(body, **kwargs):
-    kwargs.setdefault('body', body)
-    kwargs.setdefault('url', 'http://example.com')
-    kwargs.setdefault('encoding', 'utf-8')
+    kwargs.setdefault("body", body)
+    kwargs.setdefault("url", "http://example.com")
+    kwargs.setdefault("encoding", "utf-8")
     return HtmlResponse(**kwargs)
 
 
-def _qs(req, encoding='utf-8', to_unicode=False):
-    if req.method == 'POST':
+def _qs(req, encoding="utf-8", to_unicode=False):
+    if req.method == "POST":
         qs = req.body
     else:
-        qs = req.url.partition('?')[2]
+        qs = req.url.partition("?")[2]
     uqs = unquote_to_bytes(qs)
     if to_unicode:
         uqs = uqs.decode(encoding)
     return parse_qs(uqs, True)
 
 
 class XmlRpcRequestTest(RequestTest):
 
     request_class = XmlRpcRequest
-    default_method = 'POST'
-    default_headers = {b'Content-Type': [b'text/xml']}
+    default_method = "POST"
+    default_headers = {b"Content-Type": [b"text/xml"]}
 
     def _test_request(self, **kwargs):
-        r = self.request_class('http://scrapytest.org/rpc2', **kwargs)
-        self.assertEqual(r.headers[b'Content-Type'], b'text/xml')
-        self.assertEqual(r.body,
-                         to_bytes(xmlrpc.client.dumps(**kwargs),
-                                  encoding=kwargs.get('encoding', 'utf-8')))
-        self.assertEqual(r.method, 'POST')
-        self.assertEqual(r.encoding, kwargs.get('encoding', 'utf-8'))
+        r = self.request_class("http://scrapytest.org/rpc2", **kwargs)
+        self.assertEqual(r.headers[b"Content-Type"], b"text/xml")
+        self.assertEqual(
+            r.body,
+            to_bytes(
+                xmlrpc.client.dumps(**kwargs), encoding=kwargs.get("encoding", "utf-8")
+            ),
+        )
+        self.assertEqual(r.method, "POST")
+        self.assertEqual(r.encoding, kwargs.get("encoding", "utf-8"))
         self.assertTrue(r.dont_filter, True)
 
     def test_xmlrpc_dumps(self):
-        self._test_request(params=('value',))
-        self._test_request(params=('username', 'password'), methodname='login')
-        self._test_request(params=('response', ), methodresponse='login')
-        self._test_request(params=('pas',), encoding='utf-8')
+        self._test_request(params=("value",))
+        self._test_request(params=("username", "password"), methodname="login")
+        self._test_request(params=("response",), methodresponse="login")
+        self._test_request(params=("pas",), encoding="utf-8")
         self._test_request(params=(None,), allow_none=1)
         self.assertRaises(TypeError, self._test_request)
         self.assertRaises(TypeError, self._test_request, params=(None,))
 
     def test_latin1(self):
-        self._test_request(params=('pas',), encoding='latin1')
+        self._test_request(params=("pas",), encoding="latin1")
 
 
 class JsonRequestTest(RequestTest):
     request_class = JsonRequest
-    default_method = 'GET'
+    default_method = "GET"
     default_headers = {
-        b'Content-Type': [b'application/json'],
-        b'Accept': [b'application/json, text/javascript, */*; q=0.01'],
+        b"Content-Type": [b"application/json"],
+        b"Accept": [b"application/json, text/javascript, */*; q=0.01"],
     }
 
     def setUp(self):
         warnings.simplefilter("always")
         super().setUp()
 
     def test_data(self):
         r1 = self.request_class(url="http://www.example.com/")
-        self.assertEqual(r1.body, b'')
+        self.assertEqual(r1.body, b"")
 
-        body = b'body'
+        body = b"body"
         r2 = self.request_class(url="http://www.example.com/", body=body)
         self.assertEqual(r2.body, body)
 
         data = {
-            'name': 'value',
+            "name": "value",
         }
         r3 = self.request_class(url="http://www.example.com/", data=data)
         self.assertEqual(r3.body, to_bytes(json.dumps(data)))
 
         # empty data
         r4 = self.request_class(url="http://www.example.com/", data=[])
         self.assertEqual(r4.body, to_bytes(json.dumps([])))
 
     def test_data_method(self):
         # data is not passed
         r1 = self.request_class(url="http://www.example.com/")
-        self.assertEqual(r1.method, 'GET')
+        self.assertEqual(r1.method, "GET")
 
-        body = b'body'
+        body = b"body"
         r2 = self.request_class(url="http://www.example.com/", body=body)
-        self.assertEqual(r2.method, 'GET')
+        self.assertEqual(r2.method, "GET")
 
         data = {
-            'name': 'value',
+            "name": "value",
         }
         r3 = self.request_class(url="http://www.example.com/", data=data)
-        self.assertEqual(r3.method, 'POST')
+        self.assertEqual(r3.method, "POST")
 
         # method passed explicitly
-        r4 = self.request_class(url="http://www.example.com/", data=data, method='GET')
-        self.assertEqual(r4.method, 'GET')
+        r4 = self.request_class(url="http://www.example.com/", data=data, method="GET")
+        self.assertEqual(r4.method, "GET")
 
         r5 = self.request_class(url="http://www.example.com/", data=[])
-        self.assertEqual(r5.method, 'POST')
+        self.assertEqual(r5.method, "POST")
 
     def test_body_data(self):
-        """ passing both body and data should result a warning """
-        body = b'body'
+        """passing both body and data should result a warning"""
+        body = b"body"
         data = {
-            'name': 'value',
+            "name": "value",
         }
         with warnings.catch_warnings(record=True) as _warnings:
             r5 = self.request_class(url="http://www.example.com/", body=body, data=data)
             self.assertEqual(r5.body, body)
-            self.assertEqual(r5.method, 'GET')
+            self.assertEqual(r5.method, "GET")
             self.assertEqual(len(_warnings), 1)
-            self.assertIn('data will be ignored', str(_warnings[0].message))
+            self.assertIn("data will be ignored", str(_warnings[0].message))
 
     def test_empty_body_data(self):
-        """ passing any body value and data should result a warning """
+        """passing any body value and data should result a warning"""
         data = {
-            'name': 'value',
+            "name": "value",
         }
         with warnings.catch_warnings(record=True) as _warnings:
-            r6 = self.request_class(url="http://www.example.com/", body=b'', data=data)
-            self.assertEqual(r6.body, b'')
-            self.assertEqual(r6.method, 'GET')
+            r6 = self.request_class(url="http://www.example.com/", body=b"", data=data)
+            self.assertEqual(r6.body, b"")
+            self.assertEqual(r6.method, "GET")
             self.assertEqual(len(_warnings), 1)
-            self.assertIn('data will be ignored', str(_warnings[0].message))
+            self.assertIn("data will be ignored", str(_warnings[0].message))
 
     def test_body_none_data(self):
         data = {
-            'name': 'value',
+            "name": "value",
         }
         with warnings.catch_warnings(record=True) as _warnings:
             r7 = self.request_class(url="http://www.example.com/", body=None, data=data)
             self.assertEqual(r7.body, to_bytes(json.dumps(data)))
-            self.assertEqual(r7.method, 'POST')
+            self.assertEqual(r7.method, "POST")
             self.assertEqual(len(_warnings), 0)
 
     def test_body_data_none(self):
         with warnings.catch_warnings(record=True) as _warnings:
             r8 = self.request_class(url="http://www.example.com/", body=None, data=None)
-            self.assertEqual(r8.method, 'GET')
+            self.assertEqual(r8.method, "GET")
             self.assertEqual(len(_warnings), 0)
 
     def test_dumps_sort_keys(self):
-        """ Test that sort_keys=True is passed to json.dumps by default """
+        """Test that sort_keys=True is passed to json.dumps by default"""
         data = {
-            'name': 'value',
+            "name": "value",
         }
-        with mock.patch('json.dumps', return_value=b'') as mock_dumps:
+        with mock.patch("json.dumps", return_value=b"") as mock_dumps:
             self.request_class(url="http://www.example.com/", data=data)
             kwargs = mock_dumps.call_args[1]
-            self.assertEqual(kwargs['sort_keys'], True)
+            self.assertEqual(kwargs["sort_keys"], True)
 
     def test_dumps_kwargs(self):
-        """ Test that dumps_kwargs are passed to json.dumps """
+        """Test that dumps_kwargs are passed to json.dumps"""
         data = {
-            'name': 'value',
+            "name": "value",
         }
         dumps_kwargs = {
-            'ensure_ascii': True,
-            'allow_nan': True,
+            "ensure_ascii": True,
+            "allow_nan": True,
         }
-        with mock.patch('json.dumps', return_value=b'') as mock_dumps:
-            self.request_class(url="http://www.example.com/", data=data, dumps_kwargs=dumps_kwargs)
+        with mock.patch("json.dumps", return_value=b"") as mock_dumps:
+            self.request_class(
+                url="http://www.example.com/", data=data, dumps_kwargs=dumps_kwargs
+            )
             kwargs = mock_dumps.call_args[1]
-            self.assertEqual(kwargs['ensure_ascii'], True)
-            self.assertEqual(kwargs['allow_nan'], True)
+            self.assertEqual(kwargs["ensure_ascii"], True)
+            self.assertEqual(kwargs["allow_nan"], True)
 
     def test_replace_data(self):
         data1 = {
-            'name1': 'value1',
+            "name1": "value1",
         }
         data2 = {
-            'name2': 'value2',
+            "name2": "value2",
         }
         r1 = self.request_class(url="http://www.example.com/", data=data1)
         r2 = r1.replace(data=data2)
         self.assertEqual(r2.body, to_bytes(json.dumps(data2)))
 
     def test_replace_sort_keys(self):
-        """ Test that replace provides sort_keys=True to json.dumps """
+        """Test that replace provides sort_keys=True to json.dumps"""
         data1 = {
-            'name1': 'value1',
+            "name1": "value1",
         }
         data2 = {
-            'name2': 'value2',
+            "name2": "value2",
         }
         r1 = self.request_class(url="http://www.example.com/", data=data1)
-        with mock.patch('json.dumps', return_value=b'') as mock_dumps:
+        with mock.patch("json.dumps", return_value=b"") as mock_dumps:
             r1.replace(data=data2)
             kwargs = mock_dumps.call_args[1]
-            self.assertEqual(kwargs['sort_keys'], True)
+            self.assertEqual(kwargs["sort_keys"], True)
 
     def test_replace_dumps_kwargs(self):
-        """ Test that dumps_kwargs are provided to json.dumps when replace is called """
+        """Test that dumps_kwargs are provided to json.dumps when replace is called"""
         data1 = {
-            'name1': 'value1',
+            "name1": "value1",
         }
         data2 = {
-            'name2': 'value2',
+            "name2": "value2",
         }
         dumps_kwargs = {
-            'ensure_ascii': True,
-            'allow_nan': True,
+            "ensure_ascii": True,
+            "allow_nan": True,
         }
-        r1 = self.request_class(url="http://www.example.com/", data=data1, dumps_kwargs=dumps_kwargs)
-        with mock.patch('json.dumps', return_value=b'') as mock_dumps:
+        r1 = self.request_class(
+            url="http://www.example.com/", data=data1, dumps_kwargs=dumps_kwargs
+        )
+        with mock.patch("json.dumps", return_value=b"") as mock_dumps:
             r1.replace(data=data2)
             kwargs = mock_dumps.call_args[1]
-            self.assertEqual(kwargs['ensure_ascii'], True)
-            self.assertEqual(kwargs['allow_nan'], True)
+            self.assertEqual(kwargs["ensure_ascii"], True)
+            self.assertEqual(kwargs["allow_nan"], True)
 
     def tearDown(self):
         warnings.resetwarnings()
         super().tearDown()
 
 
 if __name__ == "__main__":
```

### Comparing `Scrapy-2.7.1/tests/test_http_response.py` & `Scrapy-2.8.0/tests/test_http_response.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,38 +1,62 @@
 import codecs
 import unittest
 from unittest import mock
 
+from packaging.version import Version as parse_version
+from pytest import mark
+from w3lib import __version__ as w3lib_version
 from w3lib.encoding import resolve_encoding
 
-from scrapy.http import (Request, Response, TextResponse, HtmlResponse,
-                         XmlResponse, Headers)
-from scrapy.selector import Selector
-from scrapy.utils.python import to_unicode
 from scrapy.exceptions import NotSupported
+from scrapy.http import (
+    Headers,
+    HtmlResponse,
+    Request,
+    Response,
+    TextResponse,
+    XmlResponse,
+)
 from scrapy.link import Link
+from scrapy.selector import Selector
+from scrapy.utils.python import to_unicode
 from tests import get_testdata
 
 
 class BaseResponseTest(unittest.TestCase):
 
     response_class = Response
 
     def test_init(self):
         # Response requires url in the constructor
         self.assertRaises(Exception, self.response_class)
-        self.assertTrue(isinstance(self.response_class('http://example.com/'), self.response_class))
+        self.assertTrue(
+            isinstance(self.response_class("http://example.com/"), self.response_class)
+        )
         self.assertRaises(TypeError, self.response_class, b"http://example.com")
         # body can be str or None
-        self.assertTrue(isinstance(self.response_class('http://example.com/', body=b''), self.response_class))
-        self.assertTrue(isinstance(self.response_class('http://example.com/', body=b'body'), self.response_class))
+        self.assertTrue(
+            isinstance(
+                self.response_class("http://example.com/", body=b""),
+                self.response_class,
+            )
+        )
+        self.assertTrue(
+            isinstance(
+                self.response_class("http://example.com/", body=b"body"),
+                self.response_class,
+            )
+        )
         # test presence of all optional parameters
         self.assertTrue(
             isinstance(
-                self.response_class('http://example.com/', body=b'', headers={}, status=200), self.response_class
+                self.response_class(
+                    "http://example.com/", body=b"", headers={}, status=200
+                ),
+                self.response_class,
             )
         )
 
         r = self.response_class("http://www.example.com")
         assert isinstance(r.url, str)
         self.assertEqual(r.url, "http://www.example.com")
         self.assertEqual(r.status, 200)
@@ -45,84 +69,97 @@
         r = self.response_class("http://www.example.com", headers=headers, body=body)
 
         assert r.headers is not headers
         self.assertEqual(r.headers[b"foo"], b"bar")
 
         r = self.response_class("http://www.example.com", status=301)
         self.assertEqual(r.status, 301)
-        r = self.response_class("http://www.example.com", status='301')
+        r = self.response_class("http://www.example.com", status="301")
         self.assertEqual(r.status, 301)
-        self.assertRaises(ValueError, self.response_class, "http://example.com", status='lala200')
+        self.assertRaises(
+            ValueError,
+            self.response_class,
+            "http://example.com",
+            status="lala200",
+        )
 
     def test_copy(self):
         """Test Response copy"""
 
         r1 = self.response_class("http://www.example.com", body=b"Some body")
-        r1.flags.append('cached')
+        r1.flags.append("cached")
         r2 = r1.copy()
 
         self.assertEqual(r1.status, r2.status)
         self.assertEqual(r1.body, r2.body)
 
         # make sure flags list is shallow copied
         assert r1.flags is not r2.flags, "flags must be a shallow copy, not identical"
         self.assertEqual(r1.flags, r2.flags)
 
         # make sure headers attribute is shallow copied
-        assert r1.headers is not r2.headers, "headers must be a shallow copy, not identical"
+        assert (
+            r1.headers is not r2.headers
+        ), "headers must be a shallow copy, not identical"
         self.assertEqual(r1.headers, r2.headers)
 
     def test_copy_meta(self):
         req = Request("http://www.example.com")
-        req.meta['foo'] = 'bar'
-        r1 = self.response_class("http://www.example.com", body=b"Some body", request=req)
+        req.meta["foo"] = "bar"
+        r1 = self.response_class(
+            "http://www.example.com", body=b"Some body", request=req
+        )
         assert r1.meta is req.meta
 
     def test_copy_cb_kwargs(self):
         req = Request("http://www.example.com")
-        req.cb_kwargs['foo'] = 'bar'
-        r1 = self.response_class("http://www.example.com", body=b"Some body", request=req)
+        req.cb_kwargs["foo"] = "bar"
+        r1 = self.response_class(
+            "http://www.example.com", body=b"Some body", request=req
+        )
         assert r1.cb_kwargs is req.cb_kwargs
 
     def test_unavailable_meta(self):
         r1 = self.response_class("http://www.example.com", body=b"Some body")
-        with self.assertRaisesRegex(AttributeError, r'Response\.meta not available'):
+        with self.assertRaisesRegex(AttributeError, r"Response\.meta not available"):
             r1.meta
 
     def test_unavailable_cb_kwargs(self):
         r1 = self.response_class("http://www.example.com", body=b"Some body")
-        with self.assertRaisesRegex(AttributeError, r'Response\.cb_kwargs not available'):
+        with self.assertRaisesRegex(
+            AttributeError, r"Response\.cb_kwargs not available"
+        ):
             r1.cb_kwargs
 
     def test_copy_inherited_classes(self):
         """Test Response children copies preserve their class"""
 
         class CustomResponse(self.response_class):
             pass
 
-        r1 = CustomResponse('http://www.example.com')
+        r1 = CustomResponse("http://www.example.com")
         r2 = r1.copy()
 
-        assert type(r2) is CustomResponse
+        assert isinstance(r2, CustomResponse)
 
     def test_replace(self):
         """Test Response.replace() method"""
         hdrs = Headers({"key": "value"})
         r1 = self.response_class("http://www.example.com")
         r2 = r1.replace(status=301, body=b"New body", headers=hdrs)
-        assert r1.body == b''
+        assert r1.body == b""
         self.assertEqual(r1.url, r2.url)
         self.assertEqual((r1.status, r2.status), (200, 301))
-        self.assertEqual((r1.body, r2.body), (b'', b"New body"))
+        self.assertEqual((r1.body, r2.body), (b"", b"New body"))
         self.assertEqual((r1.headers, r2.headers), ({}, hdrs))
 
         # Empty attributes (which may fail if not compared properly)
-        r3 = self.response_class("http://www.example.com", flags=['cached'])
-        r4 = r3.replace(body=b'', flags=[])
-        self.assertEqual(r4.body, b'')
+        r3 = self.response_class("http://www.example.com", flags=["cached"])
+        r4 = r3.replace(body=b"", flags=[])
+        self.assertEqual(r4.body, b"")
         self.assertEqual(r4.flags, [])
 
     def _assert_response_values(self, response, encoding, body):
         if isinstance(body, str):
             body_unicode = body
             body_bytes = body.encode(encoding)
         else:
@@ -136,89 +173,102 @@
         self.assertEqual(response.text, body_unicode)
 
     def _assert_response_encoding(self, response, encoding):
         self.assertEqual(response.encoding, resolve_encoding(encoding))
 
     def test_immutable_attributes(self):
         r = self.response_class("http://example.com")
-        self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')
-        self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')
+        self.assertRaises(AttributeError, setattr, r, "url", "http://example2.com")
+        self.assertRaises(AttributeError, setattr, r, "body", "xxx")
 
     def test_urljoin(self):
         """Test urljoin shortcut (only for existence, since behavior equals urljoin)"""
-        joined = self.response_class('http://www.example.com').urljoin('/test')
-        absolute = 'http://www.example.com/test'
+        joined = self.response_class("http://www.example.com").urljoin("/test")
+        absolute = "http://www.example.com/test"
         self.assertEqual(joined, absolute)
 
     def test_shortcut_attributes(self):
-        r = self.response_class("http://example.com", body=b'hello')
+        r = self.response_class("http://example.com", body=b"hello")
         if self.response_class == Response:
             msg = "Response content isn't text"
-            self.assertRaisesRegex(AttributeError, msg, getattr, r, 'text')
-            self.assertRaisesRegex(NotSupported, msg, r.css, 'body')
-            self.assertRaisesRegex(NotSupported, msg, r.xpath, '//body')
+            self.assertRaisesRegex(AttributeError, msg, getattr, r, "text")
+            self.assertRaisesRegex(NotSupported, msg, r.css, "body")
+            self.assertRaisesRegex(NotSupported, msg, r.xpath, "//body")
         else:
             r.text
-            r.css('body')
-            r.xpath('//body')
+            r.css("body")
+            r.xpath("//body")
 
     # Response.follow
 
     def test_follow_url_absolute(self):
-        self._assert_followed_url('http://foo.example.com',
-                                  'http://foo.example.com')
+        self._assert_followed_url("http://foo.example.com", "http://foo.example.com")
 
     def test_follow_url_relative(self):
-        self._assert_followed_url('foo',
-                                  'http://example.com/foo')
+        self._assert_followed_url("foo", "http://example.com/foo")
 
     def test_follow_link(self):
-        self._assert_followed_url(Link('http://example.com/foo'),
-                                  'http://example.com/foo')
+        self._assert_followed_url(
+            Link("http://example.com/foo"), "http://example.com/foo"
+        )
 
     def test_follow_None_url(self):
         r = self.response_class("http://example.com")
         self.assertRaises(ValueError, r.follow, None)
 
+    @mark.xfail(
+        parse_version(w3lib_version) < parse_version("2.1.1"),
+        reason="https://github.com/scrapy/w3lib/pull/207",
+        strict=True,
+    )
     def test_follow_whitespace_url(self):
-        self._assert_followed_url('foo ',
-                                  'http://example.com/foo%20')
+        self._assert_followed_url("foo ", "http://example.com/foo")
 
+    @mark.xfail(
+        parse_version(w3lib_version) < parse_version("2.1.1"),
+        reason="https://github.com/scrapy/w3lib/pull/207",
+        strict=True,
+    )
     def test_follow_whitespace_link(self):
-        self._assert_followed_url(Link('http://example.com/foo '),
-                                  'http://example.com/foo%20')
+        self._assert_followed_url(
+            Link("http://example.com/foo "), "http://example.com/foo"
+        )
 
     def test_follow_flags(self):
-        res = self.response_class('http://example.com/')
-        fol = res.follow('http://example.com/', flags=['cached', 'allowed'])
-        self.assertEqual(fol.flags, ['cached', 'allowed'])
+        res = self.response_class("http://example.com/")
+        fol = res.follow("http://example.com/", flags=["cached", "allowed"])
+        self.assertEqual(fol.flags, ["cached", "allowed"])
 
     # Response.follow_all
 
     def test_follow_all_absolute(self):
-        url_list = ['http://example.org', 'http://www.example.org',
-                    'http://example.com', 'http://www.example.com']
+        url_list = [
+            "http://example.org",
+            "http://www.example.org",
+            "http://example.com",
+            "http://www.example.com",
+        ]
         self._assert_followed_all_urls(url_list, url_list)
 
     def test_follow_all_relative(self):
-        relative = ['foo', 'bar', 'foo/bar', 'bar/foo']
+        relative = ["foo", "bar", "foo/bar", "bar/foo"]
         absolute = [
-            'http://example.com/foo',
-            'http://example.com/bar',
-            'http://example.com/foo/bar',
-            'http://example.com/bar/foo',
+            "http://example.com/foo",
+            "http://example.com/bar",
+            "http://example.com/foo/bar",
+            "http://example.com/bar/foo",
         ]
         self._assert_followed_all_urls(relative, absolute)
 
     def test_follow_all_links(self):
         absolute = [
-            'http://example.com/foo',
-            'http://example.com/bar',
-            'http://example.com/foo/bar',
-            'http://example.com/bar/foo',
+            "http://example.com/foo",
+            "http://example.com/bar",
+            "http://example.com/foo/bar",
+            "http://example.com/bar/foo",
         ]
         links = map(Link, absolute)
         self._assert_followed_all_urls(links, absolute)
 
     def test_follow_all_empty(self):
         r = self.response_class("http://example.com")
         self.assertEqual([], list(r.follow_all([])))
@@ -237,44 +287,44 @@
                 list(r.follow_all(urls=None))
             with self.assertRaises(TypeError):
                 list(r.follow_all(urls=12345))
             with self.assertRaises(ValueError):
                 list(r.follow_all(urls=[None]))
 
     def test_follow_all_whitespace(self):
-        relative = ['foo ', 'bar ', 'foo/bar ', 'bar/foo ']
+        relative = ["foo ", "bar ", "foo/bar ", "bar/foo "]
         absolute = [
-            'http://example.com/foo%20',
-            'http://example.com/bar%20',
-            'http://example.com/foo/bar%20',
-            'http://example.com/bar/foo%20',
+            "http://example.com/foo%20",
+            "http://example.com/bar%20",
+            "http://example.com/foo/bar%20",
+            "http://example.com/bar/foo%20",
         ]
         self._assert_followed_all_urls(relative, absolute)
 
     def test_follow_all_whitespace_links(self):
         absolute = [
-            'http://example.com/foo ',
-            'http://example.com/bar ',
-            'http://example.com/foo/bar ',
-            'http://example.com/bar/foo ',
+            "http://example.com/foo ",
+            "http://example.com/bar ",
+            "http://example.com/foo/bar ",
+            "http://example.com/bar/foo ",
         ]
         links = map(Link, absolute)
-        expected = [u.replace(' ', '%20') for u in absolute]
+        expected = [u.replace(" ", "%20") for u in absolute]
         self._assert_followed_all_urls(links, expected)
 
     def test_follow_all_flags(self):
-        re = self.response_class('http://www.example.com/')
+        re = self.response_class("http://www.example.com/")
         urls = [
-            'http://www.example.com/',
-            'http://www.example.com/2',
-            'http://www.example.com/foo',
+            "http://www.example.com/",
+            "http://www.example.com/2",
+            "http://www.example.com/foo",
         ]
-        fol = re.follow_all(urls, flags=['cached', 'allowed'])
+        fol = re.follow_all(urls, flags=["cached", "allowed"])
         for req in fol:
-            self.assertEqual(req.flags, ['cached', 'allowed'])
+            self.assertEqual(req.flags, ["cached", "allowed"])
 
     def _assert_followed_url(self, follow_obj, target_url, response=None):
         if response is None:
             response = self._links_response()
         req = response.follow(follow_obj)
         self.assertEqual(req.url, target_url)
         return req
@@ -284,31 +334,33 @@
             response = self._links_response()
         followed = response.follow_all(follow_obj)
         for req, target in zip(followed, target_urls):
             self.assertEqual(req.url, target)
             yield req
 
     def _links_response(self):
-        body = get_testdata('link_extractor', 'linkextractor.html')
-        resp = self.response_class('http://example.com/index', body=body)
+        body = get_testdata("link_extractor", "linkextractor.html")
+        resp = self.response_class("http://example.com/index", body=body)
         return resp
 
     def _links_response_no_href(self):
-        body = get_testdata('link_extractor', 'linkextractor_no_href.html')
-        resp = self.response_class('http://example.com/index', body=body)
+        body = get_testdata("link_extractor", "linkextractor_no_href.html")
+        resp = self.response_class("http://example.com/index", body=body)
         return resp
 
 
 class TextResponseTest(BaseResponseTest):
 
     response_class = TextResponse
 
     def test_replace(self):
         super().test_replace()
-        r1 = self.response_class("http://www.example.com", body="hello", encoding="cp852")
+        r1 = self.response_class(
+            "http://www.example.com", body="hello", encoding="cp852"
+        )
         r2 = r1.replace(url="http://www.example.com/other")
         r3 = r1.replace(url="http://www.example.com/other", encoding="latin1")
 
         assert isinstance(r2, self.response_class)
         self.assertEqual(r2.url, "http://www.example.com/other")
         self._assert_response_encoding(r2, "cp852")
         self.assertEqual(r3.url, "http://www.example.com/other")
@@ -316,169 +368,224 @@
 
     def test_unicode_url(self):
         # instantiate with unicode url without encoding (should set default encoding)
         resp = self.response_class("http://www.example.com/")
         self._assert_response_encoding(resp, self.response_class._DEFAULT_ENCODING)
 
         # make sure urls are converted to str
-        resp = self.response_class(url="http://www.example.com/", encoding='utf-8')
+        resp = self.response_class(url="http://www.example.com/", encoding="utf-8")
         assert isinstance(resp.url, str)
 
-        resp = self.response_class(url="http://www.example.com/price/\xa3", encoding='utf-8')
-        self.assertEqual(resp.url, to_unicode(b'http://www.example.com/price/\xc2\xa3'))
-        resp = self.response_class(url="http://www.example.com/price/\xa3", encoding='latin-1')
-        self.assertEqual(resp.url, 'http://www.example.com/price/\xa3')
-        resp = self.response_class("http://www.example.com/price/\xa3",
-                                   headers={"Content-type": ["text/html; charset=utf-8"]})
-        self.assertEqual(resp.url, to_unicode(b'http://www.example.com/price/\xc2\xa3'))
-        resp = self.response_class("http://www.example.com/price/\xa3",
-                                   headers={"Content-type": ["text/html; charset=iso-8859-1"]})
-        self.assertEqual(resp.url, 'http://www.example.com/price/\xa3')
+        resp = self.response_class(
+            url="http://www.example.com/price/\xa3", encoding="utf-8"
+        )
+        self.assertEqual(resp.url, to_unicode(b"http://www.example.com/price/\xc2\xa3"))
+        resp = self.response_class(
+            url="http://www.example.com/price/\xa3", encoding="latin-1"
+        )
+        self.assertEqual(resp.url, "http://www.example.com/price/\xa3")
+        resp = self.response_class(
+            "http://www.example.com/price/\xa3",
+            headers={"Content-type": ["text/html; charset=utf-8"]},
+        )
+        self.assertEqual(resp.url, to_unicode(b"http://www.example.com/price/\xc2\xa3"))
+        resp = self.response_class(
+            "http://www.example.com/price/\xa3",
+            headers={"Content-type": ["text/html; charset=iso-8859-1"]},
+        )
+        self.assertEqual(resp.url, "http://www.example.com/price/\xa3")
 
     def test_unicode_body(self):
-        unicode_string = ('\u043a\u0438\u0440\u0438\u043b\u043b\u0438\u0447\u0435\u0441\u043a\u0438\u0439 '
-                          '\u0442\u0435\u043a\u0441\u0442')
-        self.assertRaises(TypeError, self.response_class, 'http://www.example.com', body='unicode body')
+        unicode_string = (
+            "\u043a\u0438\u0440\u0438\u043b\u043b\u0438\u0447\u0435\u0441\u043a\u0438\u0439 "
+            "\u0442\u0435\u043a\u0441\u0442"
+        )
+        self.assertRaises(
+            TypeError,
+            self.response_class,
+            "http://www.example.com",
+            body="unicode body",
+        )
 
-        original_string = unicode_string.encode('cp1251')
-        r1 = self.response_class('http://www.example.com', body=original_string, encoding='cp1251')
+        original_string = unicode_string.encode("cp1251")
+        r1 = self.response_class(
+            "http://www.example.com", body=original_string, encoding="cp1251"
+        )
 
         # check response.text
         self.assertTrue(isinstance(r1.text, str))
         self.assertEqual(r1.text, unicode_string)
 
     def test_encoding(self):
-        r1 = self.response_class("http://www.example.com", body=b"\xc2\xa3",
-                                 headers={"Content-type": ["text/html; charset=utf-8"]})
-        r2 = self.response_class("http://www.example.com", encoding='utf-8', body="\xa3")
-        r3 = self.response_class("http://www.example.com", body=b"\xa3",
-                                 headers={"Content-type": ["text/html; charset=iso-8859-1"]})
+        r1 = self.response_class(
+            "http://www.example.com",
+            body=b"\xc2\xa3",
+            headers={"Content-type": ["text/html; charset=utf-8"]},
+        )
+        r2 = self.response_class(
+            "http://www.example.com", encoding="utf-8", body="\xa3"
+        )
+        r3 = self.response_class(
+            "http://www.example.com",
+            body=b"\xa3",
+            headers={"Content-type": ["text/html; charset=iso-8859-1"]},
+        )
         r4 = self.response_class("http://www.example.com", body=b"\xa2\xa3")
-        r5 = self.response_class("http://www.example.com", body=b"\xc2\xa3",
-                                 headers={"Content-type": ["text/html; charset=None"]})
-        r6 = self.response_class("http://www.example.com", body=b"\xa8D",
-                                 headers={"Content-type": ["text/html; charset=gb2312"]})
-        r7 = self.response_class("http://www.example.com", body=b"\xa8D",
-                                 headers={"Content-type": ["text/html; charset=gbk"]})
-        r8 = self.response_class("http://www.example.com", body=codecs.BOM_UTF8 + b"\xc2\xa3",
-                                 headers={"Content-type": ["text/html; charset=cp1251"]})
+        r5 = self.response_class(
+            "http://www.example.com",
+            body=b"\xc2\xa3",
+            headers={"Content-type": ["text/html; charset=None"]},
+        )
+        r6 = self.response_class(
+            "http://www.example.com",
+            body=b"\xa8D",
+            headers={"Content-type": ["text/html; charset=gb2312"]},
+        )
+        r7 = self.response_class(
+            "http://www.example.com",
+            body=b"\xa8D",
+            headers={"Content-type": ["text/html; charset=gbk"]},
+        )
+        r8 = self.response_class(
+            "http://www.example.com",
+            body=codecs.BOM_UTF8 + b"\xc2\xa3",
+            headers={"Content-type": ["text/html; charset=cp1251"]},
+        )
 
         self.assertEqual(r1._headers_encoding(), "utf-8")
         self.assertEqual(r2._headers_encoding(), None)
-        self.assertEqual(r2._declared_encoding(), 'utf-8')
-        self._assert_response_encoding(r2, 'utf-8')
+        self.assertEqual(r2._declared_encoding(), "utf-8")
+        self._assert_response_encoding(r2, "utf-8")
         self.assertEqual(r3._headers_encoding(), "cp1252")
         self.assertEqual(r3._declared_encoding(), "cp1252")
         self.assertEqual(r4._headers_encoding(), None)
         self.assertEqual(r5._headers_encoding(), None)
         self.assertEqual(r8._headers_encoding(), "cp1251")
         self.assertEqual(r8._declared_encoding(), "utf-8")
         self._assert_response_encoding(r5, "utf-8")
         self._assert_response_encoding(r8, "utf-8")
-        assert r4._body_inferred_encoding() is not None and r4._body_inferred_encoding() != 'ascii'
-        self._assert_response_values(r1, 'utf-8', "\xa3")
-        self._assert_response_values(r2, 'utf-8', "\xa3")
-        self._assert_response_values(r3, 'iso-8859-1', "\xa3")
-        self._assert_response_values(r6, 'gb18030', "\u2015")
-        self._assert_response_values(r7, 'gb18030', "\u2015")
+        assert (
+            r4._body_inferred_encoding() is not None
+            and r4._body_inferred_encoding() != "ascii"
+        )
+        self._assert_response_values(r1, "utf-8", "\xa3")
+        self._assert_response_values(r2, "utf-8", "\xa3")
+        self._assert_response_values(r3, "iso-8859-1", "\xa3")
+        self._assert_response_values(r6, "gb18030", "\u2015")
+        self._assert_response_values(r7, "gb18030", "\u2015")
 
         # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies
-        self.assertRaises(TypeError, self.response_class, "http://www.example.com", body="\xa3")
+        self.assertRaises(
+            TypeError,
+            self.response_class,
+            "http://www.example.com",
+            body="\xa3",
+        )
 
     def test_declared_encoding_invalid(self):
         """Check that unknown declared encodings are ignored"""
-        r = self.response_class("http://www.example.com",
-                                headers={"Content-type": ["text/html; charset=UNKNOWN"]},
-                                body=b"\xc2\xa3")
+        r = self.response_class(
+            "http://www.example.com",
+            headers={"Content-type": ["text/html; charset=UNKNOWN"]},
+            body=b"\xc2\xa3",
+        )
         self.assertEqual(r._declared_encoding(), None)
-        self._assert_response_values(r, 'utf-8', "\xa3")
+        self._assert_response_values(r, "utf-8", "\xa3")
 
     def test_utf16(self):
         """Test utf-16 because UnicodeDammit is known to have problems with"""
-        r = self.response_class("http://www.example.com",
-                                body=b'\xff\xfeh\x00i\x00',
-                                encoding='utf-16')
-        self._assert_response_values(r, 'utf-16', "hi")
+        r = self.response_class(
+            "http://www.example.com",
+            body=b"\xff\xfeh\x00i\x00",
+            encoding="utf-16",
+        )
+        self._assert_response_values(r, "utf-16", "hi")
 
     def test_invalid_utf8_encoded_body_with_valid_utf8_BOM(self):
-        r6 = self.response_class("http://www.example.com",
-                                 headers={"Content-type": ["text/html; charset=utf-8"]},
-                                 body=b"\xef\xbb\xbfWORD\xe3\xab")
-        self.assertEqual(r6.encoding, 'utf-8')
-        self.assertIn(r6.text, {
-            'WORD\ufffd\ufffd',  # w3lib < 1.19.0
-            'WORD\ufffd',        # w3lib >= 1.19.0
-        })
+        r6 = self.response_class(
+            "http://www.example.com",
+            headers={"Content-type": ["text/html; charset=utf-8"]},
+            body=b"\xef\xbb\xbfWORD\xe3\xab",
+        )
+        self.assertEqual(r6.encoding, "utf-8")
+        self.assertIn(
+            r6.text,
+            {
+                "WORD\ufffd\ufffd",  # w3lib < 1.19.0
+                "WORD\ufffd",  # w3lib >= 1.19.0
+            },
+        )
 
     def test_bom_is_removed_from_body(self):
         # Inferring encoding from body also cache decoded body as sideeffect,
         # this test tries to ensure that calling response.encoding and
-        # response.text in indistint order doesn't affect final
+        # response.text in indistinct order doesn't affect final
+        # response.text in indistinct order doesn't affect final
         # values for encoding and decoded body.
-        url = 'http://example.com'
+        url = "http://example.com"
         body = b"\xef\xbb\xbfWORD"
         headers = {"Content-type": ["text/html; charset=utf-8"]}
 
         # Test response without content-type and BOM encoding
         response = self.response_class(url, body=body)
-        self.assertEqual(response.encoding, 'utf-8')
-        self.assertEqual(response.text, 'WORD')
+        self.assertEqual(response.encoding, "utf-8")
+        self.assertEqual(response.text, "WORD")
         response = self.response_class(url, body=body)
-        self.assertEqual(response.text, 'WORD')
-        self.assertEqual(response.encoding, 'utf-8')
+        self.assertEqual(response.text, "WORD")
+        self.assertEqual(response.encoding, "utf-8")
 
         # Body caching sideeffect isn't triggered when encoding is declared in
         # content-type header but BOM still need to be removed from decoded
         # body
         response = self.response_class(url, headers=headers, body=body)
-        self.assertEqual(response.encoding, 'utf-8')
-        self.assertEqual(response.text, 'WORD')
+        self.assertEqual(response.encoding, "utf-8")
+        self.assertEqual(response.text, "WORD")
         response = self.response_class(url, headers=headers, body=body)
-        self.assertEqual(response.text, 'WORD')
-        self.assertEqual(response.encoding, 'utf-8')
+        self.assertEqual(response.text, "WORD")
+        self.assertEqual(response.encoding, "utf-8")
 
     def test_replace_wrong_encoding(self):
         """Test invalid chars are replaced properly"""
-        r = self.response_class("http://www.example.com", encoding='utf-8', body=b'PREFIX\xe3\xabSUFFIX')
+        r = self.response_class(
+            "http://www.example.com",
+            encoding="utf-8",
+            body=b"PREFIX\xe3\xabSUFFIX",
+        )
         # XXX: Policy for replacing invalid chars may suffer minor variations
         # but it should always contain the unicode replacement char ('\ufffd')
-        assert '\ufffd' in r.text, repr(r.text)
-        assert 'PREFIX' in r.text, repr(r.text)
-        assert 'SUFFIX' in r.text, repr(r.text)
+        assert "\ufffd" in r.text, repr(r.text)
+        assert "PREFIX" in r.text, repr(r.text)
+        assert "SUFFIX" in r.text, repr(r.text)
 
         # Do not destroy html tags due to encoding bugs
-        r = self.response_class("http://example.com", encoding='utf-8',
-                                body=b'\xf0<span>value</span>')
-        assert '<span>value</span>' in r.text, repr(r.text)
+        r = self.response_class(
+            "http://example.com",
+            encoding="utf-8",
+            body=b"\xf0<span>value</span>",
+        )
+        assert "<span>value</span>" in r.text, repr(r.text)
 
         # FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse
         # r = self.response_class("http://www.example.com", body=b'PREFIX\xe3\xabSUFFIX')
         # assert '\ufffd' in r.text, repr(r.text)
 
     def test_selector(self):
         body = b"<html><head><title>Some page</title><body></body></html>"
         response = self.response_class("http://www.example.com", body=body)
 
         self.assertIsInstance(response.selector, Selector)
-        self.assertEqual(response.selector.type, 'html')
+        self.assertEqual(response.selector.type, "html")
         self.assertIs(response.selector, response.selector)  # property is cached
         self.assertIs(response.selector.response, response)
 
         self.assertEqual(
-            response.selector.xpath("//title/text()").getall(),
-            ['Some page']
-        )
-        self.assertEqual(
-            response.selector.css("title::text").getall(),
-            ['Some page']
-        )
-        self.assertEqual(
-            response.selector.re("Some (.*)</title>"),
-            ['page']
+            response.selector.xpath("//title/text()").getall(), ["Some page"]
         )
+        self.assertEqual(response.selector.css("title::text").getall(), ["Some page"])
+        self.assertEqual(response.selector.re("Some (.*)</title>"), ["page"])
 
     def test_selector_shortcuts(self):
         body = b"<html><head><title>Some page</title><body></body></html>"
         response = self.response_class("http://www.example.com", body=body)
 
         self.assertEqual(
             response.xpath("//title/text()").getall(),
@@ -486,218 +593,246 @@
         )
         self.assertEqual(
             response.css("title::text").getall(),
             response.selector.css("title::text").getall(),
         )
 
     def test_selector_shortcuts_kwargs(self):
-        body = b"<html><head><title>Some page</title><body><p class=\"content\">A nice paragraph.</p></body></html>"
+        body = b'<html><head><title>Some page</title><body><p class="content">A nice paragraph.</p></body></html>'
         response = self.response_class("http://www.example.com", body=body)
 
         self.assertEqual(
-            response.xpath("normalize-space(//p[@class=$pclass])", pclass="content").getall(),
-            response.xpath("normalize-space(//p[@class=\"content\"])").getall(),
+            response.xpath(
+                "normalize-space(//p[@class=$pclass])", pclass="content"
+            ).getall(),
+            response.xpath('normalize-space(//p[@class="content"])').getall(),
         )
         self.assertEqual(
             response.xpath(
                 "//title[count(following::p[@class=$pclass])=$pcount]/text()",
-                pclass="content", pcount=1,
+                pclass="content",
+                pcount=1,
+            ).getall(),
+            response.xpath(
+                '//title[count(following::p[@class="content"])=1]/text()'
             ).getall(),
-            response.xpath("//title[count(following::p[@class=\"content\"])=1]/text()").getall(),
         )
 
     def test_urljoin_with_base_url(self):
         """Test urljoin shortcut which also evaluates base-url through get_base_url()."""
         body = b'<html><body><base href="https://example.net"></body></html>'
-        joined = self.response_class('http://www.example.com', body=body).urljoin('/test')
-        absolute = 'https://example.net/test'
+        joined = self.response_class("http://www.example.com", body=body).urljoin(
+            "/test"
+        )
+        absolute = "https://example.net/test"
         self.assertEqual(joined, absolute)
 
         body = b'<html><body><base href="/elsewhere"></body></html>'
-        joined = self.response_class('http://www.example.com', body=body).urljoin('test')
-        absolute = 'http://www.example.com/test'
+        joined = self.response_class("http://www.example.com", body=body).urljoin(
+            "test"
+        )
+        absolute = "http://www.example.com/test"
         self.assertEqual(joined, absolute)
 
         body = b'<html><body><base href="/elsewhere/"></body></html>'
-        joined = self.response_class('http://www.example.com', body=body).urljoin('test')
-        absolute = 'http://www.example.com/elsewhere/test'
+        joined = self.response_class("http://www.example.com", body=body).urljoin(
+            "test"
+        )
+        absolute = "http://www.example.com/elsewhere/test"
         self.assertEqual(joined, absolute)
 
     def test_follow_selector(self):
         resp = self._links_response()
         urls = [
-            'http://example.com/sample2.html',
-            'http://example.com/sample3.html',
-            'http://example.com/sample3.html',
-            'http://example.com/sample3.html#foo',
-            'http://www.google.com/something',
-            'http://example.com/innertag.html'
+            "http://example.com/sample2.html",
+            "http://example.com/sample3.html",
+            "http://example.com/sample3.html",
+            "http://example.com/sample3.html",
+            "http://example.com/sample3.html#foo",
+            "http://www.google.com/something",
+            "http://example.com/innertag.html",
         ]
 
         # select <a> elements
-        for sellist in [resp.css('a'), resp.xpath('//a')]:
+        for sellist in [resp.css("a"), resp.xpath("//a")]:
             for sel, url in zip(sellist, urls):
                 self._assert_followed_url(sel, url, response=resp)
 
         # select <link> elements
         self._assert_followed_url(
-            Selector(text='<link href="foo"></link>').css('link')[0],
-            'http://example.com/foo',
-            response=resp
+            Selector(text='<link href="foo"></link>').css("link")[0],
+            "http://example.com/foo",
+            response=resp,
         )
 
         # href attributes should work
-        for sellist in [resp.css('a::attr(href)'), resp.xpath('//a/@href')]:
+        for sellist in [resp.css("a::attr(href)"), resp.xpath("//a/@href")]:
             for sel, url in zip(sellist, urls):
                 self._assert_followed_url(sel, url, response=resp)
 
         # non-a elements are not supported
-        self.assertRaises(ValueError, resp.follow, resp.css('div')[0])
+        self.assertRaises(ValueError, resp.follow, resp.css("div")[0])
 
     def test_follow_selector_list(self):
         resp = self._links_response()
-        self.assertRaisesRegex(ValueError, 'SelectorList',
-                               resp.follow, resp.css('a'))
+        self.assertRaisesRegex(ValueError, "SelectorList", resp.follow, resp.css("a"))
 
     def test_follow_selector_invalid(self):
         resp = self._links_response()
-        self.assertRaisesRegex(ValueError, 'Unsupported',
-                               resp.follow, resp.xpath('count(//div)')[0])
+        self.assertRaisesRegex(
+            ValueError,
+            "Unsupported",
+            resp.follow,
+            resp.xpath("count(//div)")[0],
+        )
 
     def test_follow_selector_attribute(self):
         resp = self._links_response()
-        for src in resp.css('img::attr(src)'):
-            self._assert_followed_url(src, 'http://example.com/sample2.jpg')
+        for src in resp.css("img::attr(src)"):
+            self._assert_followed_url(src, "http://example.com/sample2.jpg")
 
     def test_follow_selector_no_href(self):
         resp = self.response_class(
-            url='http://example.com',
-            body=b'<html><body><a name=123>click me</a></body></html>',
+            url="http://example.com",
+            body=b"<html><body><a name=123>click me</a></body></html>",
         )
-        self.assertRaisesRegex(ValueError, 'no href',
-                               resp.follow, resp.css('a')[0])
+        self.assertRaisesRegex(ValueError, "no href", resp.follow, resp.css("a")[0])
 
     def test_follow_whitespace_selector(self):
         resp = self.response_class(
-            'http://example.com',
-            body=b'''<html><body><a href=" foo\n">click me</a></body></html>'''
+            "http://example.com",
+            body=b"""<html><body><a href=" foo\n">click me</a></body></html>""",
         )
         self._assert_followed_url(
-            resp.css('a')[0],
-            'http://example.com/foo',
-            response=resp)
+            resp.css("a")[0], "http://example.com/foo", response=resp
+        )
         self._assert_followed_url(
-            resp.css('a::attr(href)')[0],
-            'http://example.com/foo',
-            response=resp)
+            resp.css("a::attr(href)")[0],
+            "http://example.com/foo",
+            response=resp,
+        )
 
     def test_follow_encoding(self):
         resp1 = self.response_class(
-            'http://example.com',
-            encoding='utf8',
-            body='<html><body><a href="foo?">click me</a></body></html>'.encode('utf8')
+            "http://example.com",
+            encoding="utf8",
+            body='<html><body><a href="foo?">click me</a></body></html>'.encode(
+                "utf8"
+            ),
         )
         req = self._assert_followed_url(
-            resp1.css('a')[0],
-            'http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82',
+            resp1.css("a")[0],
+            "http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82",
             response=resp1,
         )
-        self.assertEqual(req.encoding, 'utf8')
+        self.assertEqual(req.encoding, "utf8")
 
         resp2 = self.response_class(
-            'http://example.com',
-            encoding='cp1251',
-            body='<html><body><a href="foo?">click me</a></body></html>'.encode('cp1251')
+            "http://example.com",
+            encoding="cp1251",
+            body='<html><body><a href="foo?">click me</a></body></html>'.encode(
+                "cp1251"
+            ),
         )
         req = self._assert_followed_url(
-            resp2.css('a')[0],
-            'http://example.com/foo?%EF%F0%E8%E2%E5%F2',
+            resp2.css("a")[0],
+            "http://example.com/foo?%EF%F0%E8%E2%E5%F2",
             response=resp2,
         )
-        self.assertEqual(req.encoding, 'cp1251')
+        self.assertEqual(req.encoding, "cp1251")
 
     def test_follow_flags(self):
-        res = self.response_class('http://example.com/')
-        fol = res.follow('http://example.com/', flags=['cached', 'allowed'])
-        self.assertEqual(fol.flags, ['cached', 'allowed'])
+        res = self.response_class("http://example.com/")
+        fol = res.follow("http://example.com/", flags=["cached", "allowed"])
+        self.assertEqual(fol.flags, ["cached", "allowed"])
 
     def test_follow_all_flags(self):
-        re = self.response_class('http://www.example.com/')
+        re = self.response_class("http://www.example.com/")
         urls = [
-            'http://www.example.com/',
-            'http://www.example.com/2',
-            'http://www.example.com/foo',
+            "http://www.example.com/",
+            "http://www.example.com/2",
+            "http://www.example.com/foo",
         ]
-        fol = re.follow_all(urls, flags=['cached', 'allowed'])
+        fol = re.follow_all(urls, flags=["cached", "allowed"])
         for req in fol:
-            self.assertEqual(req.flags, ['cached', 'allowed'])
+            self.assertEqual(req.flags, ["cached", "allowed"])
 
     def test_follow_all_css(self):
         expected = [
-            'http://example.com/sample3.html',
-            'http://example.com/innertag.html',
+            "http://example.com/sample3.html",
+            "http://example.com/innertag.html",
         ]
         response = self._links_response()
         extracted = [r.url for r in response.follow_all(css='a[href*="example.com"]')]
         self.assertEqual(expected, extracted)
 
     def test_follow_all_css_skip_invalid(self):
         expected = [
-            'http://example.com/page/1/',
-            'http://example.com/page/3/',
-            'http://example.com/page/4/',
+            "http://example.com/page/1/",
+            "http://example.com/page/3/",
+            "http://example.com/page/4/",
         ]
         response = self._links_response_no_href()
-        extracted1 = [r.url for r in response.follow_all(css='.pagination a')]
+        extracted1 = [r.url for r in response.follow_all(css=".pagination a")]
         self.assertEqual(expected, extracted1)
-        extracted2 = [r.url for r in response.follow_all(response.css('.pagination a'))]
+        extracted2 = [r.url for r in response.follow_all(response.css(".pagination a"))]
         self.assertEqual(expected, extracted2)
 
     def test_follow_all_xpath(self):
         expected = [
-            'http://example.com/sample3.html',
-            'http://example.com/innertag.html',
+            "http://example.com/sample3.html",
+            "http://example.com/innertag.html",
         ]
         response = self._links_response()
         extracted = response.follow_all(xpath='//a[contains(@href, "example.com")]')
         self.assertEqual(expected, [r.url for r in extracted])
 
     def test_follow_all_xpath_skip_invalid(self):
         expected = [
-            'http://example.com/page/1/',
-            'http://example.com/page/3/',
-            'http://example.com/page/4/',
+            "http://example.com/page/1/",
+            "http://example.com/page/3/",
+            "http://example.com/page/4/",
         ]
         response = self._links_response_no_href()
-        extracted1 = [r.url for r in response.follow_all(xpath='//div[@id="pagination"]/a')]
+        extracted1 = [
+            r.url for r in response.follow_all(xpath='//div[@id="pagination"]/a')
+        ]
         self.assertEqual(expected, extracted1)
-        extracted2 = [r.url for r in response.follow_all(response.xpath('//div[@id="pagination"]/a'))]
+        extracted2 = [
+            r.url
+            for r in response.follow_all(response.xpath('//div[@id="pagination"]/a'))
+        ]
         self.assertEqual(expected, extracted2)
 
     def test_follow_all_too_many_arguments(self):
         response = self._links_response()
         with self.assertRaises(ValueError):
-            response.follow_all(css='a[href*="example.com"]', xpath='//a[contains(@href, "example.com")]')
+            response.follow_all(
+                css='a[href*="example.com"]',
+                xpath='//a[contains(@href, "example.com")]',
+            )
 
     def test_json_response(self):
         json_body = b"""{"ip": "109.187.217.200"}"""
         json_response = self.response_class("http://www.example.com", body=json_body)
-        self.assertEqual(json_response.json(), {'ip': '109.187.217.200'})
+        self.assertEqual(json_response.json(), {"ip": "109.187.217.200"})
 
         text_body = b"""<html><body>text</body></html>"""
         text_response = self.response_class("http://www.example.com", body=text_body)
         with self.assertRaises(ValueError):
             text_response.json()
 
     def test_cache_json_response(self):
         json_valid_bodies = [b"""{"ip": "109.187.217.200"}""", b"""null"""]
         for json_body in json_valid_bodies:
-            json_response = self.response_class("http://www.example.com", body=json_body)
+            json_response = self.response_class(
+                "http://www.example.com", body=json_body
+            )
 
-            with mock.patch('json.loads') as mock_json:
+            with mock.patch("json.loads") as mock_json:
                 for _ in range(2):
                     json_response.json()
                 mock_json.assert_called_once_with(json_body.decode())
 
 
 class HtmlResponseTest(TextResponseTest):
 
@@ -706,112 +841,118 @@
     def test_html_encoding(self):
 
         body = b"""<html><head><title>Some page</title>
         <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
         </head><body>Price: \xa3100</body></html>'
         """
         r1 = self.response_class("http://www.example.com", body=body)
-        self._assert_response_values(r1, 'iso-8859-1', body)
+        self._assert_response_values(r1, "iso-8859-1", body)
 
         body = b"""<?xml version="1.0" encoding="iso-8859-1"?>
         <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
         Price: \xa3100
         """
         r2 = self.response_class("http://www.example.com", body=body)
-        self._assert_response_values(r2, 'iso-8859-1', body)
+        self._assert_response_values(r2, "iso-8859-1", body)
 
         # for conflicting declarations headers must take precedence
         body = b"""<html><head><title>Some page</title>
         <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
         </head><body>Price: \xa3100</body></html>'
         """
-        r3 = self.response_class("http://www.example.com", body=body,
-                                 headers={"Content-type": ["text/html; charset=iso-8859-1"]})
-        self._assert_response_values(r3, 'iso-8859-1', body)
+        r3 = self.response_class(
+            "http://www.example.com",
+            body=body,
+            headers={"Content-type": ["text/html; charset=iso-8859-1"]},
+        )
+        self._assert_response_values(r3, "iso-8859-1", body)
 
         # make sure replace() preserves the encoding of the original response
         body = b"New body \xa3"
         r4 = r3.replace(body=body)
-        self._assert_response_values(r4, 'iso-8859-1', body)
+        self._assert_response_values(r4, "iso-8859-1", body)
 
     def test_html5_meta_charset(self):
         body = b"""<html><head><meta charset="gb2312" /><title>Some page</title><body>bla bla</body>"""
         r1 = self.response_class("http://www.example.com", body=body)
-        self._assert_response_values(r1, 'gb2312', body)
+        self._assert_response_values(r1, "gb2312", body)
 
 
 class XmlResponseTest(TextResponseTest):
 
     response_class = XmlResponse
 
     def test_xml_encoding(self):
         body = b"<xml></xml>"
         r1 = self.response_class("http://www.example.com", body=body)
         self._assert_response_values(r1, self.response_class._DEFAULT_ENCODING, body)
 
         body = b"""<?xml version="1.0" encoding="iso-8859-1"?><xml></xml>"""
         r2 = self.response_class("http://www.example.com", body=body)
-        self._assert_response_values(r2, 'iso-8859-1', body)
+        self._assert_response_values(r2, "iso-8859-1", body)
 
         # make sure replace() preserves the explicit encoding passed in the __init__ method
         body = b"""<?xml version="1.0" encoding="iso-8859-1"?><xml></xml>"""
-        r3 = self.response_class("http://www.example.com", body=body, encoding='utf-8')
+        r3 = self.response_class("http://www.example.com", body=body, encoding="utf-8")
         body2 = b"New body"
         r4 = r3.replace(body=body2)
-        self._assert_response_values(r4, 'utf-8', body2)
+        self._assert_response_values(r4, "utf-8", body2)
 
     def test_replace_encoding(self):
         # make sure replace() keeps the previous encoding unless overridden explicitly
         body = b"""<?xml version="1.0" encoding="iso-8859-1"?><xml></xml>"""
         body2 = b"""<?xml version="1.0" encoding="utf-8"?><xml></xml>"""
         r5 = self.response_class("http://www.example.com", body=body)
         r6 = r5.replace(body=body2)
-        r7 = r5.replace(body=body2, encoding='utf-8')
-        self._assert_response_values(r5, 'iso-8859-1', body)
-        self._assert_response_values(r6, 'iso-8859-1', body2)
-        self._assert_response_values(r7, 'utf-8', body2)
+        r7 = r5.replace(body=body2, encoding="utf-8")
+        self._assert_response_values(r5, "iso-8859-1", body)
+        self._assert_response_values(r6, "iso-8859-1", body2)
+        self._assert_response_values(r7, "utf-8", body2)
 
     def test_selector(self):
         body = b'<?xml version="1.0" encoding="utf-8"?><xml><elem>value</elem></xml>'
         response = self.response_class("http://www.example.com", body=body)
 
         self.assertIsInstance(response.selector, Selector)
-        self.assertEqual(response.selector.type, 'xml')
+        self.assertEqual(response.selector.type, "xml")
         self.assertIs(response.selector, response.selector)  # property is cached
         self.assertIs(response.selector.response, response)
 
-        self.assertEqual(
-            response.selector.xpath("//elem/text()").getall(),
-            ['value']
-        )
+        self.assertEqual(response.selector.xpath("//elem/text()").getall(), ["value"])
 
     def test_selector_shortcuts(self):
         body = b'<?xml version="1.0" encoding="utf-8"?><xml><elem>value</elem></xml>'
         response = self.response_class("http://www.example.com", body=body)
 
         self.assertEqual(
             response.xpath("//elem/text()").getall(),
             response.selector.xpath("//elem/text()").getall(),
         )
 
     def test_selector_shortcuts_kwargs(self):
-        body = b'''<?xml version="1.0" encoding="utf-8"?>
+        body = b"""<?xml version="1.0" encoding="utf-8"?>
         <xml xmlns:somens="http://scrapy.org">
         <somens:elem>value</somens:elem>
-        </xml>'''
+        </xml>"""
         response = self.response_class("http://www.example.com", body=body)
 
         self.assertEqual(
-            response.xpath("//s:elem/text()", namespaces={'s': 'http://scrapy.org'}).getall(),
-            response.selector.xpath("//s:elem/text()", namespaces={'s': 'http://scrapy.org'}).getall(),
+            response.xpath(
+                "//s:elem/text()", namespaces={"s": "http://scrapy.org"}
+            ).getall(),
+            response.selector.xpath(
+                "//s:elem/text()", namespaces={"s": "http://scrapy.org"}
+            ).getall(),
         )
 
-        response.selector.register_namespace('s2', 'http://scrapy.org')
+        response.selector.register_namespace("s2", "http://scrapy.org")
         self.assertEqual(
-            response.xpath("//s1:elem/text()", namespaces={'s1': 'http://scrapy.org'}).getall(),
+            response.xpath(
+                "//s1:elem/text()", namespaces={"s1": "http://scrapy.org"}
+            ).getall(),
             response.selector.xpath("//s2:elem/text()").getall(),
         )
 
 
 class CustomResponse(TextResponse):
     attributes = TextResponse.attributes + ("foo", "bar")
 
@@ -823,25 +964,37 @@
 
 
 class CustomResponseTest(TextResponseTest):
     response_class = CustomResponse
 
     def test_copy(self):
         super().test_copy()
-        r1 = self.response_class(url="https://example.org", status=200, foo="foo", bar="bar", lost="lost")
+        r1 = self.response_class(
+            url="https://example.org",
+            status=200,
+            foo="foo",
+            bar="bar",
+            lost="lost",
+        )
         r2 = r1.copy()
         self.assertIsInstance(r2, self.response_class)
         self.assertEqual(r1.foo, r2.foo)
         self.assertEqual(r1.bar, r2.bar)
         self.assertEqual(r1.lost, "lost")
         self.assertIsNone(r2.lost)
 
     def test_replace(self):
         super().test_replace()
-        r1 = self.response_class(url="https://example.org", status=200, foo="foo", bar="bar", lost="lost")
+        r1 = self.response_class(
+            url="https://example.org",
+            status=200,
+            foo="foo",
+            bar="bar",
+            lost="lost",
+        )
 
         r2 = r1.replace(foo="new-foo", bar="new-bar", lost="new-lost")
         self.assertIsInstance(r2, self.response_class)
         self.assertEqual(r1.foo, "foo")
         self.assertEqual(r1.bar, "bar")
         self.assertEqual(r1.lost, "lost")
         self.assertEqual(r2.foo, "new-foo")
@@ -864,8 +1017,12 @@
         self.assertEqual(r1.lost, "lost")
         self.assertEqual(r4.foo, "new-foo")
         self.assertEqual(r4.bar, "bar")
         self.assertIsNone(r4.lost)
 
         with self.assertRaises(TypeError) as ctx:
             r1.replace(unknown="unknown")
-        self.assertTrue(str(ctx.exception).endswith("__init__() got an unexpected keyword argument 'unknown'"))
+        self.assertTrue(
+            str(ctx.exception).endswith(
+                "__init__() got an unexpected keyword argument 'unknown'"
+            )
+        )
```

### Comparing `Scrapy-2.7.1/tests/test_item.py` & `Scrapy-2.8.0/tests/test_item.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,288 +1,297 @@
 import unittest
 from unittest import mock
 
 from scrapy.item import ABCMeta, Field, Item, ItemMeta
 
 
 class ItemTest(unittest.TestCase):
-
     def assertSortedEqual(self, first, second, msg=None):
         return self.assertEqual(sorted(first), sorted(second), msg)
 
     def test_simple(self):
         class TestItem(Item):
             name = Field()
 
         i = TestItem()
-        i['name'] = 'name'
-        self.assertEqual(i['name'], 'name')
+        i["name"] = "name"
+        self.assertEqual(i["name"], "name")
 
     def test_init(self):
         class TestItem(Item):
             name = Field()
 
         i = TestItem()
-        self.assertRaises(KeyError, i.__getitem__, 'name')
+        self.assertRaises(KeyError, i.__getitem__, "name")
 
-        i2 = TestItem(name='john doe')
-        self.assertEqual(i2['name'], 'john doe')
+        i2 = TestItem(name="john doe")
+        self.assertEqual(i2["name"], "john doe")
 
-        i3 = TestItem({'name': 'john doe'})
-        self.assertEqual(i3['name'], 'john doe')
+        i3 = TestItem({"name": "john doe"})
+        self.assertEqual(i3["name"], "john doe")
 
         i4 = TestItem(i3)
-        self.assertEqual(i4['name'], 'john doe')
+        self.assertEqual(i4["name"], "john doe")
 
-        self.assertRaises(KeyError, TestItem, {'name': 'john doe',
-                                               'other': 'foo'})
+        self.assertRaises(KeyError, TestItem, {"name": "john doe", "other": "foo"})
 
     def test_invalid_field(self):
         class TestItem(Item):
             pass
 
         i = TestItem()
-        self.assertRaises(KeyError, i.__setitem__, 'field', 'text')
-        self.assertRaises(KeyError, i.__getitem__, 'field')
+        self.assertRaises(KeyError, i.__setitem__, "field", "text")
+        self.assertRaises(KeyError, i.__getitem__, "field")
 
     def test_repr(self):
         class TestItem(Item):
             name = Field()
             number = Field()
 
         i = TestItem()
-        i['name'] = 'John Doe'
-        i['number'] = 123
+        i["name"] = "John Doe"
+        i["number"] = 123
         itemrepr = repr(i)
 
-        self.assertEqual(itemrepr,
-                         "{'name': 'John Doe', 'number': 123}")
+        self.assertEqual(itemrepr, "{'name': 'John Doe', 'number': 123}")
 
         i2 = eval(itemrepr)
-        self.assertEqual(i2['name'], 'John Doe')
-        self.assertEqual(i2['number'], 123)
+        self.assertEqual(i2["name"], "John Doe")
+        self.assertEqual(i2["number"], 123)
 
     def test_private_attr(self):
         class TestItem(Item):
             name = Field()
 
         i = TestItem()
-        i._private = 'test'
-        self.assertEqual(i._private, 'test')
+        i._private = "test"
+        self.assertEqual(i._private, "test")
 
     def test_raise_getattr(self):
         class TestItem(Item):
             name = Field()
 
         i = TestItem()
-        self.assertRaises(AttributeError, getattr, i, 'name')
+        self.assertRaises(AttributeError, getattr, i, "name")
 
     def test_raise_setattr(self):
         class TestItem(Item):
             name = Field()
 
         i = TestItem()
-        self.assertRaises(AttributeError, setattr, i, 'name', 'john')
+        self.assertRaises(AttributeError, setattr, i, "name", "john")
 
     def test_custom_methods(self):
         class TestItem(Item):
             name = Field()
 
             def get_name(self):
-                return self['name']
+                return self["name"]
 
             def change_name(self, name):
-                self['name'] = name
+                self["name"] = name
 
         i = TestItem()
         self.assertRaises(KeyError, i.get_name)
-        i['name'] = 'lala'
-        self.assertEqual(i.get_name(), 'lala')
-        i.change_name('other')
-        self.assertEqual(i.get_name(), 'other')
+        i["name"] = "lala"
+        self.assertEqual(i.get_name(), "lala")
+        i.change_name("other")
+        self.assertEqual(i.get_name(), "other")
 
     def test_metaclass(self):
         class TestItem(Item):
             name = Field()
             keys = Field()
             values = Field()
 
         i = TestItem()
-        i['name'] = 'John'
-        self.assertEqual(list(i.keys()), ['name'])
-        self.assertEqual(list(i.values()), ['John'])
-
-        i['keys'] = 'Keys'
-        i['values'] = 'Values'
-        self.assertSortedEqual(list(i.keys()), ['keys', 'values', 'name'])
-        self.assertSortedEqual(list(i.values()), ['Keys', 'Values', 'John'])
+        i["name"] = "John"
+        self.assertEqual(list(i.keys()), ["name"])
+        self.assertEqual(list(i.values()), ["John"])
+
+        i["keys"] = "Keys"
+        i["values"] = "Values"
+        self.assertSortedEqual(list(i.keys()), ["keys", "values", "name"])
+        self.assertSortedEqual(list(i.values()), ["Keys", "Values", "John"])
 
     def test_metaclass_with_fields_attribute(self):
         class TestItem(Item):
-            fields = {'new': Field(default='X')}
+            fields = {"new": Field(default="X")}
 
-        item = TestItem(new='New')
-        self.assertSortedEqual(list(item.keys()), ['new'])
-        self.assertSortedEqual(list(item.values()), ['New'])
+        item = TestItem(new="New")
+        self.assertSortedEqual(list(item.keys()), ["new"])
+        self.assertSortedEqual(list(item.values()), ["New"])
 
     def test_metaclass_inheritance(self):
         class ParentItem(Item):
             name = Field()
             keys = Field()
             values = Field()
 
         class TestItem(ParentItem):
             keys = Field()
 
         i = TestItem()
-        i['keys'] = 3
-        self.assertEqual(list(i.keys()), ['keys'])
+        i["keys"] = 3
+        self.assertEqual(list(i.keys()), ["keys"])
         self.assertEqual(list(i.values()), [3])
 
     def test_metaclass_multiple_inheritance_simple(self):
         class A(Item):
-            fields = {'load': Field(default='A')}
-            save = Field(default='A')
+            fields = {"load": Field(default="A")}
+            save = Field(default="A")
 
         class B(A):
             pass
 
         class C(Item):
-            fields = {'load': Field(default='C')}
-            save = Field(default='C')
+            fields = {"load": Field(default="C")}
+            save = Field(default="C")
 
         class D(B, C):
             pass
 
-        item = D(save='X', load='Y')
-        self.assertEqual(item['save'], 'X')
-        self.assertEqual(item['load'], 'Y')
-        self.assertEqual(D.fields, {'load': {'default': 'A'}, 'save': {'default': 'A'}})
+        item = D(save="X", load="Y")
+        self.assertEqual(item["save"], "X")
+        self.assertEqual(item["load"], "Y")
+        self.assertEqual(D.fields, {"load": {"default": "A"}, "save": {"default": "A"}})
 
         # D class inverted
         class E(C, B):
             pass
 
-        self.assertEqual(E(save='X')['save'], 'X')
-        self.assertEqual(E(load='X')['load'], 'X')
-        self.assertEqual(E.fields, {'load': {'default': 'C'}, 'save': {'default': 'C'}})
+        self.assertEqual(E(save="X")["save"], "X")
+        self.assertEqual(E(load="X")["load"], "X")
+        self.assertEqual(E.fields, {"load": {"default": "C"}, "save": {"default": "C"}})
 
     def test_metaclass_multiple_inheritance_diamond(self):
         class A(Item):
-            fields = {'update': Field(default='A')}
-            save = Field(default='A')
-            load = Field(default='A')
+            fields = {"update": Field(default="A")}
+            save = Field(default="A")
+            load = Field(default="A")
 
         class B(A):
             pass
 
         class C(A):
-            fields = {'update': Field(default='C')}
-            save = Field(default='C')
+            fields = {"update": Field(default="C")}
+            save = Field(default="C")
 
         class D(B, C):
-            fields = {'update': Field(default='D')}
-            load = Field(default='D')
+            fields = {"update": Field(default="D")}
+            load = Field(default="D")
 
-        self.assertEqual(D(save='X')['save'], 'X')
-        self.assertEqual(D(load='X')['load'], 'X')
+        self.assertEqual(D(save="X")["save"], "X")
+        self.assertEqual(D(load="X")["load"], "X")
         self.assertEqual(
             D.fields,
-            {'save': {'default': 'C'}, 'load': {'default': 'D'}, 'update': {'default': 'D'}})
+            {
+                "save": {"default": "C"},
+                "load": {"default": "D"},
+                "update": {"default": "D"},
+            },
+        )
 
         # D class inverted
         class E(C, B):
-            load = Field(default='E')
+            load = Field(default="E")
 
-        self.assertEqual(E(save='X')['save'], 'X')
-        self.assertEqual(E(load='X')['load'], 'X')
+        self.assertEqual(E(save="X")["save"], "X")
+        self.assertEqual(E(load="X")["load"], "X")
         self.assertEqual(
             E.fields,
-            {'save': {'default': 'C'}, 'load': {'default': 'E'}, 'update': {'default': 'C'}})
+            {
+                "save": {"default": "C"},
+                "load": {"default": "E"},
+                "update": {"default": "C"},
+            },
+        )
 
     def test_metaclass_multiple_inheritance_without_metaclass(self):
         class A(Item):
-            fields = {'load': Field(default='A')}
-            save = Field(default='A')
+            fields = {"load": Field(default="A")}
+            save = Field(default="A")
 
         class B(A):
             pass
 
         class C:
-            fields = {'load': Field(default='C')}
-            not_allowed = Field(default='not_allowed')
-            save = Field(default='C')
+            fields = {"load": Field(default="C")}
+            not_allowed = Field(default="not_allowed")
+            save = Field(default="C")
 
         class D(B, C):
             pass
 
-        self.assertRaises(KeyError, D, not_allowed='value')
-        self.assertEqual(D(save='X')['save'], 'X')
-        self.assertEqual(D.fields, {'save': {'default': 'A'}, 'load': {'default': 'A'}})
+        self.assertRaises(KeyError, D, not_allowed="value")
+        self.assertEqual(D(save="X")["save"], "X")
+        self.assertEqual(D.fields, {"save": {"default": "A"}, "load": {"default": "A"}})
 
         # D class inverted
         class E(C, B):
             pass
 
-        self.assertRaises(KeyError, E, not_allowed='value')
-        self.assertEqual(E(save='X')['save'], 'X')
-        self.assertEqual(E.fields, {'save': {'default': 'A'}, 'load': {'default': 'A'}})
+        self.assertRaises(KeyError, E, not_allowed="value")
+        self.assertEqual(E(save="X")["save"], "X")
+        self.assertEqual(E.fields, {"save": {"default": "A"}, "load": {"default": "A"}})
 
     def test_to_dict(self):
         class TestItem(Item):
             name = Field()
 
         i = TestItem()
-        i['name'] = 'John'
-        self.assertEqual(dict(i), {'name': 'John'})
+        i["name"] = "John"
+        self.assertEqual(dict(i), {"name": "John"})
 
     def test_copy(self):
         class TestItem(Item):
             name = Field()
-        item = TestItem({'name': 'lower'})
+
+        item = TestItem({"name": "lower"})
         copied_item = item.copy()
         self.assertNotEqual(id(item), id(copied_item))
-        copied_item['name'] = copied_item['name'].upper()
-        self.assertNotEqual(item['name'], copied_item['name'])
+        copied_item["name"] = copied_item["name"].upper()
+        self.assertNotEqual(item["name"], copied_item["name"])
 
     def test_deepcopy(self):
         class TestItem(Item):
             tags = Field()
-        item = TestItem({'tags': ['tag1']})
+
+        item = TestItem({"tags": ["tag1"]})
         copied_item = item.deepcopy()
-        item['tags'].append('tag2')
-        assert item['tags'] != copied_item['tags']
+        item["tags"].append("tag2")
+        assert item["tags"] != copied_item["tags"]
 
 
 class ItemMetaTest(unittest.TestCase):
-
     def test_new_method_propagates_classcell(self):
         new_mock = mock.Mock(side_effect=ABCMeta.__new__)
         base = ItemMeta.__bases__[0]
 
-        with mock.patch.object(base, '__new__', new_mock):
+        with mock.patch.object(base, "__new__", new_mock):
 
             class MyItem(Item):
                 def f(self):
                     # For rationale of this see:
                     # https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222
-                    return __class__  # noqa  https://github.com/scrapy/scrapy/issues/2836
+                    return (
+                        __class__  # noqa  https://github.com/scrapy/scrapy/issues/2836
+                    )
 
             MyItem()
 
         (first_call, second_call) = new_mock.call_args_list[-2:]
 
         mcs, class_name, bases, attrs = first_call[0]
-        assert '__classcell__' not in attrs
+        assert "__classcell__" not in attrs
         mcs, class_name, bases, attrs = second_call[0]
-        assert '__classcell__' in attrs
+        assert "__classcell__" in attrs
 
 
 class ItemMetaClassCellRegression(unittest.TestCase):
-
     def test_item_meta_classcell_regression(self):
         class MyItem(Item, metaclass=ItemMeta):
             def __init__(self, *args, **kwargs):
                 # This call to super() trigger the __classcell__ propagation
                 # requirement. When not done properly raises an error:
                 # TypeError: __class__ set to <class '__main__.MyItem'>
                 # defining 'MyItem' as <class '__main__.MyItem'>
```

### Comparing `Scrapy-2.7.1/tests/test_link.py` & `Scrapy-2.8.0/tests/test_link.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 import unittest
 
 from scrapy.link import Link
 
 
 class LinkTest(unittest.TestCase):
-
     def _assert_same_links(self, link1, link2):
         self.assertEqual(link1, link2)
         self.assertEqual(hash(link1), hash(link2))
 
     def _assert_different_links(self, link1, link2):
         self.assertNotEqual(link1, link2)
         self.assertNotEqual(hash(link1), hash(link2))
@@ -26,23 +25,33 @@
         l5 = Link("http://www.example.com", text="test2")
         l6 = Link("http://www.example.com", text="test")
 
         self._assert_same_links(l4, l4)
         self._assert_different_links(l4, l5)
         self._assert_same_links(l4, l6)
 
-        l7 = Link("http://www.example.com", text="test", fragment='something', nofollow=False)
-        l8 = Link("http://www.example.com", text="test", fragment='something', nofollow=False)
-        l9 = Link("http://www.example.com", text="test", fragment='something', nofollow=True)
-        l10 = Link("http://www.example.com", text="test", fragment='other', nofollow=False)
+        l7 = Link(
+            "http://www.example.com", text="test", fragment="something", nofollow=False
+        )
+        l8 = Link(
+            "http://www.example.com", text="test", fragment="something", nofollow=False
+        )
+        l9 = Link(
+            "http://www.example.com", text="test", fragment="something", nofollow=True
+        )
+        l10 = Link(
+            "http://www.example.com", text="test", fragment="other", nofollow=False
+        )
         self._assert_same_links(l7, l8)
         self._assert_different_links(l7, l9)
         self._assert_different_links(l7, l10)
 
     def test_repr(self):
-        l1 = Link("http://www.example.com", text="test", fragment='something', nofollow=True)
+        l1 = Link(
+            "http://www.example.com", text="test", fragment="something", nofollow=True
+        )
         l2 = eval(repr(l1))
         self._assert_same_links(l1, l2)
 
     def test_bytes_url(self):
         with self.assertRaises(TypeError):
             Link(b"http://www.example.com/\xc2\xa3")
```

### Comparing `Scrapy-2.7.1/tests/test_loader.py` & `Scrapy-2.8.0/tests/test_loader.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-import unittest
 import dataclasses
+import unittest
 
 import attr
 from itemadapter import ItemAdapter
 from itemloaders.processors import Compose, Identity, MapCompose, TakeFirst
 
 from scrapy.http import HtmlResponse, Response
-from scrapy.item import Item, Field
+from scrapy.item import Field, Item
 from scrapy.loader import ItemLoader
 from scrapy.selector import Selector
 
 
 # test items
 class NameItem(Item):
     name = Field()
@@ -55,127 +55,135 @@
 
 class DefaultedItemLoader(NameItemLoader):
     default_input_processor = MapCompose(lambda v: v[:-1])
 
 
 # test processors
 def processor_with_args(value, other=None, loader_context=None):
-    if 'key' in loader_context:
-        return loader_context['key']
+    if "key" in loader_context:
+        return loader_context["key"]
     return value
 
 
 class BasicItemLoaderTest(unittest.TestCase):
-
     def test_add_value_on_unknown_field(self):
         il = TestItemLoader()
-        self.assertRaises(KeyError, il.add_value, 'wrong_field', ['lala', 'lolo'])
+        self.assertRaises(KeyError, il.add_value, "wrong_field", ["lala", "lolo"])
 
     def test_load_item_using_default_loader(self):
         i = TestItem()
-        i['summary'] = 'lala'
+        i["summary"] = "lala"
         il = ItemLoader(item=i)
-        il.add_value('name', 'marta')
+        il.add_value("name", "marta")
         item = il.load_item()
         assert item is i
-        self.assertEqual(item['summary'], ['lala'])
-        self.assertEqual(item['name'], ['marta'])
+        self.assertEqual(item["summary"], ["lala"])
+        self.assertEqual(item["name"], ["marta"])
 
     def test_load_item_using_custom_loader(self):
         il = TestItemLoader()
-        il.add_value('name', 'marta')
+        il.add_value("name", "marta")
         item = il.load_item()
-        self.assertEqual(item['name'], ['Marta'])
+        self.assertEqual(item["name"], ["Marta"])
 
 
 class InitializationTestMixin:
 
     item_class = None
 
     def test_keep_single_value(self):
         """Loaded item should contain values from the initial item"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo']})
+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {"name": ["foo"]})
 
     def test_keep_list(self):
         """Loaded item should contain values from the initial item"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar']})
+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {"name": ["foo", "bar"]})
 
     def test_add_value_singlevalue_singlevalue(self):
         """Values added after initialization should be appended"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
-        il.add_value('name', 'bar')
+        il.add_value("name", "bar")
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar']})
+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {"name": ["foo", "bar"]})
 
     def test_add_value_singlevalue_list(self):
         """Values added after initialization should be appended"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
-        il.add_value('name', ['item', 'loader'])
+        il.add_value("name", ["item", "loader"])
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'item', 'loader']})
+        self.assertEqual(
+            ItemAdapter(loaded_item).asdict(), {"name": ["foo", "item", "loader"]}
+        )
 
     def test_add_value_list_singlevalue(self):
         """Values added after initialization should be appended"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
-        il.add_value('name', 'qwerty')
+        il.add_value("name", "qwerty")
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar', 'qwerty']})
+        self.assertEqual(
+            ItemAdapter(loaded_item).asdict(), {"name": ["foo", "bar", "qwerty"]}
+        )
 
     def test_add_value_list_list(self):
         """Values added after initialization should be appended"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
-        il.add_value('name', ['item', 'loader'])
+        il.add_value("name", ["item", "loader"])
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar', 'item', 'loader']})
+        self.assertEqual(
+            ItemAdapter(loaded_item).asdict(),
+            {"name": ["foo", "bar", "item", "loader"]},
+        )
 
     def test_get_output_value_singlevalue(self):
         """Getting output value must not remove value from item"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
-        self.assertEqual(il.get_output_value('name'), ['foo'])
+        self.assertEqual(il.get_output_value("name"), ["foo"])
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(ItemAdapter(loaded_item).asdict(), dict({'name': ['foo']}))
+        self.assertEqual(ItemAdapter(loaded_item).asdict(), dict({"name": ["foo"]}))
 
     def test_get_output_value_list(self):
         """Getting output value must not remove value from item"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
-        self.assertEqual(il.get_output_value('name'), ['foo', 'bar'])
+        self.assertEqual(il.get_output_value("name"), ["foo", "bar"])
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(ItemAdapter(loaded_item).asdict(), dict({'name': ['foo', 'bar']}))
+        self.assertEqual(
+            ItemAdapter(loaded_item).asdict(), dict({"name": ["foo", "bar"]})
+        )
 
     def test_values_single(self):
         """Values from initial item must be added to loader._values"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
-        self.assertEqual(il._values.get('name'), ['foo'])
+        self.assertEqual(il._values.get("name"), ["foo"])
 
     def test_values_list(self):
         """Values from initial item must be added to loader._values"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
-        self.assertEqual(il._values.get('name'), ['foo', 'bar'])
+        self.assertEqual(il._values.get("name"), ["foo", "bar"])
 
 
 class InitializationFromDictTest(InitializationTestMixin, unittest.TestCase):
     item_class = dict
 
 
 class InitializationFromItemTest(InitializationTestMixin, unittest.TestCase):
@@ -203,325 +211,361 @@
     default_item_class = NoInputReprocessingItem
 
 
 class NoInputReprocessingFromItemTest(unittest.TestCase):
     """
     Loaders initialized from loaded items must not reprocess fields (Item instances)
     """
+
     def test_avoid_reprocessing_with_initial_values_single(self):
-        il = NoInputReprocessingItemLoader(item=NoInputReprocessingItem(title='foo'))
+        il = NoInputReprocessingItemLoader(item=NoInputReprocessingItem(title="foo"))
         il_loaded = il.load_item()
-        self.assertEqual(il_loaded, {'title': 'foo'})
-        self.assertEqual(NoInputReprocessingItemLoader(item=il_loaded).load_item(), {'title': 'foo'})
+        self.assertEqual(il_loaded, {"title": "foo"})
+        self.assertEqual(
+            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {"title": "foo"}
+        )
 
     def test_avoid_reprocessing_with_initial_values_list(self):
-        il = NoInputReprocessingItemLoader(item=NoInputReprocessingItem(title=['foo', 'bar']))
+        il = NoInputReprocessingItemLoader(
+            item=NoInputReprocessingItem(title=["foo", "bar"])
+        )
         il_loaded = il.load_item()
-        self.assertEqual(il_loaded, {'title': 'foo'})
-        self.assertEqual(NoInputReprocessingItemLoader(item=il_loaded).load_item(), {'title': 'foo'})
+        self.assertEqual(il_loaded, {"title": "foo"})
+        self.assertEqual(
+            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {"title": "foo"}
+        )
 
     def test_avoid_reprocessing_without_initial_values_single(self):
         il = NoInputReprocessingItemLoader()
-        il.add_value('title', 'FOO')
+        il.add_value("title", "FOO")
         il_loaded = il.load_item()
-        self.assertEqual(il_loaded, {'title': 'FOO'})
-        self.assertEqual(NoInputReprocessingItemLoader(item=il_loaded).load_item(), {'title': 'FOO'})
+        self.assertEqual(il_loaded, {"title": "FOO"})
+        self.assertEqual(
+            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {"title": "FOO"}
+        )
 
     def test_avoid_reprocessing_without_initial_values_list(self):
         il = NoInputReprocessingItemLoader()
-        il.add_value('title', ['foo', 'bar'])
+        il.add_value("title", ["foo", "bar"])
         il_loaded = il.load_item()
-        self.assertEqual(il_loaded, {'title': 'FOO'})
-        self.assertEqual(NoInputReprocessingItemLoader(item=il_loaded).load_item(), {'title': 'FOO'})
+        self.assertEqual(il_loaded, {"title": "FOO"})
+        self.assertEqual(
+            NoInputReprocessingItemLoader(item=il_loaded).load_item(), {"title": "FOO"}
+        )
 
 
 class TestOutputProcessorItem(unittest.TestCase):
     def test_output_processor(self):
-
         class TempItem(Item):
             temp = Field()
 
             def __init__(self, *args, **kwargs):
                 super().__init__(self, *args, **kwargs)
-                self.setdefault('temp', 0.3)
+                self.setdefault("temp", 0.3)
 
         class TempLoader(ItemLoader):
             default_item_class = TempItem
             default_input_processor = Identity()
             default_output_processor = Compose(TakeFirst())
 
         loader = TempLoader()
         item = loader.load_item()
         self.assertIsInstance(item, TempItem)
-        self.assertEqual(dict(item), {'temp': 0.3})
+        self.assertEqual(dict(item), {"temp": 0.3})
 
 
 class SelectortemLoaderTest(unittest.TestCase):
-    response = HtmlResponse(url="", encoding='utf-8', body=b"""
+    response = HtmlResponse(
+        url="",
+        encoding="utf-8",
+        body=b"""
     <html>
     <body>
     <div id="id">marta</div>
     <p>paragraph</p>
     <a href="http://www.scrapy.org">homepage</a>
     <img src="/images/logo.png" width="244" height="65" alt="Scrapy">
     </body>
     </html>
-    """)
+    """,
+    )
 
     def test_init_method(self):
         l = TestItemLoader()
         self.assertEqual(l.selector, None)
 
     def test_init_method_errors(self):
         l = TestItemLoader()
-        self.assertRaises(RuntimeError, l.add_xpath, 'url', '//a/@href')
-        self.assertRaises(RuntimeError, l.replace_xpath, 'url', '//a/@href')
-        self.assertRaises(RuntimeError, l.get_xpath, '//a/@href')
-        self.assertRaises(RuntimeError, l.add_css, 'name', '#name::text')
-        self.assertRaises(RuntimeError, l.replace_css, 'name', '#name::text')
-        self.assertRaises(RuntimeError, l.get_css, '#name::text')
+        self.assertRaises(RuntimeError, l.add_xpath, "url", "//a/@href")
+        self.assertRaises(RuntimeError, l.replace_xpath, "url", "//a/@href")
+        self.assertRaises(RuntimeError, l.get_xpath, "//a/@href")
+        self.assertRaises(RuntimeError, l.add_css, "name", "#name::text")
+        self.assertRaises(RuntimeError, l.replace_css, "name", "#name::text")
+        self.assertRaises(RuntimeError, l.get_css, "#name::text")
 
     def test_init_method_with_selector(self):
         sel = Selector(text="<html><body><div>marta</div></body></html>")
         l = TestItemLoader(selector=sel)
         self.assertIs(l.selector, sel)
 
-        l.add_xpath('name', '//div/text()')
-        self.assertEqual(l.get_output_value('name'), ['Marta'])
+        l.add_xpath("name", "//div/text()")
+        self.assertEqual(l.get_output_value("name"), ["Marta"])
 
     def test_init_method_with_selector_css(self):
         sel = Selector(text="<html><body><div>marta</div></body></html>")
         l = TestItemLoader(selector=sel)
         self.assertIs(l.selector, sel)
 
-        l.add_css('name', 'div::text')
-        self.assertEqual(l.get_output_value('name'), ['Marta'])
+        l.add_css("name", "div::text")
+        self.assertEqual(l.get_output_value("name"), ["Marta"])
 
     def test_init_method_with_base_response(self):
         """Selector should be None after initialization"""
         response = Response("https://scrapy.org")
         l = TestItemLoader(response=response)
         self.assertIs(l.selector, None)
 
     def test_init_method_with_response(self):
         l = TestItemLoader(response=self.response)
         self.assertTrue(l.selector)
 
-        l.add_xpath('name', '//div/text()')
-        self.assertEqual(l.get_output_value('name'), ['Marta'])
+        l.add_xpath("name", "//div/text()")
+        self.assertEqual(l.get_output_value("name"), ["Marta"])
 
     def test_init_method_with_response_css(self):
         l = TestItemLoader(response=self.response)
         self.assertTrue(l.selector)
 
-        l.add_css('name', 'div::text')
-        self.assertEqual(l.get_output_value('name'), ['Marta'])
+        l.add_css("name", "div::text")
+        self.assertEqual(l.get_output_value("name"), ["Marta"])
 
-        l.add_css('url', 'a::attr(href)')
-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])
+        l.add_css("url", "a::attr(href)")
+        self.assertEqual(l.get_output_value("url"), ["http://www.scrapy.org"])
 
         # combining/accumulating CSS selectors and XPath expressions
-        l.add_xpath('name', '//div/text()')
-        self.assertEqual(l.get_output_value('name'), ['Marta', 'Marta'])
+        l.add_xpath("name", "//div/text()")
+        self.assertEqual(l.get_output_value("name"), ["Marta", "Marta"])
 
-        l.add_xpath('url', '//img/@src')
-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org', '/images/logo.png'])
+        l.add_xpath("url", "//img/@src")
+        self.assertEqual(
+            l.get_output_value("url"), ["http://www.scrapy.org", "/images/logo.png"]
+        )
 
     def test_add_xpath_re(self):
         l = TestItemLoader(response=self.response)
-        l.add_xpath('name', '//div/text()', re='ma')
-        self.assertEqual(l.get_output_value('name'), ['Ma'])
+        l.add_xpath("name", "//div/text()", re="ma")
+        self.assertEqual(l.get_output_value("name"), ["Ma"])
 
     def test_replace_xpath(self):
         l = TestItemLoader(response=self.response)
         self.assertTrue(l.selector)
-        l.add_xpath('name', '//div/text()')
-        self.assertEqual(l.get_output_value('name'), ['Marta'])
-        l.replace_xpath('name', '//p/text()')
-        self.assertEqual(l.get_output_value('name'), ['Paragraph'])
+        l.add_xpath("name", "//div/text()")
+        self.assertEqual(l.get_output_value("name"), ["Marta"])
+        l.replace_xpath("name", "//p/text()")
+        self.assertEqual(l.get_output_value("name"), ["Paragraph"])
 
-        l.replace_xpath('name', ['//p/text()', '//div/text()'])
-        self.assertEqual(l.get_output_value('name'), ['Paragraph', 'Marta'])
+        l.replace_xpath("name", ["//p/text()", "//div/text()"])
+        self.assertEqual(l.get_output_value("name"), ["Paragraph", "Marta"])
 
     def test_get_xpath(self):
         l = TestItemLoader(response=self.response)
-        self.assertEqual(l.get_xpath('//p/text()'), ['paragraph'])
-        self.assertEqual(l.get_xpath('//p/text()', TakeFirst()), 'paragraph')
-        self.assertEqual(l.get_xpath('//p/text()', TakeFirst(), re='pa'), 'pa')
+        self.assertEqual(l.get_xpath("//p/text()"), ["paragraph"])
+        self.assertEqual(l.get_xpath("//p/text()", TakeFirst()), "paragraph")
+        self.assertEqual(l.get_xpath("//p/text()", TakeFirst(), re="pa"), "pa")
 
-        self.assertEqual(l.get_xpath(['//p/text()', '//div/text()']), ['paragraph', 'marta'])
+        self.assertEqual(
+            l.get_xpath(["//p/text()", "//div/text()"]), ["paragraph", "marta"]
+        )
 
     def test_replace_xpath_multi_fields(self):
         l = TestItemLoader(response=self.response)
-        l.add_xpath(None, '//div/text()', TakeFirst(), lambda x: {'name': x})
-        self.assertEqual(l.get_output_value('name'), ['Marta'])
-        l.replace_xpath(None, '//p/text()', TakeFirst(), lambda x: {'name': x})
-        self.assertEqual(l.get_output_value('name'), ['Paragraph'])
+        l.add_xpath(None, "//div/text()", TakeFirst(), lambda x: {"name": x})
+        self.assertEqual(l.get_output_value("name"), ["Marta"])
+        l.replace_xpath(None, "//p/text()", TakeFirst(), lambda x: {"name": x})
+        self.assertEqual(l.get_output_value("name"), ["Paragraph"])
 
     def test_replace_xpath_re(self):
         l = TestItemLoader(response=self.response)
         self.assertTrue(l.selector)
-        l.add_xpath('name', '//div/text()')
-        self.assertEqual(l.get_output_value('name'), ['Marta'])
-        l.replace_xpath('name', '//div/text()', re='ma')
-        self.assertEqual(l.get_output_value('name'), ['Ma'])
+        l.add_xpath("name", "//div/text()")
+        self.assertEqual(l.get_output_value("name"), ["Marta"])
+        l.replace_xpath("name", "//div/text()", re="ma")
+        self.assertEqual(l.get_output_value("name"), ["Ma"])
 
     def test_add_css_re(self):
         l = TestItemLoader(response=self.response)
-        l.add_css('name', 'div::text', re='ma')
-        self.assertEqual(l.get_output_value('name'), ['Ma'])
+        l.add_css("name", "div::text", re="ma")
+        self.assertEqual(l.get_output_value("name"), ["Ma"])
 
-        l.add_css('url', 'a::attr(href)', re='http://(.+)')
-        self.assertEqual(l.get_output_value('url'), ['www.scrapy.org'])
+        l.add_css("url", "a::attr(href)", re="http://(.+)")
+        self.assertEqual(l.get_output_value("url"), ["www.scrapy.org"])
 
     def test_replace_css(self):
         l = TestItemLoader(response=self.response)
         self.assertTrue(l.selector)
-        l.add_css('name', 'div::text')
-        self.assertEqual(l.get_output_value('name'), ['Marta'])
-        l.replace_css('name', 'p::text')
-        self.assertEqual(l.get_output_value('name'), ['Paragraph'])
-
-        l.replace_css('name', ['p::text', 'div::text'])
-        self.assertEqual(l.get_output_value('name'), ['Paragraph', 'Marta'])
-
-        l.add_css('url', 'a::attr(href)', re='http://(.+)')
-        self.assertEqual(l.get_output_value('url'), ['www.scrapy.org'])
-        l.replace_css('url', 'img::attr(src)')
-        self.assertEqual(l.get_output_value('url'), ['/images/logo.png'])
+        l.add_css("name", "div::text")
+        self.assertEqual(l.get_output_value("name"), ["Marta"])
+        l.replace_css("name", "p::text")
+        self.assertEqual(l.get_output_value("name"), ["Paragraph"])
+
+        l.replace_css("name", ["p::text", "div::text"])
+        self.assertEqual(l.get_output_value("name"), ["Paragraph", "Marta"])
+
+        l.add_css("url", "a::attr(href)", re="http://(.+)")
+        self.assertEqual(l.get_output_value("url"), ["www.scrapy.org"])
+        l.replace_css("url", "img::attr(src)")
+        self.assertEqual(l.get_output_value("url"), ["/images/logo.png"])
 
     def test_get_css(self):
         l = TestItemLoader(response=self.response)
-        self.assertEqual(l.get_css('p::text'), ['paragraph'])
-        self.assertEqual(l.get_css('p::text', TakeFirst()), 'paragraph')
-        self.assertEqual(l.get_css('p::text', TakeFirst(), re='pa'), 'pa')
-
-        self.assertEqual(l.get_css(['p::text', 'div::text']), ['paragraph', 'marta'])
-        self.assertEqual(l.get_css(['a::attr(href)', 'img::attr(src)']),
-                         ['http://www.scrapy.org', '/images/logo.png'])
+        self.assertEqual(l.get_css("p::text"), ["paragraph"])
+        self.assertEqual(l.get_css("p::text", TakeFirst()), "paragraph")
+        self.assertEqual(l.get_css("p::text", TakeFirst(), re="pa"), "pa")
+
+        self.assertEqual(l.get_css(["p::text", "div::text"]), ["paragraph", "marta"])
+        self.assertEqual(
+            l.get_css(["a::attr(href)", "img::attr(src)"]),
+            ["http://www.scrapy.org", "/images/logo.png"],
+        )
 
     def test_replace_css_multi_fields(self):
         l = TestItemLoader(response=self.response)
-        l.add_css(None, 'div::text', TakeFirst(), lambda x: {'name': x})
-        self.assertEqual(l.get_output_value('name'), ['Marta'])
-        l.replace_css(None, 'p::text', TakeFirst(), lambda x: {'name': x})
-        self.assertEqual(l.get_output_value('name'), ['Paragraph'])
-
-        l.add_css(None, 'a::attr(href)', TakeFirst(), lambda x: {'url': x})
-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])
-        l.replace_css(None, 'img::attr(src)', TakeFirst(), lambda x: {'url': x})
-        self.assertEqual(l.get_output_value('url'), ['/images/logo.png'])
+        l.add_css(None, "div::text", TakeFirst(), lambda x: {"name": x})
+        self.assertEqual(l.get_output_value("name"), ["Marta"])
+        l.replace_css(None, "p::text", TakeFirst(), lambda x: {"name": x})
+        self.assertEqual(l.get_output_value("name"), ["Paragraph"])
+
+        l.add_css(None, "a::attr(href)", TakeFirst(), lambda x: {"url": x})
+        self.assertEqual(l.get_output_value("url"), ["http://www.scrapy.org"])
+        l.replace_css(None, "img::attr(src)", TakeFirst(), lambda x: {"url": x})
+        self.assertEqual(l.get_output_value("url"), ["/images/logo.png"])
 
     def test_replace_css_re(self):
         l = TestItemLoader(response=self.response)
         self.assertTrue(l.selector)
-        l.add_css('url', 'a::attr(href)')
-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])
-        l.replace_css('url', 'a::attr(href)', re=r'http://www\.(.+)')
-        self.assertEqual(l.get_output_value('url'), ['scrapy.org'])
+        l.add_css("url", "a::attr(href)")
+        self.assertEqual(l.get_output_value("url"), ["http://www.scrapy.org"])
+        l.replace_css("url", "a::attr(href)", re=r"http://www\.(.+)")
+        self.assertEqual(l.get_output_value("url"), ["scrapy.org"])
 
 
 class SubselectorLoaderTest(unittest.TestCase):
-    response = HtmlResponse(url="", encoding='utf-8', body=b"""
+    response = HtmlResponse(
+        url="",
+        encoding="utf-8",
+        body=b"""
     <html>
     <body>
     <header>
       <div id="id">marta</div>
       <p>paragraph</p>
     </header>
     <footer class="footer">
       <a href="http://www.scrapy.org">homepage</a>
       <img src="/images/logo.png" width="244" height="65" alt="Scrapy">
     </footer>
     </body>
     </html>
-    """)
+    """,
+    )
 
     def test_nested_xpath(self):
         l = NestedItemLoader(response=self.response)
 
         nl = l.nested_xpath("//header")
-        nl.add_xpath('name', 'div/text()')
-        nl.add_css('name_div', '#id')
-        nl.add_value('name_value', nl.selector.xpath('div[@id = "id"]/text()').getall())
-
-        self.assertEqual(l.get_output_value('name'), ['marta'])
-        self.assertEqual(l.get_output_value('name_div'), ['<div id="id">marta</div>'])
-        self.assertEqual(l.get_output_value('name_value'), ['marta'])
-
-        self.assertEqual(l.get_output_value('name'), nl.get_output_value('name'))
-        self.assertEqual(l.get_output_value('name_div'), nl.get_output_value('name_div'))
-        self.assertEqual(l.get_output_value('name_value'), nl.get_output_value('name_value'))
+        nl.add_xpath("name", "div/text()")
+        nl.add_css("name_div", "#id")
+        nl.add_value("name_value", nl.selector.xpath('div[@id = "id"]/text()').getall())
+
+        self.assertEqual(l.get_output_value("name"), ["marta"])
+        self.assertEqual(l.get_output_value("name_div"), ['<div id="id">marta</div>'])
+        self.assertEqual(l.get_output_value("name_value"), ["marta"])
+
+        self.assertEqual(l.get_output_value("name"), nl.get_output_value("name"))
+        self.assertEqual(
+            l.get_output_value("name_div"), nl.get_output_value("name_div")
+        )
+        self.assertEqual(
+            l.get_output_value("name_value"), nl.get_output_value("name_value")
+        )
 
     def test_nested_css(self):
         l = NestedItemLoader(response=self.response)
         nl = l.nested_css("header")
-        nl.add_xpath('name', 'div/text()')
-        nl.add_css('name_div', '#id')
-        nl.add_value('name_value', nl.selector.xpath('div[@id = "id"]/text()').getall())
-
-        self.assertEqual(l.get_output_value('name'), ['marta'])
-        self.assertEqual(l.get_output_value('name_div'), ['<div id="id">marta</div>'])
-        self.assertEqual(l.get_output_value('name_value'), ['marta'])
-
-        self.assertEqual(l.get_output_value('name'), nl.get_output_value('name'))
-        self.assertEqual(l.get_output_value('name_div'), nl.get_output_value('name_div'))
-        self.assertEqual(l.get_output_value('name_value'), nl.get_output_value('name_value'))
+        nl.add_xpath("name", "div/text()")
+        nl.add_css("name_div", "#id")
+        nl.add_value("name_value", nl.selector.xpath('div[@id = "id"]/text()').getall())
+
+        self.assertEqual(l.get_output_value("name"), ["marta"])
+        self.assertEqual(l.get_output_value("name_div"), ['<div id="id">marta</div>'])
+        self.assertEqual(l.get_output_value("name_value"), ["marta"])
+
+        self.assertEqual(l.get_output_value("name"), nl.get_output_value("name"))
+        self.assertEqual(
+            l.get_output_value("name_div"), nl.get_output_value("name_div")
+        )
+        self.assertEqual(
+            l.get_output_value("name_value"), nl.get_output_value("name_value")
+        )
 
     def test_nested_replace(self):
         l = NestedItemLoader(response=self.response)
-        nl1 = l.nested_xpath('//footer')
-        nl2 = nl1.nested_xpath('a')
+        nl1 = l.nested_xpath("//footer")
+        nl2 = nl1.nested_xpath("a")
 
-        l.add_xpath('url', '//footer/a/@href')
-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])
-        nl1.replace_xpath('url', 'img/@src')
-        self.assertEqual(l.get_output_value('url'), ['/images/logo.png'])
-        nl2.replace_xpath('url', '@href')
-        self.assertEqual(l.get_output_value('url'), ['http://www.scrapy.org'])
+        l.add_xpath("url", "//footer/a/@href")
+        self.assertEqual(l.get_output_value("url"), ["http://www.scrapy.org"])
+        nl1.replace_xpath("url", "img/@src")
+        self.assertEqual(l.get_output_value("url"), ["/images/logo.png"])
+        nl2.replace_xpath("url", "@href")
+        self.assertEqual(l.get_output_value("url"), ["http://www.scrapy.org"])
 
     def test_nested_ordering(self):
         l = NestedItemLoader(response=self.response)
-        nl1 = l.nested_xpath('//footer')
-        nl2 = nl1.nested_xpath('a')
+        nl1 = l.nested_xpath("//footer")
+        nl2 = nl1.nested_xpath("a")
+
+        nl1.add_xpath("url", "img/@src")
+        l.add_xpath("url", "//footer/a/@href")
+        nl2.add_xpath("url", "text()")
+        l.add_xpath("url", "//footer/a/@href")
 
-        nl1.add_xpath('url', 'img/@src')
-        l.add_xpath('url', '//footer/a/@href')
-        nl2.add_xpath('url', 'text()')
-        l.add_xpath('url', '//footer/a/@href')
-
-        self.assertEqual(l.get_output_value('url'), [
-            '/images/logo.png',
-            'http://www.scrapy.org',
-            'homepage',
-            'http://www.scrapy.org',
-        ])
+        self.assertEqual(
+            l.get_output_value("url"),
+            [
+                "/images/logo.png",
+                "http://www.scrapy.org",
+                "homepage",
+                "http://www.scrapy.org",
+            ],
+        )
 
     def test_nested_load_item(self):
         l = NestedItemLoader(response=self.response)
-        nl1 = l.nested_xpath('//footer')
-        nl2 = nl1.nested_xpath('img')
+        nl1 = l.nested_xpath("//footer")
+        nl2 = nl1.nested_xpath("img")
 
-        l.add_xpath('name', '//header/div/text()')
-        nl1.add_xpath('url', 'a/@href')
-        nl2.add_xpath('image', '@src')
+        l.add_xpath("name", "//header/div/text()")
+        nl1.add_xpath("url", "a/@href")
+        nl2.add_xpath("image", "@src")
 
         item = l.load_item()
 
         assert item is l.item
         assert item is nl1.item
         assert item is nl2.item
 
-        self.assertEqual(item['name'], ['marta'])
-        self.assertEqual(item['url'], ['http://www.scrapy.org'])
-        self.assertEqual(item['image'], ['/images/logo.png'])
+        self.assertEqual(item["name"], ["marta"])
+        self.assertEqual(item["url"], ["http://www.scrapy.org"])
+        self.assertEqual(item["image"], ["/images/logo.png"])
 
 
 # Functions as processors
 
+
 def function_processor_strip(iterable):
     return [x.strip() for x in iterable]
 
 
 def function_processor_upper(iterable):
     return [x.upper() for x in iterable]
 
@@ -534,20 +578,16 @@
 
 
 class FunctionProcessorItemLoader(ItemLoader):
     default_item_class = FunctionProcessorItem
 
 
 class FunctionProcessorTestCase(unittest.TestCase):
-
     def test_processor_defined_in_item(self):
         lo = FunctionProcessorItemLoader()
-        lo.add_value('foo', '  bar  ')
-        lo.add_value('foo', ['  asdf  ', '  qwerty  '])
-        self.assertEqual(
-            dict(lo.load_item()),
-            {'foo': ['BAR', 'ASDF', 'QWERTY']}
-        )
+        lo.add_value("foo", "  bar  ")
+        lo.add_value("foo", ["  asdf  ", "  qwerty  "])
+        self.assertEqual(dict(lo.load_item()), {"foo": ["BAR", "ASDF", "QWERTY"]})
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_loader_deprecated.py` & `Scrapy-2.8.0/tests/test_loader_deprecated.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,18 +3,24 @@
 Once we remove the references from scrapy, we can remove these tests.
 """
 
 import unittest
 import warnings
 from functools import partial
 
-from itemloaders.processors import (Compose, Identity, Join,
-                                    MapCompose, SelectJmes, TakeFirst)
+from itemloaders.processors import (
+    Compose,
+    Identity,
+    Join,
+    MapCompose,
+    SelectJmes,
+    TakeFirst,
+)
 
-from scrapy.item import Item, Field
+from scrapy.item import Field, Item
 from scrapy.loader import ItemLoader
 from scrapy.loader.common import wrap_loader_context
 from scrapy.utils.deprecate import ScrapyDeprecationWarning
 from scrapy.utils.misc import extract_regex
 
 
 # test items
@@ -38,504 +44,519 @@
 
 class DefaultedItemLoader(NameItemLoader):
     default_input_processor = MapCompose(lambda v: v[:-1])
 
 
 # test processors
 def processor_with_args(value, other=None, loader_context=None):
-    if 'key' in loader_context:
-        return loader_context['key']
+    if "key" in loader_context:
+        return loader_context["key"]
     return value
 
 
 class BasicItemLoaderTest(unittest.TestCase):
-
     def test_load_item_using_default_loader(self):
         i = TestItem()
-        i['summary'] = 'lala'
+        i["summary"] = "lala"
         il = ItemLoader(item=i)
-        il.add_value('name', 'marta')
+        il.add_value("name", "marta")
         item = il.load_item()
         assert item is i
-        self.assertEqual(item['summary'], ['lala'])
-        self.assertEqual(item['name'], ['marta'])
+        self.assertEqual(item["summary"], ["lala"])
+        self.assertEqual(item["name"], ["marta"])
 
     def test_load_item_using_custom_loader(self):
         il = TestItemLoader()
-        il.add_value('name', 'marta')
+        il.add_value("name", "marta")
         item = il.load_item()
-        self.assertEqual(item['name'], ['Marta'])
+        self.assertEqual(item["name"], ["Marta"])
 
     def test_load_item_ignore_none_field_values(self):
         def validate_sku(value):
             # Let's assume a SKU is only digits.
             if value.isdigit():
                 return value
 
         class MyLoader(ItemLoader):
             name_out = Compose(lambda vs: vs[0])  # take first which allows empty values
             price_out = Compose(TakeFirst(), float)
             sku_out = Compose(TakeFirst(), validate_sku)
 
-        valid_fragment = 'SKU: 1234'
-        invalid_fragment = 'SKU: not available'
-        sku_re = 'SKU: (.+)'
+        valid_fragment = "SKU: 1234"
+        invalid_fragment = "SKU: not available"
+        sku_re = "SKU: (.+)"
 
         il = MyLoader(item={})
         # Should not return "sku: None".
-        il.add_value('sku', [invalid_fragment], re=sku_re)
+        il.add_value("sku", [invalid_fragment], re=sku_re)
         # Should not ignore empty values.
-        il.add_value('name', '')
-        il.add_value('price', ['0'])
-        self.assertEqual(il.load_item(), {
-            'name': '',
-            'price': 0.0,
-        })
+        il.add_value("name", "")
+        il.add_value("price", ["0"])
+        self.assertEqual(
+            il.load_item(),
+            {
+                "name": "",
+                "price": 0.0,
+            },
+        )
 
-        il.replace_value('sku', [valid_fragment], re=sku_re)
-        self.assertEqual(il.load_item()['sku'], '1234')
+        il.replace_value("sku", [valid_fragment], re=sku_re)
+        self.assertEqual(il.load_item()["sku"], "1234")
 
     def test_self_referencing_loader(self):
         class MyLoader(ItemLoader):
             url_out = TakeFirst()
 
             def img_url_out(self, values):
-                return (self.get_output_value('url') or '') + values[0]
+                return (self.get_output_value("url") or "") + values[0]
 
         il = MyLoader(item={})
-        il.add_value('url', 'http://example.com/')
-        il.add_value('img_url', '1234.png')
-        self.assertEqual(il.load_item(), {
-            'url': 'http://example.com/',
-            'img_url': 'http://example.com/1234.png',
-        })
+        il.add_value("url", "http://example.com/")
+        il.add_value("img_url", "1234.png")
+        self.assertEqual(
+            il.load_item(),
+            {
+                "url": "http://example.com/",
+                "img_url": "http://example.com/1234.png",
+            },
+        )
 
         il = MyLoader(item={})
-        il.add_value('img_url', '1234.png')
-        self.assertEqual(il.load_item(), {
-            'img_url': '1234.png',
-        })
+        il.add_value("img_url", "1234.png")
+        self.assertEqual(
+            il.load_item(),
+            {
+                "img_url": "1234.png",
+            },
+        )
 
     def test_add_value(self):
         il = TestItemLoader()
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_collected_values('name'), ['Marta'])
-        self.assertEqual(il.get_output_value('name'), ['Marta'])
-        il.add_value('name', 'pepe')
-        self.assertEqual(il.get_collected_values('name'), ['Marta', 'Pepe'])
-        self.assertEqual(il.get_output_value('name'), ['Marta', 'Pepe'])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_collected_values("name"), ["Marta"])
+        self.assertEqual(il.get_output_value("name"), ["Marta"])
+        il.add_value("name", "pepe")
+        self.assertEqual(il.get_collected_values("name"), ["Marta", "Pepe"])
+        self.assertEqual(il.get_output_value("name"), ["Marta", "Pepe"])
 
         # test add object value
-        il.add_value('summary', {'key': 1})
-        self.assertEqual(il.get_collected_values('summary'), [{'key': 1}])
+        il.add_value("summary", {"key": 1})
+        self.assertEqual(il.get_collected_values("summary"), [{"key": 1}])
 
-        il.add_value(None, 'Jim', lambda x: {'name': x})
-        self.assertEqual(il.get_collected_values('name'), ['Marta', 'Pepe', 'Jim'])
+        il.add_value(None, "Jim", lambda x: {"name": x})
+        self.assertEqual(il.get_collected_values("name"), ["Marta", "Pepe", "Jim"])
 
     def test_add_zero(self):
         il = NameItemLoader()
-        il.add_value('name', 0)
-        self.assertEqual(il.get_collected_values('name'), [0])
+        il.add_value("name", 0)
+        self.assertEqual(il.get_collected_values("name"), [0])
 
     def test_replace_value(self):
         il = TestItemLoader()
-        il.replace_value('name', 'marta')
-        self.assertEqual(il.get_collected_values('name'), ['Marta'])
-        self.assertEqual(il.get_output_value('name'), ['Marta'])
-        il.replace_value('name', 'pepe')
-        self.assertEqual(il.get_collected_values('name'), ['Pepe'])
-        self.assertEqual(il.get_output_value('name'), ['Pepe'])
+        il.replace_value("name", "marta")
+        self.assertEqual(il.get_collected_values("name"), ["Marta"])
+        self.assertEqual(il.get_output_value("name"), ["Marta"])
+        il.replace_value("name", "pepe")
+        self.assertEqual(il.get_collected_values("name"), ["Pepe"])
+        self.assertEqual(il.get_output_value("name"), ["Pepe"])
 
-        il.replace_value(None, 'Jim', lambda x: {'name': x})
-        self.assertEqual(il.get_collected_values('name'), ['Jim'])
+        il.replace_value(None, "Jim", lambda x: {"name": x})
+        self.assertEqual(il.get_collected_values("name"), ["Jim"])
 
     def test_get_value(self):
         il = NameItemLoader()
-        self.assertEqual('FOO', il.get_value(['foo', 'bar'], TakeFirst(), str.upper))
-        self.assertEqual(['foo', 'bar'], il.get_value(['name:foo', 'name:bar'], re='name:(.*)$'))
-        self.assertEqual('foo', il.get_value(['name:foo', 'name:bar'], TakeFirst(), re='name:(.*)$'))
-
-        il.add_value('name', ['name:foo', 'name:bar'], TakeFirst(), re='name:(.*)$')
-        self.assertEqual(['foo'], il.get_collected_values('name'))
-        il.replace_value('name', 'name:bar', re='name:(.*)$')
-        self.assertEqual(['bar'], il.get_collected_values('name'))
+        self.assertEqual("FOO", il.get_value(["foo", "bar"], TakeFirst(), str.upper))
+        self.assertEqual(
+            ["foo", "bar"], il.get_value(["name:foo", "name:bar"], re="name:(.*)$")
+        )
+        self.assertEqual(
+            "foo", il.get_value(["name:foo", "name:bar"], TakeFirst(), re="name:(.*)$")
+        )
+
+        il.add_value("name", ["name:foo", "name:bar"], TakeFirst(), re="name:(.*)$")
+        self.assertEqual(["foo"], il.get_collected_values("name"))
+        il.replace_value("name", "name:bar", re="name:(.*)$")
+        self.assertEqual(["bar"], il.get_collected_values("name"))
 
     def test_iter_on_input_processor_input(self):
         class NameFirstItemLoader(NameItemLoader):
             name_in = TakeFirst()
 
         il = NameFirstItemLoader()
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_collected_values('name'), ['marta'])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_collected_values("name"), ["marta"])
         il = NameFirstItemLoader()
-        il.add_value('name', ['marta', 'jose'])
-        self.assertEqual(il.get_collected_values('name'), ['marta'])
+        il.add_value("name", ["marta", "jose"])
+        self.assertEqual(il.get_collected_values("name"), ["marta"])
 
         il = NameFirstItemLoader()
-        il.replace_value('name', 'marta')
-        self.assertEqual(il.get_collected_values('name'), ['marta'])
+        il.replace_value("name", "marta")
+        self.assertEqual(il.get_collected_values("name"), ["marta"])
         il = NameFirstItemLoader()
-        il.replace_value('name', ['marta', 'jose'])
-        self.assertEqual(il.get_collected_values('name'), ['marta'])
+        il.replace_value("name", ["marta", "jose"])
+        self.assertEqual(il.get_collected_values("name"), ["marta"])
 
         il = NameFirstItemLoader()
-        il.add_value('name', 'marta')
-        il.add_value('name', ['jose', 'pedro'])
-        self.assertEqual(il.get_collected_values('name'), ['marta', 'jose'])
+        il.add_value("name", "marta")
+        il.add_value("name", ["jose", "pedro"])
+        self.assertEqual(il.get_collected_values("name"), ["marta", "jose"])
 
     def test_map_compose_filter(self):
         def filter_world(x):
-            return None if x == 'world' else x
+            return None if x == "world" else x
 
         proc = MapCompose(filter_world, str.upper)
-        self.assertEqual(proc(['hello', 'world', 'this', 'is', 'scrapy']),
-                         ['HELLO', 'THIS', 'IS', 'SCRAPY'])
+        self.assertEqual(
+            proc(["hello", "world", "this", "is", "scrapy"]),
+            ["HELLO", "THIS", "IS", "SCRAPY"],
+        )
 
     def test_map_compose_filter_multil(self):
         class TestItemLoader(NameItemLoader):
             name_in = MapCompose(lambda v: v.title(), lambda v: v[:-1])
 
         il = TestItemLoader()
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_output_value('name'), ['Mart'])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_output_value("name"), ["Mart"])
         item = il.load_item()
-        self.assertEqual(item['name'], ['Mart'])
+        self.assertEqual(item["name"], ["Mart"])
 
     def test_default_input_processor(self):
         il = DefaultedItemLoader()
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_output_value('name'), ['mart'])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_output_value("name"), ["mart"])
 
     def test_inherited_default_input_processor(self):
         class InheritDefaultedItemLoader(DefaultedItemLoader):
             pass
 
         il = InheritDefaultedItemLoader()
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_output_value('name'), ['mart'])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_output_value("name"), ["mart"])
 
     def test_input_processor_inheritance(self):
         class ChildItemLoader(TestItemLoader):
             url_in = MapCompose(lambda v: v.lower())
 
         il = ChildItemLoader()
-        il.add_value('url', 'HTTP://scrapy.ORG')
-        self.assertEqual(il.get_output_value('url'), ['http://scrapy.org'])
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_output_value('name'), ['Marta'])
+        il.add_value("url", "HTTP://scrapy.ORG")
+        self.assertEqual(il.get_output_value("url"), ["http://scrapy.org"])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_output_value("name"), ["Marta"])
 
         class ChildChildItemLoader(ChildItemLoader):
             url_in = MapCompose(lambda v: v.upper())
             summary_in = MapCompose(lambda v: v)
 
         il = ChildChildItemLoader()
-        il.add_value('url', 'http://scrapy.org')
-        self.assertEqual(il.get_output_value('url'), ['HTTP://SCRAPY.ORG'])
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_output_value('name'), ['Marta'])
+        il.add_value("url", "http://scrapy.org")
+        self.assertEqual(il.get_output_value("url"), ["HTTP://SCRAPY.ORG"])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_output_value("name"), ["Marta"])
 
     def test_empty_map_compose(self):
         class IdentityDefaultedItemLoader(DefaultedItemLoader):
             name_in = MapCompose()
 
         il = IdentityDefaultedItemLoader()
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_output_value('name'), ['marta'])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_output_value("name"), ["marta"])
 
     def test_identity_input_processor(self):
         class IdentityDefaultedItemLoader(DefaultedItemLoader):
             name_in = Identity()
 
         il = IdentityDefaultedItemLoader()
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_output_value('name'), ['marta'])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_output_value("name"), ["marta"])
 
     def test_extend_custom_input_processors(self):
         class ChildItemLoader(TestItemLoader):
             name_in = MapCompose(TestItemLoader.name_in, str.swapcase)
 
         il = ChildItemLoader()
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_output_value('name'), ['mARTA'])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_output_value("name"), ["mARTA"])
 
     def test_extend_default_input_processors(self):
         class ChildDefaultedItemLoader(DefaultedItemLoader):
-            name_in = MapCompose(DefaultedItemLoader.default_input_processor, str.swapcase)
+            name_in = MapCompose(
+                DefaultedItemLoader.default_input_processor, str.swapcase
+            )
 
         il = ChildDefaultedItemLoader()
-        il.add_value('name', 'marta')
-        self.assertEqual(il.get_output_value('name'), ['MART'])
+        il.add_value("name", "marta")
+        self.assertEqual(il.get_output_value("name"), ["MART"])
 
     def test_output_processor_using_function(self):
         il = TestItemLoader()
-        il.add_value('name', ['mar', 'ta'])
-        self.assertEqual(il.get_output_value('name'), ['Mar', 'Ta'])
+        il.add_value("name", ["mar", "ta"])
+        self.assertEqual(il.get_output_value("name"), ["Mar", "Ta"])
 
         class TakeFirstItemLoader(TestItemLoader):
             name_out = " ".join
 
         il = TakeFirstItemLoader()
-        il.add_value('name', ['mar', 'ta'])
-        self.assertEqual(il.get_output_value('name'), 'Mar Ta')
+        il.add_value("name", ["mar", "ta"])
+        self.assertEqual(il.get_output_value("name"), "Mar Ta")
 
     def test_output_processor_error(self):
         class TestItemLoader(ItemLoader):
             default_item_class = TestItem
             name_out = MapCompose(float)
 
         il = TestItemLoader()
-        il.add_value('name', ['$10'])
+        il.add_value("name", ["$10"])
         try:
-            float('$10')
+            float("$10")
         except Exception as e:
             expected_exc_str = str(e)
 
         exc = None
         try:
             il.load_item()
         except Exception as e:
             exc = e
         assert isinstance(exc, ValueError)
         s = str(exc)
-        assert 'name' in s, s
-        assert '$10' in s, s
-        assert 'ValueError' in s, s
+        assert "name" in s, s
+        assert "$10" in s, s
+        assert "ValueError" in s, s
         assert expected_exc_str in s, s
 
     def test_output_processor_using_classes(self):
         il = TestItemLoader()
-        il.add_value('name', ['mar', 'ta'])
-        self.assertEqual(il.get_output_value('name'), ['Mar', 'Ta'])
+        il.add_value("name", ["mar", "ta"])
+        self.assertEqual(il.get_output_value("name"), ["Mar", "Ta"])
 
         class TakeFirstItemLoader(TestItemLoader):
             name_out = Join()
 
         il = TakeFirstItemLoader()
-        il.add_value('name', ['mar', 'ta'])
-        self.assertEqual(il.get_output_value('name'), 'Mar Ta')
+        il.add_value("name", ["mar", "ta"])
+        self.assertEqual(il.get_output_value("name"), "Mar Ta")
 
         class TakeFirstItemLoader(TestItemLoader):
             name_out = Join("<br>")
 
         il = TakeFirstItemLoader()
-        il.add_value('name', ['mar', 'ta'])
-        self.assertEqual(il.get_output_value('name'), 'Mar<br>Ta')
+        il.add_value("name", ["mar", "ta"])
+        self.assertEqual(il.get_output_value("name"), "Mar<br>Ta")
 
     def test_default_output_processor(self):
         il = TestItemLoader()
-        il.add_value('name', ['mar', 'ta'])
-        self.assertEqual(il.get_output_value('name'), ['Mar', 'Ta'])
+        il.add_value("name", ["mar", "ta"])
+        self.assertEqual(il.get_output_value("name"), ["Mar", "Ta"])
 
         class LalaItemLoader(TestItemLoader):
             default_output_processor = Identity()
 
         il = LalaItemLoader()
-        il.add_value('name', ['mar', 'ta'])
-        self.assertEqual(il.get_output_value('name'), ['Mar', 'Ta'])
+        il.add_value("name", ["mar", "ta"])
+        self.assertEqual(il.get_output_value("name"), ["Mar", "Ta"])
 
     def test_loader_context_on_declaration(self):
         class ChildItemLoader(TestItemLoader):
-            url_in = MapCompose(processor_with_args, key='val')
+            url_in = MapCompose(processor_with_args, key="val")
 
         il = ChildItemLoader()
-        il.add_value('url', 'text')
-        self.assertEqual(il.get_output_value('url'), ['val'])
-        il.replace_value('url', 'text2')
-        self.assertEqual(il.get_output_value('url'), ['val'])
+        il.add_value("url", "text")
+        self.assertEqual(il.get_output_value("url"), ["val"])
+        il.replace_value("url", "text2")
+        self.assertEqual(il.get_output_value("url"), ["val"])
 
     def test_loader_context_on_instantiation(self):
         class ChildItemLoader(TestItemLoader):
             url_in = MapCompose(processor_with_args)
 
-        il = ChildItemLoader(key='val')
-        il.add_value('url', 'text')
-        self.assertEqual(il.get_output_value('url'), ['val'])
-        il.replace_value('url', 'text2')
-        self.assertEqual(il.get_output_value('url'), ['val'])
+        il = ChildItemLoader(key="val")
+        il.add_value("url", "text")
+        self.assertEqual(il.get_output_value("url"), ["val"])
+        il.replace_value("url", "text2")
+        self.assertEqual(il.get_output_value("url"), ["val"])
 
     def test_loader_context_on_assign(self):
         class ChildItemLoader(TestItemLoader):
             url_in = MapCompose(processor_with_args)
 
         il = ChildItemLoader()
-        il.context['key'] = 'val'
-        il.add_value('url', 'text')
-        self.assertEqual(il.get_output_value('url'), ['val'])
-        il.replace_value('url', 'text2')
-        self.assertEqual(il.get_output_value('url'), ['val'])
+        il.context["key"] = "val"
+        il.add_value("url", "text")
+        self.assertEqual(il.get_output_value("url"), ["val"])
+        il.replace_value("url", "text2")
+        self.assertEqual(il.get_output_value("url"), ["val"])
 
     def test_item_passed_to_input_processor_functions(self):
         def processor(value, loader_context):
-            return loader_context['item']['name']
+            return loader_context["item"]["name"]
 
         class ChildItemLoader(TestItemLoader):
             url_in = MapCompose(processor)
 
-        it = TestItem(name='marta')
+        it = TestItem(name="marta")
         il = ChildItemLoader(item=it)
-        il.add_value('url', 'text')
-        self.assertEqual(il.get_output_value('url'), ['marta'])
-        il.replace_value('url', 'text2')
-        self.assertEqual(il.get_output_value('url'), ['marta'])
+        il.add_value("url", "text")
+        self.assertEqual(il.get_output_value("url"), ["marta"])
+        il.replace_value("url", "text2")
+        self.assertEqual(il.get_output_value("url"), ["marta"])
 
     def test_compose_processor(self):
         class TestItemLoader(NameItemLoader):
             name_out = Compose(lambda v: v[0], lambda v: v.title(), lambda v: v[:-1])
 
         il = TestItemLoader()
-        il.add_value('name', ['marta', 'other'])
-        self.assertEqual(il.get_output_value('name'), 'Mart')
+        il.add_value("name", ["marta", "other"])
+        self.assertEqual(il.get_output_value("name"), "Mart")
         item = il.load_item()
-        self.assertEqual(item['name'], 'Mart')
+        self.assertEqual(item["name"], "Mart")
 
     def test_partial_processor(self):
         def join(values, sep=None, loader_context=None, ignored=None):
             if sep is not None:
                 return sep.join(values)
-            elif loader_context and 'sep' in loader_context:
-                return loader_context['sep'].join(values)
-            else:
-                return ''.join(values)
+            if loader_context and "sep" in loader_context:
+                return loader_context["sep"].join(values)
+            return "".join(values)
 
         class TestItemLoader(NameItemLoader):
-            name_out = Compose(partial(join, sep='+'))
-            url_out = Compose(partial(join, loader_context={'sep': '.'}))
-            summary_out = Compose(partial(join, ignored='foo'))
+            name_out = Compose(partial(join, sep="+"))
+            url_out = Compose(partial(join, loader_context={"sep": "."}))
+            summary_out = Compose(partial(join, ignored="foo"))
 
         il = TestItemLoader()
-        il.add_value('name', ['rabbit', 'hole'])
-        il.add_value('url', ['rabbit', 'hole'])
-        il.add_value('summary', ['rabbit', 'hole'])
+        il.add_value("name", ["rabbit", "hole"])
+        il.add_value("url", ["rabbit", "hole"])
+        il.add_value("summary", ["rabbit", "hole"])
         item = il.load_item()
-        self.assertEqual(item['name'], 'rabbit+hole')
-        self.assertEqual(item['url'], 'rabbit.hole')
-        self.assertEqual(item['summary'], 'rabbithole')
+        self.assertEqual(item["name"], "rabbit+hole")
+        self.assertEqual(item["url"], "rabbit.hole")
+        self.assertEqual(item["summary"], "rabbithole")
 
     def test_error_input_processor(self):
         class TestItem(Item):
             name = Field()
 
         class TestItemLoader(ItemLoader):
             default_item_class = TestItem
             name_in = MapCompose(float)
 
         il = TestItemLoader()
-        self.assertRaises(ValueError, il.add_value, 'name',
-                          ['marta', 'other'])
+        self.assertRaises(ValueError, il.add_value, "name", ["marta", "other"])
 
     def test_error_output_processor(self):
         class TestItem(Item):
             name = Field()
 
         class TestItemLoader(ItemLoader):
             default_item_class = TestItem
             name_out = Compose(Join(), float)
 
         il = TestItemLoader()
-        il.add_value('name', 'marta')
+        il.add_value("name", "marta")
         with self.assertRaises(ValueError):
             il.load_item()
 
     def test_error_processor_as_argument(self):
         class TestItem(Item):
             name = Field()
 
         class TestItemLoader(ItemLoader):
             default_item_class = TestItem
 
         il = TestItemLoader()
-        self.assertRaises(ValueError, il.add_value, 'name',
-                          ['marta', 'other'], Compose(float))
+        self.assertRaises(
+            ValueError, il.add_value, "name", ["marta", "other"], Compose(float)
+        )
 
 
 class InitializationFromDictTest(unittest.TestCase):
 
     item_class = dict
 
     def test_keep_single_value(self):
         """Loaded item should contain values from the initial item"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(dict(loaded_item), {'name': ['foo']})
+        self.assertEqual(dict(loaded_item), {"name": ["foo"]})
 
     def test_keep_list(self):
         """Loaded item should contain values from the initial item"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar']})
+        self.assertEqual(dict(loaded_item), {"name": ["foo", "bar"]})
 
     def test_add_value_singlevalue_singlevalue(self):
         """Values added after initialization should be appended"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
-        il.add_value('name', 'bar')
+        il.add_value("name", "bar")
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar']})
+        self.assertEqual(dict(loaded_item), {"name": ["foo", "bar"]})
 
     def test_add_value_singlevalue_list(self):
         """Values added after initialization should be appended"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
-        il.add_value('name', ['item', 'loader'])
+        il.add_value("name", ["item", "loader"])
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'item', 'loader']})
+        self.assertEqual(dict(loaded_item), {"name": ["foo", "item", "loader"]})
 
     def test_add_value_list_singlevalue(self):
         """Values added after initialization should be appended"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
-        il.add_value('name', 'qwerty')
+        il.add_value("name", "qwerty")
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar', 'qwerty']})
+        self.assertEqual(dict(loaded_item), {"name": ["foo", "bar", "qwerty"]})
 
     def test_add_value_list_list(self):
         """Values added after initialization should be appended"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
-        il.add_value('name', ['item', 'loader'])
+        il.add_value("name", ["item", "loader"])
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar', 'item', 'loader']})
+        self.assertEqual(dict(loaded_item), {"name": ["foo", "bar", "item", "loader"]})
 
     def test_get_output_value_singlevalue(self):
         """Getting output value must not remove value from item"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
-        self.assertEqual(il.get_output_value('name'), ['foo'])
+        self.assertEqual(il.get_output_value("name"), ["foo"])
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(loaded_item, dict({'name': ['foo']}))
+        self.assertEqual(loaded_item, dict({"name": ["foo"]}))
 
     def test_get_output_value_list(self):
         """Getting output value must not remove value from item"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
-        self.assertEqual(il.get_output_value('name'), ['foo', 'bar'])
+        self.assertEqual(il.get_output_value("name"), ["foo", "bar"])
         loaded_item = il.load_item()
         self.assertIsInstance(loaded_item, self.item_class)
-        self.assertEqual(loaded_item, dict({'name': ['foo', 'bar']}))
+        self.assertEqual(loaded_item, dict({"name": ["foo", "bar"]}))
 
     def test_values_single(self):
         """Values from initial item must be added to loader._values"""
-        input_item = self.item_class(name='foo')
+        input_item = self.item_class(name="foo")
         il = ItemLoader(item=input_item)
-        self.assertEqual(il._values.get('name'), ['foo'])
+        self.assertEqual(il._values.get("name"), ["foo"])
 
     def test_values_list(self):
         """Values from initial item must be added to loader._values"""
-        input_item = self.item_class(name=['foo', 'bar'])
+        input_item = self.item_class(name=["foo", "bar"])
         il = ItemLoader(item=input_item)
-        self.assertEqual(il._values.get('name'), ['foo', 'bar'])
+        self.assertEqual(il._values.get("name"), ["foo", "bar"])
 
 
 class BaseNoInputReprocessingLoader(ItemLoader):
     title_in = MapCompose(str.upper)
     title_out = TakeFirst()
 
 
@@ -543,130 +564,140 @@
     default_item_class = dict
 
 
 class NoInputReprocessingFromDictTest(unittest.TestCase):
     """
     Loaders initialized from loaded items must not reprocess fields (dict instances)
     """
+
     def test_avoid_reprocessing_with_initial_values_single(self):
-        il = NoInputReprocessingDictLoader(item=dict(title='foo'))
+        il = NoInputReprocessingDictLoader(item=dict(title="foo"))
         il_loaded = il.load_item()
-        self.assertEqual(il_loaded, dict(title='foo'))
-        self.assertEqual(NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title='foo'))
+        self.assertEqual(il_loaded, dict(title="foo"))
+        self.assertEqual(
+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title="foo")
+        )
 
     def test_avoid_reprocessing_with_initial_values_list(self):
-        il = NoInputReprocessingDictLoader(item=dict(title=['foo', 'bar']))
+        il = NoInputReprocessingDictLoader(item=dict(title=["foo", "bar"]))
         il_loaded = il.load_item()
-        self.assertEqual(il_loaded, dict(title='foo'))
-        self.assertEqual(NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title='foo'))
+        self.assertEqual(il_loaded, dict(title="foo"))
+        self.assertEqual(
+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title="foo")
+        )
 
     def test_avoid_reprocessing_without_initial_values_single(self):
         il = NoInputReprocessingDictLoader()
-        il.add_value('title', 'foo')
+        il.add_value("title", "foo")
         il_loaded = il.load_item()
-        self.assertEqual(il_loaded, dict(title='FOO'))
-        self.assertEqual(NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title='FOO'))
+        self.assertEqual(il_loaded, dict(title="FOO"))
+        self.assertEqual(
+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title="FOO")
+        )
 
     def test_avoid_reprocessing_without_initial_values_list(self):
         il = NoInputReprocessingDictLoader()
-        il.add_value('title', ['foo', 'bar'])
+        il.add_value("title", ["foo", "bar"])
         il_loaded = il.load_item()
-        self.assertEqual(il_loaded, dict(title='FOO'))
-        self.assertEqual(NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title='FOO'))
+        self.assertEqual(il_loaded, dict(title="FOO"))
+        self.assertEqual(
+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title="FOO")
+        )
 
 
 class TestOutputProcessorDict(unittest.TestCase):
     def test_output_processor(self):
-
         class TempDict(dict):
             def __init__(self, *args, **kwargs):
                 super().__init__(self, *args, **kwargs)
-                self.setdefault('temp', 0.3)
+                self.setdefault("temp", 0.3)
 
         class TempLoader(ItemLoader):
             default_item_class = TempDict
             default_input_processor = Identity()
             default_output_processor = Compose(TakeFirst())
 
         loader = TempLoader()
         item = loader.load_item()
         self.assertIsInstance(item, TempDict)
-        self.assertEqual(dict(item), {'temp': 0.3})
+        self.assertEqual(dict(item), {"temp": 0.3})
 
 
 class ProcessorsTest(unittest.TestCase):
-
     def test_take_first(self):
         proc = TakeFirst()
-        self.assertEqual(proc([None, '', 'hello', 'world']), 'hello')
-        self.assertEqual(proc([None, '', 0, 'hello', 'world']), 0)
+        self.assertEqual(proc([None, "", "hello", "world"]), "hello")
+        self.assertEqual(proc([None, "", 0, "hello", "world"]), 0)
 
     def test_identity(self):
         proc = Identity()
-        self.assertEqual(proc([None, '', 'hello', 'world']),
-                         [None, '', 'hello', 'world'])
+        self.assertEqual(
+            proc([None, "", "hello", "world"]), [None, "", "hello", "world"]
+        )
 
     def test_join(self):
         proc = Join()
-        self.assertRaises(TypeError, proc, [None, '', 'hello', 'world'])
-        self.assertEqual(proc(['', 'hello', 'world']), ' hello world')
-        self.assertEqual(proc(['hello', 'world']), 'hello world')
-        self.assertIsInstance(proc(['hello', 'world']), str)
+        self.assertRaises(TypeError, proc, [None, "", "hello", "world"])
+        self.assertEqual(proc(["", "hello", "world"]), " hello world")
+        self.assertEqual(proc(["hello", "world"]), "hello world")
+        self.assertIsInstance(proc(["hello", "world"]), str)
 
     def test_compose(self):
         proc = Compose(lambda v: v[0], str.upper)
-        self.assertEqual(proc(['hello', 'world']), 'HELLO')
+        self.assertEqual(proc(["hello", "world"]), "HELLO")
         proc = Compose(str.upper)
         self.assertEqual(proc(None), None)
         proc = Compose(str.upper, stop_on_none=False)
         self.assertRaises(ValueError, proc, None)
         proc = Compose(str.upper, lambda x: x + 1)
-        self.assertRaises(ValueError, proc, 'hello')
+        self.assertRaises(ValueError, proc, "hello")
 
     def test_mapcompose(self):
         def filter_world(x):
-            return None if x == 'world' else x
+            return None if x == "world" else x
+
         proc = MapCompose(filter_world, str.upper)
-        self.assertEqual(proc(['hello', 'world', 'this', 'is', 'scrapy']),
-                         ['HELLO', 'THIS', 'IS', 'SCRAPY'])
+        self.assertEqual(
+            proc(["hello", "world", "this", "is", "scrapy"]),
+            ["HELLO", "THIS", "IS", "SCRAPY"],
+        )
         proc = MapCompose(filter_world, str.upper)
         self.assertEqual(proc(None), [])
         proc = MapCompose(filter_world, str.upper)
         self.assertRaises(ValueError, proc, [1])
         proc = MapCompose(filter_world, lambda x: x + 1)
-        self.assertRaises(ValueError, proc, 'hello')
+        self.assertRaises(ValueError, proc, "hello")
 
 
 class SelectJmesTestCase(unittest.TestCase):
     test_list_equals = {
-        'simple': ('foo.bar', {"foo": {"bar": "baz"}}, "baz"),
-        'invalid': ('foo.bar.baz', {"foo": {"bar": "baz"}}, None),
-        'top_level': ('foo', {"foo": {"bar": "baz"}}, {"bar": "baz"}),
-        'double_vs_single_quote_string': ('foo.bar', {"foo": {"bar": "baz"}}, "baz"),
-        'dict': (
-            'foo.bar[*].name',
+        "simple": ("foo.bar", {"foo": {"bar": "baz"}}, "baz"),
+        "invalid": ("foo.bar.baz", {"foo": {"bar": "baz"}}, None),
+        "top_level": ("foo", {"foo": {"bar": "baz"}}, {"bar": "baz"}),
+        "double_vs_single_quote_string": ("foo.bar", {"foo": {"bar": "baz"}}, "baz"),
+        "dict": (
+            "foo.bar[*].name",
             {"foo": {"bar": [{"name": "one"}, {"name": "two"}]}},
-            ['one', 'two']
+            ["one", "two"],
         ),
-        'list': ('[1]', [1, 2], 2)
+        "list": ("[1]", [1, 2], 2),
     }
 
     def test_output(self):
         for tl in self.test_list_equals:
             expr, test_list, expected = self.test_list_equals[tl]
             test = SelectJmes(expr)(test_list)
             self.assertEqual(
-                test,
-                expected,
-                msg=f'test "{tl}" got {test} expected {expected}'
+                test, expected, msg=f'test "{tl}" got {test} expected {expected}'
             )
 
 
 # Functions as processors
 
+
 def function_processor_strip(iterable):
     return [x.strip() for x in iterable]
 
 
 def function_processor_upper(iterable):
     return [x.upper() for x in iterable]
 
@@ -681,40 +712,35 @@
 class FunctionProcessorDictLoader(ItemLoader):
     default_item_class = dict
     foo_in = function_processor_strip
     foo_out = function_processor_upper
 
 
 class FunctionProcessorTestCase(unittest.TestCase):
-
     def test_processor_defined_in_item_loader(self):
         lo = FunctionProcessorDictLoader()
-        lo.add_value('foo', '  bar  ')
-        lo.add_value('foo', ['  asdf  ', '  qwerty  '])
-        self.assertEqual(
-            dict(lo.load_item()),
-            {'foo': ['BAR', 'ASDF', 'QWERTY']}
-        )
+        lo.add_value("foo", "  bar  ")
+        lo.add_value("foo", ["  asdf  ", "  qwerty  "])
+        self.assertEqual(dict(lo.load_item()), {"foo": ["BAR", "ASDF", "QWERTY"]})
 
 
 class DeprecatedUtilityFunctionsTestCase(unittest.TestCase):
-
     def test_deprecated_wrap_loader_context(self):
         def function(*args):
             return None
 
         with warnings.catch_warnings(record=True) as w:
             wrap_loader_context(function, context={})
 
             assert len(w) == 1
             assert issubclass(w[0].category, ScrapyDeprecationWarning)
 
     def test_deprecated_extract_regex(self):
         with warnings.catch_warnings(record=True) as w:
-            extract_regex(r'\w+', 'this is a test')
+            extract_regex(r"\w+", "this is a test")
 
             assert len(w) == 1
             assert issubclass(w[0].category, ScrapyDeprecationWarning)
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_logformatter.py` & `Scrapy-2.8.0/tests/test_logformatter.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,170 +2,187 @@
 
 from testfixtures import LogCapture
 from twisted.internet import defer
 from twisted.python.failure import Failure
 from twisted.trial.unittest import TestCase as TwistedTestCase
 
 from scrapy.exceptions import DropItem
-from scrapy.utils.test import get_crawler
 from scrapy.http import Request, Response
-from scrapy.item import Item, Field
+from scrapy.item import Field, Item
 from scrapy.logformatter import LogFormatter
 from scrapy.spiders import Spider
+from scrapy.utils.test import get_crawler
 from tests.mockserver import MockServer
 from tests.spiders import ItemSpider
 
 
 class CustomItem(Item):
 
     name = Field()
 
     def __str__(self):
         return f"name: {self['name']}"
 
 
 class LogFormatterTestCase(unittest.TestCase):
-
     def setUp(self):
         self.formatter = LogFormatter()
-        self.spider = Spider('default')
+        self.spider = Spider("default")
 
     def test_crawled_with_referer(self):
         req = Request("http://www.example.com")
         res = Response("http://www.example.com")
         logkws = self.formatter.crawled(req, res, self.spider)
-        logline = logkws['msg'] % logkws['args']
-        self.assertEqual(logline, "Crawled (200) <GET http://www.example.com> (referer: None)")
+        logline = logkws["msg"] % logkws["args"]
+        self.assertEqual(
+            logline, "Crawled (200) <GET http://www.example.com> (referer: None)"
+        )
 
     def test_crawled_without_referer(self):
-        req = Request("http://www.example.com", headers={'referer': 'http://example.com'})
-        res = Response("http://www.example.com", flags=['cached'])
+        req = Request(
+            "http://www.example.com", headers={"referer": "http://example.com"}
+        )
+        res = Response("http://www.example.com", flags=["cached"])
         logkws = self.formatter.crawled(req, res, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         self.assertEqual(
             logline,
-            "Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']")
+            "Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']",
+        )
 
     def test_flags_in_request(self):
-        req = Request("http://www.example.com", flags=['test', 'flag'])
+        req = Request("http://www.example.com", flags=["test", "flag"])
         res = Response("http://www.example.com")
         logkws = self.formatter.crawled(req, res, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         self.assertEqual(
             logline,
-            "Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)")
+            "Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)",
+        )
 
     def test_dropped(self):
         item = {}
         exception = Exception("\u2018")
         response = Response("http://www.example.com")
         logkws = self.formatter.dropped(item, exception, response, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         lines = logline.splitlines()
         assert all(isinstance(x, str) for x in lines)
-        self.assertEqual(lines, ["Dropped: \u2018", '{}'])
+        self.assertEqual(lines, ["Dropped: \u2018", "{}"])
 
     def test_item_error(self):
         # In practice, the complete traceback is shown by passing the
         # 'exc_info' argument to the logging function
-        item = {'key': 'value'}
+        item = {"key": "value"}
         exception = Exception()
         response = Response("http://www.example.com")
         logkws = self.formatter.item_error(item, exception, response, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         self.assertEqual(logline, "Error processing {'key': 'value'}")
 
     def test_spider_error(self):
         # In practice, the complete traceback is shown by passing the
         # 'exc_info' argument to the logging function
         failure = Failure(Exception())
-        request = Request("http://www.example.com", headers={'Referer': 'http://example.org'})
+        request = Request(
+            "http://www.example.com", headers={"Referer": "http://example.org"}
+        )
         response = Response("http://www.example.com", request=request)
         logkws = self.formatter.spider_error(failure, request, response, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         self.assertEqual(
             logline,
-            "Spider error processing <GET http://www.example.com> (referer: http://example.org)"
+            "Spider error processing <GET http://www.example.com> (referer: http://example.org)",
         )
 
     def test_download_error_short(self):
         # In practice, the complete traceback is shown by passing the
         # 'exc_info' argument to the logging function
         failure = Failure(Exception())
         request = Request("http://www.example.com")
         logkws = self.formatter.download_error(failure, request, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         self.assertEqual(logline, "Error downloading <GET http://www.example.com>")
 
     def test_download_error_long(self):
         # In practice, the complete traceback is shown by passing the
         # 'exc_info' argument to the logging function
         failure = Failure(Exception())
         request = Request("http://www.example.com")
-        logkws = self.formatter.download_error(failure, request, self.spider, "Some message")
-        logline = logkws['msg'] % logkws['args']
-        self.assertEqual(logline, "Error downloading <GET http://www.example.com>: Some message")
+        logkws = self.formatter.download_error(
+            failure, request, self.spider, "Some message"
+        )
+        logline = logkws["msg"] % logkws["args"]
+        self.assertEqual(
+            logline, "Error downloading <GET http://www.example.com>: Some message"
+        )
 
     def test_scraped(self):
         item = CustomItem()
-        item['name'] = '\xa3'
+        item["name"] = "\xa3"
         response = Response("http://www.example.com")
         logkws = self.formatter.scraped(item, response, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         lines = logline.splitlines()
         assert all(isinstance(x, str) for x in lines)
-        self.assertEqual(lines, ["Scraped from <200 http://www.example.com>", 'name: \xa3'])
+        self.assertEqual(
+            lines, ["Scraped from <200 http://www.example.com>", "name: \xa3"]
+        )
 
 
 class LogFormatterSubclass(LogFormatter):
     def crawled(self, request, response, spider):
         kwargs = super().crawled(request, response, spider)
-        CRAWLEDMSG = (
-            "Crawled (%(status)s) %(request)s (referer: %(referer)s) %(flags)s"
-        )
-        log_args = kwargs['args']
-        log_args['flags'] = str(request.flags)
+        CRAWLEDMSG = "Crawled (%(status)s) %(request)s (referer: %(referer)s) %(flags)s"
+        log_args = kwargs["args"]
+        log_args["flags"] = str(request.flags)
         return {
-            'level': kwargs['level'],
-            'msg': CRAWLEDMSG,
-            'args': log_args,
+            "level": kwargs["level"],
+            "msg": CRAWLEDMSG,
+            "args": log_args,
         }
 
 
 class LogformatterSubclassTest(LogFormatterTestCase):
     def setUp(self):
         self.formatter = LogFormatterSubclass()
-        self.spider = Spider('default')
+        self.spider = Spider("default")
 
     def test_crawled_with_referer(self):
         req = Request("http://www.example.com")
         res = Response("http://www.example.com")
         logkws = self.formatter.crawled(req, res, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         self.assertEqual(
-            logline,
-            "Crawled (200) <GET http://www.example.com> (referer: None) []")
+            logline, "Crawled (200) <GET http://www.example.com> (referer: None) []"
+        )
 
     def test_crawled_without_referer(self):
-        req = Request("http://www.example.com", headers={'referer': 'http://example.com'}, flags=['cached'])
+        req = Request(
+            "http://www.example.com",
+            headers={"referer": "http://example.com"},
+            flags=["cached"],
+        )
         res = Response("http://www.example.com")
         logkws = self.formatter.crawled(req, res, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         self.assertEqual(
             logline,
-            "Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']")
+            "Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']",
+        )
 
     def test_flags_in_request(self):
-        req = Request("http://www.example.com", flags=['test', 'flag'])
+        req = Request("http://www.example.com", flags=["test", "flag"])
         res = Response("http://www.example.com")
         logkws = self.formatter.crawled(req, res, self.spider)
-        logline = logkws['msg'] % logkws['args']
+        logline = logkws["msg"] % logkws["args"]
         self.assertEqual(
             logline,
-            "Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']")
+            "Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']",
+        )
 
 
 class SkipMessagesLogFormatter(LogFormatter):
     def crawled(self, *args, **kwargs):
         return None
 
     def scraped(self, *args, **kwargs):
@@ -187,16 +204,16 @@
 
 
 class ShowOrSkipMessagesTestCase(TwistedTestCase):
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
         self.base_settings = {
-            'LOG_LEVEL': 'DEBUG',
-            'ITEM_PIPELINES': {
+            "LOG_LEVEL": "DEBUG",
+            "ITEM_PIPELINES": {
                 DropSomeItemsPipeline: 300,
             },
         }
 
     def tearDown(self):
         self.mockserver.__exit__(None, None, None)
 
@@ -208,15 +225,15 @@
         self.assertIn("Scraped from <200 http://127.0.0.1:", str(lc))
         self.assertIn("Crawled (200) <GET http://127.0.0.1:", str(lc))
         self.assertIn("Dropped: Ignoring item", str(lc))
 
     @defer.inlineCallbacks
     def test_skip_messages(self):
         settings = self.base_settings.copy()
-        settings['LOG_FORMATTER'] = SkipMessagesLogFormatter
+        settings["LOG_FORMATTER"] = SkipMessagesLogFormatter
         crawler = get_crawler(ItemSpider, settings)
         with LogCapture() as lc:
             yield crawler.crawl(mockserver=self.mockserver)
         self.assertNotIn("Scraped from <200 http://127.0.0.1:", str(lc))
         self.assertNotIn("Crawled (200) <GET http://127.0.0.1:", str(lc))
         self.assertNotIn("Dropped: Ignoring item", str(lc))
```

### Comparing `Scrapy-2.7.1/tests/test_middleware.py` & `Scrapy-2.8.0/tests/test_middleware.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,87 +1,85 @@
 from twisted.trial import unittest
 
-from scrapy.settings import Settings
 from scrapy.exceptions import NotConfigured
 from scrapy.middleware import MiddlewareManager
+from scrapy.settings import Settings
 
 
 class M1:
-
     def open_spider(self, spider):
         pass
 
     def close_spider(self, spider):
         pass
 
     def process(self, response, request, spider):
         pass
 
 
 class M2:
-
     def open_spider(self, spider):
         pass
 
     def close_spider(self, spider):
         pass
 
     pass
 
 
 class M3:
-
     def process(self, response, request, spider):
         pass
 
 
 class MOff:
-
     def open_spider(self, spider):
         pass
 
     def close_spider(self, spider):
         pass
 
     def __init__(self):
         raise NotConfigured
 
 
 class TestMiddlewareManager(MiddlewareManager):
-
     @classmethod
     def _get_mwlist_from_settings(cls, settings):
         return [M1, MOff, M3]
 
     def _add_middleware(self, mw):
         super()._add_middleware(mw)
-        if hasattr(mw, 'process'):
-            self.methods['process'].append(mw.process)
+        if hasattr(mw, "process"):
+            self.methods["process"].append(mw.process)
 
 
 class MiddlewareManagerTest(unittest.TestCase):
-
     def test_init(self):
         m1, m2, m3 = M1(), M2(), M3()
         mwman = TestMiddlewareManager(m1, m2, m3)
-        self.assertEqual(list(mwman.methods['open_spider']), [m1.open_spider, m2.open_spider])
-        self.assertEqual(list(mwman.methods['close_spider']), [m2.close_spider, m1.close_spider])
-        self.assertEqual(list(mwman.methods['process']), [m1.process, m3.process])
+        self.assertEqual(
+            list(mwman.methods["open_spider"]), [m1.open_spider, m2.open_spider]
+        )
+        self.assertEqual(
+            list(mwman.methods["close_spider"]), [m2.close_spider, m1.close_spider]
+        )
+        self.assertEqual(list(mwman.methods["process"]), [m1.process, m3.process])
 
     def test_methods(self):
         mwman = TestMiddlewareManager(M1(), M2(), M3())
         self.assertEqual(
-            [x.__self__.__class__ for x in mwman.methods['open_spider']],
-            [M1, M2])
+            [x.__self__.__class__ for x in mwman.methods["open_spider"]], [M1, M2]
+        )
         self.assertEqual(
-            [x.__self__.__class__ for x in mwman.methods['close_spider']],
-            [M2, M1])
+            [x.__self__.__class__ for x in mwman.methods["close_spider"]], [M2, M1]
+        )
         self.assertEqual(
-            [x.__self__.__class__ for x in mwman.methods['process']],
-            [M1, M3])
+            [x.__self__.__class__ for x in mwman.methods["process"]], [M1, M3]
+        )
 
     def test_enabled(self):
         m1, m2, m3 = M1(), M2(), M3()
         mwman = MiddlewareManager(m1, m2, m3)
         self.assertEqual(mwman.middlewares, (m1, m2, m3))
 
     def test_enabled_from_settings(self):
```

### Comparing `Scrapy-2.7.1/tests/test_pipeline_crawl.py` & `Scrapy-2.8.0/tests/test_pipeline_crawl.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,76 +1,80 @@
-import os
 import shutil
+from pathlib import Path
 
 from testfixtures import LogCapture
 from twisted.internet import defer
 from twisted.trial.unittest import TestCase
 from w3lib.url import add_or_replace_parameter
 
-from scrapy.crawler import CrawlerRunner
 from scrapy import signals
+from scrapy.crawler import CrawlerRunner
 from tests.mockserver import MockServer
 from tests.spiders import SimpleSpider
 
 
 class MediaDownloadSpider(SimpleSpider):
-    name = 'mediadownload'
+    name = "mediadownload"
 
     def _process_url(self, url):
         return url
 
     def parse(self, response):
         self.logger.info(response.headers)
         self.logger.info(response.text)
         item = {
             self.media_key: [],
             self.media_urls_key: [
                 self._process_url(response.urljoin(href))
                 for href in response.xpath(
                     '//table[thead/tr/th="Filename"]/tbody//a/@href'
-                ).getall()],
+                ).getall()
+            ],
         }
         yield item
 
 
 class BrokenLinksMediaDownloadSpider(MediaDownloadSpider):
-    name = 'brokenmedia'
+    name = "brokenmedia"
 
     def _process_url(self, url):
-        return url + '.foo'
+        return url + ".foo"
 
 
 class RedirectedMediaDownloadSpider(MediaDownloadSpider):
-    name = 'redirectedmedia'
+    name = "redirectedmedia"
 
     def _process_url(self, url):
-        return add_or_replace_parameter(self.mockserver.url('/redirect-to'), 'goto', url)
+        return add_or_replace_parameter(
+            self.mockserver.url("/redirect-to"), "goto", url
+        )
 
 
 class FileDownloadCrawlTestCase(TestCase):
-    pipeline_class = 'scrapy.pipelines.files.FilesPipeline'
-    store_setting_key = 'FILES_STORE'
-    media_key = 'files'
-    media_urls_key = 'file_urls'
+    pipeline_class = "scrapy.pipelines.files.FilesPipeline"
+    store_setting_key = "FILES_STORE"
+    media_key = "files"
+    media_urls_key = "file_urls"
     expected_checksums = {
-        '5547178b89448faf0015a13f904c936e',
-        'c2281c83670e31d8aaab7cb642b824db',
-        'ed3f6538dc15d4d9179dae57319edc5f'}
+        "5547178b89448faf0015a13f904c936e",
+        "c2281c83670e31d8aaab7cb642b824db",
+        "ed3f6538dc15d4d9179dae57319edc5f",
+    }
 
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
 
         # prepare a directory for storing files
-        self.tmpmediastore = self.mktemp()
-        os.mkdir(self.tmpmediastore)
+        self.tmpmediastore = Path(self.mktemp())
+        self.tmpmediastore.mkdir()
         self.settings = {
-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
-            'ITEM_PIPELINES': {self.pipeline_class: 1},
-            self.store_setting_key: self.tmpmediastore,
+            "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+            "ITEM_PIPELINES": {self.pipeline_class: 1},
+            self.store_setting_key: str(self.tmpmediastore),
         }
         self.runner = CrawlerRunner(self.settings)
         self.items = []
 
     def tearDown(self):
         shutil.rmtree(self.tmpmediastore)
         self.items = []
@@ -87,117 +91,127 @@
         return crawler
 
     def _assert_files_downloaded(self, items, logs):
         self.assertEqual(len(items), 1)
         self.assertIn(self.media_key, items[0])
 
         # check that logs show the expected number of successful file downloads
-        file_dl_success = 'File (downloaded): Downloaded file from'
+        file_dl_success = "File (downloaded): Downloaded file from"
         self.assertEqual(logs.count(file_dl_success), 3)
 
         # check that the images/files status is `downloaded`
         for item in items:
             for i in item[self.media_key]:
-                self.assertEqual(i['status'], 'downloaded')
+                self.assertEqual(i["status"], "downloaded")
 
         # check that the images/files checksums are what we know they should be
         if self.expected_checksums is not None:
             checksums = set(
-                i['checksum']
-                for item in items
-                for i in item[self.media_key]
+                i["checksum"] for item in items for i in item[self.media_key]
             )
             self.assertEqual(checksums, self.expected_checksums)
 
         # check that the image files where actually written to the media store
         for item in items:
             for i in item[self.media_key]:
-                self.assertTrue(
-                    os.path.exists(
-                        os.path.join(self.tmpmediastore, i['path'])))
+                self.assertTrue((self.tmpmediastore / i["path"]).exists())
 
     def _assert_files_download_failure(self, crawler, items, code, logs):
 
         # check that the item does NOT have the "images/files" field populated
         self.assertEqual(len(items), 1)
         self.assertIn(self.media_key, items[0])
         self.assertFalse(items[0][self.media_key])
 
         # check that there was 1 successful fetch and 3 other responses with non-200 code
-        self.assertEqual(crawler.stats.get_value('downloader/request_method_count/GET'), 4)
-        self.assertEqual(crawler.stats.get_value('downloader/response_count'), 4)
-        self.assertEqual(crawler.stats.get_value('downloader/response_status_count/200'), 1)
-        self.assertEqual(crawler.stats.get_value(f'downloader/response_status_count/{code}'), 3)
+        self.assertEqual(
+            crawler.stats.get_value("downloader/request_method_count/GET"), 4
+        )
+        self.assertEqual(crawler.stats.get_value("downloader/response_count"), 4)
+        self.assertEqual(
+            crawler.stats.get_value("downloader/response_status_count/200"), 1
+        )
+        self.assertEqual(
+            crawler.stats.get_value(f"downloader/response_status_count/{code}"), 3
+        )
 
         # check that logs do show the failure on the file downloads
-        file_dl_failure = f'File (code: {code}): Error downloading file from'
+        file_dl_failure = f"File (code: {code}): Error downloading file from"
         self.assertEqual(logs.count(file_dl_failure), 3)
 
         # check that no files were written to the media store
-        self.assertEqual(os.listdir(self.tmpmediastore), [])
+        self.assertEqual([x for x in self.tmpmediastore.iterdir()], [])
 
     @defer.inlineCallbacks
     def test_download_media(self):
         crawler = self._create_crawler(MediaDownloadSpider)
         with LogCapture() as log:
             yield crawler.crawl(
                 self.mockserver.url("/files/images/"),
                 media_key=self.media_key,
-                media_urls_key=self.media_urls_key)
+                media_urls_key=self.media_urls_key,
+            )
         self._assert_files_downloaded(self.items, str(log))
 
     @defer.inlineCallbacks
     def test_download_media_wrong_urls(self):
         crawler = self._create_crawler(BrokenLinksMediaDownloadSpider)
         with LogCapture() as log:
             yield crawler.crawl(
                 self.mockserver.url("/files/images/"),
                 media_key=self.media_key,
-                media_urls_key=self.media_urls_key)
+                media_urls_key=self.media_urls_key,
+            )
         self._assert_files_download_failure(crawler, self.items, 404, str(log))
 
     @defer.inlineCallbacks
     def test_download_media_redirected_default_failure(self):
         crawler = self._create_crawler(RedirectedMediaDownloadSpider)
         with LogCapture() as log:
             yield crawler.crawl(
                 self.mockserver.url("/files/images/"),
                 media_key=self.media_key,
                 media_urls_key=self.media_urls_key,
-                mockserver=self.mockserver)
+                mockserver=self.mockserver,
+            )
         self._assert_files_download_failure(crawler, self.items, 302, str(log))
 
     @defer.inlineCallbacks
     def test_download_media_redirected_allowed(self):
         settings = dict(self.settings)
-        settings.update({'MEDIA_ALLOW_REDIRECTS': True})
+        settings.update({"MEDIA_ALLOW_REDIRECTS": True})
         runner = CrawlerRunner(settings)
         crawler = self._create_crawler(RedirectedMediaDownloadSpider, runner=runner)
         with LogCapture() as log:
             yield crawler.crawl(
                 self.mockserver.url("/files/images/"),
                 media_key=self.media_key,
                 media_urls_key=self.media_urls_key,
-                mockserver=self.mockserver)
+                mockserver=self.mockserver,
+            )
         self._assert_files_downloaded(self.items, str(log))
-        self.assertEqual(crawler.stats.get_value('downloader/response_status_count/302'), 3)
+        self.assertEqual(
+            crawler.stats.get_value("downloader/response_status_count/302"), 3
+        )
 
 
 try:
     from PIL import Image  # noqa: imported just to check for the import error
 except ImportError:
-    skip_pillow = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'
+    skip_pillow = (
+        "Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow"
+    )
 else:
     skip_pillow = None
 
 
 class ImageDownloadCrawlTestCase(FileDownloadCrawlTestCase):
 
     skip = skip_pillow
 
-    pipeline_class = 'scrapy.pipelines.images.ImagesPipeline'
-    store_setting_key = 'IMAGES_STORE'
-    media_key = 'images'
-    media_urls_key = 'image_urls'
+    pipeline_class = "scrapy.pipelines.images.ImagesPipeline"
+    store_setting_key = "IMAGES_STORE"
+    media_key = "images"
+    media_urls_key = "image_urls"
 
     # somehow checksums for images are different for Python 3.3
     expected_checksums = None
```

### Comparing `Scrapy-2.7.1/tests/test_pipeline_files.py` & `Scrapy-2.8.0/tests/test_pipeline_files.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,17 +1,18 @@
+import dataclasses
 import os
 import random
 import time
 from datetime import datetime
 from io import BytesIO
+from pathlib import Path
 from shutil import rmtree
 from tempfile import mkdtemp
 from unittest import mock
 from urllib.parse import urlparse
-import dataclasses
 
 import attr
 from itemadapter import ItemAdapter
 from twisted.internet import defer
 from twisted.trial import unittest
 
 from scrapy.http import Request, Response
@@ -28,268 +29,334 @@
     assert_gcs_environ,
     get_crawler,
     get_ftp_content_and_delete,
     get_gcs_content_and_delete,
     skip_if_no_boto,
 )
 
-
-def _mocked_download_func(request, info):
-    response = request.meta.get('response')
-    return response() if callable(response) else response
+from .test_pipeline_media import _mocked_download_func
 
 
 class FilesPipelineTestCase(unittest.TestCase):
-
     def setUp(self):
         self.tempdir = mkdtemp()
-        settings_dict = {'FILES_STORE': self.tempdir}
+        settings_dict = {"FILES_STORE": self.tempdir}
         crawler = get_crawler(spidercls=None, settings_dict=settings_dict)
         self.pipeline = FilesPipeline.from_crawler(crawler)
         self.pipeline.download_func = _mocked_download_func
         self.pipeline.open_spider(None)
 
     def tearDown(self):
         rmtree(self.tempdir)
 
     def test_file_path(self):
         file_path = self.pipeline.file_path
         self.assertEqual(
             file_path(Request("https://dev.mydeco.com/mydeco.pdf")),
-            'full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')
+            "full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf",
+        )
         self.assertEqual(
-            file_path(Request("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt")),
-            'full/4ce274dd83db0368bafd7e406f382ae088e39219.txt')
+            file_path(
+                Request(
+                    "http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt"
+                )
+            ),
+            "full/4ce274dd83db0368bafd7e406f382ae088e39219.txt",
+        )
         self.assertEqual(
-            file_path(Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc")),
-            'full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc')
+            file_path(
+                Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc")
+            ),
+            "full/94ccc495a17b9ac5d40e3eabf3afcb8c2c9b9e1a.doc",
+        )
         self.assertEqual(
-            file_path(Request("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg")),
-            'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')
+            file_path(
+                Request(
+                    "http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg"
+                )
+            ),
+            "full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg",
+        )
         self.assertEqual(
             file_path(Request("http://www.dorma.co.uk/images/product_details/2532/")),
-            'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2')
+            "full/97ee6f8a46cbbb418ea91502fd24176865cf39b2",
+        )
         self.assertEqual(
             file_path(Request("http://www.dorma.co.uk/images/product_details/2532")),
-            'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')
+            "full/244e0dd7d96a3b7b01f54eded250c9e272577aa1",
+        )
         self.assertEqual(
-            file_path(Request("http://www.dorma.co.uk/images/product_details/2532"),
-                      response=Response("http://www.dorma.co.uk/images/product_details/2532"),
-                      info=object()),
-            'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')
+            file_path(
+                Request("http://www.dorma.co.uk/images/product_details/2532"),
+                response=Response("http://www.dorma.co.uk/images/product_details/2532"),
+                info=object(),
+            ),
+            "full/244e0dd7d96a3b7b01f54eded250c9e272577aa1",
+        )
         self.assertEqual(
-            file_path(Request("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg.bohaha")),
-            'full/76c00cef2ef669ae65052661f68d451162829507')
+            file_path(
+                Request(
+                    "http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg.bohaha"
+                )
+            ),
+            "full/76c00cef2ef669ae65052661f68d451162829507",
+        )
         self.assertEqual(
-            file_path(Request("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\
-                                    //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y")),
-            'full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png')
+            file_path(
+                Request(
+                    "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\
+                                    //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y"
+                )
+            ),
+            "full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png",
+        )
 
     def test_fs_store(self):
         assert isinstance(self.pipeline.store, FSFilesStore)
         self.assertEqual(self.pipeline.store.basedir, self.tempdir)
 
-        path = 'some/image/key.jpg'
-        fullpath = os.path.join(self.tempdir, 'some', 'image', 'key.jpg')
+        path = "some/image/key.jpg"
+        fullpath = Path(self.tempdir, "some", "image", "key.jpg")
         self.assertEqual(self.pipeline.store._get_filesystem_path(path), fullpath)
 
     @defer.inlineCallbacks
     def test_file_not_expired(self):
         item_url = "http://example.com/file.pdf"
         item = _create_item_with_files(item_url)
         patchers = [
-            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True),
-            mock.patch.object(FSFilesStore, 'stat_file', return_value={
-                'checksum': 'abc', 'last_modified': time.time()}),
-            mock.patch.object(FilesPipeline, 'get_media_requests',
-                              return_value=[_prepare_request_object(item_url)])
+            mock.patch.object(FilesPipeline, "inc_stats", return_value=True),
+            mock.patch.object(
+                FSFilesStore,
+                "stat_file",
+                return_value={"checksum": "abc", "last_modified": time.time()},
+            ),
+            mock.patch.object(
+                FilesPipeline,
+                "get_media_requests",
+                return_value=[_prepare_request_object(item_url)],
+            ),
         ]
         for p in patchers:
             p.start()
 
         result = yield self.pipeline.process_item(item, None)
-        self.assertEqual(result['files'][0]['checksum'], 'abc')
-        self.assertEqual(result['files'][0]['status'], 'uptodate')
+        self.assertEqual(result["files"][0]["checksum"], "abc")
+        self.assertEqual(result["files"][0]["status"], "uptodate")
 
         for p in patchers:
             p.stop()
 
     @defer.inlineCallbacks
     def test_file_expired(self):
         item_url = "http://example.com/file2.pdf"
         item = _create_item_with_files(item_url)
         patchers = [
-            mock.patch.object(FSFilesStore, 'stat_file', return_value={
-                'checksum': 'abc',
-                'last_modified': time.time() - (self.pipeline.expires * 60 * 60 * 24 * 2)}),
-            mock.patch.object(FilesPipeline, 'get_media_requests',
-                              return_value=[_prepare_request_object(item_url)]),
-            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True)
+            mock.patch.object(
+                FSFilesStore,
+                "stat_file",
+                return_value={
+                    "checksum": "abc",
+                    "last_modified": time.time()
+                    - (self.pipeline.expires * 60 * 60 * 24 * 2),
+                },
+            ),
+            mock.patch.object(
+                FilesPipeline,
+                "get_media_requests",
+                return_value=[_prepare_request_object(item_url)],
+            ),
+            mock.patch.object(FilesPipeline, "inc_stats", return_value=True),
         ]
         for p in patchers:
             p.start()
 
         result = yield self.pipeline.process_item(item, None)
-        self.assertNotEqual(result['files'][0]['checksum'], 'abc')
-        self.assertEqual(result['files'][0]['status'], 'downloaded')
+        self.assertNotEqual(result["files"][0]["checksum"], "abc")
+        self.assertEqual(result["files"][0]["status"], "downloaded")
 
         for p in patchers:
             p.stop()
 
     @defer.inlineCallbacks
     def test_file_cached(self):
         item_url = "http://example.com/file3.pdf"
         item = _create_item_with_files(item_url)
         patchers = [
-            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True),
-            mock.patch.object(FSFilesStore, 'stat_file', return_value={
-                'checksum': 'abc',
-                'last_modified': time.time() - (self.pipeline.expires * 60 * 60 * 24 * 2)}),
-            mock.patch.object(FilesPipeline, 'get_media_requests',
-                              return_value=[_prepare_request_object(item_url, flags=['cached'])])
+            mock.patch.object(FilesPipeline, "inc_stats", return_value=True),
+            mock.patch.object(
+                FSFilesStore,
+                "stat_file",
+                return_value={
+                    "checksum": "abc",
+                    "last_modified": time.time()
+                    - (self.pipeline.expires * 60 * 60 * 24 * 2),
+                },
+            ),
+            mock.patch.object(
+                FilesPipeline,
+                "get_media_requests",
+                return_value=[_prepare_request_object(item_url, flags=["cached"])],
+            ),
         ]
         for p in patchers:
             p.start()
 
         result = yield self.pipeline.process_item(item, None)
-        self.assertNotEqual(result['files'][0]['checksum'], 'abc')
-        self.assertEqual(result['files'][0]['status'], 'cached')
+        self.assertNotEqual(result["files"][0]["checksum"], "abc")
+        self.assertEqual(result["files"][0]["status"], "cached")
 
         for p in patchers:
             p.stop()
 
     def test_file_path_from_item(self):
         """
         Custom file path based on item data, overriding default implementation
         """
+
         class CustomFilesPipeline(FilesPipeline):
             def file_path(self, request, response=None, info=None, item=None):
                 return f'full/{item.get("path")}'
 
-        file_path = CustomFilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir})).file_path
-        item = dict(path='path-to-store-file')
+        file_path = CustomFilesPipeline.from_settings(
+            Settings({"FILES_STORE": self.tempdir})
+        ).file_path
+        item = dict(path="path-to-store-file")
         request = Request("http://example.com")
-        self.assertEqual(file_path(request, item=item), 'full/path-to-store-file')
+        self.assertEqual(file_path(request, item=item), "full/path-to-store-file")
 
 
 class FilesPipelineTestCaseFieldsMixin:
-
     def test_item_fields_default(self):
-        url = 'http://www.example.com/files/1.txt'
-        item = self.item_class(name='item1', file_urls=[url])
-        pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': 's3://example/files/'}))
+        url = "http://www.example.com/files/1.txt"
+        item = self.item_class(name="item1", file_urls=[url])
+        pipeline = FilesPipeline.from_settings(
+            Settings({"FILES_STORE": "s3://example/files/"})
+        )
         requests = list(pipeline.get_media_requests(item, None))
         self.assertEqual(requests[0].url, url)
-        results = [(True, {'url': url})]
+        results = [(True, {"url": url})]
         item = pipeline.item_completed(results, item, None)
         files = ItemAdapter(item).get("files")
         self.assertEqual(files, [results[0][1]])
         self.assertIsInstance(item, self.item_class)
 
     def test_item_fields_override_settings(self):
-        url = 'http://www.example.com/files/1.txt'
-        item = self.item_class(name='item1', custom_file_urls=[url])
-        pipeline = FilesPipeline.from_settings(Settings({
-            'FILES_STORE': 's3://example/files/',
-            'FILES_URLS_FIELD': 'custom_file_urls',
-            'FILES_RESULT_FIELD': 'custom_files'
-        }))
+        url = "http://www.example.com/files/1.txt"
+        item = self.item_class(name="item1", custom_file_urls=[url])
+        pipeline = FilesPipeline.from_settings(
+            Settings(
+                {
+                    "FILES_STORE": "s3://example/files/",
+                    "FILES_URLS_FIELD": "custom_file_urls",
+                    "FILES_RESULT_FIELD": "custom_files",
+                }
+            )
+        )
         requests = list(pipeline.get_media_requests(item, None))
         self.assertEqual(requests[0].url, url)
-        results = [(True, {'url': url})]
+        results = [(True, {"url": url})]
         item = pipeline.item_completed(results, item, None)
         custom_files = ItemAdapter(item).get("custom_files")
         self.assertEqual(custom_files, [results[0][1]])
         self.assertIsInstance(item, self.item_class)
 
 
-class FilesPipelineTestCaseFieldsDict(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):
+class FilesPipelineTestCaseFieldsDict(
+    FilesPipelineTestCaseFieldsMixin, unittest.TestCase
+):
     item_class = dict
 
 
 class FilesPipelineTestItem(Item):
     name = Field()
     # default fields
     file_urls = Field()
     files = Field()
     # overridden fields
     custom_file_urls = Field()
     custom_files = Field()
 
 
-class FilesPipelineTestCaseFieldsItem(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):
+class FilesPipelineTestCaseFieldsItem(
+    FilesPipelineTestCaseFieldsMixin, unittest.TestCase
+):
     item_class = FilesPipelineTestItem
 
 
 @dataclasses.dataclass
 class FilesPipelineTestDataClass:
     name: str
     # default fields
     file_urls: list = dataclasses.field(default_factory=list)
     files: list = dataclasses.field(default_factory=list)
     # overridden fields
     custom_file_urls: list = dataclasses.field(default_factory=list)
     custom_files: list = dataclasses.field(default_factory=list)
 
 
-class FilesPipelineTestCaseFieldsDataClass(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):
+class FilesPipelineTestCaseFieldsDataClass(
+    FilesPipelineTestCaseFieldsMixin, unittest.TestCase
+):
     item_class = FilesPipelineTestDataClass
 
 
 @attr.s
 class FilesPipelineTestAttrsItem:
     name = attr.ib(default="")
     # default fields
     file_urls = attr.ib(default=lambda: [])
     files = attr.ib(default=lambda: [])
     # overridden fields
     custom_file_urls = attr.ib(default=lambda: [])
     custom_files = attr.ib(default=lambda: [])
 
 
-class FilesPipelineTestCaseFieldsAttrsItem(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):
+class FilesPipelineTestCaseFieldsAttrsItem(
+    FilesPipelineTestCaseFieldsMixin, unittest.TestCase
+):
     item_class = FilesPipelineTestAttrsItem
 
 
 class FilesPipelineTestCaseCustomSettings(unittest.TestCase):
     default_cls_settings = {
         "EXPIRES": 90,
         "FILES_URLS_FIELD": "file_urls",
-        "FILES_RESULT_FIELD": "files"
+        "FILES_RESULT_FIELD": "files",
     }
     file_cls_attr_settings_map = {
         ("EXPIRES", "FILES_EXPIRES", "expires"),
         ("FILES_URLS_FIELD", "FILES_URLS_FIELD", "files_urls_field"),
-        ("FILES_RESULT_FIELD", "FILES_RESULT_FIELD", "files_result_field")
+        ("FILES_RESULT_FIELD", "FILES_RESULT_FIELD", "files_result_field"),
     }
 
     def setUp(self):
         self.tempdir = mkdtemp()
 
     def tearDown(self):
         rmtree(self.tempdir)
 
     def _generate_fake_settings(self, prefix=None):
-
         def random_string():
             return "".join([chr(random.randint(97, 123)) for _ in range(10)])
 
         settings = {
             "FILES_EXPIRES": random.randint(100, 1000),
             "FILES_URLS_FIELD": random_string(),
             "FILES_RESULT_FIELD": random_string(),
-            "FILES_STORE": self.tempdir
+            "FILES_STORE": self.tempdir,
         }
         if not prefix:
             return settings
 
-        return {prefix.upper() + "_" + k if k != "FILES_STORE" else k: v for k, v in settings.items()}
+        return {
+            prefix.upper() + "_" + k if k != "FILES_STORE" else k: v
+            for k, v in settings.items()
+        }
 
     def _generate_fake_pipeline(self):
-
         class UserDefinedFilePipeline(FilesPipeline):
             EXPIRES = 1001
             FILES_URLS_FIELD = "alfa"
             FILES_RESULT_FIELD = "beta"
 
         return UserDefinedFilePipeline
 
@@ -334,28 +401,32 @@
             self.assertEqual(value, setting_value)
 
     def test_no_custom_settings_for_subclasses(self):
         """
         If there are no settings for subclass and no subclass attributes, pipeline should use
         attributes of base class.
         """
+
         class UserDefinedFilesPipeline(FilesPipeline):
             pass
 
-        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings({"FILES_STORE": self.tempdir}))
+        user_pipeline = UserDefinedFilesPipeline.from_settings(
+            Settings({"FILES_STORE": self.tempdir})
+        )
         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:
             # Values from settings for custom pipeline should be set on pipeline instance.
             custom_value = self.default_cls_settings.get(pipe_attr.upper())
             self.assertEqual(getattr(user_pipeline, pipe_ins_attr), custom_value)
 
     def test_custom_settings_for_subclasses(self):
         """
         If there are custom settings for subclass and NO class attributes, pipeline should use custom
         settings.
         """
+
         class UserDefinedFilesPipeline(FilesPipeline):
             pass
 
         prefix = UserDefinedFilesPipeline.__name__.upper()
         settings = self._generate_fake_settings(prefix=prefix)
         user_pipeline = UserDefinedFilesPipeline.from_settings(Settings(settings))
         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:
@@ -369,25 +440,31 @@
         If there are custom settings for subclass AND class attributes
         setting keys are preferred and override attributes.
         """
         pipeline_cls = self._generate_fake_pipeline()
         prefix = pipeline_cls.__name__.upper()
         settings = self._generate_fake_settings(prefix=prefix)
         user_pipeline = pipeline_cls.from_settings(Settings(settings))
-        for pipe_cls_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:
+        for (
+            pipe_cls_attr,
+            settings_attr,
+            pipe_inst_attr,
+        ) in self.file_cls_attr_settings_map:
             custom_value = settings.get(prefix + "_" + settings_attr)
             self.assertNotEqual(custom_value, self.default_cls_settings[pipe_cls_attr])
             self.assertEqual(getattr(user_pipeline, pipe_inst_attr), custom_value)
 
     def test_cls_attrs_with_DEFAULT_prefix(self):
         class UserDefinedFilesPipeline(FilesPipeline):
             DEFAULT_FILES_RESULT_FIELD = "this"
             DEFAULT_FILES_URLS_FIELD = "that"
 
-        pipeline = UserDefinedFilesPipeline.from_settings(Settings({"FILES_STORE": self.tempdir}))
+        pipeline = UserDefinedFilesPipeline.from_settings(
+            Settings({"FILES_STORE": self.tempdir})
+        )
         self.assertEqual(pipeline.files_result_field, "this")
         self.assertEqual(pipeline.files_urls_field, "that")
 
     def test_user_defined_subclass_default_key_names(self):
         """Test situation when user defines subclass of FilesPipeline,
         but uses attribute names for default pipeline (without prefixing
         them with pipeline class name).
@@ -397,191 +474,199 @@
         class UserPipe(FilesPipeline):
             pass
 
         pipeline_cls = UserPipe.from_settings(Settings(settings))
 
         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:
             expected_value = settings.get(settings_attr)
-            self.assertEqual(getattr(pipeline_cls, pipe_inst_attr),
-                             expected_value)
+            self.assertEqual(getattr(pipeline_cls, pipe_inst_attr), expected_value)
 
 
 class TestS3FilesStore(unittest.TestCase):
-
     @defer.inlineCallbacks
     def test_persist(self):
         skip_if_no_boto()
 
-        bucket = 'mybucket'
-        key = 'export.csv'
-        uri = f's3://{bucket}/{key}'
+        bucket = "mybucket"
+        key = "export.csv"
+        uri = f"s3://{bucket}/{key}"
         buffer = mock.MagicMock()
-        meta = {'foo': 'bar'}
-        path = ''
-        content_type = 'image/png'
+        meta = {"foo": "bar"}
+        path = ""
+        content_type = "image/png"
 
         store = S3FilesStore(uri)
         from botocore.stub import Stubber
+
         with Stubber(store.s3_client) as stub:
             stub.add_response(
-                'put_object',
+                "put_object",
                 expected_params={
-                    'ACL': S3FilesStore.POLICY,
-                    'Body': buffer,
-                    'Bucket': bucket,
-                    'CacheControl': S3FilesStore.HEADERS['Cache-Control'],
-                    'ContentType': content_type,
-                    'Key': key,
-                    'Metadata': meta,
+                    "ACL": S3FilesStore.POLICY,
+                    "Body": buffer,
+                    "Bucket": bucket,
+                    "CacheControl": S3FilesStore.HEADERS["Cache-Control"],
+                    "ContentType": content_type,
+                    "Key": key,
+                    "Metadata": meta,
                 },
                 service_response={},
             )
 
             yield store.persist_file(
                 path,
                 buffer,
                 info=None,
                 meta=meta,
-                headers={'Content-Type': content_type},
+                headers={"Content-Type": content_type},
             )
 
             stub.assert_no_pending_responses()
             self.assertEqual(
                 buffer.method_calls,
                 [
                     mock.call.seek(0),
                     # The call to read does not happen with Stubber
-                ]
+                ],
             )
 
     @defer.inlineCallbacks
     def test_stat(self):
         skip_if_no_boto()
 
-        bucket = 'mybucket'
-        key = 'export.csv'
-        uri = f's3://{bucket}/{key}'
-        checksum = '3187896a9657a28163abb31667df64c8'
+        bucket = "mybucket"
+        key = "export.csv"
+        uri = f"s3://{bucket}/{key}"
+        checksum = "3187896a9657a28163abb31667df64c8"
         last_modified = datetime(2019, 12, 1)
 
         store = S3FilesStore(uri)
         from botocore.stub import Stubber
+
         with Stubber(store.s3_client) as stub:
             stub.add_response(
-                'head_object',
+                "head_object",
                 expected_params={
-                    'Bucket': bucket,
-                    'Key': key,
+                    "Bucket": bucket,
+                    "Key": key,
                 },
                 service_response={
-                    'ETag': f'"{checksum}"',
-                    'LastModified': last_modified,
+                    "ETag": f'"{checksum}"',
+                    "LastModified": last_modified,
                 },
             )
 
-            file_stats = yield store.stat_file('', info=None)
+            file_stats = yield store.stat_file("", info=None)
             self.assertEqual(
                 file_stats,
                 {
-                    'checksum': checksum,
-                    'last_modified': last_modified.timestamp(),
+                    "checksum": checksum,
+                    "last_modified": last_modified.timestamp(),
                 },
             )
 
             stub.assert_no_pending_responses()
 
 
 class TestGCSFilesStore(unittest.TestCase):
     @defer.inlineCallbacks
     def test_persist(self):
         assert_gcs_environ()
-        uri = os.environ.get('GCS_TEST_FILE_URI')
+        uri = os.environ.get("GCS_TEST_FILE_URI")
         if not uri:
             raise unittest.SkipTest("No GCS URI available for testing")
         data = b"TestGCSFilesStore: \xe2\x98\x83"
         buf = BytesIO(data)
-        meta = {'foo': 'bar'}
-        path = 'full/filename'
+        meta = {"foo": "bar"}
+        path = "full/filename"
         store = GCSFilesStore(uri)
-        store.POLICY = 'authenticatedRead'
-        expected_policy = {'role': 'READER', 'entity': 'allAuthenticatedUsers'}
+        store.POLICY = "authenticatedRead"
+        expected_policy = {"role": "READER", "entity": "allAuthenticatedUsers"}
         yield store.persist_file(path, buf, info=None, meta=meta, headers=None)
         s = yield store.stat_file(path, info=None)
-        self.assertIn('last_modified', s)
-        self.assertIn('checksum', s)
-        self.assertEqual(s['checksum'], 'zc2oVgXkbQr2EQdSdw3OPA==')
+        self.assertIn("last_modified", s)
+        self.assertIn("checksum", s)
+        self.assertEqual(s["checksum"], "zc2oVgXkbQr2EQdSdw3OPA==")
         u = urlparse(uri)
         content, acl, blob = get_gcs_content_and_delete(u.hostname, u.path[1:] + path)
         self.assertEqual(content, data)
-        self.assertEqual(blob.metadata, {'foo': 'bar'})
+        self.assertEqual(blob.metadata, {"foo": "bar"})
         self.assertEqual(blob.cache_control, GCSFilesStore.CACHE_CONTROL)
-        self.assertEqual(blob.content_type, 'application/octet-stream')
+        self.assertEqual(blob.content_type, "application/octet-stream")
         self.assertIn(expected_policy, acl)
 
     @defer.inlineCallbacks
     def test_blob_path_consistency(self):
         """Test to make sure that paths used to store files is the same as the one used to get
         already uploaded files.
         """
         assert_gcs_environ()
         try:
-            import google.cloud.storage # noqa
+            import google.cloud.storage  # noqa
         except ModuleNotFoundError:
             raise unittest.SkipTest("google-cloud-storage is not installed")
         else:
-            with mock.patch('google.cloud.storage') as _:
-                with mock.patch('scrapy.pipelines.files.time') as _:
-                    uri = 'gs://my_bucket/my_prefix/'
+            with mock.patch("google.cloud.storage") as _:
+                with mock.patch("scrapy.pipelines.files.time") as _:
+                    uri = "gs://my_bucket/my_prefix/"
                     store = GCSFilesStore(uri)
                     store.bucket = mock.Mock()
-                    path = 'full/my_data.txt'
-                    yield store.persist_file(path, mock.Mock(), info=None, meta=None, headers=None)
+                    path = "full/my_data.txt"
+                    yield store.persist_file(
+                        path, mock.Mock(), info=None, meta=None, headers=None
+                    )
                     yield store.stat_file(path, info=None)
                     expected_blob_path = store.prefix + path
                     store.bucket.blob.assert_called_with(expected_blob_path)
                     store.bucket.get_blob.assert_called_with(expected_blob_path)
 
 
 class TestFTPFileStore(unittest.TestCase):
     @defer.inlineCallbacks
     def test_persist(self):
-        uri = os.environ.get('FTP_TEST_FILE_URI')
+        uri = os.environ.get("FTP_TEST_FILE_URI")
         if not uri:
             raise unittest.SkipTest("No FTP URI available for testing")
         data = b"TestFTPFilesStore: \xe2\x98\x83"
         buf = BytesIO(data)
-        meta = {'foo': 'bar'}
-        path = 'full/filename'
+        meta = {"foo": "bar"}
+        path = "full/filename"
         store = FTPFilesStore(uri)
         empty_dict = yield store.stat_file(path, info=None)
         self.assertEqual(empty_dict, {})
         yield store.persist_file(path, buf, info=None, meta=meta, headers=None)
         stat = yield store.stat_file(path, info=None)
-        self.assertIn('last_modified', stat)
-        self.assertIn('checksum', stat)
-        self.assertEqual(stat['checksum'], 'd113d66b2ec7258724a268bd88eef6b6')
-        path = f'{store.basedir}/{path}'
+        self.assertIn("last_modified", stat)
+        self.assertIn("checksum", stat)
+        self.assertEqual(stat["checksum"], "d113d66b2ec7258724a268bd88eef6b6")
+        path = f"{store.basedir}/{path}"
         content = get_ftp_content_and_delete(
-            path, store.host, store.port,
-            store.username, store.password, store.USE_ACTIVE_MODE)
+            path,
+            store.host,
+            store.port,
+            store.username,
+            store.password,
+            store.USE_ACTIVE_MODE,
+        )
         self.assertEqual(data.decode(), content)
 
 
 class ItemWithFiles(Item):
     file_urls = Field()
     files = Field()
 
 
 def _create_item_with_files(*files):
     item = ItemWithFiles()
-    item['file_urls'] = files
+    item["file_urls"] = files
     return item
 
 
 def _prepare_request_object(item_url, flags=None):
     return Request(
         item_url,
-        meta={'response': Response(item_url, status=200, body=b'data', flags=flags)})
+        meta={"response": Response(item_url, status=200, body=b"data", flags=flags)},
+    )
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_pipeline_media.py` & `Scrapy-2.8.0/tests/test_pipeline_media.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,114 +1,117 @@
-from typing import Optional
 import io
+from typing import Optional
 
 from testfixtures import LogCapture
-from twisted.trial import unittest
-from twisted.python.failure import Failure
 from twisted.internet import reactor
 from twisted.internet.defer import Deferred, inlineCallbacks
+from twisted.python.failure import Failure
+from twisted.trial import unittest
 
 from scrapy import signals
 from scrapy.http import Request, Response
-from scrapy.settings import Settings
-from scrapy.spiders import Spider
+from scrapy.http.request import NO_CALLBACK
 from scrapy.pipelines.files import FileException
 from scrapy.pipelines.images import ImagesPipeline
 from scrapy.pipelines.media import MediaPipeline
+from scrapy.settings import Settings
+from scrapy.spiders import Spider
 from scrapy.utils.deprecate import ScrapyDeprecationWarning
 from scrapy.utils.log import failure_to_exc_info
 from scrapy.utils.signal import disconnect_all
 from scrapy.utils.test import get_crawler
 
-
 try:
     from PIL import Image  # noqa: imported just to check for the import error
 except ImportError:
-    skip_pillow: Optional[str] = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'
+    skip_pillow: Optional[
+        str
+    ] = "Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow"
 else:
     skip_pillow = None
 
 
 def _mocked_download_func(request, info):
-    response = request.meta.get('response')
+    assert request.callback is NO_CALLBACK
+    response = request.meta.get("response")
     return response() if callable(response) else response
 
 
 class BaseMediaPipelineTestCase(unittest.TestCase):
 
     pipeline_class = MediaPipeline
     settings = None
 
     def setUp(self):
         spider_cls = Spider
-        self.spider = spider_cls('media.com')
+        self.spider = spider_cls("media.com")
         crawler = get_crawler(spider_cls, self.settings)
         self.pipe = self.pipeline_class.from_crawler(crawler)
         self.pipe.download_func = _mocked_download_func
         self.pipe.open_spider(self.spider)
         self.info = self.pipe.spiderinfo
         self.fingerprint = crawler.request_fingerprinter.fingerprint
 
     def tearDown(self):
         for name, signal in vars(signals).items():
-            if not name.startswith('_'):
+            if not name.startswith("_"):
                 disconnect_all(signal)
 
     def test_default_media_to_download(self):
-        request = Request('http://url')
+        request = Request("http://url")
         assert self.pipe.media_to_download(request, self.info) is None
 
     def test_default_get_media_requests(self):
-        item = dict(name='name')
+        item = dict(name="name")
         assert self.pipe.get_media_requests(item, self.info) is None
 
     def test_default_media_downloaded(self):
-        request = Request('http://url')
-        response = Response('http://url', body=b'')
+        request = Request("http://url")
+        response = Response("http://url", body=b"")
         assert self.pipe.media_downloaded(response, request, self.info) is response
 
     def test_default_media_failed(self):
-        request = Request('http://url')
+        request = Request("http://url")
         fail = Failure(Exception())
         assert self.pipe.media_failed(fail, request, self.info) is fail
 
     def test_default_item_completed(self):
-        item = dict(name='name')
+        item = dict(name="name")
         assert self.pipe.item_completed([], item, self.info) is item
 
         # Check that failures are logged by default
         fail = Failure(Exception())
         results = [(True, 1), (False, fail)]
 
         with LogCapture() as log:
             new_item = self.pipe.item_completed(results, item, self.info)
 
         assert new_item is item
         assert len(log.records) == 1
         record = log.records[0]
-        assert record.levelname == 'ERROR'
+        assert record.levelname == "ERROR"
         self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))
 
         # disable failure logging and check again
         self.pipe.LOG_FAILED_RESULTS = False
         with LogCapture() as log:
             new_item = self.pipe.item_completed(results, item, self.info)
         assert new_item is item
         assert len(log.records) == 0
 
     @inlineCallbacks
     def test_default_process_item(self):
-        item = dict(name='name')
+        item = dict(name="name")
         new_item = yield self.pipe.process_item(item, self.spider)
         assert new_item is item
 
     def test_modify_media_request(self):
-        request = Request('http://url')
+        request = Request("http://url")
         self.pipe._modify_media_request(request)
-        assert request.meta == {'handle_httpstatus_all': True}
+        assert request.meta == {"handle_httpstatus_all": True}
 
     def test_should_remove_req_res_references_before_caching_the_results(self):
         """Regression test case to prevent a memory leak in the Media Pipeline.
 
         The memory leak is triggered when an exception is raised when a Response
         scheduled by the Media Pipeline is being returned. For example, when a
         FileException('download-error') is raised because the Response status
@@ -130,27 +133,27 @@
         would not detect another kind of leak happening due to old object
         references being kept inside the Media Pipeline cache.
 
         This problem does not occur in Python 2.7 since we don't have Exception
         Chaining (https://www.python.org/dev/peps/pep-3134/).
         """
         # Create sample pair of Request and Response objects
-        request = Request('http://url')
-        response = Response('http://url', body=b'', request=request)
+        request = Request("http://url")
+        response = Response("http://url", body=b"", request=request)
 
         # Simulate the Media Pipeline behavior to produce a Twisted Failure
         try:
             # Simulate a Twisted inline callback returning a Response
             raise StopIteration(response)
         except StopIteration as exc:
             def_gen_return_exc = exc
             try:
                 # Simulate the media_downloaded callback raising a FileException
                 # This usually happens when the status code is not 200 OK
-                raise FileException('download-error')
+                raise FileException("download-error")
             except Exception as exc:
                 file_exc = exc
                 # Simulate Twisted capturing the FileException
                 # It encapsulates the exception inside a Twisted Failure
                 failure = Failure(file_exc)
 
         # The Failure should encapsulate a FileException ...
@@ -167,384 +170,400 @@
         # When calling the method that caches the Request's result ...
         self.pipe._cache_result_and_execute_waiters(failure, fp, info)
         # ... it should store the Twisted Failure ...
         self.assertEqual(info.downloaded[fp], failure)
         # ... encapsulating the original FileException ...
         self.assertEqual(info.downloaded[fp].value, file_exc)
         # ... but it should not store the StopIteration exception on its context
-        context = getattr(info.downloaded[fp].value, '__context__', None)
+        context = getattr(info.downloaded[fp].value, "__context__", None)
         self.assertIsNone(context)
 
 
 class MockedMediaPipeline(MediaPipeline):
-
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self._mockcalled = []
 
     def download(self, request, info):
-        self._mockcalled.append('download')
+        self._mockcalled.append("download")
         return super().download(request, info)
 
     def media_to_download(self, request, info, *, item=None):
-        self._mockcalled.append('media_to_download')
-        if 'result' in request.meta:
-            return request.meta.get('result')
+        self._mockcalled.append("media_to_download")
+        if "result" in request.meta:
+            return request.meta.get("result")
         return super().media_to_download(request, info)
 
     def get_media_requests(self, item, info):
-        self._mockcalled.append('get_media_requests')
-        return item.get('requests')
+        self._mockcalled.append("get_media_requests")
+        return item.get("requests")
 
     def media_downloaded(self, response, request, info, *, item=None):
-        self._mockcalled.append('media_downloaded')
+        self._mockcalled.append("media_downloaded")
         return super().media_downloaded(response, request, info)
 
     def media_failed(self, failure, request, info):
-        self._mockcalled.append('media_failed')
+        self._mockcalled.append("media_failed")
         return super().media_failed(failure, request, info)
 
     def item_completed(self, results, item, info):
-        self._mockcalled.append('item_completed')
+        self._mockcalled.append("item_completed")
         item = super().item_completed(results, item, info)
-        item['results'] = results
+        item["results"] = results
         return item
 
 
 class MediaPipelineTestCase(BaseMediaPipelineTestCase):
 
     pipeline_class = MockedMediaPipeline
 
     def _callback(self, result):
-        self.pipe._mockcalled.append('request_callback')
+        self.pipe._mockcalled.append("request_callback")
         return result
 
     def _errback(self, result):
-        self.pipe._mockcalled.append('request_errback')
+        self.pipe._mockcalled.append("request_errback")
         return result
 
     @inlineCallbacks
     def test_result_succeed(self):
-        rsp = Response('http://url1')
-        req = Request('http://url1', meta=dict(response=rsp),
-                      callback=self._callback, errback=self._errback)
+        rsp = Response("http://url1")
+        req = Request(
+            "http://url1",
+            meta=dict(response=rsp),
+            callback=self._callback,
+            errback=self._errback,
+        )
         item = dict(requests=req)
         new_item = yield self.pipe.process_item(item, self.spider)
-        self.assertEqual(new_item['results'], [(True, rsp)])
+        self.assertEqual(new_item["results"], [(True, rsp)])
         self.assertEqual(
             self.pipe._mockcalled,
-            ['get_media_requests', 'media_to_download', 'media_downloaded', 'request_callback', 'item_completed'])
+            [
+                "get_media_requests",
+                "media_to_download",
+                "media_downloaded",
+                "request_callback",
+                "item_completed",
+            ],
+        )
 
     @inlineCallbacks
     def test_result_failure(self):
         self.pipe.LOG_FAILED_RESULTS = False
         fail = Failure(Exception())
-        req = Request('http://url1', meta=dict(response=fail),
-                      callback=self._callback, errback=self._errback)
+        req = Request(
+            "http://url1",
+            meta=dict(response=fail),
+            callback=self._callback,
+            errback=self._errback,
+        )
         item = dict(requests=req)
         new_item = yield self.pipe.process_item(item, self.spider)
-        self.assertEqual(new_item['results'], [(False, fail)])
+        self.assertEqual(new_item["results"], [(False, fail)])
         self.assertEqual(
             self.pipe._mockcalled,
-            ['get_media_requests', 'media_to_download', 'media_failed', 'request_errback', 'item_completed'])
+            [
+                "get_media_requests",
+                "media_to_download",
+                "media_failed",
+                "request_errback",
+                "item_completed",
+            ],
+        )
 
     @inlineCallbacks
     def test_mix_of_success_and_failure(self):
         self.pipe.LOG_FAILED_RESULTS = False
-        rsp1 = Response('http://url1')
-        req1 = Request('http://url1', meta=dict(response=rsp1))
+        rsp1 = Response("http://url1")
+        req1 = Request("http://url1", meta=dict(response=rsp1))
         fail = Failure(Exception())
-        req2 = Request('http://url2', meta=dict(response=fail))
+        req2 = Request("http://url2", meta=dict(response=fail))
         item = dict(requests=[req1, req2])
         new_item = yield self.pipe.process_item(item, self.spider)
-        self.assertEqual(new_item['results'], [(True, rsp1), (False, fail)])
+        self.assertEqual(new_item["results"], [(True, rsp1), (False, fail)])
         m = self.pipe._mockcalled
         # only once
-        self.assertEqual(m[0], 'get_media_requests')  # first hook called
-        self.assertEqual(m.count('get_media_requests'), 1)
-        self.assertEqual(m.count('item_completed'), 1)
-        self.assertEqual(m[-1], 'item_completed')  # last hook called
+        self.assertEqual(m[0], "get_media_requests")  # first hook called
+        self.assertEqual(m.count("get_media_requests"), 1)
+        self.assertEqual(m.count("item_completed"), 1)
+        self.assertEqual(m[-1], "item_completed")  # last hook called
         # twice, one per request
-        self.assertEqual(m.count('media_to_download'), 2)
+        self.assertEqual(m.count("media_to_download"), 2)
         # one to handle success and other for failure
-        self.assertEqual(m.count('media_downloaded'), 1)
-        self.assertEqual(m.count('media_failed'), 1)
+        self.assertEqual(m.count("media_downloaded"), 1)
+        self.assertEqual(m.count("media_failed"), 1)
 
     @inlineCallbacks
     def test_get_media_requests(self):
         # returns single Request (without callback)
-        req = Request('http://url')
+        req = Request("http://url")
         item = dict(requests=req)  # pass a single item
         new_item = yield self.pipe.process_item(item, self.spider)
         assert new_item is item
         self.assertIn(self.fingerprint(req), self.info.downloaded)
 
         # returns iterable of Requests
-        req1 = Request('http://url1')
-        req2 = Request('http://url2')
+        req1 = Request("http://url1")
+        req2 = Request("http://url2")
         item = dict(requests=iter([req1, req2]))
         new_item = yield self.pipe.process_item(item, self.spider)
         assert new_item is item
         assert self.fingerprint(req1) in self.info.downloaded
         assert self.fingerprint(req2) in self.info.downloaded
 
     @inlineCallbacks
     def test_results_are_cached_across_multiple_items(self):
-        rsp1 = Response('http://url1')
-        req1 = Request('http://url1', meta=dict(response=rsp1))
+        rsp1 = Response("http://url1")
+        req1 = Request("http://url1", meta=dict(response=rsp1))
         item = dict(requests=req1)
         new_item = yield self.pipe.process_item(item, self.spider)
         self.assertTrue(new_item is item)
-        self.assertEqual(new_item['results'], [(True, rsp1)])
+        self.assertEqual(new_item["results"], [(True, rsp1)])
 
         # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same
-        req2 = Request(req1.url, meta=dict(response=Response('http://donot.download.me')))
+        req2 = Request(
+            req1.url, meta=dict(response=Response("http://donot.download.me"))
+        )
         item = dict(requests=req2)
         new_item = yield self.pipe.process_item(item, self.spider)
         self.assertTrue(new_item is item)
         self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))
-        self.assertEqual(new_item['results'], [(True, rsp1)])
+        self.assertEqual(new_item["results"], [(True, rsp1)])
 
     @inlineCallbacks
     def test_results_are_cached_for_requests_of_single_item(self):
-        rsp1 = Response('http://url1')
-        req1 = Request('http://url1', meta=dict(response=rsp1))
-        req2 = Request(req1.url, meta=dict(response=Response('http://donot.download.me')))
+        rsp1 = Response("http://url1")
+        req1 = Request("http://url1", meta=dict(response=rsp1))
+        req2 = Request(
+            req1.url, meta=dict(response=Response("http://donot.download.me"))
+        )
         item = dict(requests=[req1, req2])
         new_item = yield self.pipe.process_item(item, self.spider)
         self.assertTrue(new_item is item)
-        self.assertEqual(new_item['results'], [(True, rsp1), (True, rsp1)])
+        self.assertEqual(new_item["results"], [(True, rsp1), (True, rsp1)])
 
     @inlineCallbacks
     def test_wait_if_request_is_downloading(self):
         def _check_downloading(response):
             fp = self.fingerprint(req1)
             self.assertTrue(fp in self.info.downloading)
             self.assertTrue(fp in self.info.waiting)
             self.assertTrue(fp not in self.info.downloaded)
             self.assertEqual(len(self.info.waiting[fp]), 2)
             return response
 
-        rsp1 = Response('http://url')
+        rsp1 = Response("http://url")
 
         def rsp1_func():
             dfd = Deferred().addCallback(_check_downloading)
-            reactor.callLater(.1, dfd.callback, rsp1)
+            reactor.callLater(0.1, dfd.callback, rsp1)
             return dfd
 
         def rsp2_func():
-            self.fail('it must cache rsp1 result and must not try to redownload')
+            self.fail("it must cache rsp1 result and must not try to redownload")
 
-        req1 = Request('http://url', meta=dict(response=rsp1_func))
+        req1 = Request("http://url", meta=dict(response=rsp1_func))
         req2 = Request(req1.url, meta=dict(response=rsp2_func))
         item = dict(requests=[req1, req2])
         new_item = yield self.pipe.process_item(item, self.spider)
-        self.assertEqual(new_item['results'], [(True, rsp1), (True, rsp1)])
+        self.assertEqual(new_item["results"], [(True, rsp1), (True, rsp1)])
 
     @inlineCallbacks
     def test_use_media_to_download_result(self):
-        req = Request('http://url', meta=dict(result='ITSME', response=self.fail))
+        req = Request("http://url", meta=dict(result="ITSME", response=self.fail))
         item = dict(requests=req)
         new_item = yield self.pipe.process_item(item, self.spider)
-        self.assertEqual(new_item['results'], [(True, 'ITSME')])
+        self.assertEqual(new_item["results"], [(True, "ITSME")])
         self.assertEqual(
             self.pipe._mockcalled,
-            ['get_media_requests', 'media_to_download', 'item_completed'])
+            ["get_media_requests", "media_to_download", "item_completed"],
+        )
 
 
 class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):
-
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self._mockcalled = []
 
     def get_media_requests(self, item, info):
-        item_url = item['image_urls'][0]
+        item_url = item["image_urls"][0]
         output_img = io.BytesIO()
-        img = Image.new('RGB', (60, 30), color='red')
-        img.save(output_img, format='JPEG')
+        img = Image.new("RGB", (60, 30), color="red")
+        img.save(output_img, format="JPEG")
         return Request(
             item_url,
-            meta={'response': Response(item_url, status=200, body=output_img.getvalue())}
+            meta={
+                "response": Response(item_url, status=200, body=output_img.getvalue())
+            },
         )
 
     def inc_stats(self, *args, **kwargs):
         return True
 
     def media_to_download(self, request, info):
-        self._mockcalled.append('media_to_download')
+        self._mockcalled.append("media_to_download")
         return super().media_to_download(request, info)
 
     def media_downloaded(self, response, request, info):
-        self._mockcalled.append('media_downloaded')
+        self._mockcalled.append("media_downloaded")
         return super().media_downloaded(response, request, info)
 
     def file_downloaded(self, response, request, info):
-        self._mockcalled.append('file_downloaded')
+        self._mockcalled.append("file_downloaded")
         return super().file_downloaded(response, request, info)
 
     def file_path(self, request, response=None, info=None):
-        self._mockcalled.append('file_path')
+        self._mockcalled.append("file_path")
         return super().file_path(request, response, info)
 
     def thumb_path(self, request, thumb_id, response=None, info=None):
-        self._mockcalled.append('thumb_path')
-        return super(MockedMediaPipelineDeprecatedMethods, self).thumb_path(request, thumb_id, response, info)
+        self._mockcalled.append("thumb_path")
+        return super().thumb_path(request, thumb_id, response, info)
 
     def get_images(self, response, request, info):
-        self._mockcalled.append('get_images')
-        return super(MockedMediaPipelineDeprecatedMethods, self).get_images(response, request, info)
+        self._mockcalled.append("get_images")
+        return super().get_images(response, request, info)
 
     def image_downloaded(self, response, request, info):
-        self._mockcalled.append('image_downloaded')
+        self._mockcalled.append("image_downloaded")
         return super().image_downloaded(response, request, info)
 
 
 class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):
     skip = skip_pillow
 
     def setUp(self):
         settings_dict = {
-            'IMAGES_STORE': 'store-uri',
-            'IMAGES_THUMBS': {'small': (50, 50)},
+            "IMAGES_STORE": "store-uri",
+            "IMAGES_THUMBS": {"small": (50, 50)},
         }
         crawler = get_crawler(spidercls=None, settings_dict=settings_dict)
         self.pipe = MockedMediaPipelineDeprecatedMethods.from_crawler(crawler)
         self.pipe.download_func = _mocked_download_func
         self.pipe.open_spider(None)
-        self.item = dict(image_urls=['http://picsum.photos/id/1014/200/300'], images=[])
+        self.item = dict(image_urls=["http://picsum.photos/id/1014/200/300"], images=[])
 
     def _assert_method_called_with_warnings(self, method, message, warnings):
         self.assertIn(method, self.pipe._mockcalled)
         warningShown = False
         for warning in warnings:
-            if warning['message'] == message and warning['category'] == ScrapyDeprecationWarning:
+            if (
+                warning["message"] == message
+                and warning["category"] == ScrapyDeprecationWarning
+            ):
                 warningShown = True
         self.assertTrue(warningShown)
 
     @inlineCallbacks
     def test_media_to_download_called(self):
         yield self.pipe.process_item(self.item, None)
         warnings = self.flushWarnings([MediaPipeline._compatible])
         message = (
-            'media_to_download(self, request, info) is deprecated, '
-            'please use media_to_download(self, request, info, *, item=None)'
+            "media_to_download(self, request, info) is deprecated, "
+            "please use media_to_download(self, request, info, *, item=None)"
         )
-        self._assert_method_called_with_warnings('media_to_download', message, warnings)
+        self._assert_method_called_with_warnings("media_to_download", message, warnings)
 
     @inlineCallbacks
     def test_media_downloaded_called(self):
         yield self.pipe.process_item(self.item, None)
         warnings = self.flushWarnings([MediaPipeline._compatible])
         message = (
-            'media_downloaded(self, response, request, info) is deprecated, '
-            'please use media_downloaded(self, response, request, info, *, item=None)'
+            "media_downloaded(self, response, request, info) is deprecated, "
+            "please use media_downloaded(self, response, request, info, *, item=None)"
         )
-        self._assert_method_called_with_warnings('media_downloaded', message, warnings)
+        self._assert_method_called_with_warnings("media_downloaded", message, warnings)
 
     @inlineCallbacks
     def test_file_downloaded_called(self):
         yield self.pipe.process_item(self.item, None)
         warnings = self.flushWarnings([MediaPipeline._compatible])
         message = (
-            'file_downloaded(self, response, request, info) is deprecated, '
-            'please use file_downloaded(self, response, request, info, *, item=None)'
+            "file_downloaded(self, response, request, info) is deprecated, "
+            "please use file_downloaded(self, response, request, info, *, item=None)"
         )
-        self._assert_method_called_with_warnings('file_downloaded', message, warnings)
+        self._assert_method_called_with_warnings("file_downloaded", message, warnings)
 
     @inlineCallbacks
     def test_file_path_called(self):
         yield self.pipe.process_item(self.item, None)
         warnings = self.flushWarnings([MediaPipeline._compatible])
         message = (
-            'file_path(self, request, response=None, info=None) is deprecated, '
-            'please use file_path(self, request, response=None, info=None, *, item=None)'
+            "file_path(self, request, response=None, info=None) is deprecated, "
+            "please use file_path(self, request, response=None, info=None, *, item=None)"
         )
-        self._assert_method_called_with_warnings('file_path', message, warnings)
+        self._assert_method_called_with_warnings("file_path", message, warnings)
 
     @inlineCallbacks
     def test_thumb_path_called(self):
         yield self.pipe.process_item(self.item, None)
         warnings = self.flushWarnings([MediaPipeline._compatible])
         message = (
-            'thumb_path(self, request, thumb_id, response=None, info=None) is deprecated, '
-            'please use thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)'
+            "thumb_path(self, request, thumb_id, response=None, info=None) is deprecated, "
+            "please use thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)"
         )
-        self._assert_method_called_with_warnings('thumb_path', message, warnings)
+        self._assert_method_called_with_warnings("thumb_path", message, warnings)
 
     @inlineCallbacks
     def test_get_images_called(self):
         yield self.pipe.process_item(self.item, None)
         warnings = self.flushWarnings([MediaPipeline._compatible])
         message = (
-            'get_images(self, response, request, info) is deprecated, '
-            'please use get_images(self, response, request, info, *, item=None)'
+            "get_images(self, response, request, info) is deprecated, "
+            "please use get_images(self, response, request, info, *, item=None)"
         )
-        self._assert_method_called_with_warnings('get_images', message, warnings)
+        self._assert_method_called_with_warnings("get_images", message, warnings)
 
     @inlineCallbacks
     def test_image_downloaded_called(self):
         yield self.pipe.process_item(self.item, None)
         warnings = self.flushWarnings([MediaPipeline._compatible])
         message = (
-            'image_downloaded(self, response, request, info) is deprecated, '
-            'please use image_downloaded(self, response, request, info, *, item=None)'
+            "image_downloaded(self, response, request, info) is deprecated, "
+            "please use image_downloaded(self, response, request, info, *, item=None)"
         )
-        self._assert_method_called_with_warnings('image_downloaded', message, warnings)
+        self._assert_method_called_with_warnings("image_downloaded", message, warnings)
 
 
 class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):
-
     def _assert_request_no3xx(self, pipeline_class, settings):
         pipe = pipeline_class(settings=Settings(settings))
-        request = Request('http://url')
+        request = Request("http://url")
         pipe._modify_media_request(request)
 
-        self.assertIn('handle_httpstatus_list', request.meta)
+        self.assertIn("handle_httpstatus_list", request.meta)
         for status, check in [
-                (200, True),
-
-                # These are the status codes we want
-                # the downloader to handle itself
-                (301, False),
-                (302, False),
-                (302, False),
-                (307, False),
-                (308, False),
-
-                # we still want to get 4xx and 5xx
-                (400, True),
-                (404, True),
-                (500, True)]:
+            (200, True),
+            # These are the status codes we want
+            # the downloader to handle itself
+            (301, False),
+            (302, False),
+            (302, False),
+            (307, False),
+            (308, False),
+            # we still want to get 4xx and 5xx
+            (400, True),
+            (404, True),
+            (500, True),
+        ]:
             if check:
-                self.assertIn(status, request.meta['handle_httpstatus_list'])
+                self.assertIn(status, request.meta["handle_httpstatus_list"])
             else:
-                self.assertNotIn(status, request.meta['handle_httpstatus_list'])
+                self.assertNotIn(status, request.meta["handle_httpstatus_list"])
 
     def test_standard_setting(self):
-        self._assert_request_no3xx(
-            MediaPipeline,
-            {
-                'MEDIA_ALLOW_REDIRECTS': True
-            })
+        self._assert_request_no3xx(MediaPipeline, {"MEDIA_ALLOW_REDIRECTS": True})
 
     def test_subclass_standard_setting(self):
-
         class UserDefinedPipeline(MediaPipeline):
             pass
 
-        self._assert_request_no3xx(
-            UserDefinedPipeline,
-            {
-                'MEDIA_ALLOW_REDIRECTS': True
-            })
+        self._assert_request_no3xx(UserDefinedPipeline, {"MEDIA_ALLOW_REDIRECTS": True})
 
     def test_subclass_specific_setting(self):
-
         class UserDefinedPipeline(MediaPipeline):
             pass
 
         self._assert_request_no3xx(
-            UserDefinedPipeline,
-            {
-                'USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS': True
-            })
+            UserDefinedPipeline, {"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS": True}
+        )
```

### Comparing `Scrapy-2.7.1/tests/test_pipelines.py` & `Scrapy-2.8.0/tests/test_pipelines.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,99 +1,101 @@
 import asyncio
 
 from pytest import mark
 from twisted.internet import defer
 from twisted.internet.defer import Deferred
 from twisted.trial import unittest
 
-from scrapy import Spider, signals, Request
-from scrapy.utils.defer import maybe_deferred_to_future, deferred_to_future
+from scrapy import Request, Spider, signals
+from scrapy.utils.defer import deferred_to_future, maybe_deferred_to_future
 from scrapy.utils.test import get_crawler, get_from_asyncio_queue
-
 from tests.mockserver import MockServer
 
 
 class SimplePipeline:
     def process_item(self, item, spider):
-        item['pipeline_passed'] = True
+        item["pipeline_passed"] = True
         return item
 
 
 class DeferredPipeline:
     def cb(self, item):
-        item['pipeline_passed'] = True
+        item["pipeline_passed"] = True
         return item
 
     def process_item(self, item, spider):
         d = Deferred()
         d.addCallback(self.cb)
         d.callback(item)
         return d
 
 
 class AsyncDefPipeline:
     async def process_item(self, item, spider):
         d = Deferred()
         from twisted.internet import reactor
+
         reactor.callLater(0, d.callback, None)
         await maybe_deferred_to_future(d)
-        item['pipeline_passed'] = True
+        item["pipeline_passed"] = True
         return item
 
 
 class AsyncDefAsyncioPipeline:
     async def process_item(self, item, spider):
         d = Deferred()
         from twisted.internet import reactor
+
         reactor.callLater(0, d.callback, None)
         await deferred_to_future(d)
         await asyncio.sleep(0.2)
-        item['pipeline_passed'] = await get_from_asyncio_queue(True)
+        item["pipeline_passed"] = await get_from_asyncio_queue(True)
         return item
 
 
 class AsyncDefNotAsyncioPipeline:
     async def process_item(self, item, spider):
         d1 = Deferred()
         from twisted.internet import reactor
+
         reactor.callLater(0, d1.callback, None)
         await d1
         d2 = Deferred()
         reactor.callLater(0, d2.callback, None)
         await maybe_deferred_to_future(d2)
-        item['pipeline_passed'] = True
+        item["pipeline_passed"] = True
         return item
 
 
 class ItemSpider(Spider):
-    name = 'itemspider'
+    name = "itemspider"
 
     def start_requests(self):
-        yield Request(self.mockserver.url('/status?n=200'))
+        yield Request(self.mockserver.url("/status?n=200"))
 
     def parse(self, response):
-        return {'field': 42}
+        return {"field": 42}
 
 
 class PipelineTestCase(unittest.TestCase):
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
 
     def tearDown(self):
         self.mockserver.__exit__(None, None, None)
 
     def _on_item_scraped(self, item):
         self.assertIsInstance(item, dict)
-        self.assertTrue(item.get('pipeline_passed'))
+        self.assertTrue(item.get("pipeline_passed"))
         self.items.append(item)
 
     def _create_crawler(self, pipeline_class):
         settings = {
-            'ITEM_PIPELINES': {pipeline_class: 1},
+            "ITEM_PIPELINES": {pipeline_class: 1},
         }
         crawler = get_crawler(ItemSpider, settings)
         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)
         self.items = []
         return crawler
 
     @defer.inlineCallbacks
```

### Comparing `Scrapy-2.7.1/tests/test_pqueues.py` & `Scrapy-2.8.0/tests/test_pqueues.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 import tempfile
 import unittest
 
 import queuelib
 
 from scrapy.http.request import Request
-from scrapy.pqueues import ScrapyPriorityQueue, DownloaderAwarePriorityQueue
+from scrapy.pqueues import DownloaderAwarePriorityQueue, ScrapyPriorityQueue
 from scrapy.spiders import Spider
 from scrapy.squeues import FifoMemoryQueue
 from scrapy.utils.test import get_crawler
-
 from tests.test_scheduler import MockDownloader, MockEngine
 
 
 class PriorityQueueTest(unittest.TestCase):
     def setUp(self):
         self.crawler = get_crawler(Spider)
         self.spider = self.crawler._create_spider("foo")
 
     def test_queue_push_pop_one(self):
         temp_dir = tempfile.mkdtemp()
-        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)
+        queue = ScrapyPriorityQueue.from_crawler(
+            self.crawler, FifoMemoryQueue, temp_dir
+        )
         self.assertIsNone(queue.pop())
         self.assertEqual(len(queue), 0)
         req1 = Request("https://example.org/1", priority=1)
         queue.push(req1)
         self.assertEqual(len(queue), 1)
         dequeued = queue.pop()
         self.assertEqual(len(queue), 0)
@@ -31,25 +32,32 @@
         self.assertEqual(dequeued.priority, req1.priority)
         self.assertEqual(queue.close(), [])
 
     def test_no_peek_raises(self):
         if hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
             raise unittest.SkipTest("queuelib.queue.FifoMemoryQueue.peek is defined")
         temp_dir = tempfile.mkdtemp()
-        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)
+        queue = ScrapyPriorityQueue.from_crawler(
+            self.crawler, FifoMemoryQueue, temp_dir
+        )
         queue.push(Request("https://example.org"))
-        with self.assertRaises(NotImplementedError, msg="The underlying queue class does not implement 'peek'"):
+        with self.assertRaises(
+            NotImplementedError,
+            msg="The underlying queue class does not implement 'peek'",
+        ):
             queue.peek()
         queue.close()
 
     def test_peek(self):
         if not hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
             raise unittest.SkipTest("queuelib.queue.FifoMemoryQueue.peek is undefined")
         temp_dir = tempfile.mkdtemp()
-        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir)
+        queue = ScrapyPriorityQueue.from_crawler(
+            self.crawler, FifoMemoryQueue, temp_dir
+        )
         self.assertEqual(len(queue), 0)
         self.assertIsNone(queue.peek())
         req1 = Request("https://example.org/1")
         req2 = Request("https://example.org/2")
         req3 = Request("https://example.org/3")
         queue.push(req1)
         queue.push(req2)
@@ -63,15 +71,17 @@
         self.assertEqual(len(queue), 1)
         self.assertEqual(queue.peek().url, req3.url)
         self.assertEqual(queue.pop().url, req3.url)
         self.assertEqual(queue.close(), [])
 
     def test_queue_push_pop_priorities(self):
         temp_dir = tempfile.mkdtemp()
-        queue = ScrapyPriorityQueue.from_crawler(self.crawler, FifoMemoryQueue, temp_dir, [-1, -2, -3])
+        queue = ScrapyPriorityQueue.from_crawler(
+            self.crawler, FifoMemoryQueue, temp_dir, [-1, -2, -3]
+        )
         self.assertIsNone(queue.pop())
         self.assertEqual(len(queue), 0)
         req1 = Request("https://example.org/1", priority=1)
         req2 = Request("https://example.org/2", priority=2)
         req3 = Request("https://example.org/3", priority=3)
         queue.push(req1)
         queue.push(req2)
@@ -115,15 +125,18 @@
         self.assertEqual(len(self.queue), 0)
         self.assertIsNone(self.queue.pop())
 
     def test_no_peek_raises(self):
         if hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
             raise unittest.SkipTest("queuelib.queue.FifoMemoryQueue.peek is defined")
         self.queue.push(Request("https://example.org"))
-        with self.assertRaises(NotImplementedError, msg="The underlying queue class does not implement 'peek'"):
+        with self.assertRaises(
+            NotImplementedError,
+            msg="The underlying queue class does not implement 'peek'",
+        ):
             self.queue.peek()
 
     def test_peek(self):
         if not hasattr(queuelib.queue.FifoMemoryQueue, "peek"):
             raise unittest.SkipTest("queuelib.queue.FifoMemoryQueue.peek is undefined")
         self.assertEqual(len(self.queue), 0)
         req1 = Request("https://example.org/1")
```

### Comparing `Scrapy-2.7.1/tests/test_proxy_connect.py` & `Scrapy-2.8.0/tests/test_proxy_connect.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,79 +1,88 @@
 import json
 import os
 import re
 import sys
-from subprocess import Popen, PIPE
+from pathlib import Path
+from subprocess import PIPE, Popen
 from urllib.parse import urlsplit, urlunsplit
+
 from testfixtures import LogCapture
 from twisted.internet import defer
 from twisted.trial.unittest import TestCase
 
 from scrapy.http import Request
 from scrapy.utils.test import get_crawler
-
 from tests.mockserver import MockServer
 from tests.spiders import SimpleSpider, SingleRequestSpider
 
 
 class MitmProxy:
-    auth_user = 'scrapy'
-    auth_pass = 'scrapy'
+    auth_user = "scrapy"
+    auth_pass = "scrapy"
 
     def start(self):
         from scrapy.utils.test import get_testenv
+
         script = """
 import sys
 from mitmproxy.tools.main import mitmdump
 sys.argv[0] = "mitmdump"
 sys.exit(mitmdump())
         """
-        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),
-                                 'keys', 'mitmproxy-ca.pem')
-        self.proc = Popen([sys.executable,
-                           '-c', script,
-                           '--listen-host', '127.0.0.1',
-                           '--listen-port', '0',
-                           '--proxyauth', f'{self.auth_user}:{self.auth_pass}',
-                           '--certs', cert_path,
-                           '--ssl-insecure',
-                           ],
-                          stdout=PIPE, env=get_testenv())
-        line = self.proc.stdout.readline().decode('utf-8')
-        host_port = re.search(r'listening at http://([^:]+:\d+)', line).group(1)
-        address = f'http://{self.auth_user}:{self.auth_pass}@{host_port}'
+        cert_path = Path(__file__).parent.resolve() / "keys" / "mitmproxy-ca.pem"
+        self.proc = Popen(
+            [
+                sys.executable,
+                "-c",
+                script,
+                "--listen-host",
+                "127.0.0.1",
+                "--listen-port",
+                "0",
+                "--proxyauth",
+                f"{self.auth_user}:{self.auth_pass}",
+                "--certs",
+                str(cert_path),
+                "--ssl-insecure",
+            ],
+            stdout=PIPE,
+            env=get_testenv(),
+        )
+        line = self.proc.stdout.readline().decode("utf-8")
+        host_port = re.search(r"listening at http://([^:]+:\d+)", line).group(1)
+        address = f"http://{self.auth_user}:{self.auth_pass}@{host_port}"
         return address
 
     def stop(self):
         self.proc.kill()
         self.proc.communicate()
 
 
 def _wrong_credentials(proxy_url):
     bad_auth_proxy = list(urlsplit(proxy_url))
-    bad_auth_proxy[1] = bad_auth_proxy[1].replace('scrapy:scrapy@', 'wrong:wronger@')
+    bad_auth_proxy[1] = bad_auth_proxy[1].replace("scrapy:scrapy@", "wrong:wronger@")
     return urlunsplit(bad_auth_proxy)
 
 
 class ProxyConnectTestCase(TestCase):
-
     def setUp(self):
         try:
             import mitmproxy  # noqa: F401
         except ImportError:
-            self.skipTest('mitmproxy is not installed')
+            self.skipTest("mitmproxy is not installed")
 
         self.mockserver = MockServer()
         self.mockserver.__enter__()
         self._oldenv = os.environ.copy()
 
         self._proxy = MitmProxy()
         proxy_url = self._proxy.start()
-        os.environ['https_proxy'] = proxy_url
-        os.environ['http_proxy'] = proxy_url
+        os.environ["https_proxy"] = proxy_url
+        os.environ["http_proxy"] = proxy_url
 
     def tearDown(self):
         self.mockserver.__exit__(None, None, None)
         self._proxy.stop()
         os.environ = self._oldenv
 
     @defer.inlineCallbacks
@@ -81,32 +90,32 @@
         crawler = get_crawler(SimpleSpider)
         with LogCapture() as log:
             yield crawler.crawl(self.mockserver.url("/status?n=200", is_secure=True))
         self._assert_got_response_code(200, log)
 
     @defer.inlineCallbacks
     def test_https_tunnel_auth_error(self):
-        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])
+        os.environ["https_proxy"] = _wrong_credentials(os.environ["https_proxy"])
         crawler = get_crawler(SimpleSpider)
         with LogCapture() as log:
             yield crawler.crawl(self.mockserver.url("/status?n=200", is_secure=True))
         # The proxy returns a 407 error code but it does not reach the client;
         # he just sees a TunnelError.
         self._assert_got_tunnel_error(log)
 
     @defer.inlineCallbacks
     def test_https_tunnel_without_leak_proxy_authorization_header(self):
         request = Request(self.mockserver.url("/echo", is_secure=True))
         crawler = get_crawler(SingleRequestSpider)
         with LogCapture() as log:
             yield crawler.crawl(seed=request)
         self._assert_got_response_code(200, log)
-        echo = json.loads(crawler.spider.meta['responses'][0].text)
-        self.assertTrue('Proxy-Authorization' not in echo['headers'])
+        echo = json.loads(crawler.spider.meta["responses"][0].text)
+        self.assertTrue("Proxy-Authorization" not in echo["headers"])
 
     def _assert_got_response_code(self, code, log):
         print(log)
-        self.assertEqual(str(log).count(f'Crawled ({code})'), 1)
+        self.assertEqual(str(log).count(f"Crawled ({code})"), 1)
 
     def _assert_got_tunnel_error(self, log):
         print(log)
-        self.assertIn('TunnelError', str(log))
+        self.assertIn("TunnelError", str(log))
```

### Comparing `Scrapy-2.7.1/tests/test_request_attribute_binding.py` & `Scrapy-2.8.0/tests/test_request_attribute_binding.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,40 +1,37 @@
+from testfixtures import LogCapture
 from twisted.internet import defer
 from twisted.trial.unittest import TestCase
 
 from scrapy import Request, signals
 from scrapy.http.response import Response
 from scrapy.utils.test import get_crawler
-
-from testfixtures import LogCapture
-
 from tests.mockserver import MockServer
 from tests.spiders import SingleRequestSpider
 
-
-OVERRIDEN_URL = "https://example.org"
+OVERRIDDEN_URL = "https://example.org"
 
 
 class ProcessResponseMiddleware:
     def process_response(self, request, response, spider):
-        return response.replace(request=Request(OVERRIDEN_URL))
+        return response.replace(request=Request(OVERRIDDEN_URL))
 
 
 class RaiseExceptionRequestMiddleware:
     def process_request(self, request, spider):
         1 / 0
         return request
 
 
 class CatchExceptionOverrideRequestMiddleware:
     def process_exception(self, request, exception, spider):
         return Response(
             url="http://localhost/",
             body=b"Caught " + exception.__class__.__name__.encode("utf-8"),
-            request=Request(OVERRIDEN_URL),
+            request=Request(OVERRIDDEN_URL),
         )
 
 
 class CatchExceptionDoNotOverrideRequestMiddleware:
     def process_exception(self, request, exception, spider):
         return Response(
             url="http://localhost/",
@@ -48,23 +45,22 @@
     def alt_callback(self, response, foo=None):
         self.logger.info("alt_callback was invoked with foo=%s", foo)
 
 
 class AlternativeCallbacksMiddleware:
     def process_response(self, request, response, spider):
         new_request = request.replace(
-            url=OVERRIDEN_URL,
+            url=OVERRIDDEN_URL,
             callback=spider.alt_callback,
             cb_kwargs={"foo": "bar"},
         )
         return response.replace(request=new_request)
 
 
 class CrawlTestCase(TestCase):
-
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
 
     def tearDown(self):
         self.mockserver.__exit__(None, None, None)
 
@@ -86,19 +82,22 @@
             response = failure.value.response
             self.assertEqual(failure.request.url, url)
             self.assertEqual(response.request.url, url)
 
     @defer.inlineCallbacks
     def test_downloader_middleware_raise_exception(self):
         url = self.mockserver.url("/status?n=200")
-        crawler = get_crawler(SingleRequestSpider, {
-            "DOWNLOADER_MIDDLEWARES": {
-                RaiseExceptionRequestMiddleware: 590,
+        crawler = get_crawler(
+            SingleRequestSpider,
+            {
+                "DOWNLOADER_MIDDLEWARES": {
+                    RaiseExceptionRequestMiddleware: 590,
+                },
             },
-        })
+        )
         yield crawler.crawl(seed=url, mockserver=self.mockserver)
         failure = crawler.spider.meta["failure"]
         self.assertEqual(failure.request.url, url)
         self.assertIsInstance(failure.value, ZeroDivisionError)
 
     @defer.inlineCallbacks
     def test_downloader_middleware_override_request_in_process_response(self):
@@ -112,86 +111,106 @@
         signal_params = {}
 
         def signal_handler(response, request, spider):
             signal_params["response"] = response
             signal_params["request"] = request
 
         url = self.mockserver.url("/status?n=200")
-        crawler = get_crawler(SingleRequestSpider, {
-            "DOWNLOADER_MIDDLEWARES": {
-                ProcessResponseMiddleware: 595,
-            }
-        })
+        crawler = get_crawler(
+            SingleRequestSpider,
+            {
+                "DOWNLOADER_MIDDLEWARES": {
+                    ProcessResponseMiddleware: 595,
+                }
+            },
+        )
         crawler.signals.connect(signal_handler, signal=signals.response_received)
 
         with LogCapture() as log:
             yield crawler.crawl(seed=url, mockserver=self.mockserver)
 
         response = crawler.spider.meta["responses"][0]
-        self.assertEqual(response.request.url, OVERRIDEN_URL)
+        self.assertEqual(response.request.url, OVERRIDDEN_URL)
 
         self.assertEqual(signal_params["response"].url, url)
-        self.assertEqual(signal_params["request"].url, OVERRIDEN_URL)
+        self.assertEqual(signal_params["request"].url, OVERRIDDEN_URL)
 
         log.check_present(
-            ("scrapy.core.engine", "DEBUG", f"Crawled (200) <GET {OVERRIDEN_URL}> (referer: None)"),
+            (
+                "scrapy.core.engine",
+                "DEBUG",
+                f"Crawled (200) <GET {OVERRIDDEN_URL}> (referer: None)",
+            ),
         )
 
     @defer.inlineCallbacks
     def test_downloader_middleware_override_in_process_exception(self):
         """
         An exception is raised but caught by the next middleware, which
         returns a Response with a specific 'request' attribute.
 
         The spider callback should receive the overridden response.request
         """
         url = self.mockserver.url("/status?n=200")
-        crawler = get_crawler(SingleRequestSpider, {
-            "DOWNLOADER_MIDDLEWARES": {
-                RaiseExceptionRequestMiddleware: 590,
-                CatchExceptionOverrideRequestMiddleware: 595,
+        crawler = get_crawler(
+            SingleRequestSpider,
+            {
+                "DOWNLOADER_MIDDLEWARES": {
+                    RaiseExceptionRequestMiddleware: 590,
+                    CatchExceptionOverrideRequestMiddleware: 595,
+                },
             },
-        })
+        )
         yield crawler.crawl(seed=url, mockserver=self.mockserver)
         response = crawler.spider.meta["responses"][0]
         self.assertEqual(response.body, b"Caught ZeroDivisionError")
-        self.assertEqual(response.request.url, OVERRIDEN_URL)
+        self.assertEqual(response.request.url, OVERRIDDEN_URL)
 
     @defer.inlineCallbacks
     def test_downloader_middleware_do_not_override_in_process_exception(self):
         """
         An exception is raised but caught by the next middleware, which
         returns a Response without a specific 'request' attribute.
 
         The spider callback should receive the original response.request
         """
         url = self.mockserver.url("/status?n=200")
-        crawler = get_crawler(SingleRequestSpider, {
-            "DOWNLOADER_MIDDLEWARES": {
-                RaiseExceptionRequestMiddleware: 590,
-                CatchExceptionDoNotOverrideRequestMiddleware: 595,
+        crawler = get_crawler(
+            SingleRequestSpider,
+            {
+                "DOWNLOADER_MIDDLEWARES": {
+                    RaiseExceptionRequestMiddleware: 590,
+                    CatchExceptionDoNotOverrideRequestMiddleware: 595,
+                },
             },
-        })
+        )
         yield crawler.crawl(seed=url, mockserver=self.mockserver)
         response = crawler.spider.meta["responses"][0]
         self.assertEqual(response.body, b"Caught ZeroDivisionError")
         self.assertEqual(response.request.url, url)
 
     @defer.inlineCallbacks
     def test_downloader_middleware_alternative_callback(self):
         """
         Downloader middleware which returns a response with a
         specific 'request' attribute, with an alternative callback
         """
-        crawler = get_crawler(AlternativeCallbacksSpider, {
-            "DOWNLOADER_MIDDLEWARES": {
-                AlternativeCallbacksMiddleware: 595,
-            }
-        })
+        crawler = get_crawler(
+            AlternativeCallbacksSpider,
+            {
+                "DOWNLOADER_MIDDLEWARES": {
+                    AlternativeCallbacksMiddleware: 595,
+                }
+            },
+        )
 
         with LogCapture() as log:
             url = self.mockserver.url("/status?n=200")
             yield crawler.crawl(seed=url, mockserver=self.mockserver)
 
         log.check_present(
-            ("alternative_callbacks_spider", "INFO", "alt_callback was invoked with foo=bar"),
+            (
+                "alternative_callbacks_spider",
+                "INFO",
+                "alt_callback was invoked with foo=bar",
+            ),
         )
```

### Comparing `Scrapy-2.7.1/tests/test_request_cb_kwargs.py` & `Scrapy-2.8.0/tests/test_request_cb_kwargs.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,140 +1,160 @@
 from testfixtures import LogCapture
 from twisted.internet import defer
 from twisted.trial.unittest import TestCase
 
 from scrapy.http import Request
 from scrapy.utils.test import get_crawler
-from tests.spiders import MockServerSpider
 from tests.mockserver import MockServer
+from tests.spiders import MockServerSpider
 
 
 class InjectArgumentsDownloaderMiddleware:
     """
     Make sure downloader middlewares are able to update the keyword arguments
     """
+
     def process_request(self, request, spider):
-        if request.callback.__name__ == 'parse_downloader_mw':
-            request.cb_kwargs['from_process_request'] = True
+        if request.callback.__name__ == "parse_downloader_mw":
+            request.cb_kwargs["from_process_request"] = True
         return None
 
     def process_response(self, request, response, spider):
-        if request.callback.__name__ == 'parse_downloader_mw':
-            request.cb_kwargs['from_process_response'] = True
+        if request.callback.__name__ == "parse_downloader_mw":
+            request.cb_kwargs["from_process_response"] = True
         return response
 
 
 class InjectArgumentsSpiderMiddleware:
     """
     Make sure spider middlewares are able to update the keyword arguments
     """
+
     def process_start_requests(self, start_requests, spider):
         for request in start_requests:
-            if request.callback.__name__ == 'parse_spider_mw':
-                request.cb_kwargs['from_process_start_requests'] = True
+            if request.callback.__name__ == "parse_spider_mw":
+                request.cb_kwargs["from_process_start_requests"] = True
             yield request
 
     def process_spider_input(self, response, spider):
         request = response.request
-        if request.callback.__name__ == 'parse_spider_mw':
-            request.cb_kwargs['from_process_spider_input'] = True
+        if request.callback.__name__ == "parse_spider_mw":
+            request.cb_kwargs["from_process_spider_input"] = True
         return None
 
     def process_spider_output(self, response, result, spider):
         for element in result:
-            if isinstance(element, Request) and element.callback.__name__ == 'parse_spider_mw_2':
-                element.cb_kwargs['from_process_spider_output'] = True
+            if (
+                isinstance(element, Request)
+                and element.callback.__name__ == "parse_spider_mw_2"
+            ):
+                element.cb_kwargs["from_process_spider_output"] = True
             yield element
 
 
 class KeywordArgumentsSpider(MockServerSpider):
-    name = 'kwargs'
+    name = "kwargs"
     custom_settings = {
-        'DOWNLOADER_MIDDLEWARES': {
+        "DOWNLOADER_MIDDLEWARES": {
             InjectArgumentsDownloaderMiddleware: 750,
         },
-        'SPIDER_MIDDLEWARES': {
+        "SPIDER_MIDDLEWARES": {
             InjectArgumentsSpiderMiddleware: 750,
         },
     }
 
     checks = []
 
     def start_requests(self):
-        data = {'key': 'value', 'number': 123, 'callback': 'some_callback'}
-        yield Request(self.mockserver.url('/first'), self.parse_first, cb_kwargs=data)
-        yield Request(self.mockserver.url('/general_with'), self.parse_general, cb_kwargs=data)
-        yield Request(self.mockserver.url('/general_without'), self.parse_general)
-        yield Request(self.mockserver.url('/no_kwargs'), self.parse_no_kwargs)
-        yield Request(self.mockserver.url('/default'), self.parse_default, cb_kwargs=data)
-        yield Request(self.mockserver.url('/takes_less'), self.parse_takes_less, cb_kwargs=data)
-        yield Request(self.mockserver.url('/takes_more'), self.parse_takes_more, cb_kwargs=data)
-        yield Request(self.mockserver.url('/downloader_mw'), self.parse_downloader_mw)
-        yield Request(self.mockserver.url('/spider_mw'), self.parse_spider_mw)
+        data = {"key": "value", "number": 123, "callback": "some_callback"}
+        yield Request(self.mockserver.url("/first"), self.parse_first, cb_kwargs=data)
+        yield Request(
+            self.mockserver.url("/general_with"), self.parse_general, cb_kwargs=data
+        )
+        yield Request(self.mockserver.url("/general_without"), self.parse_general)
+        yield Request(self.mockserver.url("/no_kwargs"), self.parse_no_kwargs)
+        yield Request(
+            self.mockserver.url("/default"), self.parse_default, cb_kwargs=data
+        )
+        yield Request(
+            self.mockserver.url("/takes_less"), self.parse_takes_less, cb_kwargs=data
+        )
+        yield Request(
+            self.mockserver.url("/takes_more"), self.parse_takes_more, cb_kwargs=data
+        )
+        yield Request(self.mockserver.url("/downloader_mw"), self.parse_downloader_mw)
+        yield Request(self.mockserver.url("/spider_mw"), self.parse_spider_mw)
 
     def parse_first(self, response, key, number):
-        self.checks.append(key == 'value')
+        self.checks.append(key == "value")
         self.checks.append(number == 123)
-        self.crawler.stats.inc_value('boolean_checks', 2)
+        self.crawler.stats.inc_value("boolean_checks", 2)
         yield response.follow(
-            self.mockserver.url('/two'),
+            self.mockserver.url("/two"),
             self.parse_second,
-            cb_kwargs={'new_key': 'new_value'})
+            cb_kwargs={"new_key": "new_value"},
+        )
 
     def parse_second(self, response, new_key):
-        self.checks.append(new_key == 'new_value')
-        self.crawler.stats.inc_value('boolean_checks')
+        self.checks.append(new_key == "new_value")
+        self.crawler.stats.inc_value("boolean_checks")
 
     def parse_general(self, response, **kwargs):
-        if response.url.endswith('/general_with'):
-            self.checks.append(kwargs['key'] == 'value')
-            self.checks.append(kwargs['number'] == 123)
-            self.checks.append(kwargs['callback'] == 'some_callback')
-            self.crawler.stats.inc_value('boolean_checks', 3)
-        elif response.url.endswith('/general_without'):
-            self.checks.append(kwargs == {})
-            self.crawler.stats.inc_value('boolean_checks')
+        if response.url.endswith("/general_with"):
+            self.checks.append(kwargs["key"] == "value")
+            self.checks.append(kwargs["number"] == 123)
+            self.checks.append(kwargs["callback"] == "some_callback")
+            self.crawler.stats.inc_value("boolean_checks", 3)
+        elif response.url.endswith("/general_without"):
+            self.checks.append(
+                kwargs == {}  # pylint: disable=use-implicit-booleaness-not-comparison
+            )
+            self.crawler.stats.inc_value("boolean_checks")
 
     def parse_no_kwargs(self, response):
-        self.checks.append(response.url.endswith('/no_kwargs'))
-        self.crawler.stats.inc_value('boolean_checks')
+        self.checks.append(response.url.endswith("/no_kwargs"))
+        self.crawler.stats.inc_value("boolean_checks")
 
     def parse_default(self, response, key, number=None, default=99):
-        self.checks.append(response.url.endswith('/default'))
-        self.checks.append(key == 'value')
+        self.checks.append(response.url.endswith("/default"))
+        self.checks.append(key == "value")
         self.checks.append(number == 123)
         self.checks.append(default == 99)
-        self.crawler.stats.inc_value('boolean_checks', 4)
+        self.crawler.stats.inc_value("boolean_checks", 4)
 
     def parse_takes_less(self, response, key, callback):
         """
         Should raise
         TypeError: parse_takes_less() got an unexpected keyword argument 'number'
         """
 
     def parse_takes_more(self, response, key, number, callback, other):
         """
         Should raise
         TypeError: parse_takes_more() missing 1 required positional argument: 'other'
         """
 
-    def parse_downloader_mw(self, response, from_process_request, from_process_response):
+    def parse_downloader_mw(
+        self, response, from_process_request, from_process_response
+    ):
         self.checks.append(bool(from_process_request))
         self.checks.append(bool(from_process_response))
-        self.crawler.stats.inc_value('boolean_checks', 2)
+        self.crawler.stats.inc_value("boolean_checks", 2)
 
-    def parse_spider_mw(self, response, from_process_spider_input, from_process_start_requests):
+    def parse_spider_mw(
+        self, response, from_process_spider_input, from_process_start_requests
+    ):
         self.checks.append(bool(from_process_spider_input))
         self.checks.append(bool(from_process_start_requests))
-        self.crawler.stats.inc_value('boolean_checks', 2)
-        return Request(self.mockserver.url('/spider_mw_2'), self.parse_spider_mw_2)
+        self.crawler.stats.inc_value("boolean_checks", 2)
+        return Request(self.mockserver.url("/spider_mw_2"), self.parse_spider_mw_2)
 
     def parse_spider_mw_2(self, response, from_process_spider_output):
         self.checks.append(bool(from_process_spider_output))
-        self.crawler.stats.inc_value('boolean_checks', 1)
+        self.crawler.stats.inc_value("boolean_checks", 1)
 
 
 class CallbackKeywordArgumentsTestCase(TestCase):
 
     maxDiff = None
 
     def setUp(self):
@@ -146,28 +166,30 @@
 
     @defer.inlineCallbacks
     def test_callback_kwargs(self):
         crawler = get_crawler(KeywordArgumentsSpider)
         with LogCapture() as log:
             yield crawler.crawl(mockserver=self.mockserver)
         self.assertTrue(all(crawler.spider.checks))
-        self.assertEqual(len(crawler.spider.checks), crawler.stats.get_value('boolean_checks'))
+        self.assertEqual(
+            len(crawler.spider.checks), crawler.stats.get_value("boolean_checks")
+        )
         # check exceptions for argument mismatch
         exceptions = {}
         for line in log.records:
-            for key in ('takes_less', 'takes_more'):
+            for key in ("takes_less", "takes_more"):
                 if key in line.getMessage():
                     exceptions[key] = line
-        self.assertEqual(exceptions['takes_less'].exc_info[0], TypeError)
+        self.assertEqual(exceptions["takes_less"].exc_info[0], TypeError)
         self.assertTrue(
-            str(exceptions['takes_less'].exc_info[1]).endswith(
+            str(exceptions["takes_less"].exc_info[1]).endswith(
                 "parse_takes_less() got an unexpected keyword argument 'number'"
             ),
-            msg="Exception message: " + str(exceptions['takes_less'].exc_info[1]),
+            msg="Exception message: " + str(exceptions["takes_less"].exc_info[1]),
         )
-        self.assertEqual(exceptions['takes_more'].exc_info[0], TypeError)
+        self.assertEqual(exceptions["takes_more"].exc_info[0], TypeError)
         self.assertTrue(
-            str(exceptions['takes_more'].exc_info[1]).endswith(
+            str(exceptions["takes_more"].exc_info[1]).endswith(
                 "parse_takes_more() missing 1 required positional argument: 'other'"
             ),
-            msg="Exception message: " + str(exceptions['takes_more'].exc_info[1]),
+            msg="Exception message: " + str(exceptions["takes_more"].exc_info[1]),
         )
```

### Comparing `Scrapy-2.7.1/tests/test_request_dict.py` & `Scrapy-2.8.0/tests/test_request_dict.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,45 +1,44 @@
 import sys
 import unittest
 import warnings
 from contextlib import suppress
 
-from scrapy import Spider, Request
+from scrapy import Request, Spider
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.http import FormRequest, JsonRequest
 from scrapy.utils.request import request_from_dict
 
 
 class CustomRequest(Request):
     pass
 
 
 class RequestSerializationTest(unittest.TestCase):
-
     def setUp(self):
         self.spider = TestSpider()
 
     def test_basic(self):
         r = Request("http://www.example.com")
         self._assert_serializes_ok(r)
 
     def test_all_attributes(self):
         r = Request(
             url="http://www.example.com",
             callback=self.spider.parse_item,
             errback=self.spider.handle_error,
             method="POST",
             body=b"some body",
-            headers={'content-encoding': 'text/html; charset=latin-1'},
-            cookies={'currency': ''},
-            encoding='latin-1',
+            headers={"content-encoding": "text/html; charset=latin-1"},
+            cookies={"currency": ""},
+            encoding="latin-1",
             priority=20,
-            meta={'a': 'b'},
-            cb_kwargs={'k': 'v'},
-            flags=['testFlag'],
+            meta={"a": "b"},
+            cb_kwargs={"k": "v"},
+            flags=["testFlag"],
         )
         self._assert_serializes_ok(r, spider=self.spider)
 
     def test_latin1_body(self):
         r = Request("http://www.example.com", body=b"\xa3")
         self._assert_serializes_ok(r)
 
@@ -76,76 +75,87 @@
         self._assert_serializes_ok(r1, spider=self.spider)
         r2 = CustomRequest("http://www.example.com")
         self._assert_serializes_ok(r2, spider=self.spider)
         r3 = JsonRequest("http://www.example.com", dumps_kwargs={"indent": 4})
         self._assert_serializes_ok(r3, spider=self.spider)
 
     def test_callback_serialization(self):
-        r = Request("http://www.example.com", callback=self.spider.parse_item,
-                    errback=self.spider.handle_error)
+        r = Request(
+            "http://www.example.com",
+            callback=self.spider.parse_item,
+            errback=self.spider.handle_error,
+        )
         self._assert_serializes_ok(r, spider=self.spider)
 
     def test_reference_callback_serialization(self):
-        r = Request("http://www.example.com",
-                    callback=self.spider.parse_item_reference,
-                    errback=self.spider.handle_error_reference)
+        r = Request(
+            "http://www.example.com",
+            callback=self.spider.parse_item_reference,
+            errback=self.spider.handle_error_reference,
+        )
         self._assert_serializes_ok(r, spider=self.spider)
         request_dict = r.to_dict(spider=self.spider)
-        self.assertEqual(request_dict['callback'], 'parse_item_reference')
-        self.assertEqual(request_dict['errback'], 'handle_error_reference')
+        self.assertEqual(request_dict["callback"], "parse_item_reference")
+        self.assertEqual(request_dict["errback"], "handle_error_reference")
 
     def test_private_reference_callback_serialization(self):
-        r = Request("http://www.example.com",
-                    callback=self.spider._TestSpider__parse_item_reference,
-                    errback=self.spider._TestSpider__handle_error_reference)
+        r = Request(
+            "http://www.example.com",
+            callback=self.spider._TestSpider__parse_item_reference,
+            errback=self.spider._TestSpider__handle_error_reference,
+        )
         self._assert_serializes_ok(r, spider=self.spider)
         request_dict = r.to_dict(spider=self.spider)
-        self.assertEqual(request_dict['callback'],
-                         '_TestSpider__parse_item_reference')
-        self.assertEqual(request_dict['errback'],
-                         '_TestSpider__handle_error_reference')
+        self.assertEqual(request_dict["callback"], "_TestSpider__parse_item_reference")
+        self.assertEqual(request_dict["errback"], "_TestSpider__handle_error_reference")
 
     def test_private_callback_serialization(self):
-        r = Request("http://www.example.com",
-                    callback=self.spider._TestSpider__parse_item_private,
-                    errback=self.spider.handle_error)
+        r = Request(
+            "http://www.example.com",
+            callback=self.spider._TestSpider__parse_item_private,
+            errback=self.spider.handle_error,
+        )
         self._assert_serializes_ok(r, spider=self.spider)
 
     def test_mixin_private_callback_serialization(self):
-        r = Request("http://www.example.com",
-                    callback=self.spider._TestSpiderMixin__mixin_callback,
-                    errback=self.spider.handle_error)
+        r = Request(
+            "http://www.example.com",
+            callback=self.spider._TestSpiderMixin__mixin_callback,
+            errback=self.spider.handle_error,
+        )
         self._assert_serializes_ok(r, spider=self.spider)
 
     def test_delegated_callback_serialization(self):
-        r = Request("http://www.example.com",
-                    callback=self.spider.delegated_callback,
-                    errback=self.spider.handle_error)
+        r = Request(
+            "http://www.example.com",
+            callback=self.spider.delegated_callback,
+            errback=self.spider.handle_error,
+        )
         self._assert_serializes_ok(r, spider=self.spider)
 
     def test_unserializable_callback1(self):
         r = Request("http://www.example.com", callback=lambda x: x)
         self.assertRaises(ValueError, r.to_dict, spider=self.spider)
 
     def test_unserializable_callback2(self):
         r = Request("http://www.example.com", callback=self.spider.parse_item)
         self.assertRaises(ValueError, r.to_dict, spider=None)
 
     def test_unserializable_callback3(self):
         """Parser method is removed or replaced dynamically."""
 
         class MySpider(Spider):
-            name = 'my_spider'
+            name = "my_spider"
 
             def parse(self, response):
                 pass
 
         spider = MySpider()
         r = Request("http://www.example.com", callback=spider.parse)
-        setattr(spider, 'parse', None)
+        setattr(spider, "parse", None)
         self.assertRaises(ValueError, r.to_dict, spider=spider)
 
     def test_callback_not_available(self):
         """Callback method is not available in the spider passed to from_dict"""
         spider = TestSpiderDelegation()
         r = Request("http://www.example.com", callback=spider.delegated_callback)
         d = r.to_dict(spider=spider)
@@ -153,17 +163,20 @@
 
 
 class DeprecatedMethodsRequestSerializationTest(RequestSerializationTest):
     def _assert_serializes_ok(self, request, spider=None):
         with warnings.catch_warnings(record=True) as caught:
             warnings.simplefilter("always")
             with suppress(KeyError):
-                del sys.modules["scrapy.utils.reqser"]  # delete module to reset the deprecation warning
+                del sys.modules[
+                    "scrapy.utils.reqser"
+                ]  # delete module to reset the deprecation warning
 
-            from scrapy.utils.reqser import request_from_dict as _from_dict, request_to_dict as _to_dict
+            from scrapy.utils.reqser import request_from_dict as _from_dict
+            from scrapy.utils.reqser import request_to_dict as _to_dict
 
             request_copy = _from_dict(_to_dict(request, spider), spider)
             self._assert_same_request(request, request_copy)
 
             self.assertEqual(len(caught), 1)
             self.assertTrue(issubclass(caught[0].category, ScrapyDeprecationWarning))
             self.assertEqual(
@@ -196,15 +209,15 @@
 
 
 def private_handle_error(failure):
     pass
 
 
 class TestSpider(Spider, TestSpiderMixin):
-    name = 'test'
+    name = "test"
     parse_item_reference = parse_item
     handle_error_reference = handle_error
     __parse_item_reference = private_parse_item
     __handle_error_reference = private_handle_error
 
     def __init__(self):
         self.delegated_callback = TestSpiderDelegation().delegated_callback
```

### Comparing `Scrapy-2.7.1/tests/test_request_left.py` & `Scrapy-2.8.0/tests/test_request_left.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,36 +1,35 @@
 from twisted.internet import defer
 from twisted.trial.unittest import TestCase
+
 from scrapy.signals import request_left_downloader
 from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
 from tests.mockserver import MockServer
 
 
 class SignalCatcherSpider(Spider):
-    name = 'signal_catcher'
+    name = "signal_catcher"
 
     def __init__(self, crawler, url, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        crawler.signals.connect(self.on_request_left,
-                                signal=request_left_downloader)
+        crawler.signals.connect(self.on_request_left, signal=request_left_downloader)
         self.caught_times = 0
         self.start_urls = [url]
 
     @classmethod
     def from_crawler(cls, crawler, *args, **kwargs):
         spider = cls(crawler, *args, **kwargs)
         return spider
 
     def on_request_left(self, request, spider):
         self.caught_times += 1
 
 
 class TestCatching(TestCase):
-
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
 
     def tearDown(self):
         self.mockserver.__exit__(None, None, None)
 
@@ -38,23 +37,22 @@
     def test_success(self):
         crawler = get_crawler(SignalCatcherSpider)
         yield crawler.crawl(self.mockserver.url("/status?n=200"))
         self.assertEqual(crawler.spider.caught_times, 1)
 
     @defer.inlineCallbacks
     def test_timeout(self):
-        crawler = get_crawler(SignalCatcherSpider,
-                              {'DOWNLOAD_TIMEOUT': 0.1})
+        crawler = get_crawler(SignalCatcherSpider, {"DOWNLOAD_TIMEOUT": 0.1})
         yield crawler.crawl(self.mockserver.url("/delay?n=0.2"))
         self.assertEqual(crawler.spider.caught_times, 1)
 
     @defer.inlineCallbacks
     def test_disconnect(self):
         crawler = get_crawler(SignalCatcherSpider)
         yield crawler.crawl(self.mockserver.url("/drop"))
         self.assertEqual(crawler.spider.caught_times, 1)
 
     @defer.inlineCallbacks
     def test_noconnect(self):
         crawler = get_crawler(SignalCatcherSpider)
-        yield crawler.crawl('http://thereisdefinetelynosuchdomain.com')
+        yield crawler.crawl("http://thereisdefinetelynosuchdomain.com")
         self.assertEqual(crawler.spider.caught_times, 1)
```

### Comparing `Scrapy-2.7.1/tests/test_robotstxt_interface.py` & `Scrapy-2.8.0/tests/test_robotstxt_interface.py`

 * *Files 8% similar despite different names*

```diff
@@ -30,67 +30,87 @@
 
 
 class BaseRobotParserTest:
     def _setUp(self, parser_cls):
         self.parser_cls = parser_cls
 
     def test_allowed(self):
-        robotstxt_robotstxt_body = ("User-agent: * \n"
-                                    "Disallow: /disallowed \n"
-                                    "Allow: /allowed \n"
-                                    "Crawl-delay: 10".encode('utf-8'))
-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)
+        robotstxt_robotstxt_body = (
+            "User-agent: * \n"
+            "Disallow: /disallowed \n"
+            "Allow: /allowed \n"
+            "Crawl-delay: 10".encode("utf-8")
+        )
+        rp = self.parser_cls.from_crawler(
+            crawler=None, robotstxt_body=robotstxt_robotstxt_body
+        )
         self.assertTrue(rp.allowed("https://www.site.local/allowed", "*"))
         self.assertFalse(rp.allowed("https://www.site.local/disallowed", "*"))
 
     def test_allowed_wildcards(self):
         robotstxt_robotstxt_body = """User-agent: first
                                 Disallow: /disallowed/*/end$
 
                                 User-agent: second
                                 Allow: /*allowed
                                 Disallow: /
-                                """.encode('utf-8')
-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)
+                                """.encode(
+            "utf-8"
+        )
+        rp = self.parser_cls.from_crawler(
+            crawler=None, robotstxt_body=robotstxt_robotstxt_body
+        )
 
         self.assertTrue(rp.allowed("https://www.site.local/disallowed", "first"))
-        self.assertFalse(rp.allowed("https://www.site.local/disallowed/xyz/end", "first"))
-        self.assertFalse(rp.allowed("https://www.site.local/disallowed/abc/end", "first"))
-        self.assertTrue(rp.allowed("https://www.site.local/disallowed/xyz/endinglater", "first"))
+        self.assertFalse(
+            rp.allowed("https://www.site.local/disallowed/xyz/end", "first")
+        )
+        self.assertFalse(
+            rp.allowed("https://www.site.local/disallowed/abc/end", "first")
+        )
+        self.assertTrue(
+            rp.allowed("https://www.site.local/disallowed/xyz/endinglater", "first")
+        )
 
         self.assertTrue(rp.allowed("https://www.site.local/allowed", "second"))
         self.assertTrue(rp.allowed("https://www.site.local/is_still_allowed", "second"))
         self.assertTrue(rp.allowed("https://www.site.local/is_allowed_too", "second"))
 
     def test_length_based_precedence(self):
-        robotstxt_robotstxt_body = ("User-agent: * \n"
-                                    "Disallow: / \n"
-                                    "Allow: /page".encode('utf-8'))
-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)
+        robotstxt_robotstxt_body = (
+            "User-agent: * \n" "Disallow: / \n" "Allow: /page".encode("utf-8")
+        )
+        rp = self.parser_cls.from_crawler(
+            crawler=None, robotstxt_body=robotstxt_robotstxt_body
+        )
         self.assertTrue(rp.allowed("https://www.site.local/page", "*"))
 
     def test_order_based_precedence(self):
-        robotstxt_robotstxt_body = ("User-agent: * \n"
-                                    "Disallow: / \n"
-                                    "Allow: /page".encode('utf-8'))
-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)
+        robotstxt_robotstxt_body = (
+            "User-agent: * \n" "Disallow: / \n" "Allow: /page".encode("utf-8")
+        )
+        rp = self.parser_cls.from_crawler(
+            crawler=None, robotstxt_body=robotstxt_robotstxt_body
+        )
         self.assertFalse(rp.allowed("https://www.site.local/page", "*"))
 
     def test_empty_response(self):
         """empty response should equal 'allow all'"""
-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=b'')
+        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=b"")
         self.assertTrue(rp.allowed("https://site.local/", "*"))
         self.assertTrue(rp.allowed("https://site.local/", "chrome"))
         self.assertTrue(rp.allowed("https://site.local/index.html", "*"))
         self.assertTrue(rp.allowed("https://site.local/disallowed", "*"))
 
     def test_garbage_response(self):
         """garbage response should be discarded, equal 'allow all'"""
-        robotstxt_robotstxt_body = b'GIF89a\xd3\x00\xfe\x00\xa2'
-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)
+        robotstxt_robotstxt_body = b"GIF89a\xd3\x00\xfe\x00\xa2"
+        rp = self.parser_cls.from_crawler(
+            crawler=None, robotstxt_body=robotstxt_robotstxt_body
+        )
         self.assertTrue(rp.allowed("https://site.local/", "*"))
         self.assertTrue(rp.allowed("https://site.local/", "chrome"))
         self.assertTrue(rp.allowed("https://site.local/index.html", "*"))
         self.assertTrue(rp.allowed("https://site.local/disallowed", "*"))
 
     def test_unicode_url_and_useragent(self):
         robotstxt_robotstxt_body = """
@@ -98,65 +118,85 @@
         Disallow: /admin/
         Disallow: /static/
         # taken from https://en.wikipedia.org/robots.txt
         Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:
         Disallow: /wiki/Kyttj:
 
         User-Agent: UnicdeBt
-        Disallow: /some/randome/page.html""".encode('utf-8')
-        rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=robotstxt_robotstxt_body)
+        Disallow: /some/randome/page.html""".encode(
+            "utf-8"
+        )
+        rp = self.parser_cls.from_crawler(
+            crawler=None, robotstxt_body=robotstxt_robotstxt_body
+        )
         self.assertTrue(rp.allowed("https://site.local/", "*"))
         self.assertFalse(rp.allowed("https://site.local/admin/", "*"))
         self.assertFalse(rp.allowed("https://site.local/static/", "*"))
         self.assertTrue(rp.allowed("https://site.local/admin/", "UnicdeBt"))
-        self.assertFalse(rp.allowed("https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:", "*"))
+        self.assertFalse(
+            rp.allowed("https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:", "*")
+        )
         self.assertFalse(rp.allowed("https://site.local/wiki/Kyttj:", "*"))
         self.assertTrue(rp.allowed("https://site.local/some/randome/page.html", "*"))
-        self.assertFalse(rp.allowed("https://site.local/some/randome/page.html", "UnicdeBt"))
+        self.assertFalse(
+            rp.allowed("https://site.local/some/randome/page.html", "UnicdeBt")
+        )
 
 
 class PythonRobotParserTest(BaseRobotParserTest, unittest.TestCase):
     def setUp(self):
         from scrapy.robotstxt import PythonRobotParser
+
         super()._setUp(PythonRobotParser)
 
     def test_length_based_precedence(self):
-        raise unittest.SkipTest("RobotFileParser does not support length based directives precedence.")
+        raise unittest.SkipTest(
+            "RobotFileParser does not support length based directives precedence."
+        )
 
     def test_allowed_wildcards(self):
         raise unittest.SkipTest("RobotFileParser does not support wildcards.")
 
 
 class ReppyRobotParserTest(BaseRobotParserTest, unittest.TestCase):
     if not reppy_available():
         skip = "Reppy parser is not installed"
 
     def setUp(self):
         from scrapy.robotstxt import ReppyRobotParser
+
         super()._setUp(ReppyRobotParser)
 
     def test_order_based_precedence(self):
-        raise unittest.SkipTest("Reppy does not support order based directives precedence.")
+        raise unittest.SkipTest(
+            "Reppy does not support order based directives precedence."
+        )
 
 
 class RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):
     if not rerp_available():
         skip = "Rerp parser is not installed"
 
     def setUp(self):
         from scrapy.robotstxt import RerpRobotParser
+
         super()._setUp(RerpRobotParser)
 
     def test_length_based_precedence(self):
-        raise unittest.SkipTest("Rerp does not support length based directives precedence.")
+        raise unittest.SkipTest(
+            "Rerp does not support length based directives precedence."
+        )
 
 
 class ProtegoRobotParserTest(BaseRobotParserTest, unittest.TestCase):
     if not protego_available():
         skip = "Protego parser is not installed"
 
     def setUp(self):
         from scrapy.robotstxt import ProtegoRobotParser
+
         super()._setUp(ProtegoRobotParser)
 
     def test_order_based_precedence(self):
-        raise unittest.SkipTest("Protego does not support order based directives precedence.")
+        raise unittest.SkipTest(
+            "Protego does not support order based directives precedence."
+        )
```

### Comparing `Scrapy-2.7.1/tests/test_scheduler.py` & `Scrapy-2.8.0/tests/test_scheduler.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,38 +1,37 @@
+import collections
 import shutil
 import tempfile
 import unittest
-import collections
 
 from twisted.internet import defer
 from twisted.trial.unittest import TestCase
 
-from scrapy.crawler import Crawler
 from scrapy.core.downloader import Downloader
 from scrapy.core.scheduler import Scheduler
+from scrapy.crawler import Crawler
 from scrapy.http import Request
 from scrapy.spiders import Spider
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.utils.test import get_crawler
 from tests.mockserver import MockServer
 
-
-MockEngine = collections.namedtuple('MockEngine', ['downloader'])
-MockSlot = collections.namedtuple('MockSlot', ['active'])
+MockEngine = collections.namedtuple("MockEngine", ["downloader"])
+MockSlot = collections.namedtuple("MockSlot", ["active"])
 
 
 class MockDownloader:
     def __init__(self):
         self.slots = {}
 
     def _get_slot_key(self, request, spider):
         if Downloader.DOWNLOAD_SLOT in request.meta:
             return request.meta[Downloader.DOWNLOAD_SLOT]
 
-        return urlparse_cached(request).hostname or ''
+        return urlparse_cached(request).hostname or ""
 
     def increment(self, slot_key):
         slot = self.slots.setdefault(slot_key, MockSlot(active=[]))
         slot.active.append(1)
 
     def decrement(self, slot_key):
         slot = self.slots.get(slot_key)
@@ -43,52 +42,54 @@
 
 
 class MockCrawler(Crawler):
     def __init__(self, priority_queue_cls, jobdir):
 
         settings = dict(
             SCHEDULER_DEBUG=False,
-            SCHEDULER_DISK_QUEUE='scrapy.squeues.PickleLifoDiskQueue',
-            SCHEDULER_MEMORY_QUEUE='scrapy.squeues.LifoMemoryQueue',
+            SCHEDULER_DISK_QUEUE="scrapy.squeues.PickleLifoDiskQueue",
+            SCHEDULER_MEMORY_QUEUE="scrapy.squeues.LifoMemoryQueue",
             SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,
             JOBDIR=jobdir,
-            DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter',
-            REQUEST_FINGERPRINTER_IMPLEMENTATION='2.7',
+            DUPEFILTER_CLASS="scrapy.dupefilters.BaseDupeFilter",
+            REQUEST_FINGERPRINTER_IMPLEMENTATION="2.7",
         )
         super().__init__(Spider, settings)
         self.engine = MockEngine(downloader=MockDownloader())
 
 
 class SchedulerHandler:
     priority_queue_cls = None
     jobdir = None
 
     def create_scheduler(self):
         self.mock_crawler = MockCrawler(self.priority_queue_cls, self.jobdir)
         self.scheduler = Scheduler.from_crawler(self.mock_crawler)
-        self.spider = Spider(name='spider')
+        self.spider = Spider(name="spider")
         self.scheduler.open(self.spider)
 
     def close_scheduler(self):
-        self.scheduler.close('finished')
+        self.scheduler.close("finished")
         self.mock_crawler.stop()
         self.mock_crawler.engine.downloader.close()
 
     def setUp(self):
         self.create_scheduler()
 
     def tearDown(self):
         self.close_scheduler()
 
 
-_PRIORITIES = [("http://foo.com/a", -2),
-               ("http://foo.com/d", 1),
-               ("http://foo.com/b", -1),
-               ("http://foo.com/c", 0),
-               ("http://foo.com/e", 2)]
+_PRIORITIES = [
+    ("http://foo.com/a", -2),
+    ("http://foo.com/d", 1),
+    ("http://foo.com/b", -1),
+    ("http://foo.com/c", 0),
+    ("http://foo.com/e", 2),
+]
 
 
 _URLS = {"http://foo.com/a", "http://foo.com/b", "http://foo.com/c"}
 
 
 class BaseSchedulerInMemoryTester(SchedulerHandler):
     def test_length(self):
@@ -115,20 +116,20 @@
         for url, priority in _PRIORITIES:
             self.scheduler.enqueue_request(Request(url, priority=priority))
 
         priorities = []
         while self.scheduler.has_pending_requests():
             priorities.append(self.scheduler.next_request().priority)
 
-        self.assertEqual(priorities,
-                         sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))
+        self.assertEqual(
+            priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)
+        )
 
 
 class BaseSchedulerOnDiskTester(SchedulerHandler):
-
     def setUp(self):
         self.jobdir = tempfile.mkdtemp()
         self.create_scheduler()
 
     def tearDown(self):
         self.close_scheduler()
 
@@ -168,54 +169,58 @@
         self.close_scheduler()
         self.create_scheduler()
 
         priorities = []
         while self.scheduler.has_pending_requests():
             priorities.append(self.scheduler.next_request().priority)
 
-        self.assertEqual(priorities,
-                         sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))
+        self.assertEqual(
+            priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)
+        )
 
 
 class TestSchedulerInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):
-    priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'
+    priority_queue_cls = "scrapy.pqueues.ScrapyPriorityQueue"
 
 
 class TestSchedulerOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):
-    priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'
+    priority_queue_cls = "scrapy.pqueues.ScrapyPriorityQueue"
 
 
-_URLS_WITH_SLOTS = [("http://foo.com/a", 'a'),
-                    ("http://foo.com/b", 'a'),
-                    ("http://foo.com/c", 'b'),
-                    ("http://foo.com/d", 'b'),
-                    ("http://foo.com/e", 'c'),
-                    ("http://foo.com/f", 'c')]
+_URLS_WITH_SLOTS = [
+    ("http://foo.com/a", "a"),
+    ("http://foo.com/b", "a"),
+    ("http://foo.com/c", "b"),
+    ("http://foo.com/d", "b"),
+    ("http://foo.com/e", "c"),
+    ("http://foo.com/f", "c"),
+]
 
 
 class TestMigration(unittest.TestCase):
-
     def setUp(self):
         self.tmpdir = tempfile.mkdtemp()
 
     def tearDown(self):
         shutil.rmtree(self.tmpdir)
 
     def _migration(self, tmp_dir):
         prev_scheduler_handler = SchedulerHandler()
-        prev_scheduler_handler.priority_queue_cls = 'scrapy.pqueues.ScrapyPriorityQueue'
+        prev_scheduler_handler.priority_queue_cls = "scrapy.pqueues.ScrapyPriorityQueue"
         prev_scheduler_handler.jobdir = tmp_dir
 
         prev_scheduler_handler.create_scheduler()
         for url in _URLS:
             prev_scheduler_handler.scheduler.enqueue_request(Request(url))
         prev_scheduler_handler.close_scheduler()
 
         next_scheduler_handler = SchedulerHandler()
-        next_scheduler_handler.priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'
+        next_scheduler_handler.priority_queue_cls = (
+            "scrapy.pqueues.DownloaderAwarePriorityQueue"
+        )
         next_scheduler_handler.jobdir = tmp_dir
 
         next_scheduler_handler.create_scheduler()
 
     def test_migration(self):
         with self.assertRaises(ValueError):
             self._migration(self.tmpdir)
@@ -235,23 +240,23 @@
     False
     """
     if len(dequeued_slots) != len(enqueued_slots):
         return False
 
     slots_number = len(set(enqueued_slots))
     for i in range(0, len(dequeued_slots), slots_number):
-        part = dequeued_slots[i:i + slots_number]
+        part = dequeued_slots[i : i + slots_number]
         if len(part) != len(set(part)):
             return False
 
     return True
 
 
 class DownloaderAwareSchedulerTestMixin:
-    priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'
+    priority_queue_cls = "scrapy.pqueues.DownloaderAwarePriorityQueue"
     reopen = False
 
     def test_logic(self):
         for url, slot in _URLS_WITH_SLOTS:
             request = Request(url)
             request.meta[Downloader.DOWNLOAD_SLOT] = slot
             self.scheduler.enqueue_request(request)
@@ -272,74 +277,75 @@
             requests.append(request)
 
         for request in requests:
             # pylint: disable=protected-access
             slot = downloader._get_slot_key(request, None)
             downloader.decrement(slot)
 
-        self.assertTrue(_is_scheduling_fair(list(s for u, s in _URLS_WITH_SLOTS),
-                                            dequeued_slots))
+        self.assertTrue(
+            _is_scheduling_fair(list(s for u, s in _URLS_WITH_SLOTS), dequeued_slots)
+        )
         self.assertEqual(sum(len(s.active) for s in downloader.slots.values()), 0)
 
 
-class TestSchedulerWithDownloaderAwareInMemory(DownloaderAwareSchedulerTestMixin,
-                                               BaseSchedulerInMemoryTester,
-                                               unittest.TestCase):
+class TestSchedulerWithDownloaderAwareInMemory(
+    DownloaderAwareSchedulerTestMixin, BaseSchedulerInMemoryTester, unittest.TestCase
+):
     pass
 
 
-class TestSchedulerWithDownloaderAwareOnDisk(DownloaderAwareSchedulerTestMixin,
-                                             BaseSchedulerOnDiskTester,
-                                             unittest.TestCase):
+class TestSchedulerWithDownloaderAwareOnDisk(
+    DownloaderAwareSchedulerTestMixin, BaseSchedulerOnDiskTester, unittest.TestCase
+):
     reopen = True
 
 
 class StartUrlsSpider(Spider):
-
     def __init__(self, start_urls):
         self.start_urls = start_urls
-        super().__init__(name='StartUrlsSpider')
+        super().__init__(name="StartUrlsSpider")
 
     def parse(self, response):
         pass
 
 
 class TestIntegrationWithDownloaderAwareInMemory(TestCase):
     def setUp(self):
         self.crawler = get_crawler(
             spidercls=StartUrlsSpider,
             settings_dict={
-                'SCHEDULER_PRIORITY_QUEUE': 'scrapy.pqueues.DownloaderAwarePriorityQueue',
-                'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
+                "SCHEDULER_PRIORITY_QUEUE": "scrapy.pqueues.DownloaderAwarePriorityQueue",
+                "DUPEFILTER_CLASS": "scrapy.dupefilters.BaseDupeFilter",
             },
         )
 
     @defer.inlineCallbacks
     def tearDown(self):
         yield self.crawler.stop()
 
     @defer.inlineCallbacks
     def test_integration_downloader_aware_priority_queue(self):
         with MockServer() as mockserver:
 
             url = mockserver.url("/status?n=200", is_secure=False)
             start_urls = [url] * 6
             yield self.crawler.crawl(start_urls)
-            self.assertEqual(self.crawler.stats.get_value('downloader/response_count'),
-                             len(start_urls))
+            self.assertEqual(
+                self.crawler.stats.get_value("downloader/response_count"),
+                len(start_urls),
+            )
 
 
 class TestIncompatibility(unittest.TestCase):
-
     def _incompatible(self):
         settings = dict(
-            SCHEDULER_PRIORITY_QUEUE='scrapy.pqueues.DownloaderAwarePriorityQueue',
+            SCHEDULER_PRIORITY_QUEUE="scrapy.pqueues.DownloaderAwarePriorityQueue",
             CONCURRENT_REQUESTS_PER_IP=1,
         )
         crawler = get_crawler(Spider, settings)
         scheduler = Scheduler.from_crawler(crawler)
-        spider = Spider(name='spider')
+        spider = Spider(name="spider")
         scheduler.open(spider)
 
     def test_incompatibility(self):
         with self.assertRaises(ValueError):
             self._incompatible()
```

### Comparing `Scrapy-2.7.1/tests/test_scheduler_base.py` & `Scrapy-2.8.0/tests/test_scheduler_base.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,18 +7,16 @@
 from twisted.trial.unittest import TestCase as TwistedTestCase
 
 from scrapy.core.scheduler import BaseScheduler
 from scrapy.http import Request
 from scrapy.spiders import Spider
 from scrapy.utils.request import fingerprint
 from scrapy.utils.test import get_crawler
-
 from tests.mockserver import MockServer
 
-
 PATHS = ["/a", "/b", "/c"]
 URLS = [urljoin("https://example.org", p) for p in PATHS]
 
 
 class MinimalScheduler:
     def __init__(self) -> None:
         self.requests: Dict[bytes, Request] = {}
@@ -72,15 +70,19 @@
     def setUp(self):
         self.scheduler = BaseScheduler()
 
     def test_methods(self):
         self.assertIsNone(self.scheduler.open(Spider("foo")))
         self.assertIsNone(self.scheduler.close("finished"))
         self.assertRaises(NotImplementedError, self.scheduler.has_pending_requests)
-        self.assertRaises(NotImplementedError, self.scheduler.enqueue_request, Request("https://example.org"))
+        self.assertRaises(
+            NotImplementedError,
+            self.scheduler.enqueue_request,
+            Request("https://example.org"),
+        )
         self.assertRaises(NotImplementedError, self.scheduler.next_request)
 
 
 class MinimalSchedulerTest(TestCase, InterfaceCheckMixin):
     def setUp(self):
         self.scheduler = MinimalScheduler()
```

### Comparing `Scrapy-2.7.1/tests/test_selector.py` & `Scrapy-2.8.0/tests/test_selector.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,95 +1,110 @@
 import weakref
 
 from twisted.trial import unittest
 
-from scrapy.http import TextResponse, HtmlResponse, XmlResponse
+from scrapy.http import HtmlResponse, TextResponse, XmlResponse
 from scrapy.selector import Selector
 
 
 class SelectorTestCase(unittest.TestCase):
-
     def test_simple_selection(self):
         """Simple selector tests"""
         body = b"<p><input name='a'value='1'/><input name='b'value='2'/></p>"
-        response = TextResponse(url="http://example.com", body=body, encoding='utf-8')
+        response = TextResponse(url="http://example.com", body=body, encoding="utf-8")
         sel = Selector(response)
 
-        xl = sel.xpath('//input')
+        xl = sel.xpath("//input")
         self.assertEqual(2, len(xl))
         for x in xl:
             assert isinstance(x, Selector)
 
         self.assertEqual(
-            sel.xpath('//input').getall(),
-            [x.get() for x in sel.xpath('//input')]
-        )
-        self.assertEqual(
-            [x.get() for x in sel.xpath("//input[@name='a']/@name")],
-            ['a']
+            sel.xpath("//input").getall(), [x.get() for x in sel.xpath("//input")]
         )
         self.assertEqual(
-            [x.get() for x in sel.xpath("number(concat(//input[@name='a']/@value, //input[@name='b']/@value))")],
-            ['12.0']
+            [x.get() for x in sel.xpath("//input[@name='a']/@name")], ["a"]
         )
         self.assertEqual(
-            sel.xpath("concat('xpath', 'rules')").getall(),
-            ['xpathrules']
+            [
+                x.get()
+                for x in sel.xpath(
+                    "number(concat(//input[@name='a']/@value, //input[@name='b']/@value))"
+                )
+            ],
+            ["12.0"],
         )
+        self.assertEqual(sel.xpath("concat('xpath', 'rules')").getall(), ["xpathrules"])
         self.assertEqual(
-            [x.get() for x in sel.xpath("concat(//input[@name='a']/@value, //input[@name='b']/@value)")],
-            ['12']
+            [
+                x.get()
+                for x in sel.xpath(
+                    "concat(//input[@name='a']/@value, //input[@name='b']/@value)"
+                )
+            ],
+            ["12"],
         )
 
     def test_root_base_url(self):
         body = b'<html><form action="/path"><input name="a" /></form></html>'
         url = "http://example.com"
-        response = TextResponse(url=url, body=body, encoding='utf-8')
+        response = TextResponse(url=url, body=body, encoding="utf-8")
         sel = Selector(response)
         self.assertEqual(url, sel.root.base)
 
     def test_flavor_detection(self):
         text = b'<div><img src="a.jpg"><p>Hello</div>'
-        sel = Selector(XmlResponse('http://example.com', body=text, encoding='utf-8'))
-        self.assertEqual(sel.type, 'xml')
-        self.assertEqual(sel.xpath("//div").getall(),
-                         ['<div><img src="a.jpg"><p>Hello</p></img></div>'])
-
-        sel = Selector(HtmlResponse('http://example.com', body=text, encoding='utf-8'))
-        self.assertEqual(sel.type, 'html')
-        self.assertEqual(sel.xpath("//div").getall(),
-                         ['<div><img src="a.jpg"><p>Hello</p></div>'])
+        sel = Selector(XmlResponse("http://example.com", body=text, encoding="utf-8"))
+        self.assertEqual(sel.type, "xml")
+        self.assertEqual(
+            sel.xpath("//div").getall(),
+            ['<div><img src="a.jpg"><p>Hello</p></img></div>'],
+        )
+
+        sel = Selector(HtmlResponse("http://example.com", body=text, encoding="utf-8"))
+        self.assertEqual(sel.type, "html")
+        self.assertEqual(
+            sel.xpath("//div").getall(), ['<div><img src="a.jpg"><p>Hello</p></div>']
+        )
 
     def test_http_header_encoding_precedence(self):
         # '\xa3'     = pound symbol in unicode
         # '\xc2\xa3' = pound symbol in utf-8
         # '\xa3'     = pound symbol in latin-1 (iso-8859-1)
 
-        meta = '<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">'
-        head = '<head>' + meta + '</head>'
+        meta = (
+            '<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">'
+        )
+        head = "<head>" + meta + "</head>"
         body_content = '<span id="blank">\xa3</span>'
-        body = '<body>' + body_content + '</body>'
-        html = '<html>' + head + body + '</html>'
-        encoding = 'utf-8'
+        body = "<body>" + body_content + "</body>"
+        html = "<html>" + head + body + "</html>"
+        encoding = "utf-8"
         html_utf8 = html.encode(encoding)
 
-        headers = {'Content-Type': ['text/html; charset=utf-8']}
-        response = HtmlResponse(url="http://example.com", headers=headers, body=html_utf8)
+        headers = {"Content-Type": ["text/html; charset=utf-8"]}
+        response = HtmlResponse(
+            url="http://example.com", headers=headers, body=html_utf8
+        )
         x = Selector(response)
-        self.assertEqual(x.xpath("//span[@id='blank']/text()").getall(), ['\xa3'])
+        self.assertEqual(x.xpath("//span[@id='blank']/text()").getall(), ["\xa3"])
 
     def test_badly_encoded_body(self):
         # \xe9 alone isn't valid utf8 sequence
-        r1 = TextResponse('http://www.example.com',
-                          body=b'<html><p>an Jos\xe9 de</p><html>',
-                          encoding='utf-8')
-        Selector(r1).xpath('//text()').getall()
+        r1 = TextResponse(
+            "http://www.example.com",
+            body=b"<html><p>an Jos\xe9 de</p><html>",
+            encoding="utf-8",
+        )
+        Selector(r1).xpath("//text()").getall()
 
     def test_weakref_slots(self):
         """Check that classes are using slots and are weak-referenceable"""
-        x = Selector(text='')
+        x = Selector(text="")
         weakref.ref(x)
-        assert not hasattr(x, '__dict__'), f"{x.__class__.__name__} does not use __slots__"
+        assert not hasattr(
+            x, "__dict__"
+        ), f"{x.__class__.__name__} does not use __slots__"
 
     def test_selector_bad_args(self):
-        with self.assertRaisesRegex(ValueError, 'received both response and text'):
-            Selector(TextResponse(url='http://example.com', body=b''), text='')
+        with self.assertRaisesRegex(ValueError, "received both response and text"):
+            Selector(TextResponse(url="http://example.com", body=b""), text="")
```

### Comparing `Scrapy-2.7.1/tests/test_settings/__init__.py` & `Scrapy-2.8.0/tests/test_settings/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,422 +1,444 @@
 import unittest
 from unittest import mock
 
-from scrapy.settings import (BaseSettings, Settings, SettingsAttribute,
-                             SETTINGS_PRIORITIES, get_settings_priority)
+from scrapy.settings import (
+    SETTINGS_PRIORITIES,
+    BaseSettings,
+    Settings,
+    SettingsAttribute,
+    get_settings_priority,
+)
+
 from . import default_settings
 
 
 class SettingsGlobalFuncsTest(unittest.TestCase):
-
     def test_get_settings_priority(self):
         for prio_str, prio_num in SETTINGS_PRIORITIES.items():
             self.assertEqual(get_settings_priority(prio_str), prio_num)
         self.assertEqual(get_settings_priority(99), 99)
 
 
 class SettingsAttributeTest(unittest.TestCase):
-
     def setUp(self):
-        self.attribute = SettingsAttribute('value', 10)
+        self.attribute = SettingsAttribute("value", 10)
 
     def test_set_greater_priority(self):
-        self.attribute.set('value2', 20)
-        self.assertEqual(self.attribute.value, 'value2')
+        self.attribute.set("value2", 20)
+        self.assertEqual(self.attribute.value, "value2")
         self.assertEqual(self.attribute.priority, 20)
 
     def test_set_equal_priority(self):
-        self.attribute.set('value2', 10)
-        self.assertEqual(self.attribute.value, 'value2')
+        self.attribute.set("value2", 10)
+        self.assertEqual(self.attribute.value, "value2")
         self.assertEqual(self.attribute.priority, 10)
 
     def test_set_less_priority(self):
-        self.attribute.set('value2', 0)
-        self.assertEqual(self.attribute.value, 'value')
+        self.attribute.set("value2", 0)
+        self.assertEqual(self.attribute.value, "value")
         self.assertEqual(self.attribute.priority, 10)
 
     def test_overwrite_basesettings(self):
-        original_dict = {'one': 10, 'two': 20}
+        original_dict = {"one": 10, "two": 20}
         original_settings = BaseSettings(original_dict, 0)
         attribute = SettingsAttribute(original_settings, 0)
 
-        new_dict = {'three': 11, 'four': 21}
+        new_dict = {"three": 11, "four": 21}
         attribute.set(new_dict, 10)
         self.assertIsInstance(attribute.value, BaseSettings)
         self.assertCountEqual(attribute.value, new_dict)
         self.assertCountEqual(original_settings, original_dict)
 
-        new_settings = BaseSettings({'five': 12}, 0)
+        new_settings = BaseSettings({"five": 12}, 0)
         attribute.set(new_settings, 0)  # Insufficient priority
         self.assertCountEqual(attribute.value, new_dict)
         attribute.set(new_settings, 10)
         self.assertCountEqual(attribute.value, new_settings)
 
     def test_repr(self):
-        self.assertEqual(repr(self.attribute),
-                         "<SettingsAttribute value='value' priority=10>")
+        self.assertEqual(
+            repr(self.attribute), "<SettingsAttribute value='value' priority=10>"
+        )
 
 
 class BaseSettingsTest(unittest.TestCase):
-
     def setUp(self):
         self.settings = BaseSettings()
 
     def test_set_new_attribute(self):
-        self.settings.set('TEST_OPTION', 'value', 0)
-        self.assertIn('TEST_OPTION', self.settings.attributes)
+        self.settings.set("TEST_OPTION", "value", 0)
+        self.assertIn("TEST_OPTION", self.settings.attributes)
 
-        attr = self.settings.attributes['TEST_OPTION']
+        attr = self.settings.attributes["TEST_OPTION"]
         self.assertIsInstance(attr, SettingsAttribute)
-        self.assertEqual(attr.value, 'value')
+        self.assertEqual(attr.value, "value")
         self.assertEqual(attr.priority, 0)
 
     def test_set_settingsattribute(self):
         myattr = SettingsAttribute(0, 30)  # Note priority 30
-        self.settings.set('TEST_ATTR', myattr, 10)
-        self.assertEqual(self.settings.get('TEST_ATTR'), 0)
-        self.assertEqual(self.settings.getpriority('TEST_ATTR'), 30)
+        self.settings.set("TEST_ATTR", myattr, 10)
+        self.assertEqual(self.settings.get("TEST_ATTR"), 0)
+        self.assertEqual(self.settings.getpriority("TEST_ATTR"), 30)
 
     def test_set_instance_identity_on_update(self):
-        attr = SettingsAttribute('value', 0)
-        self.settings.attributes = {'TEST_OPTION': attr}
-        self.settings.set('TEST_OPTION', 'othervalue', 10)
+        attr = SettingsAttribute("value", 0)
+        self.settings.attributes = {"TEST_OPTION": attr}
+        self.settings.set("TEST_OPTION", "othervalue", 10)
 
-        self.assertIn('TEST_OPTION', self.settings.attributes)
-        self.assertIs(attr, self.settings.attributes['TEST_OPTION'])
+        self.assertIn("TEST_OPTION", self.settings.attributes)
+        self.assertIs(attr, self.settings.attributes["TEST_OPTION"])
 
     def test_set_calls_settings_attributes_methods_on_update(self):
-        attr = SettingsAttribute('value', 10)
-        with mock.patch.object(attr, '__setattr__') as mock_setattr, mock.patch.object(attr, 'set') as mock_set:
+        attr = SettingsAttribute("value", 10)
+        with mock.patch.object(attr, "__setattr__") as mock_setattr, mock.patch.object(
+            attr, "set"
+        ) as mock_set:
 
-            self.settings.attributes = {'TEST_OPTION': attr}
+            self.settings.attributes = {"TEST_OPTION": attr}
 
             for priority in (0, 10, 20):
-                self.settings.set('TEST_OPTION', 'othervalue', priority)
-                mock_set.assert_called_once_with('othervalue', priority)
+                self.settings.set("TEST_OPTION", "othervalue", priority)
+                mock_set.assert_called_once_with("othervalue", priority)
                 self.assertFalse(mock_setattr.called)
                 mock_set.reset_mock()
                 mock_setattr.reset_mock()
 
     def test_setitem(self):
         settings = BaseSettings()
-        settings.set('key', 'a', 'default')
-        settings['key'] = 'b'
-        self.assertEqual(settings['key'], 'b')
-        self.assertEqual(settings.getpriority('key'), 20)
-        settings['key'] = 'c'
-        self.assertEqual(settings['key'], 'c')
-        settings['key2'] = 'x'
-        self.assertIn('key2', settings)
-        self.assertEqual(settings['key2'], 'x')
-        self.assertEqual(settings.getpriority('key2'), 20)
+        settings.set("key", "a", "default")
+        settings["key"] = "b"
+        self.assertEqual(settings["key"], "b")
+        self.assertEqual(settings.getpriority("key"), 20)
+        settings["key"] = "c"
+        self.assertEqual(settings["key"], "c")
+        settings["key2"] = "x"
+        self.assertIn("key2", settings)
+        self.assertEqual(settings["key2"], "x")
+        self.assertEqual(settings.getpriority("key2"), 20)
 
     def test_setdict_alias(self):
-        with mock.patch.object(self.settings, 'set') as mock_set:
-            self.settings.setdict({'TEST_1': 'value1', 'TEST_2': 'value2'}, 10)
+        with mock.patch.object(self.settings, "set") as mock_set:
+            self.settings.setdict({"TEST_1": "value1", "TEST_2": "value2"}, 10)
             self.assertEqual(mock_set.call_count, 2)
-            calls = [mock.call('TEST_1', 'value1', 10),
-                     mock.call('TEST_2', 'value2', 10)]
+            calls = [
+                mock.call("TEST_1", "value1", 10),
+                mock.call("TEST_2", "value2", 10),
+            ]
             mock_set.assert_has_calls(calls, any_order=True)
 
     def test_setmodule_only_load_uppercase_vars(self):
-        class ModuleMock():
-            UPPERCASE_VAR = 'value'
-            MIXEDcase_VAR = 'othervalue'
-            lowercase_var = 'anothervalue'
+        class ModuleMock:
+            UPPERCASE_VAR = "value"
+            MIXEDcase_VAR = "othervalue"
+            lowercase_var = "anothervalue"
 
         self.settings.attributes = {}
         self.settings.setmodule(ModuleMock(), 10)
-        self.assertIn('UPPERCASE_VAR', self.settings.attributes)
-        self.assertNotIn('MIXEDcase_VAR', self.settings.attributes)
-        self.assertNotIn('lowercase_var', self.settings.attributes)
+        self.assertIn("UPPERCASE_VAR", self.settings.attributes)
+        self.assertNotIn("MIXEDcase_VAR", self.settings.attributes)
+        self.assertNotIn("lowercase_var", self.settings.attributes)
         self.assertEqual(len(self.settings.attributes), 1)
 
     def test_setmodule_alias(self):
-        with mock.patch.object(self.settings, 'set') as mock_set:
+        with mock.patch.object(self.settings, "set") as mock_set:
             self.settings.setmodule(default_settings, 10)
-            mock_set.assert_any_call('TEST_DEFAULT', 'defvalue', 10)
-            mock_set.assert_any_call('TEST_DICT', {'key': 'val'}, 10)
+            mock_set.assert_any_call("TEST_DEFAULT", "defvalue", 10)
+            mock_set.assert_any_call("TEST_DICT", {"key": "val"}, 10)
 
     def test_setmodule_by_path(self):
         self.settings.attributes = {}
         self.settings.setmodule(default_settings, 10)
         ctrl_attributes = self.settings.attributes.copy()
 
         self.settings.attributes = {}
-        self.settings.setmodule(
-            'tests.test_settings.default_settings', 10)
+        self.settings.setmodule("tests.test_settings.default_settings", 10)
 
-        self.assertCountEqual(self.settings.attributes.keys(),
-                              ctrl_attributes.keys())
+        self.assertCountEqual(self.settings.attributes.keys(), ctrl_attributes.keys())
 
         for key in ctrl_attributes.keys():
             attr = self.settings.attributes[key]
             ctrl_attr = ctrl_attributes[key]
             self.assertEqual(attr.value, ctrl_attr.value)
             self.assertEqual(attr.priority, ctrl_attr.priority)
 
     def test_update(self):
-        settings = BaseSettings({'key_lowprio': 0}, priority=0)
-        settings.set('key_highprio', 10, priority=50)
-        custom_settings = BaseSettings({'key_lowprio': 1, 'key_highprio': 11},
-                                       priority=30)
-        custom_settings.set('newkey_one', None, priority=50)
-        custom_dict = {'key_lowprio': 2, 'key_highprio': 12, 'newkey_two': None}
+        settings = BaseSettings({"key_lowprio": 0}, priority=0)
+        settings.set("key_highprio", 10, priority=50)
+        custom_settings = BaseSettings(
+            {"key_lowprio": 1, "key_highprio": 11}, priority=30
+        )
+        custom_settings.set("newkey_one", None, priority=50)
+        custom_dict = {"key_lowprio": 2, "key_highprio": 12, "newkey_two": None}
 
         settings.update(custom_dict, priority=20)
-        self.assertEqual(settings['key_lowprio'], 2)
-        self.assertEqual(settings.getpriority('key_lowprio'), 20)
-        self.assertEqual(settings['key_highprio'], 10)
-        self.assertIn('newkey_two', settings)
-        self.assertEqual(settings.getpriority('newkey_two'), 20)
+        self.assertEqual(settings["key_lowprio"], 2)
+        self.assertEqual(settings.getpriority("key_lowprio"), 20)
+        self.assertEqual(settings["key_highprio"], 10)
+        self.assertIn("newkey_two", settings)
+        self.assertEqual(settings.getpriority("newkey_two"), 20)
 
         settings.update(custom_settings)
-        self.assertEqual(settings['key_lowprio'], 1)
-        self.assertEqual(settings.getpriority('key_lowprio'), 30)
-        self.assertEqual(settings['key_highprio'], 10)
-        self.assertIn('newkey_one', settings)
-        self.assertEqual(settings.getpriority('newkey_one'), 50)
+        self.assertEqual(settings["key_lowprio"], 1)
+        self.assertEqual(settings.getpriority("key_lowprio"), 30)
+        self.assertEqual(settings["key_highprio"], 10)
+        self.assertIn("newkey_one", settings)
+        self.assertEqual(settings.getpriority("newkey_one"), 50)
 
-        settings.update({'key_lowprio': 3}, priority=20)
-        self.assertEqual(settings['key_lowprio'], 1)
+        settings.update({"key_lowprio": 3}, priority=20)
+        self.assertEqual(settings["key_lowprio"], 1)
 
     def test_update_jsonstring(self):
-        settings = BaseSettings({'number': 0, 'dict': BaseSettings({'key': 'val'})})
+        settings = BaseSettings({"number": 0, "dict": BaseSettings({"key": "val"})})
         settings.update('{"number": 1, "newnumber": 2}')
-        self.assertEqual(settings['number'], 1)
-        self.assertEqual(settings['newnumber'], 2)
+        self.assertEqual(settings["number"], 1)
+        self.assertEqual(settings["newnumber"], 2)
         settings.set("dict", '{"key": "newval", "newkey": "newval2"}')
-        self.assertEqual(settings['dict']['key'], "newval")
-        self.assertEqual(settings['dict']['newkey'], "newval2")
+        self.assertEqual(settings["dict"]["key"], "newval")
+        self.assertEqual(settings["dict"]["newkey"], "newval2")
 
     def test_delete(self):
-        settings = BaseSettings({'key': None})
-        settings.set('key_highprio', None, priority=50)
-        settings.delete('key')
-        settings.delete('key_highprio')
-        self.assertNotIn('key', settings)
-        self.assertIn('key_highprio', settings)
-        del settings['key_highprio']
-        self.assertNotIn('key_highprio', settings)
+        settings = BaseSettings({"key": None})
+        settings.set("key_highprio", None, priority=50)
+        settings.delete("key")
+        settings.delete("key_highprio")
+        self.assertNotIn("key", settings)
+        self.assertIn("key_highprio", settings)
+        del settings["key_highprio"]
+        self.assertNotIn("key_highprio", settings)
 
     def test_get(self):
         test_configuration = {
-            'TEST_ENABLED1': '1',
-            'TEST_ENABLED2': True,
-            'TEST_ENABLED3': 1,
-            'TEST_ENABLED4': 'True',
-            'TEST_ENABLED5': 'true',
-            'TEST_ENABLED_WRONG': 'on',
-            'TEST_DISABLED1': '0',
-            'TEST_DISABLED2': False,
-            'TEST_DISABLED3': 0,
-            'TEST_DISABLED4': 'False',
-            'TEST_DISABLED5': 'false',
-            'TEST_DISABLED_WRONG': 'off',
-            'TEST_INT1': 123,
-            'TEST_INT2': '123',
-            'TEST_FLOAT1': 123.45,
-            'TEST_FLOAT2': '123.45',
-            'TEST_LIST1': ['one', 'two'],
-            'TEST_LIST2': 'one,two',
-            'TEST_STR': 'value',
-            'TEST_DICT1': {'key1': 'val1', 'ke2': 3},
-            'TEST_DICT2': '{"key1": "val1", "ke2": 3}',
+            "TEST_ENABLED1": "1",
+            "TEST_ENABLED2": True,
+            "TEST_ENABLED3": 1,
+            "TEST_ENABLED4": "True",
+            "TEST_ENABLED5": "true",
+            "TEST_ENABLED_WRONG": "on",
+            "TEST_DISABLED1": "0",
+            "TEST_DISABLED2": False,
+            "TEST_DISABLED3": 0,
+            "TEST_DISABLED4": "False",
+            "TEST_DISABLED5": "false",
+            "TEST_DISABLED_WRONG": "off",
+            "TEST_INT1": 123,
+            "TEST_INT2": "123",
+            "TEST_FLOAT1": 123.45,
+            "TEST_FLOAT2": "123.45",
+            "TEST_LIST1": ["one", "two"],
+            "TEST_LIST2": "one,two",
+            "TEST_STR": "value",
+            "TEST_DICT1": {"key1": "val1", "ke2": 3},
+            "TEST_DICT2": '{"key1": "val1", "ke2": 3}',
         }
         settings = self.settings
-        settings.attributes = {key: SettingsAttribute(value, 0) for key, value
-                               in test_configuration.items()}
+        settings.attributes = {
+            key: SettingsAttribute(value, 0)
+            for key, value in test_configuration.items()
+        }
 
-        self.assertTrue(settings.getbool('TEST_ENABLED1'))
-        self.assertTrue(settings.getbool('TEST_ENABLED2'))
-        self.assertTrue(settings.getbool('TEST_ENABLED3'))
-        self.assertTrue(settings.getbool('TEST_ENABLED4'))
-        self.assertTrue(settings.getbool('TEST_ENABLED5'))
-        self.assertFalse(settings.getbool('TEST_ENABLEDx'))
-        self.assertTrue(settings.getbool('TEST_ENABLEDx', True))
-        self.assertFalse(settings.getbool('TEST_DISABLED1'))
-        self.assertFalse(settings.getbool('TEST_DISABLED2'))
-        self.assertFalse(settings.getbool('TEST_DISABLED3'))
-        self.assertFalse(settings.getbool('TEST_DISABLED4'))
-        self.assertFalse(settings.getbool('TEST_DISABLED5'))
-        self.assertEqual(settings.getint('TEST_INT1'), 123)
-        self.assertEqual(settings.getint('TEST_INT2'), 123)
-        self.assertEqual(settings.getint('TEST_INTx'), 0)
-        self.assertEqual(settings.getint('TEST_INTx', 45), 45)
-        self.assertEqual(settings.getfloat('TEST_FLOAT1'), 123.45)
-        self.assertEqual(settings.getfloat('TEST_FLOAT2'), 123.45)
-        self.assertEqual(settings.getfloat('TEST_FLOATx'), 0.0)
-        self.assertEqual(settings.getfloat('TEST_FLOATx', 55.0), 55.0)
-        self.assertEqual(settings.getlist('TEST_LIST1'), ['one', 'two'])
-        self.assertEqual(settings.getlist('TEST_LIST2'), ['one', 'two'])
-        self.assertEqual(settings.getlist('TEST_LISTx'), [])
-        self.assertEqual(settings.getlist('TEST_LISTx', ['default']), ['default'])
-        self.assertEqual(settings['TEST_STR'], 'value')
-        self.assertEqual(settings.get('TEST_STR'), 'value')
-        self.assertEqual(settings['TEST_STRx'], None)
-        self.assertEqual(settings.get('TEST_STRx'), None)
-        self.assertEqual(settings.get('TEST_STRx', 'default'), 'default')
-        self.assertEqual(settings.getdict('TEST_DICT1'), {'key1': 'val1', 'ke2': 3})
-        self.assertEqual(settings.getdict('TEST_DICT2'), {'key1': 'val1', 'ke2': 3})
-        self.assertEqual(settings.getdict('TEST_DICT3'), {})
-        self.assertEqual(settings.getdict('TEST_DICT3', {'key1': 5}), {'key1': 5})
-        self.assertRaises(ValueError, settings.getdict, 'TEST_LIST1')
-        self.assertRaises(ValueError, settings.getbool, 'TEST_ENABLED_WRONG')
-        self.assertRaises(ValueError, settings.getbool, 'TEST_DISABLED_WRONG')
+        self.assertTrue(settings.getbool("TEST_ENABLED1"))
+        self.assertTrue(settings.getbool("TEST_ENABLED2"))
+        self.assertTrue(settings.getbool("TEST_ENABLED3"))
+        self.assertTrue(settings.getbool("TEST_ENABLED4"))
+        self.assertTrue(settings.getbool("TEST_ENABLED5"))
+        self.assertFalse(settings.getbool("TEST_ENABLEDx"))
+        self.assertTrue(settings.getbool("TEST_ENABLEDx", True))
+        self.assertFalse(settings.getbool("TEST_DISABLED1"))
+        self.assertFalse(settings.getbool("TEST_DISABLED2"))
+        self.assertFalse(settings.getbool("TEST_DISABLED3"))
+        self.assertFalse(settings.getbool("TEST_DISABLED4"))
+        self.assertFalse(settings.getbool("TEST_DISABLED5"))
+        self.assertEqual(settings.getint("TEST_INT1"), 123)
+        self.assertEqual(settings.getint("TEST_INT2"), 123)
+        self.assertEqual(settings.getint("TEST_INTx"), 0)
+        self.assertEqual(settings.getint("TEST_INTx", 45), 45)
+        self.assertEqual(settings.getfloat("TEST_FLOAT1"), 123.45)
+        self.assertEqual(settings.getfloat("TEST_FLOAT2"), 123.45)
+        self.assertEqual(settings.getfloat("TEST_FLOATx"), 0.0)
+        self.assertEqual(settings.getfloat("TEST_FLOATx", 55.0), 55.0)
+        self.assertEqual(settings.getlist("TEST_LIST1"), ["one", "two"])
+        self.assertEqual(settings.getlist("TEST_LIST2"), ["one", "two"])
+        self.assertEqual(settings.getlist("TEST_LISTx"), [])
+        self.assertEqual(settings.getlist("TEST_LISTx", ["default"]), ["default"])
+        self.assertEqual(settings["TEST_STR"], "value")
+        self.assertEqual(settings.get("TEST_STR"), "value")
+        self.assertEqual(settings["TEST_STRx"], None)
+        self.assertEqual(settings.get("TEST_STRx"), None)
+        self.assertEqual(settings.get("TEST_STRx", "default"), "default")
+        self.assertEqual(settings.getdict("TEST_DICT1"), {"key1": "val1", "ke2": 3})
+        self.assertEqual(settings.getdict("TEST_DICT2"), {"key1": "val1", "ke2": 3})
+        self.assertEqual(settings.getdict("TEST_DICT3"), {})
+        self.assertEqual(settings.getdict("TEST_DICT3", {"key1": 5}), {"key1": 5})
+        self.assertRaises(ValueError, settings.getdict, "TEST_LIST1")
+        self.assertRaises(ValueError, settings.getbool, "TEST_ENABLED_WRONG")
+        self.assertRaises(ValueError, settings.getbool, "TEST_DISABLED_WRONG")
 
     def test_getpriority(self):
-        settings = BaseSettings({'key': 'value'}, priority=99)
-        self.assertEqual(settings.getpriority('key'), 99)
-        self.assertEqual(settings.getpriority('nonexistentkey'), None)
+        settings = BaseSettings({"key": "value"}, priority=99)
+        self.assertEqual(settings.getpriority("key"), 99)
+        self.assertEqual(settings.getpriority("nonexistentkey"), None)
 
     def test_getwithbase(self):
-        s = BaseSettings({'TEST_BASE': BaseSettings({1: 1, 2: 2}, 'project'),
-                          'TEST': BaseSettings({1: 10, 3: 30}, 'default'),
-                          'HASNOBASE': BaseSettings({3: 3000}, 'default')})
-        s['TEST'].set(2, 200, 'cmdline')
-        self.assertCountEqual(s.getwithbase('TEST'), {1: 1, 2: 200, 3: 30})
-        self.assertCountEqual(s.getwithbase('HASNOBASE'), s['HASNOBASE'])
-        self.assertEqual(s.getwithbase('NONEXISTENT'), {})
+        s = BaseSettings(
+            {
+                "TEST_BASE": BaseSettings({1: 1, 2: 2}, "project"),
+                "TEST": BaseSettings({1: 10, 3: 30}, "default"),
+                "HASNOBASE": BaseSettings({3: 3000}, "default"),
+            }
+        )
+        s["TEST"].set(2, 200, "cmdline")
+        self.assertCountEqual(s.getwithbase("TEST"), {1: 1, 2: 200, 3: 30})
+        self.assertCountEqual(s.getwithbase("HASNOBASE"), s["HASNOBASE"])
+        self.assertEqual(s.getwithbase("NONEXISTENT"), {})
 
     def test_maxpriority(self):
         # Empty settings should return 'default'
         self.assertEqual(self.settings.maxpriority(), 0)
-        self.settings.set('A', 0, 10)
-        self.settings.set('B', 0, 30)
+        self.settings.set("A", 0, 10)
+        self.settings.set("B", 0, 30)
         self.assertEqual(self.settings.maxpriority(), 30)
 
     def test_copy(self):
         values = {
-            'TEST_BOOL': True,
-            'TEST_LIST': ['one', 'two'],
-            'TEST_LIST_OF_LISTS': [['first_one', 'first_two'],
-                                   ['second_one', 'second_two']]
+            "TEST_BOOL": True,
+            "TEST_LIST": ["one", "two"],
+            "TEST_LIST_OF_LISTS": [
+                ["first_one", "first_two"],
+                ["second_one", "second_two"],
+            ],
         }
         self.settings.setdict(values)
         copy = self.settings.copy()
-        self.settings.set('TEST_BOOL', False)
-        self.assertTrue(copy.get('TEST_BOOL'))
+        self.settings.set("TEST_BOOL", False)
+        self.assertTrue(copy.get("TEST_BOOL"))
 
-        test_list = self.settings.get('TEST_LIST')
-        test_list.append('three')
-        self.assertListEqual(copy.get('TEST_LIST'), ['one', 'two'])
-
-        test_list_of_lists = self.settings.get('TEST_LIST_OF_LISTS')
-        test_list_of_lists[0].append('first_three')
-        self.assertListEqual(copy.get('TEST_LIST_OF_LISTS')[0],
-                             ['first_one', 'first_two'])
+        test_list = self.settings.get("TEST_LIST")
+        test_list.append("three")
+        self.assertListEqual(copy.get("TEST_LIST"), ["one", "two"])
+
+        test_list_of_lists = self.settings.get("TEST_LIST_OF_LISTS")
+        test_list_of_lists[0].append("first_three")
+        self.assertListEqual(
+            copy.get("TEST_LIST_OF_LISTS")[0], ["first_one", "first_two"]
+        )
 
     def test_copy_to_dict(self):
-        s = BaseSettings({'TEST_STRING': 'a string',
-                          'TEST_LIST': [1, 2],
-                          'TEST_BOOLEAN': False,
-                          'TEST_BASE': BaseSettings({1: 1, 2: 2}, 'project'),
-                          'TEST': BaseSettings({1: 10, 3: 30}, 'default'),
-                          'HASNOBASE': BaseSettings({3: 3000}, 'default')})
+        s = BaseSettings(
+            {
+                "TEST_STRING": "a string",
+                "TEST_LIST": [1, 2],
+                "TEST_BOOLEAN": False,
+                "TEST_BASE": BaseSettings({1: 1, 2: 2}, "project"),
+                "TEST": BaseSettings({1: 10, 3: 30}, "default"),
+                "HASNOBASE": BaseSettings({3: 3000}, "default"),
+            }
+        )
         self.assertDictEqual(
             s.copy_to_dict(),
             {
-                'HASNOBASE': {3: 3000},
-                'TEST': {1: 10, 3: 30},
-                'TEST_BASE': {1: 1, 2: 2},
-                'TEST_LIST': [1, 2],
-                'TEST_BOOLEAN': False,
-                'TEST_STRING': 'a string',
-            }
+                "HASNOBASE": {3: 3000},
+                "TEST": {1: 10, 3: 30},
+                "TEST_BASE": {1: 1, 2: 2},
+                "TEST_LIST": [1, 2],
+                "TEST_BOOLEAN": False,
+                "TEST_STRING": "a string",
+            },
         )
 
     def test_freeze(self):
         self.settings.freeze()
         with self.assertRaises(TypeError) as cm:
-            self.settings.set('TEST_BOOL', False)
-            self.assertEqual(str(cm.exception),
-                             "Trying to modify an immutable Settings object")
+            self.settings.set("TEST_BOOL", False)
+            self.assertEqual(
+                str(cm.exception), "Trying to modify an immutable Settings object"
+            )
 
     def test_frozencopy(self):
         frozencopy = self.settings.frozencopy()
         self.assertTrue(frozencopy.frozen)
         self.assertIsNot(frozencopy, self.settings)
 
 
 class SettingsTest(unittest.TestCase):
-
     def setUp(self):
         self.settings = Settings()
 
-    @mock.patch.dict('scrapy.settings.SETTINGS_PRIORITIES', {'default': 10})
-    @mock.patch('scrapy.settings.default_settings', default_settings)
+    @mock.patch.dict("scrapy.settings.SETTINGS_PRIORITIES", {"default": 10})
+    @mock.patch("scrapy.settings.default_settings", default_settings)
     def test_initial_defaults(self):
         settings = Settings()
         self.assertEqual(len(settings.attributes), 2)
-        self.assertIn('TEST_DEFAULT', settings.attributes)
+        self.assertIn("TEST_DEFAULT", settings.attributes)
 
-        attr = settings.attributes['TEST_DEFAULT']
+        attr = settings.attributes["TEST_DEFAULT"]
         self.assertIsInstance(attr, SettingsAttribute)
-        self.assertEqual(attr.value, 'defvalue')
+        self.assertEqual(attr.value, "defvalue")
         self.assertEqual(attr.priority, 10)
 
-    @mock.patch.dict('scrapy.settings.SETTINGS_PRIORITIES', {})
-    @mock.patch('scrapy.settings.default_settings', {})
+    @mock.patch.dict("scrapy.settings.SETTINGS_PRIORITIES", {})
+    @mock.patch("scrapy.settings.default_settings", {})
     def test_initial_values(self):
-        settings = Settings({'TEST_OPTION': 'value'}, 10)
+        settings = Settings({"TEST_OPTION": "value"}, 10)
         self.assertEqual(len(settings.attributes), 1)
-        self.assertIn('TEST_OPTION', settings.attributes)
+        self.assertIn("TEST_OPTION", settings.attributes)
 
-        attr = settings.attributes['TEST_OPTION']
+        attr = settings.attributes["TEST_OPTION"]
         self.assertIsInstance(attr, SettingsAttribute)
-        self.assertEqual(attr.value, 'value')
+        self.assertEqual(attr.value, "value")
         self.assertEqual(attr.priority, 10)
 
-    @mock.patch('scrapy.settings.default_settings', default_settings)
+    @mock.patch("scrapy.settings.default_settings", default_settings)
     def test_autopromote_dicts(self):
         settings = Settings()
-        mydict = settings.get('TEST_DICT')
+        mydict = settings.get("TEST_DICT")
         self.assertIsInstance(mydict, BaseSettings)
-        self.assertIn('key', mydict)
-        self.assertEqual(mydict['key'], 'val')
-        self.assertEqual(mydict.getpriority('key'), 0)
+        self.assertIn("key", mydict)
+        self.assertEqual(mydict["key"], "val")
+        self.assertEqual(mydict.getpriority("key"), 0)
 
-    @mock.patch('scrapy.settings.default_settings', default_settings)
+    @mock.patch("scrapy.settings.default_settings", default_settings)
     def test_getdict_autodegrade_basesettings(self):
         settings = Settings()
-        mydict = settings.getdict('TEST_DICT')
+        mydict = settings.getdict("TEST_DICT")
         self.assertIsInstance(mydict, dict)
         self.assertEqual(len(mydict), 1)
-        self.assertIn('key', mydict)
-        self.assertEqual(mydict['key'], 'val')
+        self.assertIn("key", mydict)
+        self.assertEqual(mydict["key"], "val")
 
     def test_passing_objects_as_values(self):
         from scrapy.core.downloader.handlers.file import FileDownloadHandler
         from scrapy.utils.misc import create_instance
         from scrapy.utils.test import get_crawler
 
-        class TestPipeline():
+        class TestPipeline:
             def process_item(self, i, s):
                 return i
 
-        settings = Settings({
-            'ITEM_PIPELINES': {
-                TestPipeline: 800,
-            },
-            'DOWNLOAD_HANDLERS': {
-                'ftp': FileDownloadHandler,
-            },
-        })
+        settings = Settings(
+            {
+                "ITEM_PIPELINES": {
+                    TestPipeline: 800,
+                },
+                "DOWNLOAD_HANDLERS": {
+                    "ftp": FileDownloadHandler,
+                },
+            }
+        )
 
-        self.assertIn('ITEM_PIPELINES', settings.attributes)
+        self.assertIn("ITEM_PIPELINES", settings.attributes)
 
-        mypipeline, priority = settings.getdict('ITEM_PIPELINES').popitem()
+        mypipeline, priority = settings.getdict("ITEM_PIPELINES").popitem()
         self.assertEqual(priority, 800)
         self.assertEqual(mypipeline, TestPipeline)
         self.assertIsInstance(mypipeline(), TestPipeline)
-        self.assertEqual(mypipeline().process_item('item', None), 'item')
+        self.assertEqual(mypipeline().process_item("item", None), "item")
 
-        myhandler = settings.getdict('DOWNLOAD_HANDLERS').pop('ftp')
+        myhandler = settings.getdict("DOWNLOAD_HANDLERS").pop("ftp")
         self.assertEqual(myhandler, FileDownloadHandler)
         myhandler_instance = create_instance(myhandler, None, get_crawler())
         self.assertIsInstance(myhandler_instance, FileDownloadHandler)
-        self.assertTrue(hasattr(myhandler_instance, 'download_request'))
+        self.assertTrue(hasattr(myhandler_instance, "download_request"))
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_signals.py` & `Scrapy-2.8.0/tests/test_signals.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 from pytest import mark
 from twisted.internet import defer
 from twisted.trial import unittest
 
-from scrapy import signals, Request, Spider
+from scrapy import Request, Spider, signals
 from scrapy.utils.test import get_crawler, get_from_asyncio_queue
-
 from tests.mockserver import MockServer
 
 
 class ItemSpider(Spider):
-    name = 'itemspider'
+    name = "itemspider"
 
     def start_requests(self):
         for index in range(10):
-            yield Request(self.mockserver.url(f'/status?n=200&id={index}'),
-                          meta={'index': index})
+            yield Request(
+                self.mockserver.url(f"/status?n=200&id={index}"), meta={"index": index}
+            )
 
     def parse(self, response):
-        return {'index': response.meta['index']}
+        return {"index": response.meta["index"]}
 
 
 class AsyncSignalTestCase(unittest.TestCase):
     def setUp(self):
         self.mockserver = MockServer()
         self.mockserver.__enter__()
         self.items = []
@@ -37,8 +37,8 @@
     @defer.inlineCallbacks
     def test_simple_pipeline(self):
         crawler = get_crawler(ItemSpider)
         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)
         yield crawler.crawl(mockserver=self.mockserver)
         self.assertEqual(len(self.items), 10)
         for index in range(10):
-            self.assertIn({'index': index}, self.items)
+            self.assertIn({"index": index}, self.items)
```

### Comparing `Scrapy-2.7.1/tests/test_spider.py` & `Scrapy-2.8.0/tests/test_spider.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,120 +2,120 @@
 import inspect
 import warnings
 from io import BytesIO
 from unittest import mock
 
 from testfixtures import LogCapture
 from twisted.trial import unittest
+from w3lib.url import safe_url_string
 
 from scrapy import signals
+from scrapy.http import HtmlResponse, Request, Response, TextResponse, XmlResponse
+from scrapy.linkextractors import LinkExtractor
 from scrapy.settings import Settings
-from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse
-from scrapy.spiders.init import InitSpider
 from scrapy.spiders import (
-    CSVFeedSpider,
     CrawlSpider,
+    CSVFeedSpider,
     Rule,
     SitemapSpider,
     Spider,
     XMLFeedSpider,
 )
-from scrapy.linkextractors import LinkExtractor
+from scrapy.spiders.init import InitSpider
 from scrapy.utils.test import get_crawler
 from tests import get_testdata
-from w3lib.url import safe_url_string
 
 
 class SpiderTest(unittest.TestCase):
 
     spider_class = Spider
 
     def setUp(self):
         warnings.simplefilter("always")
 
     def tearDown(self):
         warnings.resetwarnings()
 
     def test_base_spider(self):
         spider = self.spider_class("example.com")
-        self.assertEqual(spider.name, 'example.com')
+        self.assertEqual(spider.name, "example.com")
         self.assertEqual(spider.start_urls, [])
 
     def test_start_requests(self):
-        spider = self.spider_class('example.com')
+        spider = self.spider_class("example.com")
         start_requests = spider.start_requests()
         self.assertTrue(inspect.isgenerator(start_requests))
         self.assertEqual(list(start_requests), [])
 
     def test_spider_args(self):
         """``__init__`` method arguments are assigned to spider attributes"""
-        spider = self.spider_class('example.com', foo='bar')
-        self.assertEqual(spider.foo, 'bar')
+        spider = self.spider_class("example.com", foo="bar")
+        self.assertEqual(spider.foo, "bar")
 
     def test_spider_without_name(self):
         """``__init__`` method arguments are assigned to spider attributes"""
         self.assertRaises(ValueError, self.spider_class)
-        self.assertRaises(ValueError, self.spider_class, somearg='foo')
+        self.assertRaises(ValueError, self.spider_class, somearg="foo")
 
     def test_from_crawler_crawler_and_settings_population(self):
         crawler = get_crawler()
-        spider = self.spider_class.from_crawler(crawler, 'example.com')
-        self.assertTrue(hasattr(spider, 'crawler'))
+        spider = self.spider_class.from_crawler(crawler, "example.com")
+        self.assertTrue(hasattr(spider, "crawler"))
         self.assertIs(spider.crawler, crawler)
-        self.assertTrue(hasattr(spider, 'settings'))
+        self.assertTrue(hasattr(spider, "settings"))
         self.assertIs(spider.settings, crawler.settings)
 
     def test_from_crawler_init_call(self):
-        with mock.patch.object(self.spider_class, '__init__',
-                               return_value=None) as mock_init:
-            self.spider_class.from_crawler(get_crawler(), 'example.com',
-                                           foo='bar')
-            mock_init.assert_called_once_with('example.com', foo='bar')
+        with mock.patch.object(
+            self.spider_class, "__init__", return_value=None
+        ) as mock_init:
+            self.spider_class.from_crawler(get_crawler(), "example.com", foo="bar")
+            mock_init.assert_called_once_with("example.com", foo="bar")
 
     def test_closed_signal_call(self):
         class TestSpider(self.spider_class):
             closed_called = False
 
             def closed(self, reason):
                 self.closed_called = True
 
         crawler = get_crawler()
-        spider = TestSpider.from_crawler(crawler, 'example.com')
-        crawler.signals.send_catch_log(signal=signals.spider_opened,
-                                       spider=spider)
-        crawler.signals.send_catch_log(signal=signals.spider_closed,
-                                       spider=spider, reason=None)
+        spider = TestSpider.from_crawler(crawler, "example.com")
+        crawler.signals.send_catch_log(signal=signals.spider_opened, spider=spider)
+        crawler.signals.send_catch_log(
+            signal=signals.spider_closed, spider=spider, reason=None
+        )
         self.assertTrue(spider.closed_called)
 
     def test_update_settings(self):
-        spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}
-        project_settings = {'TEST1': 'project', 'TEST3': 'project'}
+        spider_settings = {"TEST1": "spider", "TEST2": "spider"}
+        project_settings = {"TEST1": "project", "TEST3": "project"}
         self.spider_class.custom_settings = spider_settings
-        settings = Settings(project_settings, priority='project')
+        settings = Settings(project_settings, priority="project")
 
         self.spider_class.update_settings(settings)
-        self.assertEqual(settings.get('TEST1'), 'spider')
-        self.assertEqual(settings.get('TEST2'), 'spider')
-        self.assertEqual(settings.get('TEST3'), 'project')
+        self.assertEqual(settings.get("TEST1"), "spider")
+        self.assertEqual(settings.get("TEST2"), "spider")
+        self.assertEqual(settings.get("TEST3"), "project")
 
     def test_logger(self):
-        spider = self.spider_class('example.com')
+        spider = self.spider_class("example.com")
         with LogCapture() as lc:
-            spider.logger.info('test log msg')
-        lc.check(('example.com', 'INFO', 'test log msg'))
+            spider.logger.info("test log msg")
+        lc.check(("example.com", "INFO", "test log msg"))
 
         record = lc.records[0]
-        self.assertIn('spider', record.__dict__)
+        self.assertIn("spider", record.__dict__)
         self.assertIs(record.spider, spider)
 
     def test_log(self):
-        spider = self.spider_class('example.com')
-        with mock.patch('scrapy.spiders.Spider.logger') as mock_logger:
-            spider.log('test log msg', 'INFO')
-        mock_logger.log.assert_called_once_with('INFO', 'test log msg')
+        spider = self.spider_class("example.com")
+        with mock.patch("scrapy.spiders.Spider.logger") as mock_logger:
+            spider.log("test log msg", "INFO")
+        mock_logger.log.assert_called_once_with("INFO", "test log msg")
 
 
 class InitSpiderTest(SpiderTest):
 
     spider_class = InitSpider
 
 
@@ -128,66 +128,74 @@
         <urlset xmlns:x="http://www.google.com/schemas/sitemap/0.84"
                 xmlns:y="http://www.example.com/schemas/extras/1.0">
         <url><x:loc>http://www.example.com/Special-Offers.html</loc><y:updated>2009-08-16</updated>
             <other value="bar" y:custom="fuu"/>
         </url>
         <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</updated><other value="foo"/></url>
         </urlset>"""
-        response = XmlResponse(url='http://example.com/sitemap.xml', body=body)
+        response = XmlResponse(url="http://example.com/sitemap.xml", body=body)
 
         class _XMLSpider(self.spider_class):
-            itertag = 'url'
+            itertag = "url"
             namespaces = (
-                ('a', 'http://www.google.com/schemas/sitemap/0.84'),
-                ('b', 'http://www.example.com/schemas/extras/1.0'),
+                ("a", "http://www.google.com/schemas/sitemap/0.84"),
+                ("b", "http://www.example.com/schemas/extras/1.0"),
             )
 
             def parse_node(self, response, selector):
                 yield {
-                    'loc': selector.xpath('a:loc/text()').getall(),
-                    'updated': selector.xpath('b:updated/text()').getall(),
-                    'other': selector.xpath('other/@value').getall(),
-                    'custom': selector.xpath('other/@b:custom').getall(),
+                    "loc": selector.xpath("a:loc/text()").getall(),
+                    "updated": selector.xpath("b:updated/text()").getall(),
+                    "other": selector.xpath("other/@value").getall(),
+                    "custom": selector.xpath("other/@b:custom").getall(),
                 }
 
-        for iterator in ('iternodes', 'xml'):
-            spider = _XMLSpider('example', iterator=iterator)
+        for iterator in ("iternodes", "xml"):
+            spider = _XMLSpider("example", iterator=iterator)
             output = list(spider._parse(response))
             self.assertEqual(len(output), 2, iterator)
-            self.assertEqual(output, [
-                {'loc': ['http://www.example.com/Special-Offers.html'],
-                 'updated': ['2009-08-16'],
-                 'custom': ['fuu'],
-                 'other': ['bar']},
-                {'loc': [],
-                 'updated': ['2009-08-16'],
-                 'other': ['foo'],
-                 'custom': []},
-            ], iterator)
+            self.assertEqual(
+                output,
+                [
+                    {
+                        "loc": ["http://www.example.com/Special-Offers.html"],
+                        "updated": ["2009-08-16"],
+                        "custom": ["fuu"],
+                        "other": ["bar"],
+                    },
+                    {
+                        "loc": [],
+                        "updated": ["2009-08-16"],
+                        "other": ["foo"],
+                        "custom": [],
+                    },
+                ],
+                iterator,
+            )
 
 
 class CSVFeedSpiderTest(SpiderTest):
 
     spider_class = CSVFeedSpider
 
     def test_parse_rows(self):
-        body = get_testdata('feeds', 'feed-sample6.csv')
+        body = get_testdata("feeds", "feed-sample6.csv")
         response = Response("http://example.org/dummy.csv", body=body)
 
         class _CrawlSpider(self.spider_class):
             name = "test"
             delimiter = ","
             quotechar = "'"
 
             def parse_row(self, response, row):
                 return row
 
         spider = _CrawlSpider()
         rows = list(spider.parse_rows(response))
-        assert rows[0] == {'id': '1', 'name': 'alpha', 'value': 'foobar'}
+        assert rows[0] == {"id": "1", "name": "alpha", "value": "foobar"}
         assert len(rows) == 4
 
 
 class CrawlSpiderTest(SpiderTest):
 
     test_body = b"""<html><head><title>Page title<title>
     <body>
@@ -199,231 +207,276 @@
     <p><a href="/nofollow.html">This shouldn't be followed</a></p>
     </div>
     </body></html>"""
     spider_class = CrawlSpider
 
     def test_rule_without_link_extractor(self):
 
-        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)
+        response = HtmlResponse(
+            "http://example.org/somepage/index.html", body=self.test_body
+        )
 
         class _CrawlSpider(self.spider_class):
             name = "test"
-            allowed_domains = ['example.org']
-            rules = (
-                Rule(),
-            )
+            allowed_domains = ["example.org"]
+            rules = (Rule(),)
 
         spider = _CrawlSpider()
         output = list(spider._requests_to_follow(response))
         self.assertEqual(len(output), 3)
         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
-        self.assertEqual([r.url for r in output],
-                         ['http://example.org/somepage/item/12.html',
-                          'http://example.org/about.html',
-                          'http://example.org/nofollow.html'])
+        self.assertEqual(
+            [r.url for r in output],
+            [
+                "http://example.org/somepage/item/12.html",
+                "http://example.org/about.html",
+                "http://example.org/nofollow.html",
+            ],
+        )
 
     def test_process_links(self):
 
-        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)
+        response = HtmlResponse(
+            "http://example.org/somepage/index.html", body=self.test_body
+        )
 
         class _CrawlSpider(self.spider_class):
             name = "test"
-            allowed_domains = ['example.org']
-            rules = (
-                Rule(LinkExtractor(), process_links="dummy_process_links"),
-            )
+            allowed_domains = ["example.org"]
+            rules = (Rule(LinkExtractor(), process_links="dummy_process_links"),)
 
             def dummy_process_links(self, links):
                 return links
 
         spider = _CrawlSpider()
         output = list(spider._requests_to_follow(response))
         self.assertEqual(len(output), 3)
         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
-        self.assertEqual([r.url for r in output],
-                         ['http://example.org/somepage/item/12.html',
-                          'http://example.org/about.html',
-                          'http://example.org/nofollow.html'])
+        self.assertEqual(
+            [r.url for r in output],
+            [
+                "http://example.org/somepage/item/12.html",
+                "http://example.org/about.html",
+                "http://example.org/nofollow.html",
+            ],
+        )
 
     def test_process_links_filter(self):
 
-        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)
+        response = HtmlResponse(
+            "http://example.org/somepage/index.html", body=self.test_body
+        )
 
         class _CrawlSpider(self.spider_class):
             import re
 
             name = "test"
-            allowed_domains = ['example.org']
-            rules = (
-                Rule(LinkExtractor(), process_links="filter_process_links"),
-            )
-            _test_regex = re.compile('nofollow')
+            allowed_domains = ["example.org"]
+            rules = (Rule(LinkExtractor(), process_links="filter_process_links"),)
+            _test_regex = re.compile("nofollow")
 
             def filter_process_links(self, links):
-                return [link for link in links
-                        if not self._test_regex.search(link.url)]
+                return [link for link in links if not self._test_regex.search(link.url)]
 
         spider = _CrawlSpider()
         output = list(spider._requests_to_follow(response))
         self.assertEqual(len(output), 2)
         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
-        self.assertEqual([r.url for r in output],
-                         ['http://example.org/somepage/item/12.html',
-                          'http://example.org/about.html'])
+        self.assertEqual(
+            [r.url for r in output],
+            [
+                "http://example.org/somepage/item/12.html",
+                "http://example.org/about.html",
+            ],
+        )
 
     def test_process_links_generator(self):
 
-        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)
+        response = HtmlResponse(
+            "http://example.org/somepage/index.html", body=self.test_body
+        )
 
         class _CrawlSpider(self.spider_class):
             name = "test"
-            allowed_domains = ['example.org']
-            rules = (
-                Rule(LinkExtractor(), process_links="dummy_process_links"),
-            )
+            allowed_domains = ["example.org"]
+            rules = (Rule(LinkExtractor(), process_links="dummy_process_links"),)
 
             def dummy_process_links(self, links):
                 for link in links:
                     yield link
 
         spider = _CrawlSpider()
         output = list(spider._requests_to_follow(response))
         self.assertEqual(len(output), 3)
         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
-        self.assertEqual([r.url for r in output],
-                         ['http://example.org/somepage/item/12.html',
-                          'http://example.org/about.html',
-                          'http://example.org/nofollow.html'])
+        self.assertEqual(
+            [r.url for r in output],
+            [
+                "http://example.org/somepage/item/12.html",
+                "http://example.org/about.html",
+                "http://example.org/nofollow.html",
+            ],
+        )
 
     def test_process_request(self):
 
-        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)
+        response = HtmlResponse(
+            "http://example.org/somepage/index.html", body=self.test_body
+        )
 
         def process_request_change_domain(request, response):
-            return request.replace(url=request.url.replace('.org', '.com'))
+            return request.replace(url=request.url.replace(".org", ".com"))
 
         class _CrawlSpider(self.spider_class):
             name = "test"
-            allowed_domains = ['example.org']
+            allowed_domains = ["example.org"]
             rules = (
                 Rule(LinkExtractor(), process_request=process_request_change_domain),
             )
 
         spider = _CrawlSpider()
         output = list(spider._requests_to_follow(response))
         self.assertEqual(len(output), 3)
         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
-        self.assertEqual([r.url for r in output],
-                         ['http://example.com/somepage/item/12.html',
-                          'http://example.com/about.html',
-                          'http://example.com/nofollow.html'])
+        self.assertEqual(
+            [r.url for r in output],
+            [
+                "http://example.com/somepage/item/12.html",
+                "http://example.com/about.html",
+                "http://example.com/nofollow.html",
+            ],
+        )
 
     def test_process_request_with_response(self):
 
-        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)
+        response = HtmlResponse(
+            "http://example.org/somepage/index.html", body=self.test_body
+        )
 
         def process_request_meta_response_class(request, response):
-            request.meta['response_class'] = response.__class__.__name__
+            request.meta["response_class"] = response.__class__.__name__
             return request
 
         class _CrawlSpider(self.spider_class):
             name = "test"
-            allowed_domains = ['example.org']
+            allowed_domains = ["example.org"]
             rules = (
-                Rule(LinkExtractor(), process_request=process_request_meta_response_class),
+                Rule(
+                    LinkExtractor(), process_request=process_request_meta_response_class
+                ),
             )
 
         spider = _CrawlSpider()
         output = list(spider._requests_to_follow(response))
         self.assertEqual(len(output), 3)
         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
-        self.assertEqual([r.url for r in output],
-                         ['http://example.org/somepage/item/12.html',
-                          'http://example.org/about.html',
-                          'http://example.org/nofollow.html'])
-        self.assertEqual([r.meta['response_class'] for r in output],
-                         ['HtmlResponse', 'HtmlResponse', 'HtmlResponse'])
+        self.assertEqual(
+            [r.url for r in output],
+            [
+                "http://example.org/somepage/item/12.html",
+                "http://example.org/about.html",
+                "http://example.org/nofollow.html",
+            ],
+        )
+        self.assertEqual(
+            [r.meta["response_class"] for r in output],
+            ["HtmlResponse", "HtmlResponse", "HtmlResponse"],
+        )
 
     def test_process_request_instance_method(self):
 
-        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)
+        response = HtmlResponse(
+            "http://example.org/somepage/index.html", body=self.test_body
+        )
 
         class _CrawlSpider(self.spider_class):
             name = "test"
-            allowed_domains = ['example.org']
-            rules = (
-                Rule(LinkExtractor(), process_request='process_request_upper'),
-            )
+            allowed_domains = ["example.org"]
+            rules = (Rule(LinkExtractor(), process_request="process_request_upper"),)
 
             def process_request_upper(self, request, response):
                 return request.replace(url=request.url.upper())
 
         spider = _CrawlSpider()
         output = list(spider._requests_to_follow(response))
         self.assertEqual(len(output), 3)
         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
-        self.assertEqual([r.url for r in output],
-                         [safe_url_string('http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML'),
-                          safe_url_string('http://EXAMPLE.ORG/ABOUT.HTML'),
-                          safe_url_string('http://EXAMPLE.ORG/NOFOLLOW.HTML')])
+        self.assertEqual(
+            [r.url for r in output],
+            [
+                safe_url_string("http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML"),
+                safe_url_string("http://EXAMPLE.ORG/ABOUT.HTML"),
+                safe_url_string("http://EXAMPLE.ORG/NOFOLLOW.HTML"),
+            ],
+        )
 
     def test_process_request_instance_method_with_response(self):
 
-        response = HtmlResponse("http://example.org/somepage/index.html", body=self.test_body)
+        response = HtmlResponse(
+            "http://example.org/somepage/index.html", body=self.test_body
+        )
 
         class _CrawlSpider(self.spider_class):
             name = "test"
-            allowed_domains = ['example.org']
+            allowed_domains = ["example.org"]
             rules = (
-                Rule(LinkExtractor(), process_request='process_request_meta_response_class'),
+                Rule(
+                    LinkExtractor(),
+                    process_request="process_request_meta_response_class",
+                ),
             )
 
             def process_request_meta_response_class(self, request, response):
-                request.meta['response_class'] = response.__class__.__name__
+                request.meta["response_class"] = response.__class__.__name__
                 return request
 
         spider = _CrawlSpider()
         output = list(spider._requests_to_follow(response))
         self.assertEqual(len(output), 3)
         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))
-        self.assertEqual([r.url for r in output],
-                         ['http://example.org/somepage/item/12.html',
-                          'http://example.org/about.html',
-                          'http://example.org/nofollow.html'])
-        self.assertEqual([r.meta['response_class'] for r in output],
-                         ['HtmlResponse', 'HtmlResponse', 'HtmlResponse'])
+        self.assertEqual(
+            [r.url for r in output],
+            [
+                "http://example.org/somepage/item/12.html",
+                "http://example.org/about.html",
+                "http://example.org/nofollow.html",
+            ],
+        )
+        self.assertEqual(
+            [r.meta["response_class"] for r in output],
+            ["HtmlResponse", "HtmlResponse", "HtmlResponse"],
+        )
 
     def test_follow_links_attribute_population(self):
         crawler = get_crawler()
-        spider = self.spider_class.from_crawler(crawler, 'example.com')
-        self.assertTrue(hasattr(spider, '_follow_links'))
+        spider = self.spider_class.from_crawler(crawler, "example.com")
+        self.assertTrue(hasattr(spider, "_follow_links"))
         self.assertTrue(spider._follow_links)
 
-        settings_dict = {'CRAWLSPIDER_FOLLOW_LINKS': False}
+        settings_dict = {"CRAWLSPIDER_FOLLOW_LINKS": False}
         crawler = get_crawler(settings_dict=settings_dict)
-        spider = self.spider_class.from_crawler(crawler, 'example.com')
-        self.assertTrue(hasattr(spider, '_follow_links'))
+        spider = self.spider_class.from_crawler(crawler, "example.com")
+        self.assertTrue(hasattr(spider, "_follow_links"))
         self.assertFalse(spider._follow_links)
 
     def test_start_url(self):
         spider = self.spider_class("example.com")
-        spider.start_url = 'https://www.example.com'
+        spider.start_url = "https://www.example.com"
 
-        with self.assertRaisesRegex(AttributeError,
-                                    r'^Crawling could not start.*$'):
+        with self.assertRaisesRegex(AttributeError, r"^Crawling could not start.*$"):
             list(spider.start_requests())
 
 
 class SitemapSpiderTest(SpiderTest):
 
     spider_class = SitemapSpider
 
     BODY = b"SITEMAP"
     f = BytesIO()
-    g = gzip.GzipFile(fileobj=f, mode='w+b')
+    g = gzip.GzipFile(fileobj=f, mode="w+b")
     g.write(BODY)
     g.close()
     GZBODY = f.getvalue()
 
     def assertSitemapBody(self, response, body):
         spider = self.spider_class("example.com")
         self.assertEqual(spider._get_sitemap_body(response), body)
@@ -435,16 +488,19 @@
         r = HtmlResponse(url="http://www.example.com/", body=self.BODY)
         self.assertSitemapBody(r, None)
 
         r = Response(url="http://www.example.com/favicon.ico", body=self.BODY)
         self.assertSitemapBody(r, None)
 
     def test_get_sitemap_body_gzip_headers(self):
-        r = Response(url="http://www.example.com/sitemap", body=self.GZBODY,
-                     headers={"content-type": "application/gzip"})
+        r = Response(
+            url="http://www.example.com/sitemap",
+            body=self.GZBODY,
+            headers={"content-type": "application/gzip"},
+        )
         self.assertSitemapBody(r, self.BODY)
 
     def test_get_sitemap_body_xml_url(self):
         r = TextResponse(url="http://www.example.com/sitemap.xml", body=self.BODY)
         self.assertSitemapBody(r, self.BODY)
 
     def test_get_sitemap_body_xml_url_compressed(self):
@@ -461,19 +517,23 @@
 Sitemap: http://example.com/sitemap-product-index.xml
 Sitemap: HTTP://example.com/sitemap-uppercase.xml
 Sitemap: /sitemap-relative-url.xml
 """
 
         r = TextResponse(url="http://www.example.com/robots.txt", body=robots)
         spider = self.spider_class("example.com")
-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
-                         ['http://example.com/sitemap.xml',
-                          'http://example.com/sitemap-product-index.xml',
-                          'http://example.com/sitemap-uppercase.xml',
-                          'http://www.example.com/sitemap-relative-url.xml'])
+        self.assertEqual(
+            [req.url for req in spider._parse_sitemap(r)],
+            [
+                "http://example.com/sitemap.xml",
+                "http://example.com/sitemap-product-index.xml",
+                "http://example.com/sitemap-uppercase.xml",
+                "http://www.example.com/sitemap-relative-url.xml",
+            ],
+        )
 
     def test_alternate_url_locs(self):
         sitemap = b"""<?xml version="1.0" encoding="UTF-8"?>
     <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
         xmlns:xhtml="http://www.w3.org/1999/xhtml">
         <url>
             <loc>http://www.example.com/english/</loc>
@@ -484,23 +544,29 @@
             <xhtml:link rel="alternate" hreflang="it"
                 href="http://www.example.com/italiano/"/>
             <xhtml:link rel="alternate" hreflang="it"/><!-- wrong tag without href -->
         </url>
     </urlset>"""
         r = TextResponse(url="http://www.example.com/sitemap.xml", body=sitemap)
         spider = self.spider_class("example.com")
-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
-                         ['http://www.example.com/english/'])
+        self.assertEqual(
+            [req.url for req in spider._parse_sitemap(r)],
+            ["http://www.example.com/english/"],
+        )
 
         spider.sitemap_alternate_links = True
-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
-                         ['http://www.example.com/english/',
-                          'http://www.example.com/deutsch/',
-                          'http://www.example.com/schweiz-deutsch/',
-                          'http://www.example.com/italiano/'])
+        self.assertEqual(
+            [req.url for req in spider._parse_sitemap(r)],
+            [
+                "http://www.example.com/english/",
+                "http://www.example.com/deutsch/",
+                "http://www.example.com/schweiz-deutsch/",
+                "http://www.example.com/italiano/",
+            ],
+        )
 
     def test_sitemap_filter(self):
         sitemap = b"""<?xml version="1.0" encoding="UTF-8"?>
     <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
         xmlns:xhtml="http://www.w3.org/1999/xhtml">
         <url>
             <loc>http://www.example.com/english/</loc>
@@ -511,28 +577,32 @@
             <lastmod>2005-01-01</lastmod>
         </url>
     </urlset>"""
 
         class FilteredSitemapSpider(self.spider_class):
             def sitemap_filter(self, entries):
                 from datetime import datetime
+
                 for entry in entries:
-                    date_time = datetime.strptime(entry['lastmod'], '%Y-%m-%d')
+                    date_time = datetime.strptime(entry["lastmod"], "%Y-%m-%d")
                     if date_time.year > 2008:
                         yield entry
 
         r = TextResponse(url="http://www.example.com/sitemap.xml", body=sitemap)
         spider = self.spider_class("example.com")
-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
-                         ['http://www.example.com/english/',
-                          'http://www.example.com/portuguese/'])
+        self.assertEqual(
+            [req.url for req in spider._parse_sitemap(r)],
+            ["http://www.example.com/english/", "http://www.example.com/portuguese/"],
+        )
 
         spider = FilteredSitemapSpider("example.com")
-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
-                         ['http://www.example.com/english/'])
+        self.assertEqual(
+            [req.url for req in spider._parse_sitemap(r)],
+            ["http://www.example.com/english/"],
+        )
 
     def test_sitemap_filter_with_alternate_links(self):
         sitemap = b"""<?xml version="1.0" encoding="UTF-8"?>
     <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
         xmlns:xhtml="http://www.w3.org/1999/xhtml">
         <url>
             <loc>http://www.example.com/english/article_1/</loc>
@@ -545,29 +615,35 @@
             <lastmod>2015-01-01</lastmod>
         </url>
     </urlset>"""
 
         class FilteredSitemapSpider(self.spider_class):
             def sitemap_filter(self, entries):
                 for entry in entries:
-                    alternate_links = entry.get('alternate', tuple())
+                    alternate_links = entry.get("alternate", tuple())
                     for link in alternate_links:
-                        if '/deutsch/' in link:
-                            entry['loc'] = link
+                        if "/deutsch/" in link:
+                            entry["loc"] = link
                             yield entry
 
         r = TextResponse(url="http://www.example.com/sitemap.xml", body=sitemap)
         spider = self.spider_class("example.com")
-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
-                         ['http://www.example.com/english/article_1/',
-                          'http://www.example.com/english/article_2/'])
+        self.assertEqual(
+            [req.url for req in spider._parse_sitemap(r)],
+            [
+                "http://www.example.com/english/article_1/",
+                "http://www.example.com/english/article_2/",
+            ],
+        )
 
         spider = FilteredSitemapSpider("example.com")
-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
-                         ['http://www.example.com/deutsch/article_1/'])
+        self.assertEqual(
+            [req.url for req in spider._parse_sitemap(r)],
+            ["http://www.example.com/deutsch/article_1/"],
+        )
 
     def test_sitemapindex_filter(self):
         sitemap = b"""<?xml version="1.0" encoding="UTF-8"?>
     <sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
         <sitemap>
             <loc>http://www.example.com/sitemap1.xml</loc>
             <lastmod>2004-01-01T20:00:00+00:00</lastmod>
@@ -577,42 +653,50 @@
             <lastmod>2005-01-01</lastmod>
         </sitemap>
     </sitemapindex>"""
 
         class FilteredSitemapSpider(self.spider_class):
             def sitemap_filter(self, entries):
                 from datetime import datetime
+
                 for entry in entries:
-                    date_time = datetime.strptime(entry['lastmod'].split('T')[0], '%Y-%m-%d')
+                    date_time = datetime.strptime(
+                        entry["lastmod"].split("T")[0], "%Y-%m-%d"
+                    )
                     if date_time.year > 2004:
                         yield entry
 
         r = TextResponse(url="http://www.example.com/sitemap.xml", body=sitemap)
         spider = self.spider_class("example.com")
-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
-                         ['http://www.example.com/sitemap1.xml',
-                          'http://www.example.com/sitemap2.xml'])
+        self.assertEqual(
+            [req.url for req in spider._parse_sitemap(r)],
+            [
+                "http://www.example.com/sitemap1.xml",
+                "http://www.example.com/sitemap2.xml",
+            ],
+        )
 
         spider = FilteredSitemapSpider("example.com")
-        self.assertEqual([req.url for req in spider._parse_sitemap(r)],
-                         ['http://www.example.com/sitemap2.xml'])
+        self.assertEqual(
+            [req.url for req in spider._parse_sitemap(r)],
+            ["http://www.example.com/sitemap2.xml"],
+        )
 
 
 class DeprecationTest(unittest.TestCase):
-
     def test_crawl_spider(self):
         assert issubclass(CrawlSpider, Spider)
-        assert isinstance(CrawlSpider(name='foo'), Spider)
+        assert isinstance(CrawlSpider(name="foo"), Spider)
 
 
 class NoParseMethodSpiderTest(unittest.TestCase):
 
     spider_class = Spider
 
     def test_undefined_parse_method(self):
-        spider = self.spider_class('example.com')
-        text = b'Random text'
+        spider = self.spider_class("example.com")
+        text = b"Random text"
         resp = TextResponse(url="http://www.example.com/random_url", body=text)
 
-        exc_msg = 'Spider.parse callback is not defined'
+        exc_msg = "Spider.parse callback is not defined"
         with self.assertRaisesRegex(NotImplementedError, exc_msg):
             spider.parse(resp)
```

### Comparing `Scrapy-2.7.1/tests/test_spiderloader/__init__.py` & `Scrapy-2.8.0/tests/test_spiderloader/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,160 +1,167 @@
-import sys
-import os
 import shutil
+import sys
+import tempfile
 import warnings
+from pathlib import Path
 
-from zope.interface.verify import verifyObject
 from twisted.trial import unittest
-
+from zope.interface.verify import verifyObject
 
 # ugly hack to avoid cyclic imports of scrapy.spiders when running this test
 # alone
 import scrapy
-import tempfile
+from scrapy.crawler import CrawlerRunner
+from scrapy.http import Request
 from scrapy.interfaces import ISpiderLoader
-from scrapy.spiderloader import SpiderLoader
 from scrapy.settings import Settings
-from scrapy.http import Request
-from scrapy.crawler import CrawlerRunner
+from scrapy.spiderloader import SpiderLoader
 
-module_dir = os.path.dirname(os.path.abspath(__file__))
+module_dir = Path(__file__).resolve().parent
 
 
-def _copytree(source, target):
+def _copytree(source: Path, target: Path):
     try:
         shutil.copytree(source, target)
     except shutil.Error:
         pass
 
 
 class SpiderLoaderTest(unittest.TestCase):
-
     def setUp(self):
-        orig_spiders_dir = os.path.join(module_dir, 'test_spiders')
-        self.tmpdir = tempfile.mkdtemp()
-        self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')
+        orig_spiders_dir = module_dir / "test_spiders"
+        self.tmpdir = Path(tempfile.mkdtemp())
+        self.spiders_dir = self.tmpdir / "test_spiders_xxx"
         _copytree(orig_spiders_dir, self.spiders_dir)
-        sys.path.append(self.tmpdir)
-        settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})
+        sys.path.append(str(self.tmpdir))
+        settings = Settings({"SPIDER_MODULES": ["test_spiders_xxx"]})
         self.spider_loader = SpiderLoader.from_settings(settings)
 
     def tearDown(self):
         del self.spider_loader
-        del sys.modules['test_spiders_xxx']
-        sys.path.remove(self.tmpdir)
+        del sys.modules["test_spiders_xxx"]
+        sys.path.remove(str(self.tmpdir))
 
     def test_interface(self):
         verifyObject(ISpiderLoader, self.spider_loader)
 
     def test_list(self):
         self.assertEqual(
-            set(self.spider_loader.list()),
-            {'spider1', 'spider2', 'spider3', 'spider4'})
+            set(self.spider_loader.list()), {"spider1", "spider2", "spider3", "spider4"}
+        )
 
     def test_load(self):
         spider1 = self.spider_loader.load("spider1")
-        self.assertEqual(spider1.__name__, 'Spider1')
+        self.assertEqual(spider1.__name__, "Spider1")
 
     def test_find_by_request(self):
         self.assertEqual(
-            self.spider_loader.find_by_request(Request('http://scrapy1.org/test')),
-            ['spider1'])
-        self.assertEqual(
-            self.spider_loader.find_by_request(Request('http://scrapy2.org/test')),
-            ['spider2'])
-        self.assertEqual(
-            set(self.spider_loader.find_by_request(Request('http://scrapy3.org/test'))),
-            {'spider1', 'spider2'})
-        self.assertEqual(
-            self.spider_loader.find_by_request(Request('http://scrapy999.org/test')),
-            [])
-        self.assertEqual(
-            self.spider_loader.find_by_request(Request('http://spider3.com')),
-            [])
-        self.assertEqual(
-            self.spider_loader.find_by_request(Request('http://spider3.com/onlythis')),
-            ['spider3'])
+            self.spider_loader.find_by_request(Request("http://scrapy1.org/test")),
+            ["spider1"],
+        )
+        self.assertEqual(
+            self.spider_loader.find_by_request(Request("http://scrapy2.org/test")),
+            ["spider2"],
+        )
+        self.assertEqual(
+            set(self.spider_loader.find_by_request(Request("http://scrapy3.org/test"))),
+            {"spider1", "spider2"},
+        )
+        self.assertEqual(
+            self.spider_loader.find_by_request(Request("http://scrapy999.org/test")), []
+        )
+        self.assertEqual(
+            self.spider_loader.find_by_request(Request("http://spider3.com")), []
+        )
+        self.assertEqual(
+            self.spider_loader.find_by_request(Request("http://spider3.com/onlythis")),
+            ["spider3"],
+        )
 
     def test_load_spider_module(self):
-        module = 'tests.test_spiderloader.test_spiders.spider1'
-        settings = Settings({'SPIDER_MODULES': [module]})
+        module = "tests.test_spiderloader.test_spiders.spider1"
+        settings = Settings({"SPIDER_MODULES": [module]})
         self.spider_loader = SpiderLoader.from_settings(settings)
         assert len(self.spider_loader._spiders) == 1
 
     def test_load_spider_module_multiple(self):
-        prefix = 'tests.test_spiderloader.test_spiders.'
-        module = ','.join(prefix + s for s in ('spider1', 'spider2'))
-        settings = Settings({'SPIDER_MODULES': module})
+        prefix = "tests.test_spiderloader.test_spiders."
+        module = ",".join(prefix + s for s in ("spider1", "spider2"))
+        settings = Settings({"SPIDER_MODULES": module})
         self.spider_loader = SpiderLoader.from_settings(settings)
         assert len(self.spider_loader._spiders) == 2
 
     def test_load_base_spider(self):
-        module = 'tests.test_spiderloader.test_spiders.spider0'
-        settings = Settings({'SPIDER_MODULES': [module]})
+        module = "tests.test_spiderloader.test_spiders.spider0"
+        settings = Settings({"SPIDER_MODULES": [module]})
         self.spider_loader = SpiderLoader.from_settings(settings)
         assert len(self.spider_loader._spiders) == 0
 
     def test_crawler_runner_loading(self):
-        module = 'tests.test_spiderloader.test_spiders.spider1'
-        runner = CrawlerRunner({
-            'SPIDER_MODULES': [module],
-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
-        })
+        module = "tests.test_spiderloader.test_spiders.spider1"
+        runner = CrawlerRunner(
+            {
+                "SPIDER_MODULES": [module],
+                "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
+            }
+        )
+
+        self.assertRaisesRegex(
+            KeyError, "Spider not found", runner.create_crawler, "spider2"
+        )
 
-        self.assertRaisesRegex(KeyError, 'Spider not found',
-                               runner.create_crawler, 'spider2')
-
-        crawler = runner.create_crawler('spider1')
+        crawler = runner.create_crawler("spider1")
         self.assertTrue(issubclass(crawler.spidercls, scrapy.Spider))
-        self.assertEqual(crawler.spidercls.name, 'spider1')
+        self.assertEqual(crawler.spidercls.name, "spider1")
 
     def test_bad_spider_modules_exception(self):
 
-        module = 'tests.test_spiderloader.test_spiders.doesnotexist'
-        settings = Settings({'SPIDER_MODULES': [module]})
+        module = "tests.test_spiderloader.test_spiders.doesnotexist"
+        settings = Settings({"SPIDER_MODULES": [module]})
         self.assertRaises(ImportError, SpiderLoader.from_settings, settings)
 
     def test_bad_spider_modules_warning(self):
 
         with warnings.catch_warnings(record=True) as w:
-            module = 'tests.test_spiderloader.test_spiders.doesnotexist'
-            settings = Settings({'SPIDER_MODULES': [module],
-                                 'SPIDER_LOADER_WARN_ONLY': True})
+            module = "tests.test_spiderloader.test_spiders.doesnotexist"
+            settings = Settings(
+                {"SPIDER_MODULES": [module], "SPIDER_LOADER_WARN_ONLY": True}
+            )
             spider_loader = SpiderLoader.from_settings(settings)
             if str(w[0].message).startswith("_SixMetaPathImporter"):
                 # needed on 3.10 because of https://github.com/benjaminp/six/issues/349,
                 # at least until all six versions we can import (including botocore.vendored.six)
                 # are updated to 1.16.0+
                 w.pop(0)
             self.assertIn("Could not load spiders from module", str(w[0].message))
 
             spiders = spider_loader.list()
             self.assertEqual(spiders, [])
 
 
 class DuplicateSpiderNameLoaderTest(unittest.TestCase):
-
     def setUp(self):
-        orig_spiders_dir = os.path.join(module_dir, 'test_spiders')
-        self.tmpdir = self.mktemp()
-        os.mkdir(self.tmpdir)
-        self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')
+        orig_spiders_dir = module_dir / "test_spiders"
+        self.tmpdir = Path(self.mktemp())
+        self.tmpdir.mkdir()
+        self.spiders_dir = self.tmpdir / "test_spiders_xxx"
         _copytree(orig_spiders_dir, self.spiders_dir)
-        sys.path.append(self.tmpdir)
-        self.settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})
+        sys.path.append(str(self.tmpdir))
+        self.settings = Settings({"SPIDER_MODULES": ["test_spiders_xxx"]})
 
     def tearDown(self):
-        del sys.modules['test_spiders_xxx']
-        sys.path.remove(self.tmpdir)
+        del sys.modules["test_spiders_xxx"]
+        sys.path.remove(str(self.tmpdir))
 
     def test_dupename_warning(self):
         # copy 1 spider module so as to have duplicate spider name
-        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider3.py'),
-                        os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider3dupe.py'))
+        shutil.copyfile(
+            self.tmpdir / "test_spiders_xxx" / "spider3.py",
+            self.tmpdir / "test_spiders_xxx" / "spider3dupe.py",
+        )
 
         with warnings.catch_warnings(record=True) as w:
             spider_loader = SpiderLoader.from_settings(self.settings)
 
             self.assertEqual(len(w), 1)
             msg = str(w[0].message)
             self.assertIn("several spiders with the same name", msg)
@@ -162,23 +169,27 @@
             self.assertTrue(msg.count("'spider3'") == 2)
 
             self.assertNotIn("'spider1'", msg)
             self.assertNotIn("'spider2'", msg)
             self.assertNotIn("'spider4'", msg)
 
             spiders = set(spider_loader.list())
-            self.assertEqual(spiders, {'spider1', 'spider2', 'spider3', 'spider4'})
+            self.assertEqual(spiders, {"spider1", "spider2", "spider3", "spider4"})
 
     def test_multiple_dupename_warning(self):
         # copy 2 spider modules so as to have duplicate spider name
         # This should issue 2 warning, 1 for each duplicate spider name
-        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider1.py'),
-                        os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider1dupe.py'))
-        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider2.py'),
-                        os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider2dupe.py'))
+        shutil.copyfile(
+            self.tmpdir / "test_spiders_xxx" / "spider1.py",
+            self.tmpdir / "test_spiders_xxx" / "spider1dupe.py",
+        )
+        shutil.copyfile(
+            self.tmpdir / "test_spiders_xxx" / "spider2.py",
+            self.tmpdir / "test_spiders_xxx" / "spider2dupe.py",
+        )
 
         with warnings.catch_warnings(record=True) as w:
             spider_loader = SpiderLoader.from_settings(self.settings)
 
             self.assertEqual(len(w), 1)
             msg = str(w[0].message)
             self.assertIn("several spiders with the same name", msg)
@@ -188,8 +199,8 @@
             self.assertIn("'spider2'", msg)
             self.assertTrue(msg.count("'spider2'") == 2)
 
             self.assertNotIn("'spider3'", msg)
             self.assertNotIn("'spider4'", msg)
 
             spiders = set(spider_loader.list())
-            self.assertEqual(spiders, {'spider1', 'spider2', 'spider3', 'spider4'})
+            self.assertEqual(spiders, {"spider1", "spider2", "spider3", "spider4"})
```

### Comparing `Scrapy-2.7.1/tests/test_spidermiddleware.py` & `Scrapy-2.8.0/tests/test_spidermiddleware.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,83 +1,81 @@
 import collections.abc
 from typing import Optional
 from unittest import mock
 
 from testfixtures import LogCapture
 from twisted.internet import defer
-from twisted.trial.unittest import TestCase
 from twisted.python.failure import Failure
+from twisted.trial.unittest import TestCase
 
-from scrapy.spiders import Spider
-from scrapy.http import Request, Response
+from scrapy.core.spidermw import SpiderMiddlewareManager
 from scrapy.exceptions import _InvalidOutput
+from scrapy.http import Request, Response
+from scrapy.spiders import Spider
 from scrapy.utils.asyncgen import collect_asyncgen
 from scrapy.utils.defer import deferred_from_coro, maybe_deferred_to_future
 from scrapy.utils.test import get_crawler
-from scrapy.core.spidermw import SpiderMiddlewareManager
 
 
 class SpiderMiddlewareTestCase(TestCase):
-
     def setUp(self):
-        self.request = Request('http://example.com/index.html')
+        self.request = Request("http://example.com/index.html")
         self.response = Response(self.request.url, request=self.request)
-        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}})
-        self.spider = self.crawler._create_spider('foo')
+        self.crawler = get_crawler(Spider, {"SPIDER_MIDDLEWARES_BASE": {}})
+        self.spider = self.crawler._create_spider("foo")
         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)
 
     def _scrape_response(self):
         """Execute spider mw manager's scrape_response method and return the result.
         Raise exception in case of failure.
         """
         scrape_func = mock.MagicMock()
-        dfd = self.mwman.scrape_response(scrape_func, self.response, self.request, self.spider)
+        dfd = self.mwman.scrape_response(
+            scrape_func, self.response, self.request, self.spider
+        )
         # catch deferred result and return the value
         results = []
         dfd.addBoth(results.append)
         self._wait(dfd)
         ret = results[0]
         return ret
 
 
 class ProcessSpiderInputInvalidOutput(SpiderMiddlewareTestCase):
     """Invalid return value for process_spider_input method"""
 
     def test_invalid_process_spider_input(self):
-
         class InvalidProcessSpiderInputMiddleware:
             def process_spider_input(self, response, spider):
                 return 1
 
         self.mwman._add_middleware(InvalidProcessSpiderInputMiddleware())
         result = self._scrape_response()
         self.assertIsInstance(result, Failure)
         self.assertIsInstance(result.value, _InvalidOutput)
 
 
 class ProcessSpiderOutputInvalidOutput(SpiderMiddlewareTestCase):
     """Invalid return value for process_spider_output method"""
 
     def test_invalid_process_spider_output(self):
-
         class InvalidProcessSpiderOutputMiddleware:
             def process_spider_output(self, response, result, spider):
                 return 1
 
         self.mwman._add_middleware(InvalidProcessSpiderOutputMiddleware())
         result = self._scrape_response()
         self.assertIsInstance(result, Failure)
         self.assertIsInstance(result.value, _InvalidOutput)
 
 
 class ProcessSpiderExceptionInvalidOutput(SpiderMiddlewareTestCase):
     """Invalid return value for process_spider_exception method"""
 
     def test_invalid_process_spider_exception(self):
-
         class InvalidProcessSpiderOutputExceptionMiddleware:
             def process_spider_exception(self, response, exception, spider):
                 return 1
 
         class RaiseExceptionProcessSpiderOutputMiddleware:
             def process_spider_output(self, response, result, spider):
                 raise Exception()
@@ -89,15 +87,14 @@
         self.assertIsInstance(result.value, _InvalidOutput)
 
 
 class ProcessSpiderExceptionReRaise(SpiderMiddlewareTestCase):
     """Re raise the exception by returning None"""
 
     def test_process_spider_exception_return_none(self):
-
         class ProcessSpiderExceptionReturnNoneMiddleware:
             def process_spider_exception(self, response, exception, spider):
                 return None
 
         class RaiseExceptionProcessSpiderOutputMiddleware:
             def process_spider_output(self, response, result, spider):
                 1 / 0
@@ -106,55 +103,67 @@
         self.mwman._add_middleware(RaiseExceptionProcessSpiderOutputMiddleware())
         result = self._scrape_response()
         self.assertIsInstance(result, Failure)
         self.assertIsInstance(result.value, ZeroDivisionError)
 
 
 class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):
-    """ Helpers for testing sync, async and mixed middlewares.
+    """Helpers for testing sync, async and mixed middlewares.
 
     Should work for process_spider_output and, when it's supported, process_start_requests.
     """
 
     RESULT_COUNT = 3  # to simplify checks, let everything return 3 objects
 
     @staticmethod
     def _construct_mw_setting(*mw_classes, start_index: Optional[int] = None):
         if start_index is None:
             start_index = 10
         return {i: c for c, i in enumerate(mw_classes, start=start_index)}
 
     def _scrape_func(self, *args, **kwargs):
-        yield {'foo': 1}
-        yield {'foo': 2}
-        yield {'foo': 3}
+        yield {"foo": 1}
+        yield {"foo": 2}
+        yield {"foo": 3}
 
     @defer.inlineCallbacks
     def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):
         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)
-        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}, 'SPIDER_MIDDLEWARES': setting})
-        self.spider = self.crawler._create_spider('foo')
+        self.crawler = get_crawler(
+            Spider, {"SPIDER_MIDDLEWARES_BASE": {}, "SPIDER_MIDDLEWARES": setting}
+        )
+        self.spider = self.crawler._create_spider("foo")
         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)
-        result = yield self.mwman.scrape_response(self._scrape_func, self.response, self.request, self.spider)
+        result = yield self.mwman.scrape_response(
+            self._scrape_func, self.response, self.request, self.spider
+        )
         return result
 
     @defer.inlineCallbacks
-    def _test_simple_base(self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None):
+    def _test_simple_base(
+        self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None
+    ):
         with LogCapture() as log:
-            result = yield self._get_middleware_result(*mw_classes, start_index=start_index)
+            result = yield self._get_middleware_result(
+                *mw_classes, start_index=start_index
+            )
         self.assertIsInstance(result, collections.abc.Iterable)
         result_list = list(result)
         self.assertEqual(len(result_list), self.RESULT_COUNT)
         self.assertIsInstance(result_list[0], self.ITEM_TYPE)
         self.assertEqual("downgraded to a non-async" in str(log), downgrade)
 
     @defer.inlineCallbacks
-    def _test_asyncgen_base(self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None):
+    def _test_asyncgen_base(
+        self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None
+    ):
         with LogCapture() as log:
-            result = yield self._get_middleware_result(*mw_classes, start_index=start_index)
+            result = yield self._get_middleware_result(
+                *mw_classes, start_index=start_index
+            )
         self.assertIsInstance(result, collections.abc.AsyncIterator)
         result_list = yield deferred_from_coro(collect_asyncgen(result))
         self.assertEqual(len(result_list), self.RESULT_COUNT)
         self.assertIsInstance(result_list[0], self.ITEM_TYPE)
         self.assertEqual("downgraded to a non-async" in str(log), downgrade)
 
 
@@ -178,115 +187,104 @@
     async def process_spider_output_async(self, response, result, spider):
         async for r in result:
             yield r
 
 
 class ProcessSpiderExceptionSimpleIterableMiddleware:
     def process_spider_exception(self, response, exception, spider):
-        yield {'foo': 1}
-        yield {'foo': 2}
-        yield {'foo': 3}
+        yield {"foo": 1}
+        yield {"foo": 2}
+        yield {"foo": 3}
 
 
 class ProcessSpiderExceptionAsyncIterableMiddleware:
     async def process_spider_exception(self, response, exception, spider):
-        yield {'foo': 1}
+        yield {"foo": 1}
         d = defer.Deferred()
         from twisted.internet import reactor
+
         reactor.callLater(0, d.callback, None)
         await maybe_deferred_to_future(d)
-        yield {'foo': 2}
-        yield {'foo': 3}
+        yield {"foo": 2}
+        yield {"foo": 3}
 
 
 class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):
-    """ process_spider_output tests for simple callbacks"""
+    """process_spider_output tests for simple callbacks"""
 
     ITEM_TYPE = dict
     MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware
     MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware
     MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware
 
     def test_simple(self):
-        """ Simple mw """
+        """Simple mw"""
         return self._test_simple_base(self.MW_SIMPLE)
 
     def test_asyncgen(self):
-        """ Asyncgen mw; upgrade """
+        """Asyncgen mw; upgrade"""
         return self._test_asyncgen_base(self.MW_ASYNCGEN)
 
     def test_simple_asyncgen(self):
-        """ Simple mw -> asyncgen mw; upgrade """
-        return self._test_asyncgen_base(self.MW_ASYNCGEN,
-                                        self.MW_SIMPLE)
+        """Simple mw -> asyncgen mw; upgrade"""
+        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_SIMPLE)
 
     def test_asyncgen_simple(self):
-        """ Asyncgen mw -> simple mw; upgrade then downgrade """
-        return self._test_simple_base(self.MW_SIMPLE,
-                                      self.MW_ASYNCGEN,
-                                      downgrade=True)
+        """Asyncgen mw -> simple mw; upgrade then downgrade"""
+        return self._test_simple_base(self.MW_SIMPLE, self.MW_ASYNCGEN, downgrade=True)
 
     def test_universal(self):
-        """ Universal mw """
+        """Universal mw"""
         return self._test_simple_base(self.MW_UNIVERSAL)
 
     def test_universal_simple(self):
-        """ Universal mw -> simple mw """
-        return self._test_simple_base(self.MW_SIMPLE,
-                                      self.MW_UNIVERSAL)
+        """Universal mw -> simple mw"""
+        return self._test_simple_base(self.MW_SIMPLE, self.MW_UNIVERSAL)
 
     def test_simple_universal(self):
-        """ Simple mw -> universal mw """
-        return self._test_simple_base(self.MW_UNIVERSAL,
-                                      self.MW_SIMPLE)
+        """Simple mw -> universal mw"""
+        return self._test_simple_base(self.MW_UNIVERSAL, self.MW_SIMPLE)
 
     def test_universal_asyncgen(self):
-        """ Universal mw -> asyncgen mw; upgrade """
-        return self._test_asyncgen_base(self.MW_ASYNCGEN,
-                                        self.MW_UNIVERSAL)
+        """Universal mw -> asyncgen mw; upgrade"""
+        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_UNIVERSAL)
 
     def test_asyncgen_universal(self):
-        """ Asyncgen mw -> universal mw; upgrade """
-        return self._test_asyncgen_base(self.MW_UNIVERSAL,
-                                        self.MW_ASYNCGEN)
+        """Asyncgen mw -> universal mw; upgrade"""
+        return self._test_asyncgen_base(self.MW_UNIVERSAL, self.MW_ASYNCGEN)
 
 
 class ProcessSpiderOutputAsyncGen(ProcessSpiderOutputSimple):
-    """ process_spider_output tests for async generator callbacks """
+    """process_spider_output tests for async generator callbacks"""
 
     async def _scrape_func(self, *args, **kwargs):
         for item in super()._scrape_func():
             yield item
 
     def test_simple(self):
-        """ Simple mw; downgrade """
-        return self._test_simple_base(self.MW_SIMPLE,
-                                      downgrade=True)
+        """Simple mw; downgrade"""
+        return self._test_simple_base(self.MW_SIMPLE, downgrade=True)
 
     def test_simple_asyncgen(self):
-        """ Simple mw -> asyncgen mw; downgrade then upgrade """
-        return self._test_asyncgen_base(self.MW_ASYNCGEN,
-                                        self.MW_SIMPLE,
-                                        downgrade=True)
+        """Simple mw -> asyncgen mw; downgrade then upgrade"""
+        return self._test_asyncgen_base(
+            self.MW_ASYNCGEN, self.MW_SIMPLE, downgrade=True
+        )
 
     def test_universal(self):
-        """ Universal mw """
+        """Universal mw"""
         return self._test_asyncgen_base(self.MW_UNIVERSAL)
 
     def test_universal_simple(self):
-        """ Universal mw -> simple mw; downgrade """
-        return self._test_simple_base(self.MW_SIMPLE,
-                                      self.MW_UNIVERSAL,
-                                      downgrade=True)
+        """Universal mw -> simple mw; downgrade"""
+        return self._test_simple_base(self.MW_SIMPLE, self.MW_UNIVERSAL, downgrade=True)
 
     def test_simple_universal(self):
-        """ Simple mw -> universal mw; downgrade """
-        return self._test_simple_base(self.MW_UNIVERSAL,
-                                      self.MW_SIMPLE,
-                                      downgrade=True)
+        """Simple mw -> universal mw; downgrade"""
+        return self._test_simple_base(self.MW_UNIVERSAL, self.MW_SIMPLE, downgrade=True)
 
 
 class ProcessSpiderOutputNonIterableMiddleware:
     def process_spider_output(self, response, result, spider):
         return
 
 
@@ -295,15 +293,14 @@
         results = []
         for r in result:
             results.append(r)
         return results
 
 
 class ProcessSpiderOutputInvalidResult(BaseAsyncSpiderMiddlewareTestCase):
-
     @defer.inlineCallbacks
     def test_non_iterable(self):
         with self.assertRaisesRegex(
             _InvalidOutput,
             (
                 r"\.process_spider_output must return an iterable, got <class "
                 r"'NoneType'>"
@@ -327,35 +324,37 @@
 class ProcessStartRequestsSimpleMiddleware:
     def process_start_requests(self, start_requests, spider):
         for r in start_requests:
             yield r
 
 
 class ProcessStartRequestsSimple(BaseAsyncSpiderMiddlewareTestCase):
-    """ process_start_requests tests for simple start_requests"""
+    """process_start_requests tests for simple start_requests"""
 
     ITEM_TYPE = Request
     MW_SIMPLE = ProcessStartRequestsSimpleMiddleware
 
     def _start_requests(self):
         for i in range(3):
-            yield Request(f'https://example.com/{i}', dont_filter=True)
+            yield Request(f"https://example.com/{i}", dont_filter=True)
 
     @defer.inlineCallbacks
     def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):
         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)
-        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}, 'SPIDER_MIDDLEWARES': setting})
-        self.spider = self.crawler._create_spider('foo')
+        self.crawler = get_crawler(
+            Spider, {"SPIDER_MIDDLEWARES_BASE": {}, "SPIDER_MIDDLEWARES": setting}
+        )
+        self.spider = self.crawler._create_spider("foo")
         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)
         start_requests = iter(self._start_requests())
         results = yield self.mwman.process_start_requests(start_requests, self.spider)
         return results
 
     def test_simple(self):
-        """ Simple mw """
+        """Simple mw"""
         return self._test_simple_base(self.MW_SIMPLE)
 
 
 class UniversalMiddlewareNoSync:
     async def process_spider_output_async(self, response, result, spider):
         yield
 
@@ -379,84 +378,102 @@
 class UniversalMiddlewareManagerTest(TestCase):
     def setUp(self):
         self.mwman = SpiderMiddlewareManager()
 
     def test_simple_mw(self):
         mw = ProcessSpiderOutputSimpleMiddleware
         self.mwman._add_middleware(mw)
-        self.assertEqual(self.mwman.methods['process_spider_output'][0], mw.process_spider_output)
+        self.assertEqual(
+            self.mwman.methods["process_spider_output"][0], mw.process_spider_output
+        )
 
     def test_async_mw(self):
         mw = ProcessSpiderOutputAsyncGenMiddleware
         self.mwman._add_middleware(mw)
-        self.assertEqual(self.mwman.methods['process_spider_output'][0], mw.process_spider_output)
+        self.assertEqual(
+            self.mwman.methods["process_spider_output"][0], mw.process_spider_output
+        )
 
     def test_universal_mw(self):
         mw = ProcessSpiderOutputUniversalMiddleware
         self.mwman._add_middleware(mw)
-        self.assertEqual(self.mwman.methods['process_spider_output'][0],
-                         (mw.process_spider_output, mw.process_spider_output_async))
+        self.assertEqual(
+            self.mwman.methods["process_spider_output"][0],
+            (mw.process_spider_output, mw.process_spider_output_async),
+        )
 
     def test_universal_mw_no_sync(self):
         with LogCapture() as log:
             self.mwman._add_middleware(UniversalMiddlewareNoSync)
-        self.assertIn("UniversalMiddlewareNoSync has process_spider_output_async"
-                      " without process_spider_output", str(log))
-        self.assertEqual(self.mwman.methods['process_spider_output'][0], None)
+        self.assertIn(
+            "UniversalMiddlewareNoSync has process_spider_output_async"
+            " without process_spider_output",
+            str(log),
+        )
+        self.assertEqual(self.mwman.methods["process_spider_output"][0], None)
 
     def test_universal_mw_both_sync(self):
         mw = UniversalMiddlewareBothSync
         with LogCapture() as log:
             self.mwman._add_middleware(mw)
-        self.assertIn("UniversalMiddlewareBothSync.process_spider_output_async "
-                      "is not an async generator function", str(log))
-        self.assertEqual(self.mwman.methods['process_spider_output'][0], mw.process_spider_output)
+        self.assertIn(
+            "UniversalMiddlewareBothSync.process_spider_output_async "
+            "is not an async generator function",
+            str(log),
+        )
+        self.assertEqual(
+            self.mwman.methods["process_spider_output"][0], mw.process_spider_output
+        )
 
     def test_universal_mw_both_async(self):
         with LogCapture() as log:
             self.mwman._add_middleware(UniversalMiddlewareBothAsync)
-        self.assertIn("UniversalMiddlewareBothAsync.process_spider_output "
-                      "is an async generator function while process_spider_output_async exists",
-                      str(log))
-        self.assertEqual(self.mwman.methods['process_spider_output'][0], None)
+        self.assertIn(
+            "UniversalMiddlewareBothAsync.process_spider_output "
+            "is an async generator function while process_spider_output_async exists",
+            str(log),
+        )
+        self.assertEqual(self.mwman.methods["process_spider_output"][0], None)
 
 
 class BuiltinMiddlewareSimpleTest(BaseAsyncSpiderMiddlewareTestCase):
     ITEM_TYPE = dict
     MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware
     MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware
     MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware
 
     @defer.inlineCallbacks
     def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):
         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)
-        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES': setting})
-        self.spider = self.crawler._create_spider('foo')
+        self.crawler = get_crawler(Spider, {"SPIDER_MIDDLEWARES": setting})
+        self.spider = self.crawler._create_spider("foo")
         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)
-        result = yield self.mwman.scrape_response(self._scrape_func, self.response, self.request, self.spider)
+        result = yield self.mwman.scrape_response(
+            self._scrape_func, self.response, self.request, self.spider
+        )
         return result
 
     def test_just_builtin(self):
         return self._test_simple_base()
 
     def test_builtin_simple(self):
         return self._test_simple_base(self.MW_SIMPLE, start_index=1000)
 
     def test_builtin_async(self):
-        """ Upgrade """
+        """Upgrade"""
         return self._test_asyncgen_base(self.MW_ASYNCGEN, start_index=1000)
 
     def test_builtin_universal(self):
         return self._test_simple_base(self.MW_UNIVERSAL, start_index=1000)
 
     def test_simple_builtin(self):
         return self._test_simple_base(self.MW_SIMPLE)
 
     def test_async_builtin(self):
-        """ Upgrade """
+        """Upgrade"""
         return self._test_asyncgen_base(self.MW_ASYNCGEN)
 
     def test_universal_builtin(self):
         return self._test_simple_base(self.MW_UNIVERSAL)
 
 
 class BuiltinMiddlewareAsyncGenTest(BuiltinMiddlewareSimpleTest):
@@ -464,25 +481,25 @@
         for item in super()._scrape_func():
             yield item
 
     def test_just_builtin(self):
         return self._test_asyncgen_base()
 
     def test_builtin_simple(self):
-        """ Downgrade """
+        """Downgrade"""
         return self._test_simple_base(self.MW_SIMPLE, downgrade=True, start_index=1000)
 
     def test_builtin_async(self):
         return self._test_asyncgen_base(self.MW_ASYNCGEN, start_index=1000)
 
     def test_builtin_universal(self):
         return self._test_asyncgen_base(self.MW_UNIVERSAL, start_index=1000)
 
     def test_simple_builtin(self):
-        """ Downgrade """
+        """Downgrade"""
         return self._test_simple_base(self.MW_SIMPLE, downgrade=True)
 
     def test_async_builtin(self):
         return self._test_asyncgen_base(self.MW_ASYNCGEN)
 
     def test_universal_builtin(self):
         return self._test_asyncgen_base(self.MW_UNIVERSAL)
@@ -497,37 +514,35 @@
     MW_EXC_ASYNCGEN = ProcessSpiderExceptionAsyncIterableMiddleware
 
     def _scrape_func(self, *args, **kwargs):
         1 / 0
 
     @defer.inlineCallbacks
     def _test_asyncgen_nodowngrade(self, *mw_classes):
-        with self.assertRaisesRegex(_InvalidOutput, "Async iterable returned from .+ cannot be downgraded"):
+        with self.assertRaisesRegex(
+            _InvalidOutput, "Async iterable returned from .+ cannot be downgraded"
+        ):
             yield self._get_middleware_result(*mw_classes)
 
     def test_exc_simple(self):
-        """ Simple exc mw """
+        """Simple exc mw"""
         return self._test_simple_base(self.MW_EXC_SIMPLE)
 
     def test_exc_async(self):
-        """ Async exc mw """
+        """Async exc mw"""
         return self._test_asyncgen_base(self.MW_EXC_ASYNCGEN)
 
     def test_exc_simple_simple(self):
-        """ Simple exc mw -> simple output mw """
-        return self._test_simple_base(self.MW_SIMPLE,
-                                      self.MW_EXC_SIMPLE)
+        """Simple exc mw -> simple output mw"""
+        return self._test_simple_base(self.MW_SIMPLE, self.MW_EXC_SIMPLE)
 
     def test_exc_async_async(self):
-        """ Async exc mw -> async output mw """
-        return self._test_asyncgen_base(self.MW_ASYNCGEN,
-                                        self.MW_EXC_ASYNCGEN)
+        """Async exc mw -> async output mw"""
+        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_EXC_ASYNCGEN)
 
     def test_exc_simple_async(self):
-        """ Simple exc mw -> async output mw; upgrade """
-        return self._test_asyncgen_base(self.MW_ASYNCGEN,
-                                        self.MW_EXC_SIMPLE)
+        """Simple exc mw -> async output mw; upgrade"""
+        return self._test_asyncgen_base(self.MW_ASYNCGEN, self.MW_EXC_SIMPLE)
 
     def test_exc_async_simple(self):
-        """ Async exc mw -> simple output mw; cannot work as downgrading is not supported """
-        return self._test_asyncgen_nodowngrade(self.MW_SIMPLE,
-                                               self.MW_EXC_ASYNCGEN)
+        """Async exc mw -> simple output mw; cannot work as downgrading is not supported"""
+        return self._test_asyncgen_nodowngrade(self.MW_SIMPLE, self.MW_EXC_ASYNCGEN)
```

### Comparing `Scrapy-2.7.1/tests/test_spidermiddleware_depth.py` & `Scrapy-2.8.0/tests/test_spidermiddleware_depth.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 from unittest import TestCase
 
+from scrapy.http import Request, Response
 from scrapy.spidermiddlewares.depth import DepthMiddleware
-from scrapy.http import Response, Request
 from scrapy.spiders import Spider
 from scrapy.statscollectors import StatsCollector
 from scrapy.utils.test import get_crawler
 
 
 class TestDepthMiddleware(TestCase):
-
     def setUp(self):
         crawler = get_crawler(Spider)
-        self.spider = crawler._create_spider('scrapytest.org')
+        self.spider = crawler._create_spider("scrapytest.org")
 
         self.stats = StatsCollector(crawler)
         self.stats.open_spider(self.spider)
 
         self.mw = DepthMiddleware(1, self.stats, True)
 
     def test_process_spider_output(self):
-        req = Request('http://scrapytest.org')
-        resp = Response('http://scrapytest.org')
+        req = Request("http://scrapytest.org")
+        resp = Response("http://scrapytest.org")
         resp.request = req
-        result = [Request('http://scrapytest.org')]
+        result = [Request("http://scrapytest.org")]
 
         out = list(self.mw.process_spider_output(resp, result, self.spider))
         self.assertEqual(out, result)
 
-        rdc = self.stats.get_value('request_depth_count/1', spider=self.spider)
+        rdc = self.stats.get_value("request_depth_count/1", spider=self.spider)
         self.assertEqual(rdc, 1)
 
-        req.meta['depth'] = 1
+        req.meta["depth"] = 1
 
         out2 = list(self.mw.process_spider_output(resp, result, self.spider))
         self.assertEqual(out2, [])
 
-        rdm = self.stats.get_value('request_depth_max', spider=self.spider)
+        rdm = self.stats.get_value("request_depth_max", spider=self.spider)
         self.assertEqual(rdm, 1)
 
     def tearDown(self):
-        self.stats.close_spider(self.spider, '')
+        self.stats.close_spider(self.spider, "")
```

### Comparing `Scrapy-2.7.1/tests/test_spidermiddleware_httperror.py` & `Scrapy-2.8.0/tests/test_spidermiddleware_httperror.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 import logging
 from unittest import TestCase
 
 from testfixtures import LogCapture
-from twisted.trial.unittest import TestCase as TrialTestCase
 from twisted.internet import defer
+from twisted.trial.unittest import TestCase as TrialTestCase
 
+from scrapy.http import Request, Response
+from scrapy.settings import Settings
+from scrapy.spidermiddlewares.httperror import HttpError, HttpErrorMiddleware
+from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
 from tests.mockserver import MockServer
-from scrapy.http import Response, Request
-from scrapy.spiders import Spider
-from scrapy.spidermiddlewares.httperror import HttpErrorMiddleware, HttpError
-from scrapy.settings import Settings
 from tests.spiders import MockServerSpider
 
 
 class _HttpErrorSpider(MockServerSpider):
-    name = 'httperror'
+    name = "httperror"
     bypass_status_codes = set()
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.start_urls = [
             self.mockserver.url("/status?n=200"),
             self.mockserver.url("/status?n=404"),
@@ -55,99 +55,117 @@
         response = Response(request.url, status=code)
         response.request = request
         responses.append(response)
     return responses
 
 
 class TestHttpErrorMiddleware(TestCase):
-
     def setUp(self):
         crawler = get_crawler(Spider)
-        self.spider = Spider.from_crawler(crawler, name='foo')
+        self.spider = Spider.from_crawler(crawler, name="foo")
         self.mw = HttpErrorMiddleware(Settings({}))
-        self.req = Request('http://scrapytest.org')
+        self.req = Request("http://scrapytest.org")
         self.res200, self.res404 = _responses(self.req, [200, 404])
 
     def test_process_spider_input(self):
         self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))
-        self.assertRaises(HttpError, self.mw.process_spider_input, self.res404, self.spider)
+        self.assertRaises(
+            HttpError, self.mw.process_spider_input, self.res404, self.spider
+        )
 
     def test_process_spider_exception(self):
         self.assertEqual(
             [],
-            self.mw.process_spider_exception(self.res404, HttpError(self.res404), self.spider))
-        self.assertIsNone(self.mw.process_spider_exception(self.res404, Exception(), self.spider))
+            self.mw.process_spider_exception(
+                self.res404, HttpError(self.res404), self.spider
+            ),
+        )
+        self.assertIsNone(
+            self.mw.process_spider_exception(self.res404, Exception(), self.spider)
+        )
 
     def test_handle_httpstatus_list(self):
         res = self.res404.copy()
-        res.request = Request('http://scrapytest.org',
-                              meta={'handle_httpstatus_list': [404]})
+        res.request = Request(
+            "http://scrapytest.org", meta={"handle_httpstatus_list": [404]}
+        )
         self.assertIsNone(self.mw.process_spider_input(res, self.spider))
 
         self.spider.handle_httpstatus_list = [404]
         self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))
 
 
 class TestHttpErrorMiddlewareSettings(TestCase):
     """Similar test, but with settings"""
 
     def setUp(self):
-        self.spider = Spider('foo')
-        self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOWED_CODES': (402,)}))
-        self.req = Request('http://scrapytest.org')
+        self.spider = Spider("foo")
+        self.mw = HttpErrorMiddleware(Settings({"HTTPERROR_ALLOWED_CODES": (402,)}))
+        self.req = Request("http://scrapytest.org")
         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])
 
     def test_process_spider_input(self):
         self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))
-        self.assertRaises(HttpError, self.mw.process_spider_input, self.res404, self.spider)
+        self.assertRaises(
+            HttpError, self.mw.process_spider_input, self.res404, self.spider
+        )
         self.assertIsNone(self.mw.process_spider_input(self.res402, self.spider))
 
     def test_meta_overrides_settings(self):
-        request = Request('http://scrapytest.org', meta={'handle_httpstatus_list': [404]})
+        request = Request(
+            "http://scrapytest.org", meta={"handle_httpstatus_list": [404]}
+        )
         res404 = self.res404.copy()
         res404.request = request
         res402 = self.res402.copy()
         res402.request = request
 
         self.assertIsNone(self.mw.process_spider_input(res404, self.spider))
         self.assertRaises(HttpError, self.mw.process_spider_input, res402, self.spider)
 
     def test_spider_override_settings(self):
         self.spider.handle_httpstatus_list = [404]
         self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))
-        self.assertRaises(HttpError, self.mw.process_spider_input, self.res402, self.spider)
+        self.assertRaises(
+            HttpError, self.mw.process_spider_input, self.res402, self.spider
+        )
 
 
 class TestHttpErrorMiddlewareHandleAll(TestCase):
-
     def setUp(self):
-        self.spider = Spider('foo')
-        self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOW_ALL': True}))
-        self.req = Request('http://scrapytest.org')
+        self.spider = Spider("foo")
+        self.mw = HttpErrorMiddleware(Settings({"HTTPERROR_ALLOW_ALL": True}))
+        self.req = Request("http://scrapytest.org")
         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])
 
     def test_process_spider_input(self):
         self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))
         self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))
 
     def test_meta_overrides_settings(self):
-        request = Request('http://scrapytest.org', meta={'handle_httpstatus_list': [404]})
+        request = Request(
+            "http://scrapytest.org", meta={"handle_httpstatus_list": [404]}
+        )
         res404 = self.res404.copy()
         res404.request = request
         res402 = self.res402.copy()
         res402.request = request
 
         self.assertIsNone(self.mw.process_spider_input(res404, self.spider))
         self.assertRaises(HttpError, self.mw.process_spider_input, res402, self.spider)
 
     def test_httperror_allow_all_false(self):
         crawler = get_crawler(_HttpErrorSpider)
         mw = HttpErrorMiddleware.from_crawler(crawler)
-        request_httpstatus_false = Request('http://scrapytest.org', meta={'handle_httpstatus_all': False})
-        request_httpstatus_true = Request('http://scrapytest.org', meta={'handle_httpstatus_all': True})
+        request_httpstatus_false = Request(
+            "http://scrapytest.org", meta={"handle_httpstatus_all": False}
+        )
+        request_httpstatus_true = Request(
+            "http://scrapytest.org", meta={"handle_httpstatus_all": True}
+        )
         res404 = self.res404.copy()
         res404.request = request_httpstatus_false
         res402 = self.res402.copy()
         res402.request = request_httpstatus_true
 
         self.assertRaises(HttpError, mw.process_spider_input, res404, self.spider)
         self.assertIsNone(mw.process_spider_input(res402, self.spider))
@@ -162,55 +180,55 @@
         self.mockserver.__exit__(None, None, None)
 
     @defer.inlineCallbacks
     def test_middleware_works(self):
         crawler = get_crawler(_HttpErrorSpider)
         yield crawler.crawl(mockserver=self.mockserver)
         assert not crawler.spider.skipped, crawler.spider.skipped
-        self.assertEqual(crawler.spider.parsed, {'200'})
-        self.assertEqual(crawler.spider.failed, {'404', '402', '500'})
+        self.assertEqual(crawler.spider.parsed, {"200"})
+        self.assertEqual(crawler.spider.failed, {"404", "402", "500"})
 
         get_value = crawler.stats.get_value
-        self.assertEqual(get_value('httperror/response_ignored_count'), 3)
-        self.assertEqual(get_value('httperror/response_ignored_status_count/404'), 1)
-        self.assertEqual(get_value('httperror/response_ignored_status_count/402'), 1)
-        self.assertEqual(get_value('httperror/response_ignored_status_count/500'), 1)
+        self.assertEqual(get_value("httperror/response_ignored_count"), 3)
+        self.assertEqual(get_value("httperror/response_ignored_status_count/404"), 1)
+        self.assertEqual(get_value("httperror/response_ignored_status_count/402"), 1)
+        self.assertEqual(get_value("httperror/response_ignored_status_count/500"), 1)
 
     @defer.inlineCallbacks
     def test_logging(self):
         crawler = get_crawler(_HttpErrorSpider)
         with LogCapture() as log:
             yield crawler.crawl(mockserver=self.mockserver, bypass_status_codes={402})
-        self.assertEqual(crawler.spider.parsed, {'200', '402'})
-        self.assertEqual(crawler.spider.skipped, {'402'})
-        self.assertEqual(crawler.spider.failed, {'404', '500'})
-
-        self.assertIn('Ignoring response <404', str(log))
-        self.assertIn('Ignoring response <500', str(log))
-        self.assertNotIn('Ignoring response <200', str(log))
-        self.assertNotIn('Ignoring response <402', str(log))
+        self.assertEqual(crawler.spider.parsed, {"200", "402"})
+        self.assertEqual(crawler.spider.skipped, {"402"})
+        self.assertEqual(crawler.spider.failed, {"404", "500"})
+
+        self.assertIn("Ignoring response <404", str(log))
+        self.assertIn("Ignoring response <500", str(log))
+        self.assertNotIn("Ignoring response <200", str(log))
+        self.assertNotIn("Ignoring response <402", str(log))
 
     @defer.inlineCallbacks
     def test_logging_level(self):
         # HttpError logs ignored responses with level INFO
         crawler = get_crawler(_HttpErrorSpider)
         with LogCapture(level=logging.INFO) as log:
             yield crawler.crawl(mockserver=self.mockserver)
-        self.assertEqual(crawler.spider.parsed, {'200'})
-        self.assertEqual(crawler.spider.failed, {'404', '402', '500'})
+        self.assertEqual(crawler.spider.parsed, {"200"})
+        self.assertEqual(crawler.spider.failed, {"404", "402", "500"})
 
-        self.assertIn('Ignoring response <402', str(log))
-        self.assertIn('Ignoring response <404', str(log))
-        self.assertIn('Ignoring response <500', str(log))
-        self.assertNotIn('Ignoring response <200', str(log))
+        self.assertIn("Ignoring response <402", str(log))
+        self.assertIn("Ignoring response <404", str(log))
+        self.assertIn("Ignoring response <500", str(log))
+        self.assertNotIn("Ignoring response <200", str(log))
 
         # with level WARNING, we shouldn't capture anything from HttpError
         crawler = get_crawler(_HttpErrorSpider)
         with LogCapture(level=logging.WARNING) as log:
             yield crawler.crawl(mockserver=self.mockserver)
-        self.assertEqual(crawler.spider.parsed, {'200'})
-        self.assertEqual(crawler.spider.failed, {'404', '402', '500'})
+        self.assertEqual(crawler.spider.parsed, {"200"})
+        self.assertEqual(crawler.spider.failed, {"404", "402", "500"})
 
-        self.assertNotIn('Ignoring response <402', str(log))
-        self.assertNotIn('Ignoring response <404', str(log))
-        self.assertNotIn('Ignoring response <500', str(log))
-        self.assertNotIn('Ignoring response <200', str(log))
+        self.assertNotIn("Ignoring response <402", str(log))
+        self.assertNotIn("Ignoring response <404", str(log))
+        self.assertNotIn("Ignoring response <500", str(log))
+        self.assertNotIn("Ignoring response <200", str(log))
```

### Comparing `Scrapy-2.7.1/tests/test_spidermiddleware_offsite.py` & `Scrapy-2.8.0/tests/test_spidermiddleware_offsite.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,97 +1,102 @@
+import warnings
 from unittest import TestCase
 from urllib.parse import urlparse
-import warnings
 
-from scrapy.http import Response, Request
+from scrapy.http import Request, Response
+from scrapy.spidermiddlewares.offsite import OffsiteMiddleware, PortWarning, URLWarning
 from scrapy.spiders import Spider
-from scrapy.spidermiddlewares.offsite import OffsiteMiddleware, URLWarning, PortWarning
 from scrapy.utils.test import get_crawler
 
 
 class TestOffsiteMiddleware(TestCase):
-
     def setUp(self):
         crawler = get_crawler(Spider)
         self.spider = crawler._create_spider(**self._get_spiderargs())
         self.mw = OffsiteMiddleware.from_crawler(crawler)
         self.mw.spider_opened(self.spider)
 
     def _get_spiderargs(self):
-        return dict(name='foo', allowed_domains=['scrapytest.org', 'scrapy.org', 'scrapy.test.org'])
+        return dict(
+            name="foo",
+            allowed_domains=["scrapytest.org", "scrapy.org", "scrapy.test.org"],
+        )
 
     def test_process_spider_output(self):
-        res = Response('http://scrapytest.org')
+        res = Response("http://scrapytest.org")
 
         onsite_reqs = [
-            Request('http://scrapytest.org/1'),
-            Request('http://scrapy.org/1'),
-            Request('http://sub.scrapy.org/1'),
-            Request('http://offsite.tld/letmepass', dont_filter=True),
-            Request('http://scrapy.test.org/'),
-            Request('http://scrapy.test.org:8000/'),
+            Request("http://scrapytest.org/1"),
+            Request("http://scrapy.org/1"),
+            Request("http://sub.scrapy.org/1"),
+            Request("http://offsite.tld/letmepass", dont_filter=True),
+            Request("http://scrapy.test.org/"),
+            Request("http://scrapy.test.org:8000/"),
         ]
         offsite_reqs = [
-            Request('http://scrapy2.org'),
-            Request('http://offsite.tld/'),
-            Request('http://offsite.tld/scrapytest.org'),
-            Request('http://offsite.tld/rogue.scrapytest.org'),
-            Request('http://rogue.scrapytest.org.haha.com'),
-            Request('http://roguescrapytest.org'),
-            Request('http://test.org/'),
-            Request('http://notscrapy.test.org/'),
+            Request("http://scrapy2.org"),
+            Request("http://offsite.tld/"),
+            Request("http://offsite.tld/scrapytest.org"),
+            Request("http://offsite.tld/rogue.scrapytest.org"),
+            Request("http://rogue.scrapytest.org.haha.com"),
+            Request("http://roguescrapytest.org"),
+            Request("http://test.org/"),
+            Request("http://notscrapy.test.org/"),
         ]
         reqs = onsite_reqs + offsite_reqs
 
         out = list(self.mw.process_spider_output(res, reqs, self.spider))
         self.assertEqual(out, onsite_reqs)
 
 
 class TestOffsiteMiddleware2(TestOffsiteMiddleware):
-
     def _get_spiderargs(self):
-        return dict(name='foo', allowed_domains=None)
+        return dict(name="foo", allowed_domains=None)
 
     def test_process_spider_output(self):
-        res = Response('http://scrapytest.org')
-        reqs = [Request('http://a.com/b.html'), Request('http://b.com/1')]
+        res = Response("http://scrapytest.org")
+        reqs = [Request("http://a.com/b.html"), Request("http://b.com/1")]
         out = list(self.mw.process_spider_output(res, reqs, self.spider))
         self.assertEqual(out, reqs)
 
 
 class TestOffsiteMiddleware3(TestOffsiteMiddleware2):
-
     def _get_spiderargs(self):
-        return dict(name='foo')
+        return dict(name="foo")
 
 
 class TestOffsiteMiddleware4(TestOffsiteMiddleware3):
-
     def _get_spiderargs(self):
-        bad_hostname = urlparse('http:////scrapytest.org').hostname
-        return dict(name='foo', allowed_domains=['scrapytest.org', None, bad_hostname])
+        bad_hostname = urlparse("http:////scrapytest.org").hostname
+        return dict(name="foo", allowed_domains=["scrapytest.org", None, bad_hostname])
 
     def test_process_spider_output(self):
-        res = Response('http://scrapytest.org')
-        reqs = [Request('http://scrapytest.org/1')]
+        res = Response("http://scrapytest.org")
+        reqs = [Request("http://scrapytest.org/1")]
         out = list(self.mw.process_spider_output(res, reqs, self.spider))
         self.assertEqual(out, reqs)
 
 
 class TestOffsiteMiddleware5(TestOffsiteMiddleware4):
-
     def test_get_host_regex(self):
-        self.spider.allowed_domains = ['http://scrapytest.org', 'scrapy.org', 'scrapy.test.org']
+        self.spider.allowed_domains = [
+            "http://scrapytest.org",
+            "scrapy.org",
+            "scrapy.test.org",
+        ]
         with warnings.catch_warnings(record=True) as w:
             warnings.simplefilter("always")
             self.mw.get_host_regex(self.spider)
             assert issubclass(w[-1].category, URLWarning)
 
 
 class TestOffsiteMiddleware6(TestOffsiteMiddleware4):
-
     def test_get_host_regex(self):
-        self.spider.allowed_domains = ['scrapytest.org:8000', 'scrapy.org', 'scrapy.test.org']
+        self.spider.allowed_domains = [
+            "scrapytest.org:8000",
+            "scrapy.org",
+            "scrapy.test.org",
+        ]
         with warnings.catch_warnings(record=True) as w:
             warnings.simplefilter("always")
             self.mw.get_host_regex(self.spider)
             assert issubclass(w[-1].category, PortWarning)
```

### Comparing `Scrapy-2.7.1/tests/test_spidermiddleware_output_chain.py` & `Scrapy-2.8.0/tests/test_spidermiddleware_output_chain.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,299 +1,308 @@
 from testfixtures import LogCapture
 from twisted.internet import defer
 from twisted.trial.unittest import TestCase
 
 from scrapy import Request, Spider
 from scrapy.utils.test import get_crawler
-
 from tests.mockserver import MockServer
 
 
 class LogExceptionMiddleware:
     def process_spider_exception(self, response, exception, spider):
-        spider.logger.info('Middleware: %s exception caught', exception.__class__.__name__)
+        spider.logger.info(
+            "Middleware: %s exception caught", exception.__class__.__name__
+        )
         return None
 
 
 # ================================================================================
 # (0) recover from an exception on a spider callback
 class RecoveryMiddleware:
     def process_spider_exception(self, response, exception, spider):
-        spider.logger.info('Middleware: %s exception caught', exception.__class__.__name__)
+        spider.logger.info(
+            "Middleware: %s exception caught", exception.__class__.__name__
+        )
         return [
-            {'from': 'process_spider_exception'},
-            Request(response.url, meta={'dont_fail': True}, dont_filter=True),
+            {"from": "process_spider_exception"},
+            Request(response.url, meta={"dont_fail": True}, dont_filter=True),
         ]
 
 
 class RecoverySpider(Spider):
-    name = 'RecoverySpider'
+    name = "RecoverySpider"
     custom_settings = {
-        'SPIDER_MIDDLEWARES_BASE': {},
-        'SPIDER_MIDDLEWARES': {
+        "SPIDER_MIDDLEWARES_BASE": {},
+        "SPIDER_MIDDLEWARES": {
             RecoveryMiddleware: 10,
         },
     }
 
     def start_requests(self):
-        yield Request(self.mockserver.url('/status?n=200'))
+        yield Request(self.mockserver.url("/status?n=200"))
 
     def parse(self, response):
-        yield {'test': 1}
-        self.logger.info('DONT_FAIL: %s', response.meta.get('dont_fail'))
-        if not response.meta.get('dont_fail'):
+        yield {"test": 1}
+        self.logger.info("DONT_FAIL: %s", response.meta.get("dont_fail"))
+        if not response.meta.get("dont_fail"):
             raise TabError()
 
 
 class RecoveryAsyncGenSpider(RecoverySpider):
-    name = 'RecoveryAsyncGenSpider'
+    name = "RecoveryAsyncGenSpider"
 
     async def parse(self, response):
         for r in super().parse(response):
             yield r
 
 
 # ================================================================================
 # (1) exceptions from a spider middleware's process_spider_input method
 class FailProcessSpiderInputMiddleware:
     def process_spider_input(self, response, spider):
-        spider.logger.info('Middleware: will raise IndexError')
+        spider.logger.info("Middleware: will raise IndexError")
         raise IndexError()
 
 
 class ProcessSpiderInputSpiderWithoutErrback(Spider):
-    name = 'ProcessSpiderInputSpiderWithoutErrback'
+    name = "ProcessSpiderInputSpiderWithoutErrback"
     custom_settings = {
-        'SPIDER_MIDDLEWARES': {
+        "SPIDER_MIDDLEWARES": {
             # spider
             FailProcessSpiderInputMiddleware: 8,
             LogExceptionMiddleware: 6,
             # engine
         }
     }
 
     def start_requests(self):
-        yield Request(url=self.mockserver.url('/status?n=200'), callback=self.parse)
+        yield Request(url=self.mockserver.url("/status?n=200"), callback=self.parse)
 
     def parse(self, response):
-        return {'from': 'callback'}
+        return {"from": "callback"}
 
 
 class ProcessSpiderInputSpiderWithErrback(ProcessSpiderInputSpiderWithoutErrback):
-    name = 'ProcessSpiderInputSpiderWithErrback'
+    name = "ProcessSpiderInputSpiderWithErrback"
 
     def start_requests(self):
-        yield Request(self.mockserver.url('/status?n=200'), self.parse, errback=self.errback)
+        yield Request(
+            self.mockserver.url("/status?n=200"), self.parse, errback=self.errback
+        )
 
     def errback(self, failure):
-        self.logger.info('Got a Failure on the Request errback')
-        return {'from': 'errback'}
+        self.logger.info("Got a Failure on the Request errback")
+        return {"from": "errback"}
 
 
 # ================================================================================
 # (2) exceptions from a spider callback (generator)
 class GeneratorCallbackSpider(Spider):
-    name = 'GeneratorCallbackSpider'
+    name = "GeneratorCallbackSpider"
     custom_settings = {
-        'SPIDER_MIDDLEWARES': {
+        "SPIDER_MIDDLEWARES": {
             LogExceptionMiddleware: 10,
         },
     }
 
     def start_requests(self):
-        yield Request(self.mockserver.url('/status?n=200'))
+        yield Request(self.mockserver.url("/status?n=200"))
 
     def parse(self, response):
-        yield {'test': 1}
-        yield {'test': 2}
+        yield {"test": 1}
+        yield {"test": 2}
         raise ImportError()
 
 
 class AsyncGeneratorCallbackSpider(GeneratorCallbackSpider):
     async def parse(self, response):
-        yield {'test': 1}
-        yield {'test': 2}
+        yield {"test": 1}
+        yield {"test": 2}
         raise ImportError()
 
 
 # ================================================================================
 # (2.1) exceptions from a spider callback (generator, middleware right after callback)
 class GeneratorCallbackSpiderMiddlewareRightAfterSpider(GeneratorCallbackSpider):
-    name = 'GeneratorCallbackSpiderMiddlewareRightAfterSpider'
+    name = "GeneratorCallbackSpiderMiddlewareRightAfterSpider"
     custom_settings = {
-        'SPIDER_MIDDLEWARES': {
+        "SPIDER_MIDDLEWARES": {
             LogExceptionMiddleware: 100000,
         },
     }
 
 
 # ================================================================================
 # (3) exceptions from a spider callback (not a generator)
 class NotGeneratorCallbackSpider(Spider):
-    name = 'NotGeneratorCallbackSpider'
+    name = "NotGeneratorCallbackSpider"
     custom_settings = {
-        'SPIDER_MIDDLEWARES': {
+        "SPIDER_MIDDLEWARES": {
             LogExceptionMiddleware: 10,
         },
     }
 
     def start_requests(self):
-        yield Request(self.mockserver.url('/status?n=200'))
+        yield Request(self.mockserver.url("/status?n=200"))
 
     def parse(self, response):
-        return [{'test': 1}, {'test': 1 / 0}]
+        return [{"test": 1}, {"test": 1 / 0}]
 
 
 # ================================================================================
 # (3.1) exceptions from a spider callback (not a generator, middleware right after callback)
 class NotGeneratorCallbackSpiderMiddlewareRightAfterSpider(NotGeneratorCallbackSpider):
-    name = 'NotGeneratorCallbackSpiderMiddlewareRightAfterSpider'
+    name = "NotGeneratorCallbackSpiderMiddlewareRightAfterSpider"
     custom_settings = {
-        'SPIDER_MIDDLEWARES': {
+        "SPIDER_MIDDLEWARES": {
             LogExceptionMiddleware: 100000,
         },
     }
 
 
 # ================================================================================
 # (4) exceptions from a middleware process_spider_output method (generator)
 class _GeneratorDoNothingMiddleware:
     def process_spider_output(self, response, result, spider):
         for r in result:
-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')
+            r["processed"].append(f"{self.__class__.__name__}.process_spider_output")
             yield r
 
     def process_spider_exception(self, response, exception, spider):
-        method = f'{self.__class__.__name__}.process_spider_exception'
-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)
+        method = f"{self.__class__.__name__}.process_spider_exception"
+        spider.logger.info("%s: %s caught", method, exception.__class__.__name__)
         return None
 
 
 class GeneratorFailMiddleware:
     def process_spider_output(self, response, result, spider):
         for r in result:
-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')
+            r["processed"].append(f"{self.__class__.__name__}.process_spider_output")
             yield r
             raise LookupError()
 
     def process_spider_exception(self, response, exception, spider):
-        method = f'{self.__class__.__name__}.process_spider_exception'
-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)
-        yield {'processed': [method]}
+        method = f"{self.__class__.__name__}.process_spider_exception"
+        spider.logger.info("%s: %s caught", method, exception.__class__.__name__)
+        yield {"processed": [method]}
 
 
 class GeneratorDoNothingAfterFailureMiddleware(_GeneratorDoNothingMiddleware):
     pass
 
 
 class GeneratorRecoverMiddleware:
     def process_spider_output(self, response, result, spider):
         for r in result:
-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')
+            r["processed"].append(f"{self.__class__.__name__}.process_spider_output")
             yield r
 
     def process_spider_exception(self, response, exception, spider):
-        method = f'{self.__class__.__name__}.process_spider_exception'
-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)
-        yield {'processed': [method]}
+        method = f"{self.__class__.__name__}.process_spider_exception"
+        spider.logger.info("%s: %s caught", method, exception.__class__.__name__)
+        yield {"processed": [method]}
 
 
 class GeneratorDoNothingAfterRecoveryMiddleware(_GeneratorDoNothingMiddleware):
     pass
 
 
 class GeneratorOutputChainSpider(Spider):
-    name = 'GeneratorOutputChainSpider'
+    name = "GeneratorOutputChainSpider"
     custom_settings = {
-        'SPIDER_MIDDLEWARES': {
+        "SPIDER_MIDDLEWARES": {
             GeneratorFailMiddleware: 10,
             GeneratorDoNothingAfterFailureMiddleware: 8,
             GeneratorRecoverMiddleware: 5,
             GeneratorDoNothingAfterRecoveryMiddleware: 3,
         },
     }
 
     def start_requests(self):
-        yield Request(self.mockserver.url('/status?n=200'))
+        yield Request(self.mockserver.url("/status?n=200"))
 
     def parse(self, response):
-        yield {'processed': ['parse-first-item']}
-        yield {'processed': ['parse-second-item']}
+        yield {"processed": ["parse-first-item"]}
+        yield {"processed": ["parse-second-item"]}
 
 
 # ================================================================================
 # (5) exceptions from a middleware process_spider_output method (not generator)
 
+
 class _NotGeneratorDoNothingMiddleware:
     def process_spider_output(self, response, result, spider):
         out = []
         for r in result:
-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')
+            r["processed"].append(f"{self.__class__.__name__}.process_spider_output")
             out.append(r)
         return out
 
     def process_spider_exception(self, response, exception, spider):
-        method = f'{self.__class__.__name__}.process_spider_exception'
-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)
+        method = f"{self.__class__.__name__}.process_spider_exception"
+        spider.logger.info("%s: %s caught", method, exception.__class__.__name__)
         return None
 
 
 class NotGeneratorFailMiddleware:
     def process_spider_output(self, response, result, spider):
         out = []
         for r in result:
-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')
+            r["processed"].append(f"{self.__class__.__name__}.process_spider_output")
             out.append(r)
         raise ReferenceError()
         return out
 
     def process_spider_exception(self, response, exception, spider):
-        method = f'{self.__class__.__name__}.process_spider_exception'
-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)
-        return [{'processed': [method]}]
+        method = f"{self.__class__.__name__}.process_spider_exception"
+        spider.logger.info("%s: %s caught", method, exception.__class__.__name__)
+        return [{"processed": [method]}]
 
 
 class NotGeneratorDoNothingAfterFailureMiddleware(_NotGeneratorDoNothingMiddleware):
     pass
 
 
 class NotGeneratorRecoverMiddleware:
     def process_spider_output(self, response, result, spider):
         out = []
         for r in result:
-            r['processed'].append(f'{self.__class__.__name__}.process_spider_output')
+            r["processed"].append(f"{self.__class__.__name__}.process_spider_output")
             out.append(r)
         return out
 
     def process_spider_exception(self, response, exception, spider):
-        method = f'{self.__class__.__name__}.process_spider_exception'
-        spider.logger.info('%s: %s caught', method, exception.__class__.__name__)
-        return [{'processed': [method]}]
+        method = f"{self.__class__.__name__}.process_spider_exception"
+        spider.logger.info("%s: %s caught", method, exception.__class__.__name__)
+        return [{"processed": [method]}]
 
 
 class NotGeneratorDoNothingAfterRecoveryMiddleware(_NotGeneratorDoNothingMiddleware):
     pass
 
 
 class NotGeneratorOutputChainSpider(Spider):
-    name = 'NotGeneratorOutputChainSpider'
+    name = "NotGeneratorOutputChainSpider"
     custom_settings = {
-        'SPIDER_MIDDLEWARES': {
+        "SPIDER_MIDDLEWARES": {
             NotGeneratorFailMiddleware: 10,
             NotGeneratorDoNothingAfterFailureMiddleware: 8,
             NotGeneratorRecoverMiddleware: 5,
             NotGeneratorDoNothingAfterRecoveryMiddleware: 3,
         },
     }
 
     def start_requests(self):
-        return [Request(self.mockserver.url('/status?n=200'))]
+        return [Request(self.mockserver.url("/status?n=200"))]
 
     def parse(self, response):
-        return [{'processed': ['parse-first-item']}, {'processed': ['parse-second-item']}]
+        return [
+            {"processed": ["parse-first-item"]},
+            {"processed": ["parse-second-item"]},
+        ]
 
 
 # ================================================================================
 class TestSpiderMiddleware(TestCase):
     @classmethod
     def setUpClass(cls):
         cls.mockserver = MockServer()
@@ -399,15 +408,17 @@
 
     @defer.inlineCallbacks
     def test_not_a_generator_callback_right_after_callback(self):
         """
         (3.1) Special case of (3): Exceptions should be caught
         even if the middleware is placed right after the spider
         """
-        log31 = yield self.crawl_log(NotGeneratorCallbackSpiderMiddlewareRightAfterSpider)
+        log31 = yield self.crawl_log(
+            NotGeneratorCallbackSpiderMiddlewareRightAfterSpider
+        )
         self.assertIn("Middleware: ZeroDivisionError exception caught", str(log31))
         self.assertNotIn("item_scraped_count", str(log31))
 
     @defer.inlineCallbacks
     def test_generator_output_chain(self):
         """
         (4) An exception from a middleware's process_spider_output method should be sent
@@ -415,56 +426,79 @@
         The result of the recovery by the process_spider_exception method should be handled
         by the process_spider_output method from the next middleware.
         The final item count should be 2 (one from the spider callback and one from the
         process_spider_exception chain)
         """
         log4 = yield self.crawl_log(GeneratorOutputChainSpider)
         self.assertIn("'item_scraped_count': 2", str(log4))
-        self.assertIn("GeneratorRecoverMiddleware.process_spider_exception: LookupError caught", str(log4))
+        self.assertIn(
+            "GeneratorRecoverMiddleware.process_spider_exception: LookupError caught",
+            str(log4),
+        )
         self.assertIn(
             "GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: LookupError caught",
-            str(log4))
+            str(log4),
+        )
         self.assertNotIn(
             "GeneratorFailMiddleware.process_spider_exception: LookupError caught",
-            str(log4))
+            str(log4),
+        )
         self.assertNotIn(
             "GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: LookupError caught",
-            str(log4))
-        item_from_callback = {'processed': [
-            'parse-first-item',
-            'GeneratorFailMiddleware.process_spider_output',
-            'GeneratorDoNothingAfterFailureMiddleware.process_spider_output',
-            'GeneratorRecoverMiddleware.process_spider_output',
-            'GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output']}
-        item_recovered = {'processed': [
-            'GeneratorRecoverMiddleware.process_spider_exception',
-            'GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output']}
+            str(log4),
+        )
+        item_from_callback = {
+            "processed": [
+                "parse-first-item",
+                "GeneratorFailMiddleware.process_spider_output",
+                "GeneratorDoNothingAfterFailureMiddleware.process_spider_output",
+                "GeneratorRecoverMiddleware.process_spider_output",
+                "GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output",
+            ]
+        }
+        item_recovered = {
+            "processed": [
+                "GeneratorRecoverMiddleware.process_spider_exception",
+                "GeneratorDoNothingAfterRecoveryMiddleware.process_spider_output",
+            ]
+        }
         self.assertIn(str(item_from_callback), str(log4))
         self.assertIn(str(item_recovered), str(log4))
-        self.assertNotIn('parse-second-item', str(log4))
+        self.assertNotIn("parse-second-item", str(log4))
 
     @defer.inlineCallbacks
     def test_not_a_generator_output_chain(self):
         """
         (5) An exception from a middleware's process_spider_output method should be sent
         to the process_spider_exception method from the next middleware in the chain.
         The result of the recovery by the process_spider_exception method should be handled
         by the process_spider_output method from the next middleware.
         The final item count should be 1 (from the process_spider_exception chain, the items
         from the spider callback are lost)
         """
         log5 = yield self.crawl_log(NotGeneratorOutputChainSpider)
         self.assertIn("'item_scraped_count': 1", str(log5))
-        self.assertIn("GeneratorRecoverMiddleware.process_spider_exception: ReferenceError caught", str(log5))
+        self.assertIn(
+            "GeneratorRecoverMiddleware.process_spider_exception: ReferenceError caught",
+            str(log5),
+        )
         self.assertIn(
             "GeneratorDoNothingAfterFailureMiddleware.process_spider_exception: ReferenceError caught",
-            str(log5))
-        self.assertNotIn("GeneratorFailMiddleware.process_spider_exception: ReferenceError caught", str(log5))
+            str(log5),
+        )
+        self.assertNotIn(
+            "GeneratorFailMiddleware.process_spider_exception: ReferenceError caught",
+            str(log5),
+        )
         self.assertNotIn(
             "GeneratorDoNothingAfterRecoveryMiddleware.process_spider_exception: ReferenceError caught",
-            str(log5))
-        item_recovered = {'processed': [
-            'NotGeneratorRecoverMiddleware.process_spider_exception',
-            'NotGeneratorDoNothingAfterRecoveryMiddleware.process_spider_output']}
+            str(log5),
+        )
+        item_recovered = {
+            "processed": [
+                "NotGeneratorRecoverMiddleware.process_spider_exception",
+                "NotGeneratorDoNothingAfterRecoveryMiddleware.process_spider_output",
+            ]
+        }
         self.assertIn(str(item_recovered), str(log5))
-        self.assertNotIn('parse-first-item', str(log5))
-        self.assertNotIn('parse-second-item', str(log5))
+        self.assertNotIn("parse-first-item", str(log5))
+        self.assertNotIn("parse-second-item", str(log5))
```

### Comparing `Scrapy-2.7.1/tests/test_spidermiddleware_urllength.py` & `Scrapy-2.8.0/tests/test_spidermiddleware_urllength.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,41 +1,44 @@
 from unittest import TestCase
 
 from testfixtures import LogCapture
 
+from scrapy.http import Request, Response
+from scrapy.settings import Settings
 from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware
-from scrapy.http import Response, Request
 from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
-from scrapy.settings import Settings
 
 
 class TestUrlLengthMiddleware(TestCase):
-
     def setUp(self):
         self.maxlength = 25
-        settings = Settings({'URLLENGTH_LIMIT': self.maxlength})
+        settings = Settings({"URLLENGTH_LIMIT": self.maxlength})
 
         crawler = get_crawler(Spider)
-        self.spider = crawler._create_spider('foo')
+        self.spider = crawler._create_spider("foo")
         self.stats = crawler.stats
         self.mw = UrlLengthMiddleware.from_settings(settings)
 
-        self.response = Response('http://scrapytest.org')
-        self.short_url_req = Request('http://scrapytest.org/')
-        self.long_url_req = Request('http://scrapytest.org/this_is_a_long_url')
+        self.response = Response("http://scrapytest.org")
+        self.short_url_req = Request("http://scrapytest.org/")
+        self.long_url_req = Request("http://scrapytest.org/this_is_a_long_url")
         self.reqs = [self.short_url_req, self.long_url_req]
 
     def process_spider_output(self):
-        return list(self.mw.process_spider_output(self.response, self.reqs, self.spider))
+        return list(
+            self.mw.process_spider_output(self.response, self.reqs, self.spider)
+        )
 
     def test_middleware_works(self):
         self.assertEqual(self.process_spider_output(), [self.short_url_req])
 
     def test_logging(self):
         with LogCapture() as log:
             self.process_spider_output()
 
-        ric = self.stats.get_value('urllength/request_ignored_count', spider=self.spider)
+        ric = self.stats.get_value(
+            "urllength/request_ignored_count", spider=self.spider
+        )
         self.assertEqual(ric, 1)
 
-        self.assertIn(f'Ignoring link (url length > {self.maxlength})', str(log))
+        self.assertIn(f"Ignoring link (url length > {self.maxlength})", str(log))
```

### Comparing `Scrapy-2.7.1/tests/test_spiderstate.py` & `Scrapy-2.8.0/tests/test_spiderstate.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-import os
-from datetime import datetime
 import shutil
+from datetime import datetime
+from pathlib import Path
+
 from twisted.trial import unittest
 
+from scrapy.exceptions import NotConfigured
 from scrapy.extensions.spiderstate import SpiderState
 from scrapy.spiders import Spider
-from scrapy.exceptions import NotConfigured
 from scrapy.utils.test import get_crawler
 
 
 class SpiderStateTest(unittest.TestCase):
-
     def test_store_load(self):
         jobdir = self.mktemp()
-        os.mkdir(jobdir)
+        Path(jobdir).mkdir()
         try:
-            spider = Spider(name='default')
+            spider = Spider(name="default")
             dt = datetime.now()
 
             ss = SpiderState(jobdir)
             ss.spider_opened(spider)
-            spider.state['one'] = 1
-            spider.state['dt'] = dt
+            spider.state["one"] = 1
+            spider.state["dt"] = dt
             ss.spider_closed(spider)
 
-            spider2 = Spider(name='default')
+            spider2 = Spider(name="default")
             ss2 = SpiderState(jobdir)
             ss2.spider_opened(spider2)
-            self.assertEqual(spider.state, {'one': 1, 'dt': dt})
+            self.assertEqual(spider.state, {"one": 1, "dt": dt})
             ss2.spider_closed(spider2)
         finally:
             shutil.rmtree(jobdir)
 
     def test_state_attribute(self):
         # state attribute must be present if jobdir is not set, to provide a
         # consistent interface
-        spider = Spider(name='default')
+        spider = Spider(name="default")
         ss = SpiderState()
         ss.spider_opened(spider)
         self.assertEqual(spider.state, {})
         ss.spider_closed(spider)
 
     def test_not_configured(self):
         crawler = get_crawler(Spider)
```

### Comparing `Scrapy-2.7.1/tests/test_squeues.py` & `Scrapy-2.8.0/tests/test_squeues.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 import pickle
 import sys
 
 from queuelib.tests import test_queue as t
+
+from scrapy.http import Request
+from scrapy.item import Field, Item
+from scrapy.loader import ItemLoader
+from scrapy.selector import Selector
 from scrapy.squeues import (
     _MarshalFifoSerializationDiskQueue,
     _MarshalLifoSerializationDiskQueue,
     _PickleFifoSerializationDiskQueue,
     _PickleLifoSerializationDiskQueue,
 )
-from scrapy.item import Item, Field
-from scrapy.http import Request
-from scrapy.loader import ItemLoader
-from scrapy.selector import Selector
 
 
 class TestItem(Item):
     name = Field()
 
 
 def _test_procesor(x):
@@ -27,28 +28,27 @@
     name_out = staticmethod(_test_procesor)
 
 
 def nonserializable_object_test(self):
     q = self.queue()
     self.assertRaises(ValueError, q.push, lambda x: x)
     # Selectors should fail (lxml.html.HtmlElement objects can't be pickled)
-    sel = Selector(text='<html><body><p>some text</p></body></html>')
+    sel = Selector(text="<html><body><p>some text</p></body></html>")
     self.assertRaises(ValueError, q.push, sel)
 
 
 class FifoDiskQueueTestMixin:
-
     def test_serialize(self):
         q = self.queue()
-        q.push('a')
+        q.push("a")
         q.push(123)
-        q.push({'a': 'dict'})
-        self.assertEqual(q.pop(), 'a')
+        q.push({"a": "dict"})
+        self.assertEqual(q.pop(), "a")
         self.assertEqual(q.pop(), 123)
-        self.assertEqual(q.pop(), {'a': 'dict'})
+        self.assertEqual(q.pop(), {"a": "dict"})
 
     test_nonserializable_object = nonserializable_object_test
 
 
 class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):
     chunksize = 100000
 
@@ -77,49 +77,49 @@
     chunksize = 100000
 
     def queue(self):
         return _PickleFifoSerializationDiskQueue(self.qpath, chunksize=self.chunksize)
 
     def test_serialize_item(self):
         q = self.queue()
-        i = TestItem(name='foo')
+        i = TestItem(name="foo")
         q.push(i)
         i2 = q.pop()
         assert isinstance(i2, TestItem)
         self.assertEqual(i, i2)
 
     def test_serialize_loader(self):
         q = self.queue()
         loader = TestLoader()
         q.push(loader)
         loader2 = q.pop()
         assert isinstance(loader2, TestLoader)
         assert loader2.default_item_class is TestItem
-        self.assertEqual(loader2.name_out('x'), 'xx')
+        self.assertEqual(loader2.name_out("x"), "xx")
 
     def test_serialize_request_recursive(self):
         q = self.queue()
-        r = Request('http://www.example.com')
-        r.meta['request'] = r
+        r = Request("http://www.example.com")
+        r.meta["request"] = r
         q.push(r)
         r2 = q.pop()
         assert isinstance(r2, Request)
         self.assertEqual(r.url, r2.url)
-        assert r2.meta['request'] is r2
+        assert r2.meta["request"] is r2
 
     def test_non_pickable_object(self):
         q = self.queue()
         try:
             q.push(lambda x: x)
         except ValueError as exc:
             if hasattr(sys, "pypy_version_info"):
                 self.assertIsInstance(exc.__context__, pickle.PicklingError)
             else:
                 self.assertIsInstance(exc.__context__, AttributeError)
-        sel = Selector(text='<html><body><p>some text</p></body></html>')
+        sel = Selector(text="<html><body><p>some text</p></body></html>")
         try:
             q.push(sel)
         except ValueError as exc:
             self.assertIsInstance(exc.__context__, TypeError)
 
 
 class ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
@@ -135,57 +135,54 @@
 
 
 class ChunkSize4PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
     chunksize = 4
 
 
 class LifoDiskQueueTestMixin:
-
     def test_serialize(self):
         q = self.queue()
-        q.push('a')
+        q.push("a")
         q.push(123)
-        q.push({'a': 'dict'})
-        self.assertEqual(q.pop(), {'a': 'dict'})
+        q.push({"a": "dict"})
+        self.assertEqual(q.pop(), {"a": "dict"})
         self.assertEqual(q.pop(), 123)
-        self.assertEqual(q.pop(), 'a')
+        self.assertEqual(q.pop(), "a")
 
     test_nonserializable_object = nonserializable_object_test
 
 
 class MarshalLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):
-
     def queue(self):
         return _MarshalLifoSerializationDiskQueue(self.qpath)
 
 
 class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):
-
     def queue(self):
         return _PickleLifoSerializationDiskQueue(self.qpath)
 
     def test_serialize_item(self):
         q = self.queue()
-        i = TestItem(name='foo')
+        i = TestItem(name="foo")
         q.push(i)
         i2 = q.pop()
         assert isinstance(i2, TestItem)
         self.assertEqual(i, i2)
 
     def test_serialize_loader(self):
         q = self.queue()
         loader = TestLoader()
         q.push(loader)
         loader2 = q.pop()
         assert isinstance(loader2, TestLoader)
         assert loader2.default_item_class is TestItem
-        self.assertEqual(loader2.name_out('x'), 'xx')
+        self.assertEqual(loader2.name_out("x"), "xx")
 
     def test_serialize_request_recursive(self):
         q = self.queue()
-        r = Request('http://www.example.com')
-        r.meta['request'] = r
+        r = Request("http://www.example.com")
+        r.meta["request"] = r
         q.push(r)
         r2 = q.pop()
         assert isinstance(r2, Request)
         self.assertEqual(r.url, r2.url)
-        assert r2.meta['request'] is r2
+        assert r2.meta["request"] is r2
```

### Comparing `Scrapy-2.7.1/tests/test_squeues_request.py` & `Scrapy-2.8.0/tests/test_squeues_request.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 import shutil
 import tempfile
 import unittest
 
 import queuelib
 
+from scrapy.http import Request
+from scrapy.spiders import Spider
 from scrapy.squeues import (
-    PickleFifoDiskQueue,
-    PickleLifoDiskQueue,
-    MarshalFifoDiskQueue,
-    MarshalLifoDiskQueue,
     FifoMemoryQueue,
     LifoMemoryQueue,
+    MarshalFifoDiskQueue,
+    MarshalLifoDiskQueue,
+    PickleFifoDiskQueue,
+    PickleLifoDiskQueue,
 )
-from scrapy.http import Request
-from scrapy.spiders import Spider
 from scrapy.utils.test import get_crawler
 
-
 """
 Queues that handle requests
 """
 
 
 class BaseQueueTestCase(unittest.TestCase):
     def setUp(self):
@@ -66,15 +65,18 @@
             raise unittest.SkipTest("The queuelib queues define peek")
         q = self.queue()
         self.assertEqual(len(q), 0)
         self.assertIsNone(q.pop())
         req = Request("http://www.example.com")
         q.push(req)
         self.assertEqual(len(q), 1)
-        with self.assertRaises(NotImplementedError, msg="The underlying queue class does not implement 'peek'"):
+        with self.assertRaises(
+            NotImplementedError,
+            msg="The underlying queue class does not implement 'peek'",
+        ):
             q.peek()
         self.assertEqual(q.pop().url, req.url)
         self.assertEqual(len(q), 0)
         self.assertIsNone(q.pop())
         q.close()
 
 
@@ -114,15 +116,18 @@
         self.assertIsNone(q.pop())
         req1 = Request("http://www.example.com/1")
         req2 = Request("http://www.example.com/2")
         req3 = Request("http://www.example.com/3")
         q.push(req1)
         q.push(req2)
         q.push(req3)
-        with self.assertRaises(NotImplementedError, msg="The underlying queue class does not implement 'peek'"):
+        with self.assertRaises(
+            NotImplementedError,
+            msg="The underlying queue class does not implement 'peek'",
+        ):
             q.peek()
         self.assertEqual(len(q), 3)
         self.assertEqual(q.pop().url, req1.url)
         self.assertEqual(len(q), 2)
         self.assertEqual(q.pop().url, req2.url)
         self.assertEqual(len(q), 1)
         self.assertEqual(q.pop().url, req3.url)
@@ -167,15 +172,18 @@
         self.assertIsNone(q.pop())
         req1 = Request("http://www.example.com/1")
         req2 = Request("http://www.example.com/2")
         req3 = Request("http://www.example.com/3")
         q.push(req1)
         q.push(req2)
         q.push(req3)
-        with self.assertRaises(NotImplementedError, msg="The underlying queue class does not implement 'peek'"):
+        with self.assertRaises(
+            NotImplementedError,
+            msg="The underlying queue class does not implement 'peek'",
+        ):
             q.peek()
         self.assertEqual(len(q), 3)
         self.assertEqual(q.pop().url, req3.url)
         self.assertEqual(len(q), 2)
         self.assertEqual(q.pop().url, req2.url)
         self.assertEqual(len(q), 1)
         self.assertEqual(q.pop().url, req1.url)
@@ -192,20 +200,24 @@
 class PickleLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):
     def queue(self):
         return PickleLifoDiskQueue.from_crawler(crawler=self.crawler, key="pickle/lifo")
 
 
 class MarshalFifoDiskQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):
     def queue(self):
-        return MarshalFifoDiskQueue.from_crawler(crawler=self.crawler, key="marshal/fifo")
+        return MarshalFifoDiskQueue.from_crawler(
+            crawler=self.crawler, key="marshal/fifo"
+        )
 
 
 class MarshalLifoDiskQueueRequestTest(LifoQueueMixin, BaseQueueTestCase):
     def queue(self):
-        return MarshalLifoDiskQueue.from_crawler(crawler=self.crawler, key="marshal/lifo")
+        return MarshalLifoDiskQueue.from_crawler(
+            crawler=self.crawler, key="marshal/lifo"
+        )
 
 
 class FifoMemoryQueueRequestTest(FifoQueueMixin, BaseQueueTestCase):
     def queue(self):
         return FifoMemoryQueue.from_crawler(crawler=self.crawler)
```

### Comparing `Scrapy-2.7.1/tests/test_stats.py` & `Scrapy-2.8.0/tests/test_stats.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,98 +1,96 @@
-from datetime import datetime
 import unittest
+from datetime import datetime
 from unittest import mock
 
 from scrapy.extensions.corestats import CoreStats
 from scrapy.spiders import Spider
-from scrapy.statscollectors import StatsCollector, DummyStatsCollector
+from scrapy.statscollectors import DummyStatsCollector, StatsCollector
 from scrapy.utils.test import get_crawler
 
 
 class CoreStatsExtensionTest(unittest.TestCase):
-
     def setUp(self):
         self.crawler = get_crawler(Spider)
-        self.spider = self.crawler._create_spider('foo')
+        self.spider = self.crawler._create_spider("foo")
 
-    @mock.patch('scrapy.extensions.corestats.datetime')
+    @mock.patch("scrapy.extensions.corestats.datetime")
     def test_core_stats_default_stats_collector(self, mock_datetime):
         fixed_datetime = datetime(2019, 12, 1, 11, 38)
         mock_datetime.utcnow = mock.Mock(return_value=fixed_datetime)
         self.crawler.stats = StatsCollector(self.crawler)
         ext = CoreStats.from_crawler(self.crawler)
         ext.spider_opened(self.spider)
         ext.item_scraped({}, self.spider)
         ext.response_received(self.spider)
         ext.item_dropped({}, self.spider, ZeroDivisionError())
-        ext.spider_closed(self.spider, 'finished')
+        ext.spider_closed(self.spider, "finished")
         self.assertEqual(
             ext.stats._stats,
             {
-                'start_time': fixed_datetime,
-                'finish_time': fixed_datetime,
-                'item_scraped_count': 1,
-                'response_received_count': 1,
-                'item_dropped_count': 1,
-                'item_dropped_reasons_count/ZeroDivisionError': 1,
-                'finish_reason': 'finished',
-                'elapsed_time_seconds': 0.0,
-            }
+                "start_time": fixed_datetime,
+                "finish_time": fixed_datetime,
+                "item_scraped_count": 1,
+                "response_received_count": 1,
+                "item_dropped_count": 1,
+                "item_dropped_reasons_count/ZeroDivisionError": 1,
+                "finish_reason": "finished",
+                "elapsed_time_seconds": 0.0,
+            },
         )
 
     def test_core_stats_dummy_stats_collector(self):
         self.crawler.stats = DummyStatsCollector(self.crawler)
         ext = CoreStats.from_crawler(self.crawler)
         ext.spider_opened(self.spider)
         ext.item_scraped({}, self.spider)
         ext.response_received(self.spider)
         ext.item_dropped({}, self.spider, ZeroDivisionError())
-        ext.spider_closed(self.spider, 'finished')
+        ext.spider_closed(self.spider, "finished")
         self.assertEqual(ext.stats._stats, {})
 
 
 class StatsCollectorTest(unittest.TestCase):
-
     def setUp(self):
         self.crawler = get_crawler(Spider)
-        self.spider = self.crawler._create_spider('foo')
+        self.spider = self.crawler._create_spider("foo")
 
     def test_collector(self):
         stats = StatsCollector(self.crawler)
         self.assertEqual(stats.get_stats(), {})
-        self.assertEqual(stats.get_value('anything'), None)
-        self.assertEqual(stats.get_value('anything', 'default'), 'default')
-        stats.set_value('test', 'value')
-        self.assertEqual(stats.get_stats(), {'test': 'value'})
-        stats.set_value('test2', 23)
-        self.assertEqual(stats.get_stats(), {'test': 'value', 'test2': 23})
-        self.assertEqual(stats.get_value('test2'), 23)
-        stats.inc_value('test2')
-        self.assertEqual(stats.get_value('test2'), 24)
-        stats.inc_value('test2', 6)
-        self.assertEqual(stats.get_value('test2'), 30)
-        stats.max_value('test2', 6)
-        self.assertEqual(stats.get_value('test2'), 30)
-        stats.max_value('test2', 40)
-        self.assertEqual(stats.get_value('test2'), 40)
-        stats.max_value('test3', 1)
-        self.assertEqual(stats.get_value('test3'), 1)
-        stats.min_value('test2', 60)
-        self.assertEqual(stats.get_value('test2'), 40)
-        stats.min_value('test2', 35)
-        self.assertEqual(stats.get_value('test2'), 35)
-        stats.min_value('test4', 7)
-        self.assertEqual(stats.get_value('test4'), 7)
+        self.assertEqual(stats.get_value("anything"), None)
+        self.assertEqual(stats.get_value("anything", "default"), "default")
+        stats.set_value("test", "value")
+        self.assertEqual(stats.get_stats(), {"test": "value"})
+        stats.set_value("test2", 23)
+        self.assertEqual(stats.get_stats(), {"test": "value", "test2": 23})
+        self.assertEqual(stats.get_value("test2"), 23)
+        stats.inc_value("test2")
+        self.assertEqual(stats.get_value("test2"), 24)
+        stats.inc_value("test2", 6)
+        self.assertEqual(stats.get_value("test2"), 30)
+        stats.max_value("test2", 6)
+        self.assertEqual(stats.get_value("test2"), 30)
+        stats.max_value("test2", 40)
+        self.assertEqual(stats.get_value("test2"), 40)
+        stats.max_value("test3", 1)
+        self.assertEqual(stats.get_value("test3"), 1)
+        stats.min_value("test2", 60)
+        self.assertEqual(stats.get_value("test2"), 40)
+        stats.min_value("test2", 35)
+        self.assertEqual(stats.get_value("test2"), 35)
+        stats.min_value("test4", 7)
+        self.assertEqual(stats.get_value("test4"), 7)
 
     def test_dummy_collector(self):
         stats = DummyStatsCollector(self.crawler)
         self.assertEqual(stats.get_stats(), {})
-        self.assertEqual(stats.get_value('anything'), None)
-        self.assertEqual(stats.get_value('anything', 'default'), 'default')
-        stats.set_value('test', 'value')
-        stats.inc_value('v1')
-        stats.max_value('v2', 100)
-        stats.min_value('v3', 100)
-        stats.open_spider('a')
-        stats.set_value('test', 'value', spider=self.spider)
+        self.assertEqual(stats.get_value("anything"), None)
+        self.assertEqual(stats.get_value("anything", "default"), "default")
+        stats.set_value("test", "value")
+        stats.inc_value("v1")
+        stats.max_value("v2", 100)
+        stats.min_value("v3", 100)
+        stats.open_spider("a")
+        stats.set_value("test", "value", spider=self.spider)
         self.assertEqual(stats.get_stats(), {})
-        self.assertEqual(stats.get_stats('a'), {})
+        self.assertEqual(stats.get_stats("a"), {})
```

### Comparing `Scrapy-2.7.1/tests/test_toplevel.py` & `Scrapy-2.8.0/tests/test_toplevel.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,30 +1,33 @@
 from unittest import TestCase
 
 import scrapy
 
 
 class ToplevelTestCase(TestCase):
-
     def test_version(self):
         self.assertIs(type(scrapy.__version__), str)
 
     def test_version_info(self):
         self.assertIs(type(scrapy.version_info), tuple)
 
     def test_request_shortcut(self):
-        from scrapy.http import Request, FormRequest
+        from scrapy.http import FormRequest, Request
+
         self.assertIs(scrapy.Request, Request)
         self.assertIs(scrapy.FormRequest, FormRequest)
 
     def test_spider_shortcut(self):
         from scrapy.spiders import Spider
+
         self.assertIs(scrapy.Spider, Spider)
 
     def test_selector_shortcut(self):
         from scrapy.selector import Selector
+
         self.assertIs(scrapy.Selector, Selector)
 
     def test_item_shortcut(self):
-        from scrapy.item import Item, Field
+        from scrapy.item import Field, Item
+
         self.assertIs(scrapy.Item, Item)
         self.assertIs(scrapy.Field, Field)
```

### Comparing `Scrapy-2.7.1/tests/test_utils_asyncgen.py` & `Scrapy-2.8.0/tests/test_utils_asyncgen.py`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/test_utils_asyncio.py` & `Scrapy-2.8.0/tests/test_utils_asyncio.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,19 +1,25 @@
 import warnings
 from unittest import TestCase
 
 from pytest import mark
 
-from scrapy.utils.reactor import is_asyncio_reactor_installed, install_reactor
+from scrapy.utils.reactor import install_reactor, is_asyncio_reactor_installed
 
 
-@mark.usefixtures('reactor_pytest')
+@mark.usefixtures("reactor_pytest")
 class AsyncioTest(TestCase):
-
     def test_is_asyncio_reactor_installed(self):
         # the result should depend only on the pytest --reactor argument
-        self.assertEqual(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')
+        self.assertEqual(
+            is_asyncio_reactor_installed(), self.reactor_pytest == "asyncio"
+        )
 
     def test_install_asyncio_reactor(self):
+        from twisted.internet import reactor as original_reactor
+
         with warnings.catch_warnings(record=True) as w:
             install_reactor("twisted.internet.asyncioreactor.AsyncioSelectorReactor")
             self.assertEqual(len(w), 0)
+        from twisted.internet import reactor
+
+        assert original_reactor == reactor
```

### Comparing `Scrapy-2.7.1/tests/test_utils_conf.py` & `Scrapy-2.8.0/tests/test_utils_conf.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,207 +1,249 @@
 import unittest
 import warnings
 
-from scrapy.exceptions import UsageError, ScrapyDeprecationWarning
+from scrapy.exceptions import ScrapyDeprecationWarning, UsageError
 from scrapy.settings import BaseSettings, Settings
 from scrapy.utils.conf import (
     arglist_to_dict,
     build_component_list,
     feed_complete_default_values_from_settings,
-    feed_process_params_from_cli
+    feed_process_params_from_cli,
 )
 
 
 class BuildComponentListTest(unittest.TestCase):
-
     def test_build_dict(self):
-        d = {'one': 1, 'two': None, 'three': 8, 'four': 4}
-        self.assertEqual(build_component_list(d, convert=lambda x: x),
-                         ['one', 'four', 'three'])
+        d = {"one": 1, "two": None, "three": 8, "four": 4}
+        self.assertEqual(
+            build_component_list(d, convert=lambda x: x), ["one", "four", "three"]
+        )
 
     def test_backward_compatible_build_dict(self):
-        base = {'one': 1, 'two': 2, 'three': 3, 'five': 5, 'six': None}
-        custom = {'two': None, 'three': 8, 'four': 4}
-        self.assertEqual(build_component_list(base, custom,
-                                              convert=lambda x: x),
-                         ['one', 'four', 'five', 'three'])
+        base = {"one": 1, "two": 2, "three": 3, "five": 5, "six": None}
+        custom = {"two": None, "three": 8, "four": 4}
+        self.assertEqual(
+            build_component_list(base, custom, convert=lambda x: x),
+            ["one", "four", "five", "three"],
+        )
 
     def test_return_list(self):
-        custom = ['a', 'b', 'c']
-        self.assertEqual(build_component_list(None, custom,
-                                              convert=lambda x: x),
-                         custom)
+        custom = ["a", "b", "c"]
+        self.assertEqual(
+            build_component_list(None, custom, convert=lambda x: x), custom
+        )
 
     def test_map_dict(self):
-        custom = {'one': 1, 'two': 2, 'three': 3}
-        self.assertEqual(build_component_list({}, custom,
-                                              convert=lambda x: x.upper()),
-                         ['ONE', 'TWO', 'THREE'])
+        custom = {"one": 1, "two": 2, "three": 3}
+        self.assertEqual(
+            build_component_list({}, custom, convert=lambda x: x.upper()),
+            ["ONE", "TWO", "THREE"],
+        )
 
     def test_map_list(self):
-        custom = ['a', 'b', 'c']
-        self.assertEqual(build_component_list(None, custom,
-                                              lambda x: x.upper()),
-                         ['A', 'B', 'C'])
+        custom = ["a", "b", "c"]
+        self.assertEqual(
+            build_component_list(None, custom, lambda x: x.upper()), ["A", "B", "C"]
+        )
 
     def test_duplicate_components_in_dict(self):
-        duplicate_dict = {'one': 1, 'two': 2, 'ONE': 4}
-        self.assertRaises(ValueError, build_component_list, {}, duplicate_dict,
-                          convert=lambda x: x.lower())
+        duplicate_dict = {"one": 1, "two": 2, "ONE": 4}
+        self.assertRaises(
+            ValueError,
+            build_component_list,
+            {},
+            duplicate_dict,
+            convert=lambda x: x.lower(),
+        )
 
     def test_duplicate_components_in_list(self):
-        duplicate_list = ['a', 'b', 'a']
+        duplicate_list = ["a", "b", "a"]
         with self.assertRaises(ValueError) as cm:
             build_component_list(None, duplicate_list, convert=lambda x: x)
         self.assertIn(str(duplicate_list), str(cm.exception))
 
     def test_duplicate_components_in_basesettings(self):
         # Higher priority takes precedence
-        duplicate_bs = BaseSettings({'one': 1, 'two': 2}, priority=0)
-        duplicate_bs.set('ONE', 4, priority=10)
-        self.assertEqual(build_component_list(duplicate_bs,
-                                              convert=lambda x: x.lower()),
-                         ['two', 'one'])
-        duplicate_bs.set('one', duplicate_bs['one'], priority=20)
-        self.assertEqual(build_component_list(duplicate_bs,
-                                              convert=lambda x: x.lower()),
-                         ['one', 'two'])
+        duplicate_bs = BaseSettings({"one": 1, "two": 2}, priority=0)
+        duplicate_bs.set("ONE", 4, priority=10)
+        self.assertEqual(
+            build_component_list(duplicate_bs, convert=lambda x: x.lower()),
+            ["two", "one"],
+        )
+        duplicate_bs.set("one", duplicate_bs["one"], priority=20)
+        self.assertEqual(
+            build_component_list(duplicate_bs, convert=lambda x: x.lower()),
+            ["one", "two"],
+        )
         # Same priority raises ValueError
-        duplicate_bs.set('ONE', duplicate_bs['ONE'], priority=20)
-        self.assertRaises(ValueError, build_component_list, duplicate_bs,
-                          convert=lambda x: x.lower())
+        duplicate_bs.set("ONE", duplicate_bs["ONE"], priority=20)
+        self.assertRaises(
+            ValueError, build_component_list, duplicate_bs, convert=lambda x: x.lower()
+        )
 
     def test_valid_numbers(self):
         # work well with None and numeric values
-        d = {'a': 10, 'b': None, 'c': 15, 'd': 5.0}
-        self.assertEqual(build_component_list(d, convert=lambda x: x),
-                         ['d', 'a', 'c'])
-        d = {'a': 33333333333333333333, 'b': 11111111111111111111, 'c': 22222222222222222222}
-        self.assertEqual(build_component_list(d, convert=lambda x: x),
-                         ['b', 'c', 'a'])
+        d = {"a": 10, "b": None, "c": 15, "d": 5.0}
+        self.assertEqual(build_component_list(d, convert=lambda x: x), ["d", "a", "c"])
+        d = {
+            "a": 33333333333333333333,
+            "b": 11111111111111111111,
+            "c": 22222222222222222222,
+        }
+        self.assertEqual(build_component_list(d, convert=lambda x: x), ["b", "c", "a"])
         # raise exception for invalid values
-        d = {'one': '5'}
+        d = {"one": "5"}
         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)
-        d = {'one': '1.0'}
+        d = {"one": "1.0"}
         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)
-        d = {'one': [1, 2, 3]}
+        d = {"one": [1, 2, 3]}
         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)
-        d = {'one': {'a': 'a', 'b': 2}}
+        d = {"one": {"a": "a", "b": 2}}
         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)
-        d = {'one': 'lorem ipsum'}
+        d = {"one": "lorem ipsum"}
         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)
 
 
 class UtilsConfTestCase(unittest.TestCase):
-
     def test_arglist_to_dict(self):
         self.assertEqual(
-            arglist_to_dict(['arg1=val1', 'arg2=val2']),
-            {'arg1': 'val1', 'arg2': 'val2'})
+            arglist_to_dict(["arg1=val1", "arg2=val2"]),
+            {"arg1": "val1", "arg2": "val2"},
+        )
 
 
 class FeedExportConfigTestCase(unittest.TestCase):
-
     def test_feed_export_config_invalid_format(self):
         settings = Settings()
-        self.assertRaises(UsageError, feed_process_params_from_cli, settings, ['items.dat'], 'noformat')
+        self.assertRaises(
+            UsageError,
+            feed_process_params_from_cli,
+            settings,
+            ["items.dat"],
+            "noformat",
+        )
 
     def test_feed_export_config_mismatch(self):
         settings = Settings()
         self.assertRaises(
             UsageError,
-            feed_process_params_from_cli, settings, ['items1.dat', 'items2.dat'], 'noformat'
+            feed_process_params_from_cli,
+            settings,
+            ["items1.dat", "items2.dat"],
+            "noformat",
         )
 
     def test_feed_export_config_backward_compatible(self):
         with warnings.catch_warnings(record=True) as cw:
             settings = Settings()
             self.assertEqual(
-                {'items.dat': {'format': 'csv'}},
-                feed_process_params_from_cli(settings, ['items.dat'], 'csv')
+                {"items.dat": {"format": "csv"}},
+                feed_process_params_from_cli(settings, ["items.dat"], "csv"),
             )
             self.assertEqual(cw[0].category, ScrapyDeprecationWarning)
 
     def test_feed_export_config_explicit_formats(self):
         settings = Settings()
         self.assertEqual(
-            {'items_1.dat': {'format': 'json'}, 'items_2.dat': {'format': 'xml'}, 'items_3.dat': {'format': 'csv'}},
-            feed_process_params_from_cli(settings, ['items_1.dat:json', 'items_2.dat:xml', 'items_3.dat:csv'])
+            {
+                "items_1.dat": {"format": "json"},
+                "items_2.dat": {"format": "xml"},
+                "items_3.dat": {"format": "csv"},
+            },
+            feed_process_params_from_cli(
+                settings, ["items_1.dat:json", "items_2.dat:xml", "items_3.dat:csv"]
+            ),
         )
 
     def test_feed_export_config_implicit_formats(self):
         settings = Settings()
         self.assertEqual(
-            {'items_1.json': {'format': 'json'}, 'items_2.xml': {'format': 'xml'}, 'items_3.csv': {'format': 'csv'}},
-            feed_process_params_from_cli(settings, ['items_1.json', 'items_2.xml', 'items_3.csv'])
+            {
+                "items_1.json": {"format": "json"},
+                "items_2.xml": {"format": "xml"},
+                "items_3.csv": {"format": "csv"},
+            },
+            feed_process_params_from_cli(
+                settings, ["items_1.json", "items_2.xml", "items_3.csv"]
+            ),
         )
 
     def test_feed_export_config_stdout(self):
         settings = Settings()
         self.assertEqual(
-            {'stdout:': {'format': 'pickle'}},
-            feed_process_params_from_cli(settings, ['-:pickle'])
+            {"stdout:": {"format": "pickle"}},
+            feed_process_params_from_cli(settings, ["-:pickle"]),
         )
 
     def test_feed_export_config_overwrite(self):
         settings = Settings()
         self.assertEqual(
-            {'output.json': {'format': 'json', 'overwrite': True}},
-            feed_process_params_from_cli(settings, [], None, ['output.json'])
+            {"output.json": {"format": "json", "overwrite": True}},
+            feed_process_params_from_cli(settings, [], None, ["output.json"]),
         )
 
     def test_output_and_overwrite_output(self):
         with self.assertRaises(UsageError):
             feed_process_params_from_cli(
                 Settings(),
-                ['output1.json'],
+                ["output1.json"],
                 None,
-                ['output2.json'],
+                ["output2.json"],
             )
 
     def test_feed_complete_default_values_from_settings_empty(self):
         feed = {}
-        settings = Settings({
-            "FEED_EXPORT_ENCODING": "custom encoding",
-            "FEED_EXPORT_FIELDS": ["f1", "f2", "f3"],
-            "FEED_EXPORT_INDENT": 42,
-            "FEED_STORE_EMPTY": True,
-            "FEED_URI_PARAMS": (1, 2, 3, 4),
-            "FEED_EXPORT_BATCH_ITEM_COUNT": 2,
-        })
+        settings = Settings(
+            {
+                "FEED_EXPORT_ENCODING": "custom encoding",
+                "FEED_EXPORT_FIELDS": ["f1", "f2", "f3"],
+                "FEED_EXPORT_INDENT": 42,
+                "FEED_STORE_EMPTY": True,
+                "FEED_URI_PARAMS": (1, 2, 3, 4),
+                "FEED_EXPORT_BATCH_ITEM_COUNT": 2,
+            }
+        )
         new_feed = feed_complete_default_values_from_settings(feed, settings)
-        self.assertEqual(new_feed, {
-            "encoding": "custom encoding",
-            "fields": ["f1", "f2", "f3"],
-            "indent": 42,
-            "store_empty": True,
-            "uri_params": (1, 2, 3, 4),
-            "batch_item_count": 2,
-            "item_export_kwargs": {},
-        })
+        self.assertEqual(
+            new_feed,
+            {
+                "encoding": "custom encoding",
+                "fields": ["f1", "f2", "f3"],
+                "indent": 42,
+                "store_empty": True,
+                "uri_params": (1, 2, 3, 4),
+                "batch_item_count": 2,
+                "item_export_kwargs": {},
+            },
+        )
 
     def test_feed_complete_default_values_from_settings_non_empty(self):
         feed = {
             "encoding": "other encoding",
             "fields": None,
         }
-        settings = Settings({
-            "FEED_EXPORT_ENCODING": "custom encoding",
-            "FEED_EXPORT_FIELDS": ["f1", "f2", "f3"],
-            "FEED_EXPORT_INDENT": 42,
-            "FEED_STORE_EMPTY": True,
-            "FEED_EXPORT_BATCH_ITEM_COUNT": 2,
-        })
+        settings = Settings(
+            {
+                "FEED_EXPORT_ENCODING": "custom encoding",
+                "FEED_EXPORT_FIELDS": ["f1", "f2", "f3"],
+                "FEED_EXPORT_INDENT": 42,
+                "FEED_STORE_EMPTY": True,
+                "FEED_EXPORT_BATCH_ITEM_COUNT": 2,
+            }
+        )
         new_feed = feed_complete_default_values_from_settings(feed, settings)
-        self.assertEqual(new_feed, {
-            "encoding": "other encoding",
-            "fields": None,
-            "indent": 42,
-            "store_empty": True,
-            "uri_params": None,
-            "batch_item_count": 2,
-            "item_export_kwargs": {},
-        })
+        self.assertEqual(
+            new_feed,
+            {
+                "encoding": "other encoding",
+                "fields": None,
+                "indent": 42,
+                "store_empty": True,
+                "uri_params": None,
+                "batch_item_count": 2,
+                "item_export_kwargs": {},
+            },
+        )
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_utils_console.py` & `Scrapy-2.8.0/tests/test_utils_console.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,45 +1,47 @@
 import unittest
 
 from scrapy.utils.console import get_shell_embed_func
+
 try:
     import bpython
+
     bpy = True
     del bpython
 except ImportError:
     bpy = False
 try:
     import IPython
+
     ipy = True
     del IPython
 except ImportError:
     ipy = False
 
 
 class UtilsConsoleTestCase(unittest.TestCase):
-
     def test_get_shell_embed_func(self):
 
-        shell = get_shell_embed_func(['invalid'])
+        shell = get_shell_embed_func(["invalid"])
         self.assertEqual(shell, None)
 
-        shell = get_shell_embed_func(['invalid', 'python'])
+        shell = get_shell_embed_func(["invalid", "python"])
         self.assertTrue(callable(shell))
-        self.assertEqual(shell.__name__, '_embed_standard_shell')
+        self.assertEqual(shell.__name__, "_embed_standard_shell")
 
-    @unittest.skipIf(not bpy, 'bpython not available in testenv')
+    @unittest.skipIf(not bpy, "bpython not available in testenv")
     def test_get_shell_embed_func2(self):
 
-        shell = get_shell_embed_func(['bpython'])
+        shell = get_shell_embed_func(["bpython"])
         self.assertTrue(callable(shell))
-        self.assertEqual(shell.__name__, '_embed_bpython_shell')
+        self.assertEqual(shell.__name__, "_embed_bpython_shell")
 
-    @unittest.skipIf(not ipy, 'IPython not available in testenv')
+    @unittest.skipIf(not ipy, "IPython not available in testenv")
     def test_get_shell_embed_func3(self):
 
         # default shell should be 'ipython'
         shell = get_shell_embed_func()
-        self.assertEqual(shell.__name__, '_embed_ipython_shell')
+        self.assertEqual(shell.__name__, "_embed_ipython_shell")
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_utils_curl.py` & `Scrapy-2.8.0/tests/test_utils_curl.py`

 * *Files 6% similar despite different names*

```diff
@@ -30,18 +30,15 @@
 
     def test_get_basic_auth(self):
         curl_command = 'curl "https://api.test.com/" -u "some_username:some_password"'
         expected_result = {
             "method": "GET",
             "url": "https://api.test.com/",
             "headers": [
-                (
-                    "Authorization",
-                    basic_auth_header("some_username", "some_password")
-                )
+                ("Authorization", basic_auth_header("some_username", "some_password"))
             ],
         }
         self._test_command(curl_command, expected_result)
 
     def test_get_complex(self):
         curl_command = (
             "curl 'http://httpbin.org/get' -H 'Accept-Encoding: gzip, deflate'"
@@ -73,19 +70,19 @@
                     "text/html,application/xhtml+xml,application/xml;q=0.9,ima"
                     "ge/webp,image/apng,*/*;q=0.8",
                 ),
                 ("Referer", "http://httpbin.org/"),
                 ("Connection", "keep-alive"),
             ],
             "cookies": {
-                '_gauges_unique_year': '1',
-                '_gauges_unique_hour': '1',
-                '_gauges_unique_day': '1',
-                '_gauges_unique': '1',
-                '_gauges_unique_month': '1'
+                "_gauges_unique_year": "1",
+                "_gauges_unique_hour": "1",
+                "_gauges_unique_day": "1",
+                "_gauges_unique": "1",
+                "_gauges_unique_month": "1",
             },
         }
         self._test_command(curl_command, expected_result)
 
     def test_post(self):
         curl_command = (
             "curl 'http://httpbin.org/post' -X POST -H 'Cookie: _gauges_unique"
@@ -103,22 +100,22 @@
             "40example.org&size=small&topping=cheese&topping=onion&delivery=12"
             "%3A15&comments=' --compressed"
         )
         expected_result = {
             "method": "POST",
             "url": "http://httpbin.org/post",
             "body": "custname=John+Smith&custtel=500&custemail=jsmith%40exampl"
-                    "e.org&size=small&topping=cheese&topping=onion&delivery=12"
-                    "%3A15&comments=",
+            "e.org&size=small&topping=cheese&topping=onion&delivery=12"
+            "%3A15&comments=",
             "cookies": {
-                '_gauges_unique_year': '1',
-                '_gauges_unique_hour': '1',
-                '_gauges_unique_day': '1',
-                '_gauges_unique': '1',
-                '_gauges_unique_month': '1'
+                "_gauges_unique_year": "1",
+                "_gauges_unique_hour": "1",
+                "_gauges_unique_day": "1",
+                "_gauges_unique": "1",
+                "_gauges_unique_month": "1",
             },
             "headers": [
                 ("Origin", "http://httpbin.org"),
                 ("Accept-Encoding", "gzip, deflate"),
                 ("Accept-Language", "en-US,en;q=0.9,ru;q=0.8,es;q=0.7"),
                 ("Upgrade-Insecure-Requests", "1"),
                 (
@@ -148,24 +145,25 @@
         )
         expected_result = {
             "method": "POST",
             "url": "https://www.example.org/",
             "body": (
                 "excerptLength=200&enableDidYouMean=true&sortCriteria=ffirstz3"
                 "2xnamez32x201740686%20ascending&queryFunctions=%5B%5D&ranking"
-                "Functions=%5B%5D")
+                "Functions=%5B%5D"
+            ),
         }
         self._test_command(curl_command, expected_result)
 
     def test_explicit_get_with_data(self):
-        curl_command = 'curl httpbin.org/anything -X GET --data asdf'
+        curl_command = "curl httpbin.org/anything -X GET --data asdf"
         expected_result = {
             "method": "GET",
             "url": "http://httpbin.org/anything",
-            "body": "asdf"
+            "body": "asdf",
         }
         self._test_command(curl_command, expected_result)
 
     def test_patch(self):
         curl_command = (
             'curl "https://example.com/api/fake" -u "username:password" -H "Ac'
             'cept: application/vnd.go.cd.v4+json" -H "Content-Type: applicatio'
@@ -178,24 +176,22 @@
             "url": "https://example.com/api/fake",
             "headers": [
                 ("Accept", "application/vnd.go.cd.v4+json"),
                 ("Content-Type", "application/json"),
                 ("Authorization", basic_auth_header("username", "password")),
             ],
             "body": '{"hostname": "agent02.example.com",  "agent_config_state"'
-                    ': "Enabled", "resources": ["Java","Linux"], "environments'
-                    '": ["Dev"]}',
+            ': "Enabled", "resources": ["Java","Linux"], "environments'
+            '": ["Dev"]}',
         }
         self._test_command(curl_command, expected_result)
 
     def test_delete(self):
         curl_command = 'curl -X "DELETE" https://www.url.com/page'
-        expected_result = {
-            "method": "DELETE", "url": "https://www.url.com/page"
-        }
+        expected_result = {"method": "DELETE", "url": "https://www.url.com/page"}
         self._test_command(curl_command, expected_result)
 
     def test_get_silent(self):
         curl_command = 'curl --silent "www.example.com"'
         expected_result = {"method": "GET", "url": "http://www.example.com"}
         self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)
 
@@ -205,27 +201,26 @@
             r"too few arguments|the following arguments are required:\s*url",
             lambda: curl_to_request_kwargs("curl"),
         )
 
     def test_ignore_unknown_options(self):
         # case 1: ignore_unknown_options=True:
         with warnings.catch_warnings():  # avoid warning when executing tests
-            warnings.simplefilter('ignore')
-            curl_command = 'curl --bar --baz http://www.example.com'
+            warnings.simplefilter("ignore")
+            curl_command = "curl --bar --baz http://www.example.com"
             expected_result = {"method": "GET", "url": "http://www.example.com"}
             self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)
 
         # case 2: ignore_unknown_options=False (raise exception):
         self.assertRaisesRegex(
             ValueError,
             "Unrecognized options:.*--bar.*--baz",
             lambda: curl_to_request_kwargs(
-                "curl --bar --baz http://www.example.com",
-                ignore_unknown_options=False
+                "curl --bar --baz http://www.example.com", ignore_unknown_options=False
             ),
         )
 
     def test_must_start_with_curl_error(self):
         self.assertRaises(
             ValueError,
-            lambda: curl_to_request_kwargs("carl -X POST http://example.org")
+            lambda: curl_to_request_kwargs("carl -X POST http://example.org"),
         )
```

### Comparing `Scrapy-2.7.1/tests/test_utils_datatypes.py` & `Scrapy-2.8.0/tests/test_utils_datatypes.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,32 +1,35 @@
 import copy
 import unittest
 from collections.abc import Mapping, MutableMapping
 
 from scrapy.http import Request
-from scrapy.utils.datatypes import CaselessDict, LocalCache, LocalWeakReferencedCache, SequenceExclude
+from scrapy.utils.datatypes import (
+    CaselessDict,
+    LocalCache,
+    LocalWeakReferencedCache,
+    SequenceExclude,
+)
 from scrapy.utils.python import garbage_collect
 
-
-__doctests__ = ['scrapy.utils.datatypes']
+__doctests__ = ["scrapy.utils.datatypes"]
 
 
 class CaselessDictTest(unittest.TestCase):
-
     def test_init_dict(self):
-        seq = {'red': 1, 'black': 3}
+        seq = {"red": 1, "black": 3}
         d = CaselessDict(seq)
-        self.assertEqual(d['red'], 1)
-        self.assertEqual(d['black'], 3)
+        self.assertEqual(d["red"], 1)
+        self.assertEqual(d["black"], 3)
 
     def test_init_pair_sequence(self):
-        seq = (('red', 1), ('black', 3))
+        seq = (("red", 1), ("black", 3))
         d = CaselessDict(seq)
-        self.assertEqual(d['red'], 1)
-        self.assertEqual(d['black'], 3)
+        self.assertEqual(d["red"], 1)
+        self.assertEqual(d["black"], 3)
 
     def test_init_mapping(self):
         class MyMapping(Mapping):
             def __init__(self, **kwargs):
                 self._d = kwargs
 
             def __getitem__(self, key):
@@ -36,16 +39,16 @@
                 return iter(self._d)
 
             def __len__(self):
                 return len(self._d)
 
         seq = MyMapping(red=1, black=3)
         d = CaselessDict(seq)
-        self.assertEqual(d['red'], 1)
-        self.assertEqual(d['black'], 3)
+        self.assertEqual(d["red"], 1)
+        self.assertEqual(d["black"], 3)
 
     def test_init_mutable_mapping(self):
         class MyMutableMapping(MutableMapping):
             def __init__(self, **kwargs):
                 self._d = kwargs
 
             def __getitem__(self, key):
@@ -61,129 +64,128 @@
                 return iter(self._d)
 
             def __len__(self):
                 return len(self._d)
 
         seq = MyMutableMapping(red=1, black=3)
         d = CaselessDict(seq)
-        self.assertEqual(d['red'], 1)
-        self.assertEqual(d['black'], 3)
+        self.assertEqual(d["red"], 1)
+        self.assertEqual(d["black"], 3)
 
     def test_caseless(self):
         d = CaselessDict()
-        d['key_Lower'] = 1
-        self.assertEqual(d['KEy_loWer'], 1)
-        self.assertEqual(d.get('KEy_loWer'), 1)
-
-        d['KEY_LOWER'] = 3
-        self.assertEqual(d['key_Lower'], 3)
-        self.assertEqual(d.get('key_Lower'), 3)
+        d["key_Lower"] = 1
+        self.assertEqual(d["KEy_loWer"], 1)
+        self.assertEqual(d.get("KEy_loWer"), 1)
+
+        d["KEY_LOWER"] = 3
+        self.assertEqual(d["key_Lower"], 3)
+        self.assertEqual(d.get("key_Lower"), 3)
 
     def test_delete(self):
-        d = CaselessDict({'key_lower': 1})
-        del d['key_LOWER']
-        self.assertRaises(KeyError, d.__getitem__, 'key_LOWER')
-        self.assertRaises(KeyError, d.__getitem__, 'key_lower')
+        d = CaselessDict({"key_lower": 1})
+        del d["key_LOWER"]
+        self.assertRaises(KeyError, d.__getitem__, "key_LOWER")
+        self.assertRaises(KeyError, d.__getitem__, "key_lower")
 
     def test_getdefault(self):
         d = CaselessDict()
-        self.assertEqual(d.get('c', 5), 5)
-        d['c'] = 10
-        self.assertEqual(d.get('c', 5), 10)
+        self.assertEqual(d.get("c", 5), 5)
+        d["c"] = 10
+        self.assertEqual(d.get("c", 5), 10)
 
     def test_setdefault(self):
-        d = CaselessDict({'a': 1, 'b': 2})
+        d = CaselessDict({"a": 1, "b": 2})
 
-        r = d.setdefault('A', 5)
+        r = d.setdefault("A", 5)
         self.assertEqual(r, 1)
-        self.assertEqual(d['A'], 1)
+        self.assertEqual(d["A"], 1)
 
-        r = d.setdefault('c', 5)
+        r = d.setdefault("c", 5)
         self.assertEqual(r, 5)
-        self.assertEqual(d['C'], 5)
+        self.assertEqual(d["C"], 5)
 
     def test_fromkeys(self):
-        keys = ('a', 'b')
+        keys = ("a", "b")
 
         d = CaselessDict.fromkeys(keys)
-        self.assertEqual(d['A'], None)
-        self.assertEqual(d['B'], None)
+        self.assertEqual(d["A"], None)
+        self.assertEqual(d["B"], None)
 
         d = CaselessDict.fromkeys(keys, 1)
-        self.assertEqual(d['A'], 1)
-        self.assertEqual(d['B'], 1)
+        self.assertEqual(d["A"], 1)
+        self.assertEqual(d["B"], 1)
 
         instance = CaselessDict()
         d = instance.fromkeys(keys)
-        self.assertEqual(d['A'], None)
-        self.assertEqual(d['B'], None)
+        self.assertEqual(d["A"], None)
+        self.assertEqual(d["B"], None)
 
         d = instance.fromkeys(keys, 1)
-        self.assertEqual(d['A'], 1)
-        self.assertEqual(d['B'], 1)
+        self.assertEqual(d["A"], 1)
+        self.assertEqual(d["B"], 1)
 
     def test_contains(self):
         d = CaselessDict()
-        d['a'] = 1
-        assert 'a' in d
+        d["a"] = 1
+        assert "a" in d
 
     def test_pop(self):
         d = CaselessDict()
-        d['a'] = 1
-        self.assertEqual(d.pop('A'), 1)
-        self.assertRaises(KeyError, d.pop, 'A')
+        d["a"] = 1
+        self.assertEqual(d.pop("A"), 1)
+        self.assertRaises(KeyError, d.pop, "A")
 
     def test_normkey(self):
         class MyDict(CaselessDict):
             def normkey(self, key):
                 return key.title()
 
         d = MyDict()
-        d['key-one'] = 2
-        self.assertEqual(list(d.keys()), ['Key-One'])
+        d["key-one"] = 2
+        self.assertEqual(list(d.keys()), ["Key-One"])
 
     def test_normvalue(self):
         class MyDict(CaselessDict):
             def normvalue(self, value):
                 if value is not None:
                     return value + 1
 
-        d = MyDict({'key': 1})
-        self.assertEqual(d['key'], 2)
-        self.assertEqual(d.get('key'), 2)
+        d = MyDict({"key": 1})
+        self.assertEqual(d["key"], 2)
+        self.assertEqual(d.get("key"), 2)
 
         d = MyDict()
-        d['key'] = 1
-        self.assertEqual(d['key'], 2)
-        self.assertEqual(d.get('key'), 2)
+        d["key"] = 1
+        self.assertEqual(d["key"], 2)
+        self.assertEqual(d.get("key"), 2)
 
         d = MyDict()
-        d.setdefault('key', 1)
-        self.assertEqual(d['key'], 2)
-        self.assertEqual(d.get('key'), 2)
+        d.setdefault("key", 1)
+        self.assertEqual(d["key"], 2)
+        self.assertEqual(d.get("key"), 2)
 
         d = MyDict()
-        d.update({'key': 1})
-        self.assertEqual(d['key'], 2)
-        self.assertEqual(d.get('key'), 2)
-
-        d = MyDict.fromkeys(('key',), 1)
-        self.assertEqual(d['key'], 2)
-        self.assertEqual(d.get('key'), 2)
+        d.update({"key": 1})
+        self.assertEqual(d["key"], 2)
+        self.assertEqual(d.get("key"), 2)
+
+        d = MyDict.fromkeys(("key",), 1)
+        self.assertEqual(d["key"], 2)
+        self.assertEqual(d.get("key"), 2)
 
     def test_copy(self):
-        h1 = CaselessDict({'header1': 'value'})
+        h1 = CaselessDict({"header1": "value"})
         h2 = copy.copy(h1)
         self.assertEqual(h1, h2)
-        self.assertEqual(h1.get('header1'), h2.get('header1'))
+        self.assertEqual(h1.get("header1"), h2.get("header1"))
         assert isinstance(h2, CaselessDict)
 
 
 class SequenceExcludeTest(unittest.TestCase):
-
     def test_list(self):
         seq = [1, 2, 3]
         d = SequenceExclude(seq)
         self.assertIn(0, d)
         self.assertIn(4, d)
         self.assertNotIn(2, d)
 
@@ -222,52 +224,50 @@
         self.assertIn(0, d)
         self.assertIn("foo", d)
         self.assertIn(3.14, d)
         self.assertIn(set("bar"), d)
 
         # supplied sequence is a set, so checking for list (non)inclusion fails
         self.assertRaises(TypeError, (0, 1, 2) in d)
-        self.assertRaises(TypeError, d.__contains__, ['a', 'b', 'c'])
+        self.assertRaises(TypeError, d.__contains__, ["a", "b", "c"])
 
         for v in [-3, "test", 1.1]:
             self.assertNotIn(v, d)
 
 
 class LocalCacheTest(unittest.TestCase):
-
     def test_cache_with_limit(self):
         cache = LocalCache(limit=2)
-        cache['a'] = 1
-        cache['b'] = 2
-        cache['c'] = 3
+        cache["a"] = 1
+        cache["b"] = 2
+        cache["c"] = 3
         self.assertEqual(len(cache), 2)
-        self.assertNotIn('a', cache)
-        self.assertIn('b', cache)
-        self.assertIn('c', cache)
-        self.assertEqual(cache['b'], 2)
-        self.assertEqual(cache['c'], 3)
+        self.assertNotIn("a", cache)
+        self.assertIn("b", cache)
+        self.assertIn("c", cache)
+        self.assertEqual(cache["b"], 2)
+        self.assertEqual(cache["c"], 3)
 
     def test_cache_without_limit(self):
         maximum = 10**4
         cache = LocalCache()
         for x in range(maximum):
             cache[str(x)] = x
         self.assertEqual(len(cache), maximum)
         for x in range(maximum):
             self.assertIn(str(x), cache)
             self.assertEqual(cache[str(x)], x)
 
 
 class LocalWeakReferencedCacheTest(unittest.TestCase):
-
     def test_cache_with_limit(self):
         cache = LocalWeakReferencedCache(limit=2)
-        r1 = Request('https://example.org')
-        r2 = Request('https://example.com')
-        r3 = Request('https://example.net')
+        r1 = Request("https://example.org")
+        r2 = Request("https://example.com")
+        r3 = Request("https://example.net")
         cache[r1] = 1
         cache[r2] = 2
         cache[r3] = 3
         self.assertEqual(len(cache), 2)
         self.assertNotIn(r1, cache)
         self.assertIn(r2, cache)
         self.assertIn(r3, cache)
@@ -295,15 +295,15 @@
         self.assertEqual(len(cache), 0)
 
     def test_cache_without_limit(self):
         max = 10**4
         cache = LocalWeakReferencedCache()
         refs = []
         for x in range(max):
-            refs.append(Request(f'https://example.org/{x}'))
+            refs.append(Request(f"https://example.org/{x}"))
             cache[refs[-1]] = x
         self.assertEqual(len(cache), max)
         for i, r in enumerate(refs):
             self.assertIn(r, cache)
             self.assertEqual(cache[r], i)
         del r  # delete reference to the last object in the list
```

### Comparing `Scrapy-2.7.1/tests/test_utils_defer.py` & `Scrapy-2.8.0/tests/test_utils_defer.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import random
 
 from pytest import mark
-from twisted.trial import unittest
-from twisted.internet import reactor, defer
+from twisted.internet import defer, reactor
 from twisted.python.failure import Failure
+from twisted.trial import unittest
 
-from scrapy.utils.asyncgen import collect_asyncgen, as_async_generator
+from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen
 from scrapy.utils.defer import (
     aiter_errback,
     deferred_f_from_coro_f,
     iter_errback,
     maybe_deferred_to_future,
     mustbe_deferred,
     parallel_async,
@@ -64,49 +64,51 @@
 
 
 def eb1(failure, arg1, arg2):
     return f"(eb1 {failure.value.__class__.__name__} {arg1} {arg2})"
 
 
 class DeferUtilsTest(unittest.TestCase):
-
     @defer.inlineCallbacks
     def test_process_chain(self):
-        x = yield process_chain([cb1, cb2, cb3], 'res', 'v1', 'v2')
+        x = yield process_chain([cb1, cb2, cb3], "res", "v1", "v2")
         self.assertEqual(x, "(cb3 (cb2 (cb1 res v1 v2) v1 v2) v1 v2)")
 
         gotexc = False
         try:
-            yield process_chain([cb1, cb_fail, cb3], 'res', 'v1', 'v2')
+            yield process_chain([cb1, cb_fail, cb3], "res", "v1", "v2")
         except TypeError:
             gotexc = True
         self.assertTrue(gotexc)
 
     @defer.inlineCallbacks
     def test_process_chain_both(self):
-        x = yield process_chain_both([cb_fail, cb2, cb3], [None, eb1, None], 'res', 'v1', 'v2')
+        x = yield process_chain_both(
+            [cb_fail, cb2, cb3], [None, eb1, None], "res", "v1", "v2"
+        )
         self.assertEqual(x, "(cb3 (eb1 TypeError v1 v2) v1 v2)")
 
         fail = Failure(ZeroDivisionError())
-        x = yield process_chain_both([eb1, cb2, cb3], [eb1, None, None], fail, 'v1', 'v2')
+        x = yield process_chain_both(
+            [eb1, cb2, cb3], [eb1, None, None], fail, "v1", "v2"
+        )
         self.assertEqual(x, "(cb3 (cb2 (eb1 ZeroDivisionError v1 v2) v1 v2) v1 v2)")
 
     @defer.inlineCallbacks
     def test_process_parallel(self):
-        x = yield process_parallel([cb1, cb2, cb3], 'res', 'v1', 'v2')
-        self.assertEqual(x, ['(cb1 res v1 v2)', '(cb2 res v1 v2)', '(cb3 res v1 v2)'])
+        x = yield process_parallel([cb1, cb2, cb3], "res", "v1", "v2")
+        self.assertEqual(x, ["(cb1 res v1 v2)", "(cb2 res v1 v2)", "(cb3 res v1 v2)"])
 
     def test_process_parallel_failure(self):
-        d = process_parallel([cb1, cb_fail, cb3], 'res', 'v1', 'v2')
+        d = process_parallel([cb1, cb_fail, cb3], "res", "v1", "v2")
         self.failUnlessFailure(d, TypeError)
         return d
 
 
 class IterErrbackTest(unittest.TestCase):
-
     def test_iter_errback_good(self):
         def itergood():
             for x in range(10):
                 yield x
 
         errors = []
         out = list(iter_errback(itergood(), errors.append))
@@ -124,15 +126,14 @@
         out = list(iter_errback(iterbad(), errors.append))
         self.assertEqual(out, [0, 1, 2, 3, 4])
         self.assertEqual(len(errors), 1)
         self.assertIsInstance(errors[0].value, ZeroDivisionError)
 
 
 class AiterErrbackTest(unittest.TestCase):
-
     @deferred_f_from_coro_f
     async def test_aiter_errback_good(self):
         async def itergood():
             for x in range(10):
                 yield x
 
         errors = []
@@ -167,39 +168,39 @@
     @mark.xfail(reason="Checks that the test is actually executed", strict=True)
     @deferred_f_from_coro_f
     async def test_deferred_f_from_coro_f_xfail(self):
         raise Exception("This is expected to be raised")
 
 
 class AsyncCooperatorTest(unittest.TestCase):
-    """ This tests _AsyncCooperatorAdapter by testing parallel_async which is its only usage.
+    """This tests _AsyncCooperatorAdapter by testing parallel_async which is its only usage.
 
     parallel_async is called with the results of a callback (so an iterable of items, requests and None,
     with arbitrary delays between values), and it uses Scraper._process_spidermw_output as the callable
     (so a callable that returns a Deferred for an item, which will fire after pipelines process it, and
     None for everything else). The concurrent task count is the CONCURRENT_ITEMS setting.
 
     We want to test different concurrency values compared to the iterable length.
     We also want to simulate the real usage, with arbitrary delays between getting the values
     from the iterable. We also want to simulate sync and async results from the callable.
     """
+
     CONCURRENT_ITEMS = 50
 
     @staticmethod
     def callable(o, results):
         if random.random() < 0.4:
             # simulate async processing
             dfd = defer.Deferred()
             dfd.addCallback(lambda _: results.append(o))
             delay = random.random() / 8
             reactor.callLater(delay, dfd.callback, None)
             return dfd
-        else:
-            # simulate trivial sync processing
-            results.append(o)
+        # simulate trivial sync processing
+        results.append(o)
 
     @staticmethod
     def get_async_iterable(length):
         # simulate a simple callback without delays between results
         return as_async_generator(range(length))
 
     @staticmethod
```

### Comparing `Scrapy-2.7.1/tests/test_utils_deprecate.py` & `Scrapy-2.8.0/tests/test_utils_deprecate.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 import inspect
 import unittest
-from unittest import mock
 import warnings
+from unittest import mock
+
 from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.utils.deprecate import create_deprecated_class, update_classpath
 
 
 class MyWarning(UserWarning):
     pass
 
@@ -15,136 +16,145 @@
 
 
 class NewName(SomeBaseClass):
     pass
 
 
 class WarnWhenSubclassedTest(unittest.TestCase):
-
     def _mywarnings(self, w, category=MyWarning):
         return [x for x in w if x.category is MyWarning]
 
     def test_no_warning_on_definition(self):
         with warnings.catch_warnings(record=True) as w:
-            create_deprecated_class('Deprecated', NewName)
+            create_deprecated_class("Deprecated", NewName)
 
         w = self._mywarnings(w)
         self.assertEqual(w, [])
 
     def test_subclassing_warning_message(self):
-        Deprecated = create_deprecated_class('Deprecated', NewName,
-                                             warn_category=MyWarning)
+        Deprecated = create_deprecated_class(
+            "Deprecated", NewName, warn_category=MyWarning
+        )
 
         with warnings.catch_warnings(record=True) as w:
+
             class UserClass(Deprecated):
                 pass
 
         w = self._mywarnings(w)
         self.assertEqual(len(w), 1)
         self.assertEqual(
             str(w[0].message),
             "tests.test_utils_deprecate.UserClass inherits from "
             "deprecated class tests.test_utils_deprecate.Deprecated, "
             "please inherit from tests.test_utils_deprecate.NewName."
-            " (warning only on first subclass, there may be others)"
+            " (warning only on first subclass, there may be others)",
         )
         self.assertEqual(w[0].lineno, inspect.getsourcelines(UserClass)[1])
 
     def test_custom_class_paths(self):
-        Deprecated = create_deprecated_class('Deprecated', NewName,
-                                             new_class_path='foo.NewClass',
-                                             old_class_path='bar.OldClass',
-                                             warn_category=MyWarning)
+        Deprecated = create_deprecated_class(
+            "Deprecated",
+            NewName,
+            new_class_path="foo.NewClass",
+            old_class_path="bar.OldClass",
+            warn_category=MyWarning,
+        )
 
         with warnings.catch_warnings(record=True) as w:
+
             class UserClass(Deprecated):
                 pass
 
             _ = Deprecated()
 
         w = self._mywarnings(w)
         self.assertEqual(len(w), 2)
-        self.assertIn('foo.NewClass', str(w[0].message))
-        self.assertIn('bar.OldClass', str(w[0].message))
-        self.assertIn('foo.NewClass', str(w[1].message))
-        self.assertIn('bar.OldClass', str(w[1].message))
-
-    def test_subclassing_warns_only_on_direct_childs(self):
-        Deprecated = create_deprecated_class('Deprecated', NewName,
-                                             warn_once=False,
-                                             warn_category=MyWarning)
+        self.assertIn("foo.NewClass", str(w[0].message))
+        self.assertIn("bar.OldClass", str(w[0].message))
+        self.assertIn("foo.NewClass", str(w[1].message))
+        self.assertIn("bar.OldClass", str(w[1].message))
+
+    def test_subclassing_warns_only_on_direct_children(self):
+        Deprecated = create_deprecated_class(
+            "Deprecated", NewName, warn_once=False, warn_category=MyWarning
+        )
 
         with warnings.catch_warnings(record=True) as w:
+
             class UserClass(Deprecated):
                 pass
 
             class NoWarnOnMe(UserClass):
                 pass
 
         w = self._mywarnings(w)
         self.assertEqual(len(w), 1)
-        self.assertIn('UserClass', str(w[0].message))
+        self.assertIn("UserClass", str(w[0].message))
 
     def test_subclassing_warns_once_by_default(self):
-        Deprecated = create_deprecated_class('Deprecated', NewName,
-                                             warn_category=MyWarning)
+        Deprecated = create_deprecated_class(
+            "Deprecated", NewName, warn_category=MyWarning
+        )
 
         with warnings.catch_warnings(record=True) as w:
+
             class UserClass(Deprecated):
                 pass
 
             class FooClass(Deprecated):
                 pass
 
             class BarClass(Deprecated):
                 pass
 
         w = self._mywarnings(w)
         self.assertEqual(len(w), 1)
-        self.assertIn('UserClass', str(w[0].message))
+        self.assertIn("UserClass", str(w[0].message))
 
     def test_warning_on_instance(self):
-        Deprecated = create_deprecated_class('Deprecated', NewName,
-                                             warn_category=MyWarning)
+        Deprecated = create_deprecated_class(
+            "Deprecated", NewName, warn_category=MyWarning
+        )
 
         # ignore subclassing warnings
         with warnings.catch_warnings():
-            warnings.simplefilter('ignore', MyWarning)
+            warnings.simplefilter("ignore", MyWarning)
 
             class UserClass(Deprecated):
                 pass
 
         with warnings.catch_warnings(record=True) as w:
             _, lineno = Deprecated(), inspect.getlineno(inspect.currentframe())
             _ = UserClass()  # subclass instances don't warn
 
         w = self._mywarnings(w)
         self.assertEqual(len(w), 1)
         self.assertEqual(
             str(w[0].message),
             "tests.test_utils_deprecate.Deprecated is deprecated, "
-            "instantiate tests.test_utils_deprecate.NewName instead."
+            "instantiate tests.test_utils_deprecate.NewName instead.",
         )
         self.assertEqual(w[0].lineno, lineno)
 
     def test_warning_auto_message(self):
         with warnings.catch_warnings(record=True) as w:
-            Deprecated = create_deprecated_class('Deprecated', NewName)
+            Deprecated = create_deprecated_class("Deprecated", NewName)
 
             class UserClass2(Deprecated):
                 pass
 
         msg = str(w[0].message)
         self.assertIn("tests.test_utils_deprecate.NewName", msg)
         self.assertIn("tests.test_utils_deprecate.Deprecated", msg)
 
     def test_issubclass(self):
         with warnings.catch_warnings():
-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)
-            DeprecatedName = create_deprecated_class('DeprecatedName', NewName)
+            warnings.simplefilter("ignore", ScrapyDeprecationWarning)
+            DeprecatedName = create_deprecated_class("DeprecatedName", NewName)
 
             class UpdatedUserClass1(NewName):
                 pass
 
             class UpdatedUserClass1a(NewName):
                 pass
 
@@ -171,16 +181,16 @@
         assert not issubclass(OutdatedUserClass1, OutdatedUserClass1a)
         assert not issubclass(OutdatedUserClass1a, OutdatedUserClass1)
 
         self.assertRaises(TypeError, issubclass, object(), DeprecatedName)
 
     def test_isinstance(self):
         with warnings.catch_warnings():
-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)
-            DeprecatedName = create_deprecated_class('DeprecatedName', NewName)
+            warnings.simplefilter("ignore", ScrapyDeprecationWarning)
+            DeprecatedName = create_deprecated_class("DeprecatedName", NewName)
 
             class UpdatedUserClass2(NewName):
                 pass
 
             class UpdatedUserClass2a(NewName):
                 pass
 
@@ -205,77 +215,84 @@
         assert not isinstance(OutdatedUserClass2a(), OutdatedUserClass2)
         assert not isinstance(OutdatedUserClass2(), OutdatedUserClass2a)
         assert not isinstance(UnrelatedClass(), DeprecatedName)
         assert not isinstance(OldStyleClass(), DeprecatedName)
 
     def test_clsdict(self):
         with warnings.catch_warnings():
-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)
-            Deprecated = create_deprecated_class('Deprecated', NewName, {'foo': 'bar'})
+            warnings.simplefilter("ignore", ScrapyDeprecationWarning)
+            Deprecated = create_deprecated_class("Deprecated", NewName, {"foo": "bar"})
 
-        self.assertEqual(Deprecated.foo, 'bar')
+        self.assertEqual(Deprecated.foo, "bar")
 
     def test_deprecate_a_class_with_custom_metaclass(self):
-        Meta1 = type('Meta1', (type,), {})
-        New = Meta1('New', (), {})
-        create_deprecated_class('Deprecated', New)
+        Meta1 = type("Meta1", (type,), {})
+        New = Meta1("New", (), {})
+        create_deprecated_class("Deprecated", New)
 
     def test_deprecate_subclass_of_deprecated_class(self):
         with warnings.catch_warnings(record=True) as w:
-            warnings.simplefilter('always')
-            Deprecated = create_deprecated_class('Deprecated', NewName,
-                                                 warn_category=MyWarning)
-            AlsoDeprecated = create_deprecated_class('AlsoDeprecated', Deprecated,
-                                                     new_class_path='foo.Bar',
-                                                     warn_category=MyWarning)
+            warnings.simplefilter("always")
+            Deprecated = create_deprecated_class(
+                "Deprecated", NewName, warn_category=MyWarning
+            )
+            AlsoDeprecated = create_deprecated_class(
+                "AlsoDeprecated",
+                Deprecated,
+                new_class_path="foo.Bar",
+                warn_category=MyWarning,
+            )
 
         w = self._mywarnings(w)
         self.assertEqual(len(w), 0, str(map(str, w)))
 
         with warnings.catch_warnings(record=True) as w:
             AlsoDeprecated()
 
             class UserClass(AlsoDeprecated):
                 pass
 
         w = self._mywarnings(w)
         self.assertEqual(len(w), 2)
-        self.assertIn('AlsoDeprecated', str(w[0].message))
-        self.assertIn('foo.Bar', str(w[0].message))
-        self.assertIn('AlsoDeprecated', str(w[1].message))
-        self.assertIn('foo.Bar', str(w[1].message))
+        self.assertIn("AlsoDeprecated", str(w[0].message))
+        self.assertIn("foo.Bar", str(w[0].message))
+        self.assertIn("AlsoDeprecated", str(w[1].message))
+        self.assertIn("foo.Bar", str(w[1].message))
 
     def test_inspect_stack(self):
-        with mock.patch('inspect.stack', side_effect=IndexError):
+        with mock.patch("inspect.stack", side_effect=IndexError):
             with warnings.catch_warnings(record=True) as w:
-                DeprecatedName = create_deprecated_class('DeprecatedName', NewName)
+                DeprecatedName = create_deprecated_class("DeprecatedName", NewName)
 
                 class SubClass(DeprecatedName):
                     pass
 
         self.assertIn("Error detecting parent module", str(w[0].message))
 
 
-@mock.patch('scrapy.utils.deprecate.DEPRECATION_RULES',
-            [('scrapy.contrib.pipeline.', 'scrapy.pipelines.'),
-             ('scrapy.contrib.', 'scrapy.extensions.')])
+@mock.patch(
+    "scrapy.utils.deprecate.DEPRECATION_RULES",
+    [
+        ("scrapy.contrib.pipeline.", "scrapy.pipelines."),
+        ("scrapy.contrib.", "scrapy.extensions."),
+    ],
+)
 class UpdateClassPathTest(unittest.TestCase):
-
     def test_old_path_gets_fixed(self):
         with warnings.catch_warnings(record=True) as w:
-            output = update_classpath('scrapy.contrib.debug.Debug')
-        self.assertEqual(output, 'scrapy.extensions.debug.Debug')
+            output = update_classpath("scrapy.contrib.debug.Debug")
+        self.assertEqual(output, "scrapy.extensions.debug.Debug")
         self.assertEqual(len(w), 1)
         self.assertIn("scrapy.contrib.debug.Debug", str(w[0].message))
         self.assertIn("scrapy.extensions.debug.Debug", str(w[0].message))
 
     def test_sorted_replacement(self):
         with warnings.catch_warnings():
-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)
-            output = update_classpath('scrapy.contrib.pipeline.Pipeline')
-        self.assertEqual(output, 'scrapy.pipelines.Pipeline')
+            warnings.simplefilter("ignore", ScrapyDeprecationWarning)
+            output = update_classpath("scrapy.contrib.pipeline.Pipeline")
+        self.assertEqual(output, "scrapy.pipelines.Pipeline")
 
     def test_unmatched_path_stays_the_same(self):
         with warnings.catch_warnings(record=True) as w:
-            output = update_classpath('scrapy.unmatched.Path')
-        self.assertEqual(output, 'scrapy.unmatched.Path')
+            output = update_classpath("scrapy.unmatched.Path")
+        self.assertEqual(output, "scrapy.unmatched.Path")
         self.assertEqual(len(w), 0)
```

### Comparing `Scrapy-2.7.1/tests/test_utils_display.py` & `Scrapy-2.8.0/tests/test_utils_display.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,78 +1,90 @@
 from io import StringIO
-
-from unittest import mock, TestCase
+from unittest import TestCase, mock
 
 from scrapy.utils.display import pformat, pprint
 
 
 class TestDisplay(TestCase):
-    object = {'a': 1}
-    colorized_string = (
-        "{\x1b[33m'\x1b[39;49;00m\x1b[33ma\x1b[39;49;00m\x1b[33m'"
-        "\x1b[39;49;00m: \x1b[34m1\x1b[39;49;00m}\n"
-    )
+    object = {"a": 1}
+    colorized_strings = {
+        (
+            (
+                "{\x1b[33m'\x1b[39;49;00m\x1b[33ma\x1b[39;49;00m\x1b[33m'"
+                "\x1b[39;49;00m: \x1b[34m1\x1b[39;49;00m}"
+            )
+            + suffix
+        )
+        for suffix in (
+            # https://github.com/pygments/pygments/issues/2313
+            "\n",  # pygments  2.13
+            "\x1b[37m\x1b[39;49;00m\n",  # pygments  2.14
+        )
+    }
     plain_string = "{'a': 1}"
 
-    @mock.patch('sys.platform', 'linux')
+    @mock.patch("sys.platform", "linux")
     @mock.patch("sys.stdout.isatty")
     def test_pformat(self, isatty):
         isatty.return_value = True
-        self.assertEqual(pformat(self.object), self.colorized_string)
+        self.assertIn(pformat(self.object), self.colorized_strings)
 
     @mock.patch("sys.stdout.isatty")
     def test_pformat_dont_colorize(self, isatty):
         isatty.return_value = True
         self.assertEqual(pformat(self.object, colorize=False), self.plain_string)
 
     def test_pformat_not_tty(self):
         self.assertEqual(pformat(self.object), self.plain_string)
 
-    @mock.patch('sys.platform', 'win32')
-    @mock.patch('platform.version')
+    @mock.patch("sys.platform", "win32")
+    @mock.patch("platform.version")
     @mock.patch("sys.stdout.isatty")
     def test_pformat_old_windows(self, isatty, version):
         isatty.return_value = True
-        version.return_value = '10.0.14392'
-        self.assertEqual(pformat(self.object), self.colorized_string)
+        version.return_value = "10.0.14392"
+        self.assertIn(pformat(self.object), self.colorized_strings)
 
-    @mock.patch('sys.platform', 'win32')
-    @mock.patch('scrapy.utils.display._enable_windows_terminal_processing')
-    @mock.patch('platform.version')
-    @mock.patch("sys.stdout.isatty")
-    def test_pformat_windows_no_terminal_processing(self, isatty, version, terminal_processing):
+    @mock.patch("sys.platform", "win32")
+    @mock.patch("scrapy.utils.display._enable_windows_terminal_processing")
+    @mock.patch("platform.version")
+    @mock.patch("sys.stdout.isatty")
+    def test_pformat_windows_no_terminal_processing(
+        self, isatty, version, terminal_processing
+    ):
         isatty.return_value = True
-        version.return_value = '10.0.14393'
+        version.return_value = "10.0.14393"
         terminal_processing.return_value = False
         self.assertEqual(pformat(self.object), self.plain_string)
 
-    @mock.patch('sys.platform', 'win32')
-    @mock.patch('scrapy.utils.display._enable_windows_terminal_processing')
-    @mock.patch('platform.version')
+    @mock.patch("sys.platform", "win32")
+    @mock.patch("scrapy.utils.display._enable_windows_terminal_processing")
+    @mock.patch("platform.version")
     @mock.patch("sys.stdout.isatty")
     def test_pformat_windows(self, isatty, version, terminal_processing):
         isatty.return_value = True
-        version.return_value = '10.0.14393'
+        version.return_value = "10.0.14393"
         terminal_processing.return_value = True
-        self.assertEqual(pformat(self.object), self.colorized_string)
+        self.assertIn(pformat(self.object), self.colorized_strings)
 
-    @mock.patch('sys.platform', 'linux')
+    @mock.patch("sys.platform", "linux")
     @mock.patch("sys.stdout.isatty")
     def test_pformat_no_pygments(self, isatty):
         isatty.return_value = True
 
         import builtins
+
         real_import = builtins.__import__
 
         def mock_import(name, globals, locals, fromlist, level):
-            if 'pygments' in name:
+            if "pygments" in name:
                 raise ImportError
             return real_import(name, globals, locals, fromlist, level)
 
         builtins.__import__ = mock_import
         self.assertEqual(pformat(self.object), self.plain_string)
         builtins.__import__ = real_import
 
     def test_pprint(self):
-        with mock.patch('sys.stdout', new=StringIO()) as mock_out:
+        with mock.patch("sys.stdout", new=StringIO()) as mock_out:
             pprint(self.object)
             self.assertEqual(mock_out.getvalue(), "{'a': 1}\n")
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `Scrapy-2.7.1/tests/test_utils_gz.py` & `Scrapy-2.8.0/tests/test_utils_gz.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,53 +1,57 @@
 import unittest
-from os.path import join
+from pathlib import Path
 
 from w3lib.encoding import html_to_unicode
 
-from scrapy.utils.gz import gunzip, gzip_magic_number
 from scrapy.http import Response
+from scrapy.utils.gz import gunzip, gzip_magic_number
 from tests import tests_datadir
 
-
-SAMPLEDIR = join(tests_datadir, 'compressed')
+SAMPLEDIR = Path(tests_datadir, "compressed")
 
 
 class GunzipTest(unittest.TestCase):
-
     def test_gunzip_basic(self):
-        with open(join(SAMPLEDIR, 'feed-sample1.xml.gz'), 'rb') as f:
-            r1 = Response("http://www.example.com", body=f.read())
-            self.assertTrue(gzip_magic_number(r1))
-
-            r2 = Response("http://www.example.com", body=gunzip(r1.body))
-            self.assertFalse(gzip_magic_number(r2))
-            self.assertEqual(len(r2.body), 9950)
+        r1 = Response(
+            "http://www.example.com",
+            body=(SAMPLEDIR / "feed-sample1.xml.gz").read_bytes(),
+        )
+        self.assertTrue(gzip_magic_number(r1))
+
+        r2 = Response("http://www.example.com", body=gunzip(r1.body))
+        self.assertFalse(gzip_magic_number(r2))
+        self.assertEqual(len(r2.body), 9950)
 
     def test_gunzip_truncated(self):
-        with open(join(SAMPLEDIR, 'truncated-crc-error.gz'), 'rb') as f:
-            text = gunzip(f.read())
-            assert text.endswith(b'</html')
+        text = gunzip((SAMPLEDIR / "truncated-crc-error.gz").read_bytes())
+        assert text.endswith(b"</html")
 
     def test_gunzip_no_gzip_file_raises(self):
-        with open(join(SAMPLEDIR, 'feed-sample1.xml'), 'rb') as f:
-            self.assertRaises(IOError, gunzip, f.read())
+        self.assertRaises(
+            IOError, gunzip, (SAMPLEDIR / "feed-sample1.xml").read_bytes()
+        )
 
     def test_gunzip_truncated_short(self):
-        with open(join(SAMPLEDIR, 'truncated-crc-error-short.gz'), 'rb') as f:
-            r1 = Response("http://www.example.com", body=f.read())
-            self.assertTrue(gzip_magic_number(r1))
-
-            r2 = Response("http://www.example.com", body=gunzip(r1.body))
-            assert r2.body.endswith(b'</html>')
-            self.assertFalse(gzip_magic_number(r2))
+        r1 = Response(
+            "http://www.example.com",
+            body=(SAMPLEDIR / "truncated-crc-error-short.gz").read_bytes(),
+        )
+        self.assertTrue(gzip_magic_number(r1))
+
+        r2 = Response("http://www.example.com", body=gunzip(r1.body))
+        assert r2.body.endswith(b"</html>")
+        self.assertFalse(gzip_magic_number(r2))
 
     def test_is_gzipped_empty(self):
         r1 = Response("http://www.example.com")
         self.assertFalse(gzip_magic_number(r1))
 
     def test_gunzip_illegal_eof(self):
-        with open(join(SAMPLEDIR, 'unexpected-eof.gz'), 'rb') as f:
-            text = html_to_unicode('charset=cp1252', gunzip(f.read()))[1]
-            with open(join(SAMPLEDIR, 'unexpected-eof-output.txt'), 'rb') as o:
-                expected_text = o.read().decode("utf-8")
-                self.assertEqual(len(text), len(expected_text))
-                self.assertEqual(text, expected_text)
+        text = html_to_unicode(
+            "charset=cp1252", gunzip((SAMPLEDIR / "unexpected-eof.gz").read_bytes())
+        )[1]
+        expected_text = (SAMPLEDIR / "unexpected-eof-output.txt").read_text(
+            encoding="utf-8"
+        )
+        self.assertEqual(len(text), len(expected_text))
+        self.assertEqual(text, expected_text)
```

### Comparing `Scrapy-2.7.1/tests/test_utils_httpobj.py` & `Scrapy-2.8.0/tests/test_utils_httpobj.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,15 +2,14 @@
 from urllib.parse import urlparse
 
 from scrapy.http import Request
 from scrapy.utils.httpobj import urlparse_cached
 
 
 class HttpobjUtilsTest(unittest.TestCase):
-
     def test_urlparse_cached(self):
         url = "http://www.example.com/index.html"
         request1 = Request(url)
         request2 = Request(url)
         req1a = urlparse_cached(request1)
         req1b = urlparse_cached(request1)
         req2 = urlparse_cached(request2)
```

### Comparing `Scrapy-2.7.1/tests/test_utils_iterators.py` & `Scrapy-2.8.0/tests/test_utils_iterators.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,14 +1,12 @@
-import os
-
 from pytest import mark
 from twisted.trial import unittest
 
-from scrapy.utils.iterators import csviter, xmliter, _body_or_str, xmliter_lxml
-from scrapy.http import XmlResponse, TextResponse, Response
+from scrapy.http import Response, TextResponse, XmlResponse
+from scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml
 from tests import get_testdata
 
 
 class XmliterTestCase(unittest.TestCase):
 
     xmliter = staticmethod(xmliter)
 
@@ -26,33 +24,39 @@
                 <name>Name 2</name>
               </product>
             </products>
         """
 
         response = XmlResponse(url="http://example.com", body=body)
         attrs = []
-        for x in self.xmliter(response, 'product'):
-            attrs.append((
-                x.attrib['id'],
-                x.xpath("name/text()").getall(),
-                x.xpath("./type/text()").getall()))
+        for x in self.xmliter(response, "product"):
+            attrs.append(
+                (
+                    x.attrib["id"],
+                    x.xpath("name/text()").getall(),
+                    x.xpath("./type/text()").getall(),
+                )
+            )
 
-        self.assertEqual(attrs,
-                         [('001', ['Name 1'], ['Type 1']), ('002', ['Name 2'], ['Type 2'])])
+        self.assertEqual(
+            attrs, [("001", ["Name 1"], ["Type 1"]), ("002", ["Name 2"], ["Type 2"])]
+        )
 
     def test_xmliter_unusual_node(self):
         body = b"""<?xml version="1.0" encoding="UTF-8"?>
             <root>
                 <matchme...></matchme...>
                 <matchmenot></matchmenot>
             </root>
         """
         response = XmlResponse(url="http://example.com", body=body)
-        nodenames = [e.xpath('name()').getall() for e in self.xmliter(response, 'matchme...')]
-        self.assertEqual(nodenames, [['matchme...']])
+        nodenames = [
+            e.xpath("name()").getall() for e in self.xmliter(response, "matchme...")
+        ]
+        self.assertEqual(nodenames, [["matchme..."]])
 
     def test_xmliter_unicode(self):
         # example taken from https://github.com/scrapy/scrapy/issues/1665
         body = """<?xml version="1.0" encoding="UTF-8"?>
             <ingflokkar>
                <ingflokkur id="26">
                   <heiti />
@@ -86,37 +90,43 @@
                      <sastaing>120</sastaing>
                   </tmabil>
                </ingflokkur>
             </ingflokkar>"""
 
         for r in (
             # with bytes
-            XmlResponse(url="http://example.com", body=body.encode('utf-8')),
+            XmlResponse(url="http://example.com", body=body.encode("utf-8")),
             # Unicode body needs encoding information
-            XmlResponse(url="http://example.com", body=body, encoding='utf-8'),
+            XmlResponse(url="http://example.com", body=body, encoding="utf-8"),
         ):
             attrs = []
-            for x in self.xmliter(r, 'ingflokkur'):
-                attrs.append((x.attrib['id'],
-                              x.xpath('./skammstafanir/stuttskammstfun/text()').getall(),
-                              x.xpath('./tmabil/fyrstaing/text()').getall()))
-
-            self.assertEqual(attrs,
-                             [('26', ['-'], ['80']),
-                              ('21', ['Ab'], ['76']),
-                              ('27', ['A'], ['27'])])
+            for x in self.xmliter(r, "ingflokkur"):
+                attrs.append(
+                    (
+                        x.attrib["id"],
+                        x.xpath("./skammstafanir/stuttskammstfun/text()").getall(),
+                        x.xpath("./tmabil/fyrstaing/text()").getall(),
+                    )
+                )
+
+            self.assertEqual(
+                attrs,
+                [("26", ["-"], ["80"]), ("21", ["Ab"], ["76"]), ("27", ["A"], ["27"])],
+            )
 
     def test_xmliter_text(self):
         body = (
             '<?xml version="1.0" encoding="UTF-8"?>'
-            '<products><product>one</product><product>two</product></products>'
+            "<products><product>one</product><product>two</product></products>"
         )
 
-        self.assertEqual([x.xpath("text()").getall() for x in self.xmliter(body, 'product')],
-                         [['one'], ['two']])
+        self.assertEqual(
+            [x.xpath("text()").getall() for x in self.xmliter(body, "product")],
+            [["one"], ["two"]],
+        )
 
     def test_xmliter_namespaces(self):
         body = b"""
             <?xml version="1.0" encoding="UTF-8"?>
             <rss version="2.0" xmlns:g="http://base.google.com/ns/1.0">
                 <channel>
                 <title>My Dummy Company</title>
@@ -129,30 +139,33 @@
                     <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>
                     <g:id>ITEM_1</g:id>
                     <g:price>400</g:price>
                 </item>
                 </channel>
             </rss>
         """
-        response = XmlResponse(url='http://mydummycompany.com', body=body)
-        my_iter = self.xmliter(response, 'item')
+        response = XmlResponse(url="http://mydummycompany.com", body=body)
+        my_iter = self.xmliter(response, "item")
         node = next(my_iter)
-        node.register_namespace('g', 'http://base.google.com/ns/1.0')
-        self.assertEqual(node.xpath('title/text()').getall(), ['Item 1'])
-        self.assertEqual(node.xpath('description/text()').getall(), ['This is item 1'])
-        self.assertEqual(node.xpath('link/text()').getall(), ['http://www.mydummycompany.com/items/1'])
-        self.assertEqual(
-            node.xpath('g:image_link/text()').getall(),
-            ['http://www.mydummycompany.com/images/item1.jpg']
-        )
-        self.assertEqual(node.xpath('g:id/text()').getall(), ['ITEM_1'])
-        self.assertEqual(node.xpath('g:price/text()').getall(), ['400'])
-        self.assertEqual(node.xpath('image_link/text()').getall(), [])
-        self.assertEqual(node.xpath('id/text()').getall(), [])
-        self.assertEqual(node.xpath('price/text()').getall(), [])
+        node.register_namespace("g", "http://base.google.com/ns/1.0")
+        self.assertEqual(node.xpath("title/text()").getall(), ["Item 1"])
+        self.assertEqual(node.xpath("description/text()").getall(), ["This is item 1"])
+        self.assertEqual(
+            node.xpath("link/text()").getall(),
+            ["http://www.mydummycompany.com/items/1"],
+        )
+        self.assertEqual(
+            node.xpath("g:image_link/text()").getall(),
+            ["http://www.mydummycompany.com/images/item1.jpg"],
+        )
+        self.assertEqual(node.xpath("g:id/text()").getall(), ["ITEM_1"])
+        self.assertEqual(node.xpath("g:price/text()").getall(), ["400"])
+        self.assertEqual(node.xpath("image_link/text()").getall(), [])
+        self.assertEqual(node.xpath("id/text()").getall(), [])
+        self.assertEqual(node.xpath("price/text()").getall(), [])
 
     def test_xmliter_namespaced_nodename(self):
         body = b"""
             <?xml version="1.0" encoding="UTF-8"?>
             <rss version="2.0" xmlns:g="http://base.google.com/ns/1.0">
                 <channel>
                 <title>My Dummy Company</title>
@@ -165,19 +178,22 @@
                     <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>
                     <g:id>ITEM_1</g:id>
                     <g:price>400</g:price>
                 </item>
                 </channel>
             </rss>
         """
-        response = XmlResponse(url='http://mydummycompany.com', body=body)
-        my_iter = self.xmliter(response, 'g:image_link')
+        response = XmlResponse(url="http://mydummycompany.com", body=body)
+        my_iter = self.xmliter(response, "g:image_link")
         node = next(my_iter)
-        node.register_namespace('g', 'http://base.google.com/ns/1.0')
-        self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])
+        node.register_namespace("g", "http://base.google.com/ns/1.0")
+        self.assertEqual(
+            node.xpath("text()").extract(),
+            ["http://www.mydummycompany.com/images/item1.jpg"],
+        )
 
     def test_xmliter_namespaced_nodename_missing(self):
         body = b"""
             <?xml version="1.0" encoding="UTF-8"?>
             <rss version="2.0" xmlns:g="http://base.google.com/ns/1.0">
                 <channel>
                 <title>My Dummy Company</title>
@@ -190,53 +206,53 @@
                     <g:image_link>http://www.mydummycompany.com/images/item1.jpg</g:image_link>
                     <g:id>ITEM_1</g:id>
                     <g:price>400</g:price>
                 </item>
                 </channel>
             </rss>
         """
-        response = XmlResponse(url='http://mydummycompany.com', body=body)
-        my_iter = self.xmliter(response, 'g:link_image')
+        response = XmlResponse(url="http://mydummycompany.com", body=body)
+        my_iter = self.xmliter(response, "g:link_image")
         with self.assertRaises(StopIteration):
             next(my_iter)
 
     def test_xmliter_exception(self):
         body = (
             '<?xml version="1.0" encoding="UTF-8"?>'
-            '<products><product>one</product><product>two</product></products>'
+            "<products><product>one</product><product>two</product></products>"
         )
 
-        iter = self.xmliter(body, 'product')
+        iter = self.xmliter(body, "product")
         next(iter)
         next(iter)
 
         self.assertRaises(StopIteration, next, iter)
 
     def test_xmliter_objtype_exception(self):
-        i = self.xmliter(42, 'product')
+        i = self.xmliter(42, "product")
         self.assertRaises(TypeError, next, i)
 
     def test_xmliter_encoding(self):
         body = (
             b'<?xml version="1.0" encoding="ISO-8859-9"?>\n'
-            b'<xml>\n'
-            b'    <item>Some Turkish Characters \xd6\xc7\xde\xdd\xd0\xdc \xfc\xf0\xfd\xfe\xe7\xf6</item>\n'
-            b'</xml>\n\n'
+            b"<xml>\n"
+            b"    <item>Some Turkish Characters \xd6\xc7\xde\xdd\xd0\xdc \xfc\xf0\xfd\xfe\xe7\xf6</item>\n"
+            b"</xml>\n\n"
         )
-        response = XmlResponse('http://www.example.com', body=body)
+        response = XmlResponse("http://www.example.com", body=body)
         self.assertEqual(
-            next(self.xmliter(response, 'item')).get(),
-            '<item>Some Turkish Characters \xd6\xc7\u015e\u0130\u011e\xdc \xfc\u011f\u0131\u015f\xe7\xf6</item>'
+            next(self.xmliter(response, "item")).get(),
+            "<item>Some Turkish Characters \xd6\xc7\u015e\u0130\u011e\xdc \xfc\u011f\u0131\u015f\xe7\xf6</item>",
         )
 
 
 class LxmlXmliterTestCase(XmliterTestCase):
     xmliter = staticmethod(xmliter_lxml)
 
-    @mark.xfail(reason='known bug of the current implementation')
+    @mark.xfail(reason="known bug of the current implementation")
     def test_xmliter_namespaced_nodename(self):
         super().test_xmliter_namespaced_nodename()
 
     def test_xmliter_iterate_namespace(self):
         body = b"""
             <?xml version="1.0" encoding="UTF-8"?>
             <rss version="2.0" xmlns="http://base.google.com/ns/1.0">
@@ -250,24 +266,32 @@
                     <link>http://www.mydummycompany.com/items/1</link>
                     <image_link>http://www.mydummycompany.com/images/item1.jpg</image_link>
                     <image_link>http://www.mydummycompany.com/images/item2.jpg</image_link>
                 </item>
                 </channel>
             </rss>
         """
-        response = XmlResponse(url='http://mydummycompany.com', body=body)
+        response = XmlResponse(url="http://mydummycompany.com", body=body)
 
-        no_namespace_iter = self.xmliter(response, 'image_link')
+        no_namespace_iter = self.xmliter(response, "image_link")
         self.assertEqual(len(list(no_namespace_iter)), 0)
 
-        namespace_iter = self.xmliter(response, 'image_link', 'http://base.google.com/ns/1.0')
+        namespace_iter = self.xmliter(
+            response, "image_link", "http://base.google.com/ns/1.0"
+        )
         node = next(namespace_iter)
-        self.assertEqual(node.xpath('text()').getall(), ['http://www.mydummycompany.com/images/item1.jpg'])
+        self.assertEqual(
+            node.xpath("text()").getall(),
+            ["http://www.mydummycompany.com/images/item1.jpg"],
+        )
         node = next(namespace_iter)
-        self.assertEqual(node.xpath('text()').getall(), ['http://www.mydummycompany.com/images/item2.jpg'])
+        self.assertEqual(
+            node.xpath("text()").getall(),
+            ["http://www.mydummycompany.com/images/item2.jpg"],
+        )
 
     def test_xmliter_namespaces_prefix(self):
         body = b"""
         <?xml version="1.0" encoding="UTF-8"?>
         <root>
             <h:table xmlns:h="http://www.w3.org/TR/html4/">
               <h:tr>
@@ -280,191 +304,231 @@
               <f:name>African Coffee Table</f:name>
               <f:width>80</f:width>
               <f:length>120</f:length>
             </f:table>
 
         </root>
         """
-        response = XmlResponse(url='http://mydummycompany.com', body=body)
-        my_iter = self.xmliter(response, 'table', 'http://www.w3.org/TR/html4/', 'h')
+        response = XmlResponse(url="http://mydummycompany.com", body=body)
+        my_iter = self.xmliter(response, "table", "http://www.w3.org/TR/html4/", "h")
 
         node = next(my_iter)
-        self.assertEqual(len(node.xpath('h:tr/h:td').getall()), 2)
-        self.assertEqual(node.xpath('h:tr/h:td[1]/text()').getall(), ['Apples'])
-        self.assertEqual(node.xpath('h:tr/h:td[2]/text()').getall(), ['Bananas'])
+        self.assertEqual(len(node.xpath("h:tr/h:td").getall()), 2)
+        self.assertEqual(node.xpath("h:tr/h:td[1]/text()").getall(), ["Apples"])
+        self.assertEqual(node.xpath("h:tr/h:td[2]/text()").getall(), ["Bananas"])
 
-        my_iter = self.xmliter(response, 'table', 'http://www.w3schools.com/furniture', 'f')
+        my_iter = self.xmliter(
+            response, "table", "http://www.w3schools.com/furniture", "f"
+        )
 
         node = next(my_iter)
-        self.assertEqual(node.xpath('f:name/text()').getall(), ['African Coffee Table'])
+        self.assertEqual(node.xpath("f:name/text()").getall(), ["African Coffee Table"])
 
     def test_xmliter_objtype_exception(self):
-        i = self.xmliter(42, 'product')
+        i = self.xmliter(42, "product")
         self.assertRaises(TypeError, next, i)
 
 
 class UtilsCsvTestCase(unittest.TestCase):
-    sample_feeds_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds')
-    sample_feed_path = os.path.join(sample_feeds_dir, 'feed-sample3.csv')
-    sample_feed2_path = os.path.join(sample_feeds_dir, 'feed-sample4.csv')
-    sample_feed3_path = os.path.join(sample_feeds_dir, 'feed-sample5.csv')
-
     def test_csviter_defaults(self):
-        body = get_testdata('feeds', 'feed-sample3.csv')
+        body = get_testdata("feeds", "feed-sample3.csv")
         response = TextResponse(url="http://example.com/", body=body)
         csv = csviter(response)
 
         result = [row for row in csv]
-        self.assertEqual(result,
-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},
-                          {'id': '2', 'name': 'unicode', 'value': '\xfan\xedc\xf3d\xe9\u203d'},
-                          {'id': '3', 'name': 'multi', 'value': "foo\nbar"},
-                          {'id': '4', 'name': 'empty', 'value': ''}])
+        self.assertEqual(
+            result,
+            [
+                {"id": "1", "name": "alpha", "value": "foobar"},
+                {"id": "2", "name": "unicode", "value": "\xfan\xedc\xf3d\xe9\u203d"},
+                {"id": "3", "name": "multi", "value": "foo\nbar"},
+                {"id": "4", "name": "empty", "value": ""},
+            ],
+        )
 
         # explicit type check cuz' we no like stinkin' autocasting! yarrr
         for result_row in result:
             self.assertTrue(all((isinstance(k, str) for k in result_row.keys())))
             self.assertTrue(all((isinstance(v, str) for v in result_row.values())))
 
     def test_csviter_delimiter(self):
-        body = get_testdata('feeds', 'feed-sample3.csv').replace(b',', b'\t')
+        body = get_testdata("feeds", "feed-sample3.csv").replace(b",", b"\t")
         response = TextResponse(url="http://example.com/", body=body)
-        csv = csviter(response, delimiter='\t')
+        csv = csviter(response, delimiter="\t")
 
-        self.assertEqual([row for row in csv],
-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},
-                          {'id': '2', 'name': 'unicode', 'value': '\xfan\xedc\xf3d\xe9\u203d'},
-                          {'id': '3', 'name': 'multi', 'value': "foo\nbar"},
-                          {'id': '4', 'name': 'empty', 'value': ''}])
+        self.assertEqual(
+            [row for row in csv],
+            [
+                {"id": "1", "name": "alpha", "value": "foobar"},
+                {"id": "2", "name": "unicode", "value": "\xfan\xedc\xf3d\xe9\u203d"},
+                {"id": "3", "name": "multi", "value": "foo\nbar"},
+                {"id": "4", "name": "empty", "value": ""},
+            ],
+        )
 
     def test_csviter_quotechar(self):
-        body1 = get_testdata('feeds', 'feed-sample6.csv')
-        body2 = get_testdata('feeds', 'feed-sample6.csv').replace(b',', b'|')
+        body1 = get_testdata("feeds", "feed-sample6.csv")
+        body2 = get_testdata("feeds", "feed-sample6.csv").replace(b",", b"|")
 
         response1 = TextResponse(url="http://example.com/", body=body1)
         csv1 = csviter(response1, quotechar="'")
 
-        self.assertEqual([row for row in csv1],
-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},
-                          {'id': '2', 'name': 'unicode', 'value': '\xfan\xedc\xf3d\xe9\u203d'},
-                          {'id': '3', 'name': 'multi', 'value': "foo\nbar"},
-                          {'id': '4', 'name': 'empty', 'value': ''}])
+        self.assertEqual(
+            [row for row in csv1],
+            [
+                {"id": "1", "name": "alpha", "value": "foobar"},
+                {"id": "2", "name": "unicode", "value": "\xfan\xedc\xf3d\xe9\u203d"},
+                {"id": "3", "name": "multi", "value": "foo\nbar"},
+                {"id": "4", "name": "empty", "value": ""},
+            ],
+        )
 
         response2 = TextResponse(url="http://example.com/", body=body2)
         csv2 = csviter(response2, delimiter="|", quotechar="'")
 
-        self.assertEqual([row for row in csv2],
-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},
-                          {'id': '2', 'name': 'unicode', 'value': '\xfan\xedc\xf3d\xe9\u203d'},
-                          {'id': '3', 'name': 'multi', 'value': "foo\nbar"},
-                          {'id': '4', 'name': 'empty', 'value': ''}])
+        self.assertEqual(
+            [row for row in csv2],
+            [
+                {"id": "1", "name": "alpha", "value": "foobar"},
+                {"id": "2", "name": "unicode", "value": "\xfan\xedc\xf3d\xe9\u203d"},
+                {"id": "3", "name": "multi", "value": "foo\nbar"},
+                {"id": "4", "name": "empty", "value": ""},
+            ],
+        )
 
     def test_csviter_wrong_quotechar(self):
-        body = get_testdata('feeds', 'feed-sample6.csv')
+        body = get_testdata("feeds", "feed-sample6.csv")
         response = TextResponse(url="http://example.com/", body=body)
         csv = csviter(response)
 
-        self.assertEqual([row for row in csv],
-                         [{"'id'": "1", "'name'": "'alpha'", "'value'": "'foobar'"},
-                          {"'id'": "2", "'name'": "'unicode'", "'value'": "'\xfan\xedc\xf3d\xe9\u203d'"},
-                          {"'id'": "'3'", "'name'": "'multi'", "'value'": "'foo"},
-                          {"'id'": "4", "'name'": "'empty'", "'value'": ""}])
+        self.assertEqual(
+            [row for row in csv],
+            [
+                {"'id'": "1", "'name'": "'alpha'", "'value'": "'foobar'"},
+                {
+                    "'id'": "2",
+                    "'name'": "'unicode'",
+                    "'value'": "'\xfan\xedc\xf3d\xe9\u203d'",
+                },
+                {"'id'": "'3'", "'name'": "'multi'", "'value'": "'foo"},
+                {"'id'": "4", "'name'": "'empty'", "'value'": ""},
+            ],
+        )
 
     def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):
-        body = get_testdata('feeds', 'feed-sample3.csv').replace(b',', b'\t')
+        body = get_testdata("feeds", "feed-sample3.csv").replace(b",", b"\t")
         response = Response(url="http://example.com/", body=body)
-        csv = csviter(response, delimiter='\t')
+        csv = csviter(response, delimiter="\t")
 
-        self.assertEqual([row for row in csv],
-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},
-                          {'id': '2', 'name': 'unicode', 'value': '\xfan\xedc\xf3d\xe9\u203d'},
-                          {'id': '3', 'name': 'multi', 'value': "foo\nbar"},
-                          {'id': '4', 'name': 'empty', 'value': ''}])
+        self.assertEqual(
+            [row for row in csv],
+            [
+                {"id": "1", "name": "alpha", "value": "foobar"},
+                {"id": "2", "name": "unicode", "value": "\xfan\xedc\xf3d\xe9\u203d"},
+                {"id": "3", "name": "multi", "value": "foo\nbar"},
+                {"id": "4", "name": "empty", "value": ""},
+            ],
+        )
 
     def test_csviter_headers(self):
-        sample = get_testdata('feeds', 'feed-sample3.csv').splitlines()
-        headers, body = sample[0].split(b','), b'\n'.join(sample[1:])
+        sample = get_testdata("feeds", "feed-sample3.csv").splitlines()
+        headers, body = sample[0].split(b","), b"\n".join(sample[1:])
 
         response = TextResponse(url="http://example.com/", body=body)
-        csv = csviter(response, headers=[h.decode('utf-8') for h in headers])
+        csv = csviter(response, headers=[h.decode("utf-8") for h in headers])
 
-        self.assertEqual([row for row in csv],
-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},
-                          {'id': '2', 'name': 'unicode', 'value': '\xfan\xedc\xf3d\xe9\u203d'},
-                          {'id': '3', 'name': 'multi', 'value': 'foo\nbar'},
-                          {'id': '4', 'name': 'empty', 'value': ''}])
+        self.assertEqual(
+            [row for row in csv],
+            [
+                {"id": "1", "name": "alpha", "value": "foobar"},
+                {"id": "2", "name": "unicode", "value": "\xfan\xedc\xf3d\xe9\u203d"},
+                {"id": "3", "name": "multi", "value": "foo\nbar"},
+                {"id": "4", "name": "empty", "value": ""},
+            ],
+        )
 
     def test_csviter_falserow(self):
-        body = get_testdata('feeds', 'feed-sample3.csv')
-        body = b'\n'.join((body, b'a,b', b'a,b,c,d'))
+        body = get_testdata("feeds", "feed-sample3.csv")
+        body = b"\n".join((body, b"a,b", b"a,b,c,d"))
 
         response = TextResponse(url="http://example.com/", body=body)
         csv = csviter(response)
 
-        self.assertEqual([row for row in csv],
-                         [{'id': '1', 'name': 'alpha', 'value': 'foobar'},
-                          {'id': '2', 'name': 'unicode', 'value': '\xfan\xedc\xf3d\xe9\u203d'},
-                          {'id': '3', 'name': 'multi', 'value': "foo\nbar"},
-                          {'id': '4', 'name': 'empty', 'value': ''}])
+        self.assertEqual(
+            [row for row in csv],
+            [
+                {"id": "1", "name": "alpha", "value": "foobar"},
+                {"id": "2", "name": "unicode", "value": "\xfan\xedc\xf3d\xe9\u203d"},
+                {"id": "3", "name": "multi", "value": "foo\nbar"},
+                {"id": "4", "name": "empty", "value": ""},
+            ],
+        )
 
     def test_csviter_exception(self):
-        body = get_testdata('feeds', 'feed-sample3.csv')
+        body = get_testdata("feeds", "feed-sample3.csv")
 
         response = TextResponse(url="http://example.com/", body=body)
         iter = csviter(response)
         next(iter)
         next(iter)
         next(iter)
         next(iter)
 
         self.assertRaises(StopIteration, next, iter)
 
     def test_csviter_encoding(self):
-        body1 = get_testdata('feeds', 'feed-sample4.csv')
-        body2 = get_testdata('feeds', 'feed-sample5.csv')
+        body1 = get_testdata("feeds", "feed-sample4.csv")
+        body2 = get_testdata("feeds", "feed-sample5.csv")
 
-        response = TextResponse(url="http://example.com/", body=body1, encoding='latin1')
+        response = TextResponse(
+            url="http://example.com/", body=body1, encoding="latin1"
+        )
         csv = csviter(response)
         self.assertEqual(
             list(csv),
             [
-                {'id': '1', 'name': 'latin1', 'value': 'test'},
-                {'id': '2', 'name': 'something', 'value': '\xf1\xe1\xe9\xf3'},
-            ]
+                {"id": "1", "name": "latin1", "value": "test"},
+                {"id": "2", "name": "something", "value": "\xf1\xe1\xe9\xf3"},
+            ],
         )
 
-        response = TextResponse(url="http://example.com/", body=body2, encoding='cp852')
+        response = TextResponse(url="http://example.com/", body=body2, encoding="cp852")
         csv = csviter(response)
         self.assertEqual(
             list(csv),
             [
-                {'id': '1', 'name': 'cp852', 'value': 'test'},
-                {'id': '2', 'name': 'something', 'value': '\u255a\u2569\u2569\u2569\u2550\u2550\u2557'},
-            ]
+                {"id": "1", "name": "cp852", "value": "test"},
+                {
+                    "id": "2",
+                    "name": "something",
+                    "value": "\u255a\u2569\u2569\u2569\u2550\u2550\u2557",
+                },
+            ],
         )
 
 
 class TestHelper(unittest.TestCase):
-    bbody = b'utf8-body'
-    ubody = bbody.decode('utf8')
-    txtresponse = TextResponse(url='http://example.org/', body=bbody, encoding='utf-8')
-    response = Response(url='http://example.org/', body=bbody)
+    bbody = b"utf8-body"
+    ubody = bbody.decode("utf8")
+    txtresponse = TextResponse(url="http://example.org/", body=bbody, encoding="utf-8")
+    response = Response(url="http://example.org/", body=bbody)
 
     def test_body_or_str(self):
         for obj in (self.bbody, self.ubody, self.txtresponse, self.response):
             r1 = _body_or_str(obj)
             self._assert_type_and_value(r1, self.ubody, obj)
             r2 = _body_or_str(obj, unicode=True)
             self._assert_type_and_value(r2, self.ubody, obj)
             r3 = _body_or_str(obj, unicode=False)
             self._assert_type_and_value(r3, self.bbody, obj)
             self.assertTrue(type(r1) is type(r2))
             self.assertTrue(type(r1) is not type(r3))
 
     def _assert_type_and_value(self, a, b, obj):
-        self.assertTrue(type(a) is type(b),
-                        f'Got {type(a)}, expected {type(b)} for { obj!r}')
+        self.assertTrue(
+            type(a) is type(b), f"Got {type(a)}, expected {type(b)} for { obj!r}"
+        )
         self.assertEqual(a, b)
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_utils_log.py` & `Scrapy-2.8.0/tests/test_utils_log.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,108 +1,108 @@
-import sys
 import logging
+import sys
 import unittest
 
 from testfixtures import LogCapture
 from twisted.python.failure import Failure
 
-from scrapy.utils.log import (failure_to_exc_info, TopLevelFormatter,
-                              LogCounterHandler, StreamLogger)
-from scrapy.utils.test import get_crawler
 from scrapy.extensions import telnet
+from scrapy.utils.log import (
+    LogCounterHandler,
+    StreamLogger,
+    TopLevelFormatter,
+    failure_to_exc_info,
+)
+from scrapy.utils.test import get_crawler
 
 
 class FailureToExcInfoTest(unittest.TestCase):
-
     def test_failure(self):
         try:
             0 / 0
         except ZeroDivisionError:
             exc_info = sys.exc_info()
             failure = Failure()
 
         self.assertTupleEqual(exc_info, failure_to_exc_info(failure))
 
     def test_non_failure(self):
-        self.assertIsNone(failure_to_exc_info('test'))
+        self.assertIsNone(failure_to_exc_info("test"))
 
 
 class TopLevelFormatterTest(unittest.TestCase):
-
     def setUp(self):
         self.handler = LogCapture()
-        self.handler.addFilter(TopLevelFormatter(['test']))
+        self.handler.addFilter(TopLevelFormatter(["test"]))
 
     def test_top_level_logger(self):
-        logger = logging.getLogger('test')
+        logger = logging.getLogger("test")
         with self.handler as log:
-            logger.warning('test log msg')
-        log.check(('test', 'WARNING', 'test log msg'))
+            logger.warning("test log msg")
+        log.check(("test", "WARNING", "test log msg"))
 
     def test_children_logger(self):
-        logger = logging.getLogger('test.test1')
+        logger = logging.getLogger("test.test1")
         with self.handler as log:
-            logger.warning('test log msg')
-        log.check(('test', 'WARNING', 'test log msg'))
+            logger.warning("test log msg")
+        log.check(("test", "WARNING", "test log msg"))
 
     def test_overlapping_name_logger(self):
-        logger = logging.getLogger('test2')
+        logger = logging.getLogger("test2")
         with self.handler as log:
-            logger.warning('test log msg')
-        log.check(('test2', 'WARNING', 'test log msg'))
+            logger.warning("test log msg")
+        log.check(("test2", "WARNING", "test log msg"))
 
     def test_different_name_logger(self):
-        logger = logging.getLogger('different')
+        logger = logging.getLogger("different")
         with self.handler as log:
-            logger.warning('test log msg')
-        log.check(('different', 'WARNING', 'test log msg'))
+            logger.warning("test log msg")
+        log.check(("different", "WARNING", "test log msg"))
 
 
 class LogCounterHandlerTest(unittest.TestCase):
-
     def setUp(self):
-        settings = {'LOG_LEVEL': 'WARNING'}
+        settings = {"LOG_LEVEL": "WARNING"}
         if not telnet.TWISTED_CONCH_AVAILABLE:
             # disable it to avoid the extra warning
-            settings['TELNETCONSOLE_ENABLED'] = False
-        self.logger = logging.getLogger('test')
+            settings["TELNETCONSOLE_ENABLED"] = False
+        self.logger = logging.getLogger("test")
         self.logger.setLevel(logging.NOTSET)
         self.logger.propagate = False
         self.crawler = get_crawler(settings_dict=settings)
         self.handler = LogCounterHandler(self.crawler)
         self.logger.addHandler(self.handler)
 
     def tearDown(self):
         self.logger.propagate = True
         self.logger.removeHandler(self.handler)
 
     def test_init(self):
-        self.assertIsNone(self.crawler.stats.get_value('log_count/DEBUG'))
-        self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))
-        self.assertIsNone(self.crawler.stats.get_value('log_count/WARNING'))
-        self.assertIsNone(self.crawler.stats.get_value('log_count/ERROR'))
-        self.assertIsNone(self.crawler.stats.get_value('log_count/CRITICAL'))
+        self.assertIsNone(self.crawler.stats.get_value("log_count/DEBUG"))
+        self.assertIsNone(self.crawler.stats.get_value("log_count/INFO"))
+        self.assertIsNone(self.crawler.stats.get_value("log_count/WARNING"))
+        self.assertIsNone(self.crawler.stats.get_value("log_count/ERROR"))
+        self.assertIsNone(self.crawler.stats.get_value("log_count/CRITICAL"))
 
     def test_accepted_level(self):
-        self.logger.error('test log msg')
-        self.assertEqual(self.crawler.stats.get_value('log_count/ERROR'), 1)
+        self.logger.error("test log msg")
+        self.assertEqual(self.crawler.stats.get_value("log_count/ERROR"), 1)
 
     def test_filtered_out_level(self):
-        self.logger.debug('test log msg')
-        self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))
+        self.logger.debug("test log msg")
+        self.assertIsNone(self.crawler.stats.get_value("log_count/INFO"))
 
 
 class StreamLoggerTest(unittest.TestCase):
-
     def setUp(self):
         self.stdout = sys.stdout
-        logger = logging.getLogger('test')
+        logger = logging.getLogger("test")
         logger.setLevel(logging.WARNING)
         sys.stdout = StreamLogger(logger, logging.ERROR)
 
     def tearDown(self):
         sys.stdout = self.stdout
 
     def test_redirect(self):
         with LogCapture() as log:
-            print('test log msg')
-        log.check(('test', 'ERROR', 'test log msg'))
+            print("test log msg")
+        log.check(("test", "ERROR", "test log msg"))
```

### Comparing `Scrapy-2.7.1/tests/test_utils_misc/__init__.py` & `Scrapy-2.8.0/tests/test_utils_misc/__init__.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,176 +1,179 @@
-import sys
 import os
+import sys
 import unittest
+from pathlib import Path
 from unittest import mock
 
-from scrapy.item import Item, Field
-from scrapy.utils.misc import arg_to_iter, create_instance, load_object, rel_has_nofollow, set_environ, walk_modules
-
+from scrapy.item import Field, Item
+from scrapy.utils.misc import (
+    arg_to_iter,
+    create_instance,
+    load_object,
+    rel_has_nofollow,
+    set_environ,
+    walk_modules,
+)
 
-__doctests__ = ['scrapy.utils.misc']
+__doctests__ = ["scrapy.utils.misc"]
 
 
 class UtilsMiscTestCase(unittest.TestCase):
-
     def test_load_object_class(self):
         obj = load_object(Field)
         self.assertIs(obj, Field)
-        obj = load_object('scrapy.item.Field')
+        obj = load_object("scrapy.item.Field")
         self.assertIs(obj, Field)
 
     def test_load_object_function(self):
         obj = load_object(load_object)
         self.assertIs(obj, load_object)
-        obj = load_object('scrapy.utils.misc.load_object')
+        obj = load_object("scrapy.utils.misc.load_object")
         self.assertIs(obj, load_object)
 
     def test_load_object_exceptions(self):
-        self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')
-        self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')
+        self.assertRaises(ImportError, load_object, "nomodule999.mod.function")
+        self.assertRaises(NameError, load_object, "scrapy.utils.misc.load_object999")
         self.assertRaises(TypeError, load_object, {})
 
     def test_walk_modules(self):
-        mods = walk_modules('tests.test_utils_misc.test_walk_modules')
+        mods = walk_modules("tests.test_utils_misc.test_walk_modules")
         expected = [
-            'tests.test_utils_misc.test_walk_modules',
-            'tests.test_utils_misc.test_walk_modules.mod',
-            'tests.test_utils_misc.test_walk_modules.mod.mod0',
-            'tests.test_utils_misc.test_walk_modules.mod1',
+            "tests.test_utils_misc.test_walk_modules",
+            "tests.test_utils_misc.test_walk_modules.mod",
+            "tests.test_utils_misc.test_walk_modules.mod.mod0",
+            "tests.test_utils_misc.test_walk_modules.mod1",
         ]
         self.assertEqual({m.__name__ for m in mods}, set(expected))
 
-        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod')
+        mods = walk_modules("tests.test_utils_misc.test_walk_modules.mod")
         expected = [
-            'tests.test_utils_misc.test_walk_modules.mod',
-            'tests.test_utils_misc.test_walk_modules.mod.mod0',
+            "tests.test_utils_misc.test_walk_modules.mod",
+            "tests.test_utils_misc.test_walk_modules.mod.mod0",
         ]
         self.assertEqual({m.__name__ for m in mods}, set(expected))
 
-        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod1')
+        mods = walk_modules("tests.test_utils_misc.test_walk_modules.mod1")
         expected = [
-            'tests.test_utils_misc.test_walk_modules.mod1',
+            "tests.test_utils_misc.test_walk_modules.mod1",
         ]
         self.assertEqual({m.__name__ for m in mods}, set(expected))
 
-        self.assertRaises(ImportError, walk_modules, 'nomodule999')
+        self.assertRaises(ImportError, walk_modules, "nomodule999")
 
     def test_walk_modules_egg(self):
-        egg = os.path.join(os.path.dirname(__file__), 'test.egg')
+        egg = str(Path(__file__).parent / "test.egg")
         sys.path.append(egg)
         try:
-            mods = walk_modules('testegg')
+            mods = walk_modules("testegg")
             expected = [
-                'testegg.spiders',
-                'testegg.spiders.a',
-                'testegg.spiders.b',
-                'testegg'
+                "testegg.spiders",
+                "testegg.spiders.a",
+                "testegg.spiders.b",
+                "testegg",
             ]
             self.assertEqual({m.__name__ for m in mods}, set(expected))
         finally:
             sys.path.remove(egg)
 
     def test_arg_to_iter(self):
-
         class TestItem(Item):
             name = Field()
 
-        assert hasattr(arg_to_iter(None), '__iter__')
-        assert hasattr(arg_to_iter(100), '__iter__')
-        assert hasattr(arg_to_iter('lala'), '__iter__')
-        assert hasattr(arg_to_iter([1, 2, 3]), '__iter__')
-        assert hasattr(arg_to_iter(c for c in 'abcd'), '__iter__')
+        assert hasattr(arg_to_iter(None), "__iter__")
+        assert hasattr(arg_to_iter(100), "__iter__")
+        assert hasattr(arg_to_iter("lala"), "__iter__")
+        assert hasattr(arg_to_iter([1, 2, 3]), "__iter__")
+        assert hasattr(arg_to_iter(c for c in "abcd"), "__iter__")
 
         self.assertEqual(list(arg_to_iter(None)), [])
-        self.assertEqual(list(arg_to_iter('lala')), ['lala'])
+        self.assertEqual(list(arg_to_iter("lala")), ["lala"])
         self.assertEqual(list(arg_to_iter(100)), [100])
-        self.assertEqual(list(arg_to_iter(c for c in 'abc')), ['a', 'b', 'c'])
+        self.assertEqual(list(arg_to_iter(c for c in "abc")), ["a", "b", "c"])
         self.assertEqual(list(arg_to_iter([1, 2, 3])), [1, 2, 3])
-        self.assertEqual(list(arg_to_iter({'a': 1})), [{'a': 1}])
-        self.assertEqual(list(arg_to_iter(TestItem(name="john"))), [TestItem(name="john")])
+        self.assertEqual(list(arg_to_iter({"a": 1})), [{"a": 1}])
+        self.assertEqual(
+            list(arg_to_iter(TestItem(name="john"))), [TestItem(name="john")]
+        )
 
     def test_create_instance(self):
         settings = mock.MagicMock()
-        crawler = mock.MagicMock(spec_set=['settings'])
-        args = (True, 100.)
-        kwargs = {'key': 'val'}
+        crawler = mock.MagicMock(spec_set=["settings"])
+        args = (True, 100.0)
+        kwargs = {"key": "val"}
 
         def _test_with_settings(mock, settings):
             create_instance(mock, settings, None, *args, **kwargs)
-            if hasattr(mock, 'from_crawler'):
+            if hasattr(mock, "from_crawler"):
                 self.assertEqual(mock.from_crawler.call_count, 0)
-            if hasattr(mock, 'from_settings'):
-                mock.from_settings.assert_called_once_with(settings, *args,
-                                                           **kwargs)
+            if hasattr(mock, "from_settings"):
+                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)
                 self.assertEqual(mock.call_count, 0)
             else:
                 mock.assert_called_once_with(*args, **kwargs)
 
         def _test_with_crawler(mock, settings, crawler):
             create_instance(mock, settings, crawler, *args, **kwargs)
-            if hasattr(mock, 'from_crawler'):
-                mock.from_crawler.assert_called_once_with(crawler, *args,
-                                                          **kwargs)
-                if hasattr(mock, 'from_settings'):
+            if hasattr(mock, "from_crawler"):
+                mock.from_crawler.assert_called_once_with(crawler, *args, **kwargs)
+                if hasattr(mock, "from_settings"):
                     self.assertEqual(mock.from_settings.call_count, 0)
                 self.assertEqual(mock.call_count, 0)
-            elif hasattr(mock, 'from_settings'):
-                mock.from_settings.assert_called_once_with(settings, *args,
-                                                           **kwargs)
+            elif hasattr(mock, "from_settings"):
+                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)
                 self.assertEqual(mock.call_count, 0)
             else:
                 mock.assert_called_once_with(*args, **kwargs)
 
         # Check usage of correct constructor using four mocks:
         #   1. with no alternative constructors
         #   2. with from_settings() constructor
         #   3. with from_crawler() constructor
         #   4. with from_settings() and from_crawler() constructor
         spec_sets = (
-            ['__qualname__'],
-            ['__qualname__', 'from_settings'],
-            ['__qualname__', 'from_crawler'],
-            ['__qualname__', 'from_settings', 'from_crawler'],
+            ["__qualname__"],
+            ["__qualname__", "from_settings"],
+            ["__qualname__", "from_crawler"],
+            ["__qualname__", "from_settings", "from_crawler"],
         )
         for specs in spec_sets:
             m = mock.MagicMock(spec_set=specs)
             _test_with_settings(m, settings)
             m.reset_mock()
             _test_with_crawler(m, settings, crawler)
 
         # Check adoption of crawler settings
-        m = mock.MagicMock(spec_set=['__qualname__', 'from_settings'])
+        m = mock.MagicMock(spec_set=["__qualname__", "from_settings"])
         create_instance(m, None, crawler, *args, **kwargs)
-        m.from_settings.assert_called_once_with(crawler.settings, *args,
-                                                **kwargs)
+        m.from_settings.assert_called_once_with(crawler.settings, *args, **kwargs)
 
         with self.assertRaises(ValueError):
             create_instance(m, None, None)
 
         m.from_settings.return_value = None
         with self.assertRaises(TypeError):
             create_instance(m, settings, None)
 
     def test_set_environ(self):
-        assert os.environ.get('some_test_environ') is None
-        with set_environ(some_test_environ='test_value'):
-            assert os.environ.get('some_test_environ') == 'test_value'
-        assert os.environ.get('some_test_environ') is None
-
-        os.environ['some_test_environ'] = 'test'
-        assert os.environ.get('some_test_environ') == 'test'
-        with set_environ(some_test_environ='test_value'):
-            assert os.environ.get('some_test_environ') == 'test_value'
-        assert os.environ.get('some_test_environ') == 'test'
+        assert os.environ.get("some_test_environ") is None
+        with set_environ(some_test_environ="test_value"):
+            assert os.environ.get("some_test_environ") == "test_value"
+        assert os.environ.get("some_test_environ") is None
+
+        os.environ["some_test_environ"] = "test"
+        assert os.environ.get("some_test_environ") == "test"
+        with set_environ(some_test_environ="test_value"):
+            assert os.environ.get("some_test_environ") == "test_value"
+        assert os.environ.get("some_test_environ") == "test"
 
     def test_rel_has_nofollow(self):
-        assert rel_has_nofollow('ugc nofollow') is True
-        assert rel_has_nofollow('ugc,nofollow') is True
-        assert rel_has_nofollow('ugc') is False
-        assert rel_has_nofollow('nofollow') is True
-        assert rel_has_nofollow('nofollowfoo') is False
-        assert rel_has_nofollow('foonofollow') is False
-        assert rel_has_nofollow('ugc,  ,  nofollow') is True
+        assert rel_has_nofollow("ugc nofollow") is True
+        assert rel_has_nofollow("ugc,nofollow") is True
+        assert rel_has_nofollow("ugc") is False
+        assert rel_has_nofollow("nofollow") is True
+        assert rel_has_nofollow("nofollowfoo") is False
+        assert rel_has_nofollow("foonofollow") is False
+        assert rel_has_nofollow("ugc,  ,  nofollow") is True
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_utils_misc/test.egg` & `Scrapy-2.8.0/tests/test_utils_misc/test.egg`

 * *Files identical despite different names*

### Comparing `Scrapy-2.7.1/tests/test_utils_misc/test_return_with_argument_inside_generator.py` & `Scrapy-2.8.0/tests/test_utils_misc/test_return_with_argument_inside_generator.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,33 +1,36 @@
 import unittest
 import warnings
 from functools import partial
 from unittest import mock
 
-from scrapy.utils.misc import is_generator_with_return_value, warn_on_generator_with_return_value
+from scrapy.utils.misc import (
+    is_generator_with_return_value,
+    warn_on_generator_with_return_value,
+)
 
 
 def _indentation_error(*args, **kwargs):
     raise IndentationError()
 
 
 def top_level_return_something():
     """
-docstring
+    docstring
     """
     url = """
 https://example.org
 """
     yield url
     return 1
 
 
 def top_level_return_none():
     """
-docstring
+    docstring
     """
     url = """
 https://example.org
 """
     yield url
     return
 
@@ -35,15 +38,14 @@
 def generator_that_returns_stuff():
     yield 1
     yield 2
     return 3
 
 
 class UtilsMiscPy3TestCase(unittest.TestCase):
-
     def test_generators_return_something(self):
         def f1():
             yield 1
             return 2
 
         def g1():
             yield 1
@@ -56,15 +58,15 @@
                 return 0
 
             yield helper()
             return 2
 
         def i1():
             """
-docstring
+            docstring
             """
             url = """
 https://example.org
         """
             yield url
             return 1
 
@@ -73,15 +75,18 @@
         assert is_generator_with_return_value(g1)
         assert is_generator_with_return_value(h1)
         assert is_generator_with_return_value(i1)
 
         with warnings.catch_warnings(record=True) as w:
             warn_on_generator_with_return_value(None, top_level_return_something)
             self.assertEqual(len(w), 1)
-            self.assertIn('The "NoneType.top_level_return_something" method is a generator', str(w[0].message))
+            self.assertIn(
+                'The "NoneType.top_level_return_something" method is a generator',
+                str(w[0].message),
+            )
         with warnings.catch_warnings(record=True) as w:
             warn_on_generator_with_return_value(None, f1)
             self.assertEqual(len(w), 1)
             self.assertIn('The "NoneType.f1" method is a generator', str(w[0].message))
         with warnings.catch_warnings(record=True) as w:
             warn_on_generator_with_return_value(None, g1)
             self.assertEqual(len(w), 1)
@@ -117,15 +122,15 @@
             def helper():
                 return 0
 
             yield helper()
 
         def k2():
             """
-docstring
+            docstring
             """
             url = """
 https://example.org
         """
             yield url
             return
 
@@ -166,14 +171,15 @@
             warn_on_generator_with_return_value(None, l2)
             self.assertEqual(len(w), 0)
 
     def test_generators_return_none_with_decorator(self):
         def decorator(func):
             def inner_func():
                 func()
+
             return inner_func
 
         @decorator
         def f3():
             yield 1
             return None
 
@@ -199,15 +205,15 @@
                 return 0
 
             yield helper()
 
         @decorator
         def k3():
             """
-docstring
+            docstring
             """
             url = """
 https://example.org
         """
             yield url
             return
 
@@ -245,20 +251,22 @@
         with warnings.catch_warnings(record=True) as w:
             warn_on_generator_with_return_value(None, k3)
             self.assertEqual(len(w), 0)
         with warnings.catch_warnings(record=True) as w:
             warn_on_generator_with_return_value(None, l3)
             self.assertEqual(len(w), 0)
 
-    @mock.patch("scrapy.utils.misc.is_generator_with_return_value", new=_indentation_error)
+    @mock.patch(
+        "scrapy.utils.misc.is_generator_with_return_value", new=_indentation_error
+    )
     def test_indentation_error(self):
         with warnings.catch_warnings(record=True) as w:
             warn_on_generator_with_return_value(None, top_level_return_none)
             self.assertEqual(len(w), 1)
-            self.assertIn('Unable to determine', str(w[0].message))
+            self.assertIn("Unable to determine", str(w[0].message))
 
     def test_partial(self):
         def cb(arg1, arg2):
             yield {}
 
         partial_cb = partial(cb, arg1=42)
         assert not is_generator_with_return_value(partial_cb)
```

### Comparing `Scrapy-2.7.1/tests/test_utils_project.py` & `Scrapy-2.8.0/tests/test_utils_project.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,54 +1,44 @@
-import unittest
+import contextlib
 import os
-import tempfile
 import shutil
-import contextlib
+import tempfile
+import unittest
 import warnings
+from pathlib import Path
 
-from pytest import warns
-
-from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.utils.project import data_path, get_project_settings
 
 
 @contextlib.contextmanager
 def inside_a_project():
     prev_dir = os.getcwd()
     project_dir = tempfile.mkdtemp()
 
     try:
         os.chdir(project_dir)
-        with open('scrapy.cfg', 'w') as f:
-            # create an empty scrapy.cfg
-            f.close()
+        Path("scrapy.cfg").touch()
 
         yield project_dir
     finally:
         os.chdir(prev_dir)
         shutil.rmtree(project_dir)
 
 
 class ProjectUtilsTest(unittest.TestCase):
     def test_data_path_outside_project(self):
-        self.assertEqual(
-            os.path.join('.scrapy', 'somepath'),
-            data_path('somepath')
-        )
-        abspath = os.path.join(os.path.sep, 'absolute', 'path')
+        self.assertEqual(str(Path(".scrapy", "somepath")), data_path("somepath"))
+        abspath = str(Path(os.path.sep, "absolute", "path"))
         self.assertEqual(abspath, data_path(abspath))
 
     def test_data_path_inside_project(self):
         with inside_a_project() as proj_path:
-            expected = os.path.join(proj_path, '.scrapy', 'somepath')
-            self.assertEqual(
-                os.path.realpath(expected),
-                os.path.realpath(data_path('somepath'))
-            )
-            abspath = os.path.join(os.path.sep, 'absolute', 'path')
+            expected = Path(proj_path, ".scrapy", "somepath")
+            self.assertEqual(expected.resolve(), Path(data_path("somepath")).resolve())
+            abspath = str(Path(os.path.sep, "absolute", "path").resolve())
             self.assertEqual(abspath, data_path(abspath))
 
 
 @contextlib.contextmanager
 def set_env(**update):
     modified = set(update.keys()) & set(os.environ.keys())
     update_after = {k: os.environ[k] for k in modified}
@@ -59,40 +49,38 @@
     finally:
         os.environ.update(update_after)
         for k in remove_after:
             os.environ.pop(k)
 
 
 class GetProjectSettingsTestCase(unittest.TestCase):
-
     def test_valid_envvar(self):
-        value = 'tests.test_cmdline.settings'
+        value = "tests.test_cmdline.settings"
         envvars = {
-            'SCRAPY_SETTINGS_MODULE': value,
+            "SCRAPY_SETTINGS_MODULE": value,
         }
         with warnings.catch_warnings():
             warnings.simplefilter("error")
             with set_env(**envvars):
                 settings = get_project_settings()
 
-        assert settings.get('SETTINGS_MODULE') == value
+        assert settings.get("SETTINGS_MODULE") == value
 
     def test_invalid_envvar(self):
         envvars = {
-            'SCRAPY_FOO': 'bar',
+            "SCRAPY_FOO": "bar",
         }
-        with warns(ScrapyDeprecationWarning, match=': FOO') as record:
-            with set_env(**envvars):
-                get_project_settings()
-        assert len(record) == 1
+        with set_env(**envvars):
+            settings = get_project_settings()
+
+        assert settings.get("SCRAPY_FOO") is None
 
     def test_valid_and_invalid_envvars(self):
-        value = 'tests.test_cmdline.settings'
+        value = "tests.test_cmdline.settings"
         envvars = {
-            'SCRAPY_FOO': 'bar',
-            'SCRAPY_SETTINGS_MODULE': value,
+            "SCRAPY_FOO": "bar",
+            "SCRAPY_SETTINGS_MODULE": value,
         }
-        with warns(ScrapyDeprecationWarning, match=': FOO') as record:
-            with set_env(**envvars):
-                settings = get_project_settings()
-        assert len(record) == 1
-        assert settings.get('SETTINGS_MODULE') == value
+        with set_env(**envvars):
+            settings = get_project_settings()
+        assert settings.get("SETTINGS_MODULE") == value
+        assert settings.get("SCRAPY_FOO") is None
```

### Comparing `Scrapy-2.7.1/tests/test_utils_python.py` & `Scrapy-2.8.0/tests/test_utils_python.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,42 +1,39 @@
 import functools
-import gc
 import operator
 import platform
-from itertools import count
-from warnings import catch_warnings, filterwarnings
 
 from twisted.trial import unittest
 
-from scrapy.exceptions import ScrapyDeprecationWarning
 from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen
-from scrapy.utils.defer import deferred_f_from_coro_f, aiter_errback
+from scrapy.utils.defer import aiter_errback, deferred_f_from_coro_f
 from scrapy.utils.python import (
-    memoizemethod_noargs, binary_is_text, equal_attributes,
-    WeakKeyCache, get_func_args, to_bytes, to_unicode,
-    without_none_values, MutableChain, MutableAsyncChain)
+    MutableAsyncChain,
+    MutableChain,
+    binary_is_text,
+    equal_attributes,
+    get_func_args,
+    memoizemethod_noargs,
+    to_bytes,
+    to_unicode,
+    without_none_values,
+)
 
-
-__doctests__ = ['scrapy.utils.python']
+__doctests__ = ["scrapy.utils.python"]
 
 
 class MutableChainTest(unittest.TestCase):
     def test_mutablechain(self):
         m = MutableChain(range(2), [2, 3], (4, 5))
         m.extend(range(6, 7))
         m.extend([7, 8])
         m.extend([9, 10], (11, 12))
         self.assertEqual(next(m), 0)
         self.assertEqual(m.__next__(), 1)
-        with catch_warnings(record=True) as warnings:
-            self.assertEqual(m.next(), 2)
-            self.assertEqual(len(warnings), 1)
-            self.assertIn('scrapy.utils.python.MutableChain.__next__',
-                          str(warnings[0].message))
-        self.assertEqual(list(m), list(range(3, 13)))
+        self.assertEqual(list(m), list(range(2, 13)))
 
 
 class MutableAsyncChainTest(unittest.TestCase):
     @staticmethod
     async def g1():
         for i in range(3):
             yield i
@@ -84,56 +81,49 @@
 
         results = await collect_asyncgen(aiter_errback(m, lambda _: None))
         self.assertEqual(results, list(range(5)))
 
 
 class ToUnicodeTest(unittest.TestCase):
     def test_converting_an_utf8_encoded_string_to_unicode(self):
-        self.assertEqual(to_unicode(b'lel\xc3\xb1e'), 'lel\xf1e')
+        self.assertEqual(to_unicode(b"lel\xc3\xb1e"), "lel\xf1e")
 
     def test_converting_a_latin_1_encoded_string_to_unicode(self):
-        self.assertEqual(to_unicode(b'lel\xf1e', 'latin-1'), 'lel\xf1e')
+        self.assertEqual(to_unicode(b"lel\xf1e", "latin-1"), "lel\xf1e")
 
     def test_converting_a_unicode_to_unicode_should_return_the_same_object(self):
-        self.assertEqual(to_unicode('\xf1e\xf1e\xf1e'), '\xf1e\xf1e\xf1e')
+        self.assertEqual(to_unicode("\xf1e\xf1e\xf1e"), "\xf1e\xf1e\xf1e")
 
     def test_converting_a_strange_object_should_raise_TypeError(self):
         self.assertRaises(TypeError, to_unicode, 423)
 
     def test_errors_argument(self):
-        self.assertEqual(
-            to_unicode(b'a\xedb', 'utf-8', errors='replace'),
-            'a\ufffdb'
-        )
+        self.assertEqual(to_unicode(b"a\xedb", "utf-8", errors="replace"), "a\ufffdb")
 
 
 class ToBytesTest(unittest.TestCase):
     def test_converting_a_unicode_object_to_an_utf_8_encoded_string(self):
-        self.assertEqual(to_bytes('\xa3 49'), b'\xc2\xa3 49')
+        self.assertEqual(to_bytes("\xa3 49"), b"\xc2\xa3 49")
 
     def test_converting_a_unicode_object_to_a_latin_1_encoded_string(self):
-        self.assertEqual(to_bytes('\xa3 49', 'latin-1'), b'\xa3 49')
+        self.assertEqual(to_bytes("\xa3 49", "latin-1"), b"\xa3 49")
 
     def test_converting_a_regular_bytes_to_bytes_should_return_the_same_object(self):
-        self.assertEqual(to_bytes(b'lel\xf1e'), b'lel\xf1e')
+        self.assertEqual(to_bytes(b"lel\xf1e"), b"lel\xf1e")
 
     def test_converting_a_strange_object_should_raise_TypeError(self):
         self.assertRaises(TypeError, to_bytes, unittest)
 
     def test_errors_argument(self):
-        self.assertEqual(
-            to_bytes('a\ufffdb', 'latin-1', errors='replace'),
-            b'a?b'
-        )
+        self.assertEqual(to_bytes("a\ufffdb", "latin-1", errors="replace"), b"a?b")
 
 
 class MemoizedMethodTest(unittest.TestCase):
     def test_memoizemethod_noargs(self):
         class A:
-
             @memoizemethod_noargs
             def cached(self):
                 return object()
 
             def noncached(self):
                 return object()
 
@@ -146,93 +136,71 @@
 
 
 class BinaryIsTextTest(unittest.TestCase):
     def test_binaryistext(self):
         assert binary_is_text(b"hello")
 
     def test_utf_16_strings_contain_null_bytes(self):
-        assert binary_is_text("hello".encode('utf-16'))
+        assert binary_is_text("hello".encode("utf-16"))
 
     def test_one_with_encoding(self):
         assert binary_is_text(b"<div>Price \xa3</div>")
 
     def test_real_binary_bytes(self):
         assert not binary_is_text(b"\x02\xa3")
 
 
 class UtilsPythonTestCase(unittest.TestCase):
-
     def test_equal_attributes(self):
         class Obj:
             pass
 
         a = Obj()
         b = Obj()
         # no attributes given return False
         self.assertFalse(equal_attributes(a, b, []))
-        # not existent attributes
-        self.assertFalse(equal_attributes(a, b, ['x', 'y']))
+        # nonexistent attributes
+        self.assertFalse(equal_attributes(a, b, ["x", "y"]))
 
         a.x = 1
         b.x = 1
         # equal attribute
-        self.assertTrue(equal_attributes(a, b, ['x']))
+        self.assertTrue(equal_attributes(a, b, ["x"]))
 
         b.y = 2
         # obj1 has no attribute y
-        self.assertFalse(equal_attributes(a, b, ['x', 'y']))
+        self.assertFalse(equal_attributes(a, b, ["x", "y"]))
 
         a.y = 2
         # equal attributes
-        self.assertTrue(equal_attributes(a, b, ['x', 'y']))
+        self.assertTrue(equal_attributes(a, b, ["x", "y"]))
 
         a.y = 1
         # differente attributes
-        self.assertFalse(equal_attributes(a, b, ['x', 'y']))
+        self.assertFalse(equal_attributes(a, b, ["x", "y"]))
 
         # test callable
         a.meta = {}
         b.meta = {}
-        self.assertTrue(equal_attributes(a, b, ['meta']))
+        self.assertTrue(equal_attributes(a, b, ["meta"]))
 
         # compare ['meta']['a']
-        a.meta['z'] = 1
-        b.meta['z'] = 1
+        a.meta["z"] = 1
+        b.meta["z"] = 1
 
-        get_z = operator.itemgetter('z')
-        get_meta = operator.attrgetter('meta')
+        get_z = operator.itemgetter("z")
+        get_meta = operator.attrgetter("meta")
 
         def compare_z(obj):
             return get_z(get_meta(obj))
 
-        self.assertTrue(equal_attributes(a, b, [compare_z, 'x']))
+        self.assertTrue(equal_attributes(a, b, [compare_z, "x"]))
         # fail z equality
-        a.meta['z'] = 2
-        self.assertFalse(equal_attributes(a, b, [compare_z, 'x']))
-
-    def test_weakkeycache(self):
-        class _Weakme:
-            pass
-
-        _values = count()
-
-        with catch_warnings():
-            filterwarnings("ignore", category=ScrapyDeprecationWarning)
-            wk = WeakKeyCache(lambda k: next(_values))
-
-        k = _Weakme()
-        v = wk[k]
-        self.assertEqual(v, wk[k])
-        self.assertNotEqual(v, wk[_Weakme()])
-        self.assertEqual(v, wk[k])
-        del k
-        for _ in range(100):
-            if wk._weakdict:
-                gc.collect()
-        self.assertFalse(len(wk._weakdict))
+        a.meta["z"] = 2
+        self.assertFalse(equal_attributes(a, b, [compare_z, "x"]))
 
     def test_get_func_args(self):
         def f1(a, b, c):
             pass
 
         def f2(a, b=None, c=None):
             pass
@@ -244,48 +212,52 @@
             def __init__(self, a, b, c):
                 pass
 
             def method(self, a, b, c):
                 pass
 
         class Callable:
-
             def __call__(self, a, b, c):
                 pass
 
         a = A(1, 2, 3)
         cal = Callable()
         partial_f1 = functools.partial(f1, None)
         partial_f2 = functools.partial(f1, b=None)
         partial_f3 = functools.partial(partial_f2, None)
 
-        self.assertEqual(get_func_args(f1), ['a', 'b', 'c'])
-        self.assertEqual(get_func_args(f2), ['a', 'b', 'c'])
-        self.assertEqual(get_func_args(f3), ['a', 'b', 'c'])
-        self.assertEqual(get_func_args(A), ['a', 'b', 'c'])
-        self.assertEqual(get_func_args(a.method), ['a', 'b', 'c'])
-        self.assertEqual(get_func_args(partial_f1), ['b', 'c'])
-        self.assertEqual(get_func_args(partial_f2), ['a', 'c'])
-        self.assertEqual(get_func_args(partial_f3), ['c'])
-        self.assertEqual(get_func_args(cal), ['a', 'b', 'c'])
+        self.assertEqual(get_func_args(f1), ["a", "b", "c"])
+        self.assertEqual(get_func_args(f2), ["a", "b", "c"])
+        self.assertEqual(get_func_args(f3), ["a", "b", "c"])
+        self.assertEqual(get_func_args(A), ["a", "b", "c"])
+        self.assertEqual(get_func_args(a.method), ["a", "b", "c"])
+        self.assertEqual(get_func_args(partial_f1), ["b", "c"])
+        self.assertEqual(get_func_args(partial_f2), ["a", "c"])
+        self.assertEqual(get_func_args(partial_f3), ["c"])
+        self.assertEqual(get_func_args(cal), ["a", "b", "c"])
         self.assertEqual(get_func_args(object), [])
 
-        if platform.python_implementation() == 'CPython':
+        if platform.python_implementation() == "CPython":
             # TODO: how do we fix this to return the actual argument names?
             self.assertEqual(get_func_args(str.split), [])
             self.assertEqual(get_func_args(" ".join), [])
             self.assertEqual(get_func_args(operator.itemgetter(2)), [])
-        elif platform.python_implementation() == 'PyPy':
-            self.assertEqual(get_func_args(str.split, stripself=True), ['sep', 'maxsplit'])
-            self.assertEqual(get_func_args(operator.itemgetter(2), stripself=True), ['obj'])
-            self.assertEqual(get_func_args(" ".join, stripself=True), ['iterable'])
+        elif platform.python_implementation() == "PyPy":
+            self.assertEqual(
+                get_func_args(str.split, stripself=True), ["sep", "maxsplit"]
+            )
+            self.assertEqual(
+                get_func_args(operator.itemgetter(2), stripself=True), ["obj"]
+            )
+            self.assertEqual(get_func_args(" ".join, stripself=True), ["iterable"])
 
     def test_without_none_values(self):
         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])
         self.assertEqual(without_none_values((1, None, 3, 4)), (1, 3, 4))
         self.assertEqual(
-            without_none_values({'one': 1, 'none': None, 'three': 3, 'four': 4}),
-            {'one': 1, 'three': 3, 'four': 4})
+            without_none_values({"one": 1, "none": None, "three": 3, "four": 4}),
+            {"one": 1, "three": 3, "four": 4},
+        )
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_utils_request.py` & `Scrapy-2.8.0/tests/test_utils_request.py`

 * *Files 7% similar despite different names*

```diff
@@ -19,32 +19,40 @@
     request_fingerprint,
     request_httprepr,
 )
 from scrapy.utils.test import get_crawler
 
 
 class UtilsRequestTest(unittest.TestCase):
-
     def test_request_authenticate(self):
         r = Request("http://www.example.com")
-        request_authenticate(r, 'someuser', 'somepass')
-        self.assertEqual(r.headers['Authorization'], b'Basic c29tZXVzZXI6c29tZXBhc3M=')
+        request_authenticate(r, "someuser", "somepass")
+        self.assertEqual(r.headers["Authorization"], b"Basic c29tZXVzZXI6c29tZXBhc3M=")
 
     def test_request_httprepr(self):
         r1 = Request("http://www.example.com")
-        self.assertEqual(request_httprepr(r1), b'GET / HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
+        self.assertEqual(
+            request_httprepr(r1), b"GET / HTTP/1.1\r\nHost: www.example.com\r\n\r\n"
+        )
 
         r1 = Request("http://www.example.com/some/page.html?arg=1")
-        self.assertEqual(request_httprepr(r1), b'GET /some/page.html?arg=1 HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
+        self.assertEqual(
+            request_httprepr(r1),
+            b"GET /some/page.html?arg=1 HTTP/1.1\r\nHost: www.example.com\r\n\r\n",
+        )
 
-        r1 = Request("http://www.example.com", method='POST',
-                     headers={"Content-type": b"text/html"}, body=b"Some body")
+        r1 = Request(
+            "http://www.example.com",
+            method="POST",
+            headers={"Content-type": b"text/html"},
+            body=b"Some body",
+        )
         self.assertEqual(
             request_httprepr(r1),
-            b'POST / HTTP/1.1\r\nHost: www.example.com\r\nContent-Type: text/html\r\n\r\nSome body'
+            b"POST / HTTP/1.1\r\nHost: www.example.com\r\nContent-Type: text/html\r\n\r\nSome body",
         )
 
     def test_request_httprepr_for_non_http_request(self):
         # the representation is not important but it must not fail.
         request_httprepr(Request("file:///tmp/foo.txt"))
         request_httprepr(Request("ftp://localhost/tmp/foo.txt"))
 
@@ -57,145 +65,145 @@
         "WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]",
         "WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]",
     ] = _fingerprint_cache
     default_cache_key = (None, False)
     known_hashes: Tuple[Tuple[Request, Union[bytes, str], Dict], ...] = (
         (
             Request("http://example.org"),
-            b'xs\xd7\x0c3uj\x15\xfe\xd7d\x9b\xa9\t\xe0d\xbf\x9cXD',
+            b"xs\xd7\x0c3uj\x15\xfe\xd7d\x9b\xa9\t\xe0d\xbf\x9cXD",
             {},
         ),
         (
             Request("https://example.org"),
-            b'\xc04\x85P,\xaa\x91\x06\xf8t\xb4\xbd*\xd9\xe9\x8a:m\xc3l',
+            b"\xc04\x85P,\xaa\x91\x06\xf8t\xb4\xbd*\xd9\xe9\x8a:m\xc3l",
             {},
         ),
         (
             Request("https://example.org?a"),
-            b'G\xad\xb8Ck\x19\x1c\xed\x838,\x01\xc4\xde;\xee\xa5\x94a\x0c',
+            b"G\xad\xb8Ck\x19\x1c\xed\x838,\x01\xc4\xde;\xee\xa5\x94a\x0c",
             {},
         ),
         (
             Request("https://example.org?a=b"),
-            b'\x024MYb\x8a\xc2\x1e\xbc>\xd6\xac*\xda\x9cF\xc1r\x7f\x17',
+            b"\x024MYb\x8a\xc2\x1e\xbc>\xd6\xac*\xda\x9cF\xc1r\x7f\x17",
             {},
         ),
         (
             Request("https://example.org?a=b&a"),
-            b't+\xe8*\xfb\x84\xe3v\x1a}\x88p\xc0\xccB\xd7\x9d\xfez\x96',
+            b"t+\xe8*\xfb\x84\xe3v\x1a}\x88p\xc0\xccB\xd7\x9d\xfez\x96",
             {},
         ),
         (
             Request("https://example.org?a=b&a=c"),
-            b'\xda\x1ec\xd0\x9c\x08s`\xb4\x9b\xe2\xb6R\xf8k\xef\xeaQG\xef',
+            b"\xda\x1ec\xd0\x9c\x08s`\xb4\x9b\xe2\xb6R\xf8k\xef\xeaQG\xef",
             {},
         ),
         (
-            Request("https://example.org", method='POST'),
-            b'\x9d\xcdA\x0fT\x02:\xca\xa0}\x90\xda\x05B\xded\x8aN7\x1d',
+            Request("https://example.org", method="POST"),
+            b"\x9d\xcdA\x0fT\x02:\xca\xa0}\x90\xda\x05B\xded\x8aN7\x1d",
             {},
         ),
         (
-            Request("https://example.org", body=b'a'),
-            b'\xc34z>\xd8\x99\x8b\xda7\x05r\x99I\xa8\xa0x;\xa41_',
+            Request("https://example.org", body=b"a"),
+            b"\xc34z>\xd8\x99\x8b\xda7\x05r\x99I\xa8\xa0x;\xa41_",
             {},
         ),
         (
-            Request("https://example.org", method='POST', body=b'a'),
-            b'5`\xe2y4\xd0\x9d\xee\xe0\xbatw\x87Q\xe8O\xd78\xfc\xe7',
+            Request("https://example.org", method="POST", body=b"a"),
+            b"5`\xe2y4\xd0\x9d\xee\xe0\xbatw\x87Q\xe8O\xd78\xfc\xe7",
             {},
         ),
         (
-            Request("https://example.org#a", headers={'A': b'B'}),
-            b'\xc04\x85P,\xaa\x91\x06\xf8t\xb4\xbd*\xd9\xe9\x8a:m\xc3l',
+            Request("https://example.org#a", headers={"A": b"B"}),
+            b"\xc04\x85P,\xaa\x91\x06\xf8t\xb4\xbd*\xd9\xe9\x8a:m\xc3l",
             {},
         ),
         (
-            Request("https://example.org#a", headers={'A': b'B'}),
-            b']\xc7\x1f\xf2\xafG2\xbc\xa4\xfa\x99\n33\xda\x18\x94\x81U.',
-            {'include_headers': ['A']},
+            Request("https://example.org#a", headers={"A": b"B"}),
+            b"]\xc7\x1f\xf2\xafG2\xbc\xa4\xfa\x99\n33\xda\x18\x94\x81U.",
+            {"include_headers": ["A"]},
         ),
         (
-            Request("https://example.org#a", headers={'A': b'B'}),
-            b'<\x1a\xeb\x85y\xdeW\xfb\xdcq\x88\xee\xaf\x17\xdd\x0c\xbfH\x18\x1f',
-            {'keep_fragments': True},
+            Request("https://example.org#a", headers={"A": b"B"}),
+            b"<\x1a\xeb\x85y\xdeW\xfb\xdcq\x88\xee\xaf\x17\xdd\x0c\xbfH\x18\x1f",
+            {"keep_fragments": True},
         ),
         (
-            Request("https://example.org#a", headers={'A': b'B'}),
-            b'\xc1\xef~\x94\x9bS\xc1\x83\t\xdcz8\x9f\xdc{\x11\x16I.\x11',
-            {'include_headers': ['A'], 'keep_fragments': True},
+            Request("https://example.org#a", headers={"A": b"B"}),
+            b"\xc1\xef~\x94\x9bS\xc1\x83\t\xdcz8\x9f\xdc{\x11\x16I.\x11",
+            {"include_headers": ["A"], "keep_fragments": True},
         ),
         (
             Request("https://example.org/ab"),
-            b'N\xe5l\xb8\x12@iw\xe2\xf3\x1bp\xea\xffp!u\xe2\x8a\xc6',
+            b"N\xe5l\xb8\x12@iw\xe2\xf3\x1bp\xea\xffp!u\xe2\x8a\xc6",
             {},
         ),
         (
-            Request("https://example.org/a", body=b'b'),
-            b'_NOv\xbco$6\xfcW\x9f\xb24g\x9f\xbb\xdd\xa82\xc5',
+            Request("https://example.org/a", body=b"b"),
+            b"_NOv\xbco$6\xfcW\x9f\xb24g\x9f\xbb\xdd\xa82\xc5",
             {},
         ),
     )
 
     def test_query_string_key_order(self):
         r1 = Request("http://www.example.com/query?id=111&cat=222")
         r2 = Request("http://www.example.com/query?cat=222&id=111")
         self.assertEqual(self.function(r1), self.function(r1))
         self.assertEqual(self.function(r1), self.function(r2))
 
     def test_query_string_key_without_value(self):
-        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78132,199')
-        r2 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')
+        r1 = Request("http://www.example.com/hnnoticiaj1.aspx?78132,199")
+        r2 = Request("http://www.example.com/hnnoticiaj1.aspx?78160,199")
         self.assertNotEqual(self.function(r1), self.function(r2))
 
     def test_caching(self):
-        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')
-        self.assertEqual(
-            self.function(r1),
-            self.cache[r1][self.default_cache_key]
-        )
+        r1 = Request("http://www.example.com/hnnoticiaj1.aspx?78160,199")
+        self.assertEqual(self.function(r1), self.cache[r1][self.default_cache_key])
 
     def test_header(self):
         r1 = Request("http://www.example.com/members/offers.html")
         r2 = Request("http://www.example.com/members/offers.html")
-        r2.headers['SESSIONID'] = b"somehash"
+        r2.headers["SESSIONID"] = b"somehash"
         self.assertEqual(self.function(r1), self.function(r2))
 
     def test_headers(self):
         r1 = Request("http://www.example.com/")
         r2 = Request("http://www.example.com/")
-        r2.headers['Accept-Language'] = b'en'
+        r2.headers["Accept-Language"] = b"en"
         r3 = Request("http://www.example.com/")
-        r3.headers['Accept-Language'] = b'en'
-        r3.headers['SESSIONID'] = b"somehash"
+        r3.headers["Accept-Language"] = b"en"
+        r3.headers["SESSIONID"] = b"somehash"
 
         self.assertEqual(self.function(r1), self.function(r2), self.function(r3))
 
-        self.assertEqual(self.function(r1),
-                         self.function(r1, include_headers=['Accept-Language']))
+        self.assertEqual(
+            self.function(r1), self.function(r1, include_headers=["Accept-Language"])
+        )
 
         self.assertNotEqual(
-            self.function(r1),
-            self.function(r2, include_headers=['Accept-Language']))
+            self.function(r1), self.function(r2, include_headers=["Accept-Language"])
+        )
 
-        self.assertEqual(self.function(r3, include_headers=['accept-language', 'sessionid']),
-                         self.function(r3, include_headers=['SESSIONID', 'Accept-Language']))
+        self.assertEqual(
+            self.function(r3, include_headers=["accept-language", "sessionid"]),
+            self.function(r3, include_headers=["SESSIONID", "Accept-Language"]),
+        )
 
     def test_fragment(self):
         r1 = Request("http://www.example.com/test.html")
         r2 = Request("http://www.example.com/test.html#fragment")
         self.assertEqual(self.function(r1), self.function(r2))
         self.assertEqual(self.function(r1), self.function(r1, keep_fragments=True))
         self.assertNotEqual(self.function(r2), self.function(r2, keep_fragments=True))
         self.assertNotEqual(self.function(r1), self.function(r2, keep_fragments=True))
 
     def test_method_and_body(self):
         r1 = Request("http://www.example.com")
-        r2 = Request("http://www.example.com", method='POST')
-        r3 = Request("http://www.example.com", method='POST', body=b'request body')
+        r2 = Request("http://www.example.com", method="POST")
+        r3 = Request("http://www.example.com", method="POST", body=b"request body")
 
         self.assertNotEqual(self.function(r1), self.function(r2))
         self.assertNotEqual(self.function(r2), self.function(r3))
 
     def test_request_replace(self):
         # cached fingerprint must be cleared on request copy
         r1 = Request("http://www.example.com")
@@ -205,175 +213,161 @@
         self.assertNotEqual(fp1, fp2)
 
     def test_part_separation(self):
         # An old implementation used to serialize request data in a way that
         # would put the body right after the URL.
         r1 = Request("http://www.example.com/foo")
         fp1 = self.function(r1)
-        r2 = Request("http://www.example.com/f", body=b'oo')
+        r2 = Request("http://www.example.com/f", body=b"oo")
         fp2 = self.function(r2)
         self.assertNotEqual(fp1, fp2)
 
     def test_hashes(self):
         """Test hardcoded hashes, to make sure future changes to not introduce
         backward incompatibilities."""
         actual = [
-            self.function(request, **kwargs)
-            for request, _, kwargs in self.known_hashes
-        ]
-        expected = [
-            _fingerprint
-            for _, _fingerprint, _ in self.known_hashes
+            self.function(request, **kwargs) for request, _, kwargs in self.known_hashes
         ]
+        expected = [_fingerprint for _, _fingerprint, _ in self.known_hashes]
         self.assertEqual(actual, expected)
 
 
 class RequestFingerprintTest(FingerprintTest):
     function = staticmethod(request_fingerprint)
     cache = _deprecated_fingerprint_cache
     known_hashes: Tuple[Tuple[Request, Union[bytes, str], Dict], ...] = (
         (
             Request("http://example.org"),
-            'b2e5245ef826fd9576c93bd6e392fce3133fab62',
+            "b2e5245ef826fd9576c93bd6e392fce3133fab62",
             {},
         ),
         (
             Request("https://example.org"),
-            'bd10a0a89ea32cdee77917320f1309b0da87e892',
+            "bd10a0a89ea32cdee77917320f1309b0da87e892",
             {},
         ),
         (
             Request("https://example.org?a"),
-            '2fb7d48ae02f04b749f40caa969c0bc3c43204ce',
+            "2fb7d48ae02f04b749f40caa969c0bc3c43204ce",
             {},
         ),
         (
             Request("https://example.org?a=b"),
-            '42e5fe149b147476e3f67ad0670c57b4cc57856a',
+            "42e5fe149b147476e3f67ad0670c57b4cc57856a",
             {},
         ),
         (
             Request("https://example.org?a=b&a"),
-            'd23a9787cb56c6375c2cae4453c5a8c634526942',
+            "d23a9787cb56c6375c2cae4453c5a8c634526942",
             {},
         ),
         (
             Request("https://example.org?a=b&a=c"),
-            '9a18a7a8552a9182b7f1e05d33876409e421e5c5',
+            "9a18a7a8552a9182b7f1e05d33876409e421e5c5",
             {},
         ),
         (
-            Request("https://example.org", method='POST'),
-            'ba20a80cb5c5ca460021ceefb3c2467b2bfd1bc6',
+            Request("https://example.org", method="POST"),
+            "ba20a80cb5c5ca460021ceefb3c2467b2bfd1bc6",
             {},
         ),
         (
-            Request("https://example.org", body=b'a'),
-            '4bb136e54e715a4ea7a9dd1101831765d33f2d60',
+            Request("https://example.org", body=b"a"),
+            "4bb136e54e715a4ea7a9dd1101831765d33f2d60",
             {},
         ),
         (
-            Request("https://example.org", method='POST', body=b'a'),
-            '6c6595374a304b293be762f7b7be3f54e9947c65',
+            Request("https://example.org", method="POST", body=b"a"),
+            "6c6595374a304b293be762f7b7be3f54e9947c65",
             {},
         ),
         (
-            Request("https://example.org#a", headers={'A': b'B'}),
-            'bd10a0a89ea32cdee77917320f1309b0da87e892',
+            Request("https://example.org#a", headers={"A": b"B"}),
+            "bd10a0a89ea32cdee77917320f1309b0da87e892",
             {},
         ),
         (
-            Request("https://example.org#a", headers={'A': b'B'}),
-            '515b633cb3ca502a33a9d8c890e889ec1e425e65',
-            {'include_headers': ['A']},
+            Request("https://example.org#a", headers={"A": b"B"}),
+            "515b633cb3ca502a33a9d8c890e889ec1e425e65",
+            {"include_headers": ["A"]},
         ),
         (
-            Request("https://example.org#a", headers={'A': b'B'}),
-            '505c96e7da675920dfef58725e8c957dfdb38f47',
-            {'keep_fragments': True},
+            Request("https://example.org#a", headers={"A": b"B"}),
+            "505c96e7da675920dfef58725e8c957dfdb38f47",
+            {"keep_fragments": True},
         ),
         (
-            Request("https://example.org#a", headers={'A': b'B'}),
-            'd6f673cdcb661b7970c2b9a00ee63e87d1e2e5da',
-            {'include_headers': ['A'], 'keep_fragments': True},
+            Request("https://example.org#a", headers={"A": b"B"}),
+            "d6f673cdcb661b7970c2b9a00ee63e87d1e2e5da",
+            {"include_headers": ["A"], "keep_fragments": True},
         ),
         (
             Request("https://example.org/ab"),
-            '4e2870fee58582d6f81755e9b8fdefe3cba0c951',
+            "4e2870fee58582d6f81755e9b8fdefe3cba0c951",
             {},
         ),
         (
-            Request("https://example.org/a", body=b'b'),
-            '4e2870fee58582d6f81755e9b8fdefe3cba0c951',
+            Request("https://example.org/a", body=b"b"),
+            "4e2870fee58582d6f81755e9b8fdefe3cba0c951",
             {},
         ),
     )
 
     def setUp(self) -> None:
         warnings.simplefilter("ignore", ScrapyDeprecationWarning)
 
     def tearDown(self) -> None:
         warnings.simplefilter("default", ScrapyDeprecationWarning)
 
-    @pytest.mark.xfail(reason='known bug kept for backward compatibility', strict=True)
+    @pytest.mark.xfail(reason="known bug kept for backward compatibility", strict=True)
     def test_part_separation(self):
         super().test_part_separation()
 
 
 class RequestFingerprintDeprecationTest(unittest.TestCase):
-
     def test_deprecation_default_parameters(self):
         with pytest.warns(ScrapyDeprecationWarning) as warnings:
             request_fingerprint(Request("http://www.example.com"))
         messages = [str(warning.message) for warning in warnings]
         self.assertTrue(
-            any(
-                'Call to deprecated function' in message
-                for message in messages
-            )
+            any("Call to deprecated function" in message for message in messages)
         )
-        self.assertFalse(any('non-default' in message for message in messages))
+        self.assertFalse(any("non-default" in message for message in messages))
 
     def test_deprecation_non_default_parameters(self):
         with pytest.warns(ScrapyDeprecationWarning) as warnings:
             request_fingerprint(Request("http://www.example.com"), keep_fragments=True)
         messages = [str(warning.message) for warning in warnings]
         self.assertTrue(
-            any(
-                'Call to deprecated function' in message
-                for message in messages
-            )
+            any("Call to deprecated function" in message for message in messages)
         )
-        self.assertTrue(any('non-default' in message for message in messages))
+        self.assertTrue(any("non-default" in message for message in messages))
 
 
 class RequestFingerprintAsBytesTest(FingerprintTest):
     function = staticmethod(_request_fingerprint_as_bytes)
     cache = _deprecated_fingerprint_cache
     known_hashes = RequestFingerprintTest.known_hashes
 
     def test_caching(self):
-        r1 = Request('http://www.example.com/hnnoticiaj1.aspx?78160,199')
+        r1 = Request("http://www.example.com/hnnoticiaj1.aspx?78160,199")
         self.assertEqual(
-            self.function(r1),
-            bytes.fromhex(self.cache[r1][self.default_cache_key])
+            self.function(r1), bytes.fromhex(self.cache[r1][self.default_cache_key])
         )
 
-    @pytest.mark.xfail(reason='known bug kept for backward compatibility', strict=True)
+    @pytest.mark.xfail(reason="known bug kept for backward compatibility", strict=True)
     def test_part_separation(self):
         super().test_part_separation()
 
     def test_hashes(self):
         actual = [
-            self.function(request, **kwargs)
-            for request, _, kwargs in self.known_hashes
+            self.function(request, **kwargs) for request, _, kwargs in self.known_hashes
         ]
         expected = [
-            bytes.fromhex(_fingerprint)
-            for _, _fingerprint, _ in self.known_hashes
+            bytes.fromhex(_fingerprint) for _, _fingerprint, _ in self.known_hashes
         ]
         self.assertEqual(actual, expected)
 
 
 _fingerprint_cache_2_6: Mapping[Request, Tuple[None, bool]] = WeakKeyDictionary()
 
 
@@ -381,63 +375,64 @@
     if include_headers:
         include_headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers))
     cache = _fingerprint_cache_2_6.setdefault(request, {})
     cache_key = (include_headers, keep_fragments)
     if cache_key not in cache:
         fp = sha1()
         fp.update(to_bytes(request.method))
-        fp.update(to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments)))
-        fp.update(request.body or b'')
+        fp.update(
+            to_bytes(canonicalize_url(request.url, keep_fragments=keep_fragments))
+        )
+        fp.update(request.body or b"")
         if include_headers:
             for hdr in include_headers:
                 if hdr in request.headers:
                     fp.update(hdr)
                     for v in request.headers.getlist(hdr):
                         fp.update(v)
         cache[cache_key] = fp.hexdigest()
     return cache[cache_key]
 
 
 REQUEST_OBJECTS_TO_TEST = (
     Request("http://www.example.com/"),
     Request("http://www.example.com/query?id=111&cat=222"),
     Request("http://www.example.com/query?cat=222&id=111"),
-    Request('http://www.example.com/hnnoticiaj1.aspx?78132,199'),
-    Request('http://www.example.com/hnnoticiaj1.aspx?78160,199'),
+    Request("http://www.example.com/hnnoticiaj1.aspx?78132,199"),
+    Request("http://www.example.com/hnnoticiaj1.aspx?78160,199"),
     Request("http://www.example.com/members/offers.html"),
     Request(
         "http://www.example.com/members/offers.html",
-        headers={'SESSIONID': b"somehash"},
+        headers={"SESSIONID": b"somehash"},
     ),
     Request(
         "http://www.example.com/",
-        headers={'Accept-Language': b"en"},
+        headers={"Accept-Language": b"en"},
     ),
     Request(
         "http://www.example.com/",
         headers={
-            'Accept-Language': b"en",
-            'SESSIONID': b"somehash",
+            "Accept-Language": b"en",
+            "SESSIONID": b"somehash",
         },
     ),
     Request("http://www.example.com/test.html"),
     Request("http://www.example.com/test.html#fragment"),
-    Request("http://www.example.com", method='POST'),
-    Request("http://www.example.com", method='POST', body=b'request body'),
+    Request("http://www.example.com", method="POST"),
+    Request("http://www.example.com", method="POST", body=b"request body"),
 )
 
 
 class BackwardCompatibilityTestCase(unittest.TestCase):
-
     def test_function_backward_compatibility(self):
         include_headers_to_test = (
             None,
-            ['Accept-Language'],
-            ['accept-language', 'sessionid'],
-            ['SESSIONID', 'Accept-Language'],
+            ["Accept-Language"],
+            ["accept-language", "sessionid"],
+            ["SESSIONID", "Accept-Language"],
         )
         for request_object in REQUEST_OBJECTS_TO_TEST:
             for include_headers in include_headers_to_test:
                 for keep_fragments in (False, True):
                     with warnings.catch_warnings():
                         warnings.simplefilter("ignore")
                         fp = request_fingerprint(
@@ -471,219 +466,207 @@
             cache = WeakKeyDictionary()
 
             def fingerprint(self, request):
                 if request not in self.cache:
                     fp = sha1()
                     fp.update(to_bytes(request.method))
                     fp.update(to_bytes(canonicalize_url(request.url)))
-                    fp.update(request.body or b'')
+                    fp.update(request.body or b"")
                     self.cache[request] = fp.digest()
                 return self.cache[request]
 
         for request_object in REQUEST_OBJECTS_TO_TEST:
             with warnings.catch_warnings() as logged_warnings:
                 settings = {
-                    'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,
+                    "REQUEST_FINGERPRINTER_CLASS": RequestFingerprinter,
                 }
                 crawler = get_crawler(settings_dict=settings)
                 fp = crawler.request_fingerprinter.fingerprint(request_object)
             old_fp = request_fingerprint_2_6(request_object)
             self.assertEqual(fp.hex(), old_fp)
             self.assertFalse(logged_warnings)
 
 
 class RequestFingerprinterTestCase(unittest.TestCase):
-
     def test_default_implementation(self):
         with warnings.catch_warnings(record=True) as logged_warnings:
             crawler = get_crawler(prevent_warnings=False)
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         self.assertEqual(
             crawler.request_fingerprinter.fingerprint(request),
             _request_fingerprint_as_bytes(request),
         )
         self.assertTrue(logged_warnings)
 
     def test_deprecated_implementation(self):
         settings = {
-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.6',
+            "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.6",
         }
         with warnings.catch_warnings(record=True) as logged_warnings:
             crawler = get_crawler(settings_dict=settings)
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         self.assertEqual(
             crawler.request_fingerprinter.fingerprint(request),
             _request_fingerprint_as_bytes(request),
         )
         self.assertTrue(logged_warnings)
 
     def test_recommended_implementation(self):
         settings = {
-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
+            "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.7",
         }
         with warnings.catch_warnings(record=True) as logged_warnings:
             crawler = get_crawler(settings_dict=settings)
-        request = Request('https://example.com')
+        request = Request("https://example.com")
         self.assertEqual(
             crawler.request_fingerprinter.fingerprint(request),
             fingerprint(request),
         )
         self.assertFalse(logged_warnings)
 
     def test_unknown_implementation(self):
         settings = {
-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.5',
+            "REQUEST_FINGERPRINTER_IMPLEMENTATION": "2.5",
         }
         with self.assertRaises(ValueError):
             get_crawler(settings_dict=settings)
 
 
 class CustomRequestFingerprinterTestCase(unittest.TestCase):
-
     def test_include_headers(self):
-
         class RequestFingerprinter:
-
             def fingerprint(self, request):
-                return fingerprint(request, include_headers=['X-ID'])
+                return fingerprint(request, include_headers=["X-ID"])
 
         settings = {
-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,
+            "REQUEST_FINGERPRINTER_CLASS": RequestFingerprinter,
         }
         crawler = get_crawler(settings_dict=settings)
 
-        r1 = Request("http://www.example.com", headers={'X-ID': '1'})
+        r1 = Request("http://www.example.com", headers={"X-ID": "1"})
         fp1 = crawler.request_fingerprinter.fingerprint(r1)
-        r2 = Request("http://www.example.com", headers={'X-ID': '2'})
+        r2 = Request("http://www.example.com", headers={"X-ID": "2"})
         fp2 = crawler.request_fingerprinter.fingerprint(r2)
         self.assertNotEqual(fp1, fp2)
 
     def test_dont_canonicalize(self):
-
         class RequestFingerprinter:
             cache = WeakKeyDictionary()
 
             def fingerprint(self, request):
                 if request not in self.cache:
                     fp = sha1()
                     fp.update(to_bytes(request.url))
                     self.cache[request] = fp.digest()
                 return self.cache[request]
 
         settings = {
-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,
+            "REQUEST_FINGERPRINTER_CLASS": RequestFingerprinter,
         }
         crawler = get_crawler(settings_dict=settings)
 
         r1 = Request("http://www.example.com?a=1&a=2")
         fp1 = crawler.request_fingerprinter.fingerprint(r1)
         r2 = Request("http://www.example.com?a=2&a=1")
         fp2 = crawler.request_fingerprinter.fingerprint(r2)
         self.assertNotEqual(fp1, fp2)
 
     def test_meta(self):
-
         class RequestFingerprinter:
-
             def fingerprint(self, request):
-                if 'fingerprint' in request.meta:
-                    return request.meta['fingerprint']
+                if "fingerprint" in request.meta:
+                    return request.meta["fingerprint"]
                 return fingerprint(request)
 
         settings = {
-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,
+            "REQUEST_FINGERPRINTER_CLASS": RequestFingerprinter,
         }
         crawler = get_crawler(settings_dict=settings)
 
         r1 = Request("http://www.example.com")
         fp1 = crawler.request_fingerprinter.fingerprint(r1)
-        r2 = Request("http://www.example.com", meta={'fingerprint': 'a'})
+        r2 = Request("http://www.example.com", meta={"fingerprint": "a"})
         fp2 = crawler.request_fingerprinter.fingerprint(r2)
-        r3 = Request("http://www.example.com", meta={'fingerprint': 'a'})
+        r3 = Request("http://www.example.com", meta={"fingerprint": "a"})
         fp3 = crawler.request_fingerprinter.fingerprint(r3)
-        r4 = Request("http://www.example.com", meta={'fingerprint': 'b'})
+        r4 = Request("http://www.example.com", meta={"fingerprint": "b"})
         fp4 = crawler.request_fingerprinter.fingerprint(r4)
         self.assertNotEqual(fp1, fp2)
         self.assertNotEqual(fp1, fp4)
         self.assertNotEqual(fp2, fp4)
         self.assertEqual(fp2, fp3)
 
     def test_from_crawler(self):
-
         class RequestFingerprinter:
-
             @classmethod
             def from_crawler(cls, crawler):
                 return cls(crawler)
 
             def __init__(self, crawler):
-                self._fingerprint = crawler.settings['FINGERPRINT']
+                self._fingerprint = crawler.settings["FINGERPRINT"]
 
             def fingerprint(self, request):
                 return self._fingerprint
 
         settings = {
-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,
-            'FINGERPRINT': b'fingerprint',
+            "REQUEST_FINGERPRINTER_CLASS": RequestFingerprinter,
+            "FINGERPRINT": b"fingerprint",
         }
         crawler = get_crawler(settings_dict=settings)
 
         request = Request("http://www.example.com")
         fingerprint = crawler.request_fingerprinter.fingerprint(request)
-        self.assertEqual(fingerprint, settings['FINGERPRINT'])
+        self.assertEqual(fingerprint, settings["FINGERPRINT"])
 
     def test_from_settings(self):
-
         class RequestFingerprinter:
-
             @classmethod
             def from_settings(cls, settings):
                 return cls(settings)
 
             def __init__(self, settings):
-                self._fingerprint = settings['FINGERPRINT']
+                self._fingerprint = settings["FINGERPRINT"]
 
             def fingerprint(self, request):
                 return self._fingerprint
 
         settings = {
-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,
-            'FINGERPRINT': b'fingerprint',
+            "REQUEST_FINGERPRINTER_CLASS": RequestFingerprinter,
+            "FINGERPRINT": b"fingerprint",
         }
         crawler = get_crawler(settings_dict=settings)
 
         request = Request("http://www.example.com")
         fingerprint = crawler.request_fingerprinter.fingerprint(request)
-        self.assertEqual(fingerprint, settings['FINGERPRINT'])
+        self.assertEqual(fingerprint, settings["FINGERPRINT"])
 
     def test_from_crawler_and_settings(self):
-
         class RequestFingerprinter:
 
             # This method is ignored due to the presence of from_crawler
             @classmethod
             def from_settings(cls, settings):
                 return cls(settings)
 
             @classmethod
             def from_crawler(cls, crawler):
                 return cls(crawler)
 
             def __init__(self, crawler):
-                self._fingerprint = crawler.settings['FINGERPRINT']
+                self._fingerprint = crawler.settings["FINGERPRINT"]
 
             def fingerprint(self, request):
                 return self._fingerprint
 
         settings = {
-            'REQUEST_FINGERPRINTER_CLASS': RequestFingerprinter,
-            'FINGERPRINT': b'fingerprint',
+            "REQUEST_FINGERPRINTER_CLASS": RequestFingerprinter,
+            "FINGERPRINT": b"fingerprint",
         }
         crawler = get_crawler(settings_dict=settings)
 
         request = Request("http://www.example.com")
         fingerprint = crawler.request_fingerprinter.fingerprint(request)
-        self.assertEqual(fingerprint, settings['FINGERPRINT'])
+        self.assertEqual(fingerprint, settings["FINGERPRINT"])
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_utils_response.py` & `Scrapy-2.8.0/tests/test_utils_response.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,147 +1,200 @@
-import os
 import unittest
 import warnings
+from pathlib import Path
 from urllib.parse import urlparse
 
 from scrapy.exceptions import ScrapyDeprecationWarning
-from scrapy.http import Response, TextResponse, HtmlResponse
+from scrapy.http import HtmlResponse, Response, TextResponse
 from scrapy.utils.python import to_bytes
-from scrapy.utils.response import (response_httprepr, open_in_browser,
-                                   get_meta_refresh, get_base_url, response_status_message)
+from scrapy.utils.response import (
+    get_base_url,
+    get_meta_refresh,
+    open_in_browser,
+    response_httprepr,
+    response_status_message,
+)
 
-
-__doctests__ = ['scrapy.utils.response']
+__doctests__ = ["scrapy.utils.response"]
 
 
 class ResponseUtilsTest(unittest.TestCase):
-    dummy_response = TextResponse(url='http://example.org/', body=b'dummy_response')
+    dummy_response = TextResponse(url="http://example.org/", body=b"dummy_response")
 
     def test_response_httprepr(self):
         with warnings.catch_warnings():
             warnings.simplefilter("ignore", ScrapyDeprecationWarning)
 
             r1 = Response("http://www.example.com")
-            self.assertEqual(response_httprepr(r1), b'HTTP/1.1 200 OK\r\n\r\n')
+            self.assertEqual(response_httprepr(r1), b"HTTP/1.1 200 OK\r\n\r\n")
 
-            r1 = Response("http://www.example.com", status=404,
-                          headers={"Content-type": "text/html"}, body=b"Some body")
-            self.assertEqual(response_httprepr(r1),
-                             b'HTTP/1.1 404 Not Found\r\nContent-Type: text/html\r\n\r\nSome body')
-
-            r1 = Response("http://www.example.com", status=6666,
-                          headers={"Content-type": "text/html"}, body=b"Some body")
-            self.assertEqual(response_httprepr(r1),
-                             b'HTTP/1.1 6666 \r\nContent-Type: text/html\r\n\r\nSome body')
+            r1 = Response(
+                "http://www.example.com",
+                status=404,
+                headers={"Content-type": "text/html"},
+                body=b"Some body",
+            )
+            self.assertEqual(
+                response_httprepr(r1),
+                b"HTTP/1.1 404 Not Found\r\nContent-Type: text/html\r\n\r\nSome body",
+            )
+
+            r1 = Response(
+                "http://www.example.com",
+                status=6666,
+                headers={"Content-type": "text/html"},
+                body=b"Some body",
+            )
+            self.assertEqual(
+                response_httprepr(r1),
+                b"HTTP/1.1 6666 \r\nContent-Type: text/html\r\n\r\nSome body",
+            )
 
     def test_open_in_browser(self):
         url = "http:///www.example.com/some/page.html"
         body = b"<html> <head> <title>test page</title> </head> <body>test body</body> </html>"
 
         def browser_open(burl):
             path = urlparse(burl).path
-            if not os.path.exists(path):
-                path = burl.replace('file://', '')
-            with open(path, "rb") as f:
-                bbody = f.read()
+            if not path or not Path(path).exists():
+                path = burl.replace("file://", "")
+            bbody = Path(path).read_bytes()
             self.assertIn(b'<base href="' + to_bytes(url) + b'">', bbody)
             return True
+
         response = HtmlResponse(url, body=body)
         assert open_in_browser(response, _openfunc=browser_open), "Browser not called"
 
         resp = Response(url, body=body)
         self.assertRaises(TypeError, open_in_browser, resp, debug=True)
 
     def test_get_meta_refresh(self):
-        r1 = HtmlResponse("http://www.example.com", body=b"""
+        r1 = HtmlResponse(
+            "http://www.example.com",
+            body=b"""
         <html>
         <head><title>Dummy</title><meta http-equiv="refresh" content="5;url=http://example.org/newpage" /></head>
         <body>blahablsdfsal&amp;</body>
-        </html>""")
-        r2 = HtmlResponse("http://www.example.com", body=b"""
+        </html>""",
+        )
+        r2 = HtmlResponse(
+            "http://www.example.com",
+            body=b"""
         <html>
         <head><title>Dummy</title><noScript>
         <meta http-equiv="refresh" content="5;url=http://example.org/newpage" /></head>
         </noSCRIPT>
         <body>blahablsdfsal&amp;</body>
-        </html>""")
-        r3 = HtmlResponse("http://www.example.com", body=b"""
+        </html>""",
+        )
+        r3 = HtmlResponse(
+            "http://www.example.com",
+            body=b"""
     <noscript><meta http-equiv="REFRESH" content="0;url=http://www.example.com/newpage</noscript>
     <script type="text/javascript">
     if(!checkCookies()){
         document.write('<meta http-equiv="REFRESH" content="0;url=http://www.example.com/newpage">');
     }
     </script>
-        """)
-        self.assertEqual(get_meta_refresh(r1), (5.0, 'http://example.org/newpage'))
+        """,
+        )
+        self.assertEqual(get_meta_refresh(r1), (5.0, "http://example.org/newpage"))
         self.assertEqual(get_meta_refresh(r2), (None, None))
         self.assertEqual(get_meta_refresh(r3), (None, None))
 
     def test_get_base_url(self):
-        resp = HtmlResponse("http://www.example.com", body=b"""
+        resp = HtmlResponse(
+            "http://www.example.com",
+            body=b"""
         <html>
         <head><base href="http://www.example.com/img/" target="_blank"></head>
         <body>blahablsdfsal&amp;</body>
-        </html>""")
+        </html>""",
+        )
         self.assertEqual(get_base_url(resp), "http://www.example.com/img/")
 
-        resp2 = HtmlResponse("http://www.example.com", body=b"""
-        <html><body>blahablsdfsal&amp;</body></html>""")
+        resp2 = HtmlResponse(
+            "http://www.example.com",
+            body=b"""
+        <html><body>blahablsdfsal&amp;</body></html>""",
+        )
         self.assertEqual(get_base_url(resp2), "http://www.example.com")
 
     def test_response_status_message(self):
-        self.assertEqual(response_status_message(200), '200 OK')
-        self.assertEqual(response_status_message(404), '404 Not Found')
+        self.assertEqual(response_status_message(200), "200 OK")
+        self.assertEqual(response_status_message(404), "404 Not Found")
         self.assertEqual(response_status_message(573), "573 Unknown Status")
 
     def test_inject_base_url(self):
         url = "http://www.example.com"
 
         def check_base_url(burl):
             path = urlparse(burl).path
-            if not os.path.exists(path):
-                path = burl.replace('file://', '')
-            with open(path, "rb") as f:
-                bbody = f.read()
+            if not path or not Path(path).exists():
+                path = burl.replace("file://", "")
+            bbody = Path(path).read_bytes()
             self.assertEqual(bbody.count(b'<base href="' + to_bytes(url) + b'">'), 1)
             return True
 
-        r1 = HtmlResponse(url, body=b"""
+        r1 = HtmlResponse(
+            url,
+            body=b"""
         <html>
             <head><title>Dummy</title></head>
             <body><p>Hello world.</p></body>
-        </html>""")
-        r2 = HtmlResponse(url, body=b"""
+        </html>""",
+        )
+        r2 = HtmlResponse(
+            url,
+            body=b"""
         <html>
             <head id="foo"><title>Dummy</title></head>
             <body>Hello world.</body>
-        </html>""")
-        r3 = HtmlResponse(url, body=b"""
+        </html>""",
+        )
+        r3 = HtmlResponse(
+            url,
+            body=b"""
         <html>
             <head><title>Dummy</title></head>
             <body>
                 <header>Hello header</header>
                 <p>Hello world.</p>
             </body>
-        </html>""")
-        r4 = HtmlResponse(url, body=b"""
+        </html>""",
+        )
+        r4 = HtmlResponse(
+            url,
+            body=b"""
         <html>
             <!-- <head>Dummy comment</head> -->
             <head><title>Dummy</title></head>
             <body><p>Hello world.</p></body>
-        </html>""")
-        r5 = HtmlResponse(url, body=b"""
+        </html>""",
+        )
+        r5 = HtmlResponse(
+            url,
+            body=b"""
         <html>
             <!--[if IE]>
             <head><title>IE head</title></head>
             <![endif]-->
             <!--[if !IE]>-->
             <head><title>Standard head</title></head>
             <!--<![endif]-->
             <body><p>Hello world.</p></body>
-        </html>""")
+        </html>""",
+        )
 
         assert open_in_browser(r1, _openfunc=check_base_url), "Inject base url"
-        assert open_in_browser(r2, _openfunc=check_base_url), "Inject base url with argumented head"
-        assert open_in_browser(r3, _openfunc=check_base_url), "Inject unique base url with misleading tag"
-        assert open_in_browser(r4, _openfunc=check_base_url), "Inject unique base url with misleading comment"
-        assert open_in_browser(r5, _openfunc=check_base_url), "Inject unique base url with conditional comment"
+        assert open_in_browser(
+            r2, _openfunc=check_base_url
+        ), "Inject base url with argumented head"
+        assert open_in_browser(
+            r3, _openfunc=check_base_url
+        ), "Inject unique base url with misleading tag"
+        assert open_in_browser(
+            r4, _openfunc=check_base_url
+        ), "Inject unique base url with misleading comment"
+        assert open_in_browser(
+            r5, _openfunc=check_base_url
+        ), "Inject unique base url with conditional comment"
```

### Comparing `Scrapy-2.7.1/tests/test_utils_serialize.py` & `Scrapy-2.8.0/tests/test_utils_serialize.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,47 +1,54 @@
+import dataclasses
 import datetime
 import json
 import unittest
-import dataclasses
 from decimal import Decimal
 
 import attr
 from twisted.internet import defer
 
 from scrapy.http import Request, Response
 from scrapy.utils.serialize import ScrapyJSONEncoder
 
 
 class JsonEncoderTestCase(unittest.TestCase):
-
     def setUp(self):
         self.encoder = ScrapyJSONEncoder(sort_keys=True)
 
     def test_encode_decode(self):
         dt = datetime.datetime(2010, 1, 2, 10, 11, 12)
         dts = "2010-01-02 10:11:12"
         d = datetime.date(2010, 1, 2)
         ds = "2010-01-02"
         t = datetime.time(10, 11, 12)
         ts = "10:11:12"
         dec = Decimal("1000.12")
         decs = "1000.12"
-        s = {'foo'}
-        ss = ['foo']
+        s = {"foo"}
+        ss = ["foo"]
         dt_set = {dt}
         dt_sets = [dts]
 
-        for input, output in [('foo', 'foo'), (d, ds), (t, ts), (dt, dts),
-                              (dec, decs), (['foo', d], ['foo', ds]), (s, ss),
-                              (dt_set, dt_sets)]:
-            self.assertEqual(self.encoder.encode(input),
-                             json.dumps(output, sort_keys=True))
+        for input, output in [
+            ("foo", "foo"),
+            (d, ds),
+            (t, ts),
+            (dt, dts),
+            (dec, decs),
+            (["foo", d], ["foo", ds]),
+            (s, ss),
+            (dt_set, dt_sets),
+        ]:
+            self.assertEqual(
+                self.encoder.encode(input), json.dumps(output, sort_keys=True)
+            )
 
     def test_encode_deferred(self):
-        self.assertIn('Deferred', self.encoder.encode(defer.Deferred()))
+        self.assertIn("Deferred", self.encoder.encode(defer.Deferred()))
 
     def test_encode_request(self):
         r = Request("http://www.example.com/lala")
         rs = self.encoder.encode(r)
         self.assertIn(r.method, rs)
         self.assertIn(r.url, rs)
 
@@ -57,24 +64,22 @@
             name: str
             url: str
             price: int
 
         item = TestDataClass(name="Product", url="http://product.org", price=1)
         encoded = self.encoder.encode(item)
         self.assertEqual(
-            encoded,
-            '{"name": "Product", "price": 1, "url": "http://product.org"}'
+            encoded, '{"name": "Product", "price": 1, "url": "http://product.org"}'
         )
 
     def test_encode_attrs_item(self):
         @attr.s
         class AttrsItem:
             name = attr.ib(type=str)
             url = attr.ib(type=str)
             price = attr.ib(type=int)
 
         item = AttrsItem(name="Product", url="http://product.org", price=1)
         encoded = self.encoder.encode(item)
         self.assertEqual(
-            encoded,
-            '{"name": "Product", "price": 1, "url": "http://product.org"}'
+            encoded, '{"name": "Product", "price": 1, "url": "http://product.org"}'
         )
```

### Comparing `Scrapy-2.7.1/tests/test_utils_signal.py` & `Scrapy-2.8.0/tests/test_utils_signal.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,34 +8,35 @@
 from twisted.trial import unittest
 
 from scrapy.utils.signal import send_catch_log, send_catch_log_deferred
 from scrapy.utils.test import get_from_asyncio_queue
 
 
 class SendCatchLogTest(unittest.TestCase):
-
     @defer.inlineCallbacks
     def test_send_catch_log(self):
         test_signal = object()
         handlers_called = set()
 
         dispatcher.connect(self.error_handler, signal=test_signal)
         dispatcher.connect(self.ok_handler, signal=test_signal)
         with LogCapture() as log:
             result = yield defer.maybeDeferred(
-                self._get_result, test_signal, arg='test',
-                handlers_called=handlers_called
+                self._get_result,
+                test_signal,
+                arg="test",
+                handlers_called=handlers_called,
             )
 
         assert self.error_handler in handlers_called
         assert self.ok_handler in handlers_called
         self.assertEqual(len(log.records), 1)
         record = log.records[0]
-        self.assertIn('error_handler', record.getMessage())
-        self.assertEqual(record.levelname, 'ERROR')
+        self.assertIn("error_handler", record.getMessage())
+        self.assertEqual(record.levelname, "ERROR")
         self.assertEqual(result[0][0], self.error_handler)
         self.assertIsInstance(result[0][1], Failure)
         self.assertEqual(result[1], (self.ok_handler, "OK"))
 
         dispatcher.disconnect(self.error_handler, signal=test_signal)
         dispatcher.disconnect(self.ok_handler, signal=test_signal)
 
@@ -44,62 +45,57 @@
 
     def error_handler(self, arg, handlers_called):
         handlers_called.add(self.error_handler)
         1 / 0
 
     def ok_handler(self, arg, handlers_called):
         handlers_called.add(self.ok_handler)
-        assert arg == 'test'
+        assert arg == "test"
         return "OK"
 
 
 class SendCatchLogDeferredTest(SendCatchLogTest):
-
     def _get_result(self, signal, *a, **kw):
         return send_catch_log_deferred(signal, *a, **kw)
 
 
 class SendCatchLogDeferredTest2(SendCatchLogDeferredTest):
-
     def ok_handler(self, arg, handlers_called):
         handlers_called.add(self.ok_handler)
-        assert arg == 'test'
+        assert arg == "test"
         d = defer.Deferred()
         reactor.callLater(0, d.callback, "OK")
         return d
 
 
-@mark.usefixtures('reactor_pytest')
+@mark.usefixtures("reactor_pytest")
 class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):
-
     async def ok_handler(self, arg, handlers_called):
         handlers_called.add(self.ok_handler)
-        assert arg == 'test'
+        assert arg == "test"
         await defer.succeed(42)
         return "OK"
 
     def test_send_catch_log(self):
         return super().test_send_catch_log()
 
 
 @mark.only_asyncio()
 class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):
-
     async def ok_handler(self, arg, handlers_called):
         handlers_called.add(self.ok_handler)
-        assert arg == 'test'
+        assert arg == "test"
         await asyncio.sleep(0.2)
         return await get_from_asyncio_queue("OK")
 
     def test_send_catch_log(self):
         return super().test_send_catch_log()
 
 
 class SendCatchLogTest2(unittest.TestCase):
-
     def test_error_logged_if_deferred_not_supported(self):
         def test_handler():
             return defer.Deferred()
 
         test_signal = object()
         dispatcher.connect(test_handler, test_signal)
         with LogCapture() as log:
```

### Comparing `Scrapy-2.7.1/tests/test_utils_spider.py` & `Scrapy-2.8.0/tests/test_utils_spider.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,36 +1,36 @@
 import unittest
 
 from scrapy import Spider
 from scrapy.http import Request
 from scrapy.item import Item
-from scrapy.utils.spider import iterate_spider_output, iter_spider_classes
+from scrapy.utils.spider import iter_spider_classes, iterate_spider_output
 
 
 class MySpider1(Spider):
-    name = 'myspider1'
+    name = "myspider1"
 
 
 class MySpider2(Spider):
-    name = 'myspider2'
+    name = "myspider2"
 
 
 class UtilsSpidersTestCase(unittest.TestCase):
-
     def test_iterate_spider_output(self):
         i = Item()
-        r = Request('http://scrapytest.org')
+        r = Request("http://scrapytest.org")
         o = object()
 
         self.assertEqual(list(iterate_spider_output(i)), [i])
         self.assertEqual(list(iterate_spider_output(r)), [r])
         self.assertEqual(list(iterate_spider_output(o)), [o])
         self.assertEqual(list(iterate_spider_output([r, i, o])), [r, i, o])
 
     def test_iter_spider_classes(self):
         import tests.test_utils_spider
+
         it = iter_spider_classes(tests.test_utils_spider)
         self.assertEqual(set(it), {MySpider1, MySpider2})
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_utils_trackref.py` & `Scrapy-2.8.0/tests/test_utils_trackref.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,73 +13,77 @@
 
 
 class Bar(trackref.object_ref):
     pass
 
 
 class TrackrefTestCase(unittest.TestCase):
-
     def setUp(self):
         trackref.live_refs.clear()
 
     def test_format_live_refs(self):
         o1 = Foo()  # NOQA
         o2 = Bar()  # NOQA
         o3 = Foo()  # NOQA
         self.assertEqual(
             trackref.format_live_refs(),
-            '''\
+            """\
 Live References
 
 Bar                                 1   oldest: 0s ago
 Foo                                 2   oldest: 0s ago
-''')
+""",
+        )
 
         self.assertEqual(
             trackref.format_live_refs(ignore=Foo),
-            '''\
+            """\
 Live References
 
 Bar                                 1   oldest: 0s ago
-''')
+""",
+        )
 
-    @mock.patch('sys.stdout', new_callable=StringIO)
+    @mock.patch("sys.stdout", new_callable=StringIO)
     def test_print_live_refs_empty(self, stdout):
         trackref.print_live_refs()
-        self.assertEqual(stdout.getvalue(), 'Live References\n\n\n')
+        self.assertEqual(stdout.getvalue(), "Live References\n\n\n")
 
-    @mock.patch('sys.stdout', new_callable=StringIO)
+    @mock.patch("sys.stdout", new_callable=StringIO)
     def test_print_live_refs_with_objects(self, stdout):
         o1 = Foo()  # NOQA
         trackref.print_live_refs()
-        self.assertEqual(stdout.getvalue(), '''\
+        self.assertEqual(
+            stdout.getvalue(),
+            """\
 Live References
 
-Foo                                 1   oldest: 0s ago\n\n''')
+Foo                                 1   oldest: 0s ago\n\n""",
+        )
 
     def test_get_oldest(self):
         o1 = Foo()  # NOQA
 
         o1_time = time()
 
         o2 = Bar()  # NOQA
 
         o3_time = time()
         if o3_time <= o1_time:
             sleep(0.01)
             o3_time = time()
         if o3_time <= o1_time:
-            raise SkipTest('time.time is not precise enough')
+            raise SkipTest("time.time is not precise enough")
 
         o3 = Foo()  # NOQA
-        self.assertIs(trackref.get_oldest('Foo'), o1)
-        self.assertIs(trackref.get_oldest('Bar'), o2)
-        self.assertIsNone(trackref.get_oldest('XXX'))
+        self.assertIs(trackref.get_oldest("Foo"), o1)
+        self.assertIs(trackref.get_oldest("Bar"), o2)
+        self.assertIsNone(trackref.get_oldest("XXX"))
 
     def test_iter_all(self):
         o1 = Foo()  # NOQA
         o2 = Bar()  # NOQA
         o3 = Foo()  # NOQA
         self.assertEqual(
-            set(trackref.iter_all('Foo')),
+            set(trackref.iter_all("Foo")),
             {o1, o3},
         )
```

### Comparing `Scrapy-2.7.1/tests/test_utils_url.py` & `Scrapy-2.8.0/tests/test_utils_url.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,461 +1,596 @@
 import unittest
 
 from scrapy.linkextractors import IGNORED_EXTENSIONS
 from scrapy.spiders import Spider
 from scrapy.utils.misc import arg_to_iter
 from scrapy.utils.url import (
+    _is_filesystem_path,
     add_http_if_no_scheme,
     guess_scheme,
-    _is_filesystem_path,
     strip_url,
+    url_has_any_extension,
     url_is_from_any_domain,
     url_is_from_spider,
-    url_has_any_extension,
 )
 
-__doctests__ = ['scrapy.utils.url']
+__doctests__ = ["scrapy.utils.url"]
 
 
 class UrlUtilsTest(unittest.TestCase):
-
     def test_url_is_from_any_domain(self):
-        url = 'http://www.wheele-bin-art.co.uk/get/product/123'
-        self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.co.uk']))
-        self.assertFalse(url_is_from_any_domain(url, ['art.co.uk']))
-
-        url = 'http://wheele-bin-art.co.uk/get/product/123'
-        self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.co.uk']))
-        self.assertFalse(url_is_from_any_domain(url, ['art.co.uk']))
-
-        url = 'http://www.Wheele-Bin-Art.co.uk/get/product/123'
-        self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.CO.UK']))
-        self.assertTrue(url_is_from_any_domain(url, ['WHEELE-BIN-ART.CO.UK']))
-
-        url = 'http://192.169.0.15:8080/mypage.html'
-        self.assertTrue(url_is_from_any_domain(url, ['192.169.0.15:8080']))
-        self.assertFalse(url_is_from_any_domain(url, ['192.169.0.15']))
+        url = "http://www.wheele-bin-art.co.uk/get/product/123"
+        self.assertTrue(url_is_from_any_domain(url, ["wheele-bin-art.co.uk"]))
+        self.assertFalse(url_is_from_any_domain(url, ["art.co.uk"]))
+
+        url = "http://wheele-bin-art.co.uk/get/product/123"
+        self.assertTrue(url_is_from_any_domain(url, ["wheele-bin-art.co.uk"]))
+        self.assertFalse(url_is_from_any_domain(url, ["art.co.uk"]))
+
+        url = "http://www.Wheele-Bin-Art.co.uk/get/product/123"
+        self.assertTrue(url_is_from_any_domain(url, ["wheele-bin-art.CO.UK"]))
+        self.assertTrue(url_is_from_any_domain(url, ["WHEELE-BIN-ART.CO.UK"]))
+
+        url = "http://192.169.0.15:8080/mypage.html"
+        self.assertTrue(url_is_from_any_domain(url, ["192.169.0.15:8080"]))
+        self.assertFalse(url_is_from_any_domain(url, ["192.169.0.15"]))
 
         url = (
-            'javascript:%20document.orderform_2581_1190810811.mode.value=%27add%27;%20'
-            'javascript:%20document.orderform_2581_1190810811.submit%28%29'
+            "javascript:%20document.orderform_2581_1190810811.mode.value=%27add%27;%20"
+            "javascript:%20document.orderform_2581_1190810811.submit%28%29"
+        )
+        self.assertFalse(url_is_from_any_domain(url, ["testdomain.com"]))
+        self.assertFalse(
+            url_is_from_any_domain(url + ".testdomain.com", ["testdomain.com"])
         )
-        self.assertFalse(url_is_from_any_domain(url, ['testdomain.com']))
-        self.assertFalse(url_is_from_any_domain(url + '.testdomain.com', ['testdomain.com']))
 
     def test_url_is_from_spider(self):
-        spider = Spider(name='example.com')
-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))
-        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))
-        self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', spider))
-        self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html', spider))
+        spider = Spider(name="example.com")
+        self.assertTrue(
+            url_is_from_spider("http://www.example.com/some/page.html", spider)
+        )
+        self.assertTrue(
+            url_is_from_spider("http://sub.example.com/some/page.html", spider)
+        )
+        self.assertFalse(
+            url_is_from_spider("http://www.example.org/some/page.html", spider)
+        )
+        self.assertFalse(
+            url_is_from_spider("http://www.example.net/some/page.html", spider)
+        )
 
     def test_url_is_from_spider_class_attributes(self):
         class MySpider(Spider):
-            name = 'example.com'
-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySpider))
-        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySpider))
-        self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', MySpider))
-        self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html', MySpider))
+            name = "example.com"
+
+        self.assertTrue(
+            url_is_from_spider("http://www.example.com/some/page.html", MySpider)
+        )
+        self.assertTrue(
+            url_is_from_spider("http://sub.example.com/some/page.html", MySpider)
+        )
+        self.assertFalse(
+            url_is_from_spider("http://www.example.org/some/page.html", MySpider)
+        )
+        self.assertFalse(
+            url_is_from_spider("http://www.example.net/some/page.html", MySpider)
+        )
 
     def test_url_is_from_spider_with_allowed_domains(self):
-        spider = Spider(name='example.com', allowed_domains=['example.org', 'example.net'])
-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))
-        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))
-        self.assertTrue(url_is_from_spider('http://example.com/some/page.html', spider))
-        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', spider))
-        self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', spider))
-        self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', spider))
+        spider = Spider(
+            name="example.com", allowed_domains=["example.org", "example.net"]
+        )
+        self.assertTrue(
+            url_is_from_spider("http://www.example.com/some/page.html", spider)
+        )
+        self.assertTrue(
+            url_is_from_spider("http://sub.example.com/some/page.html", spider)
+        )
+        self.assertTrue(url_is_from_spider("http://example.com/some/page.html", spider))
+        self.assertTrue(
+            url_is_from_spider("http://www.example.org/some/page.html", spider)
+        )
+        self.assertTrue(
+            url_is_from_spider("http://www.example.net/some/page.html", spider)
+        )
+        self.assertFalse(
+            url_is_from_spider("http://www.example.us/some/page.html", spider)
+        )
 
-        spider = Spider(name='example.com', allowed_domains={'example.com', 'example.net'})
-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))
+        spider = Spider(
+            name="example.com", allowed_domains={"example.com", "example.net"}
+        )
+        self.assertTrue(
+            url_is_from_spider("http://www.example.com/some/page.html", spider)
+        )
 
-        spider = Spider(name='example.com', allowed_domains=('example.com', 'example.net'))
-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))
+        spider = Spider(
+            name="example.com", allowed_domains=("example.com", "example.net")
+        )
+        self.assertTrue(
+            url_is_from_spider("http://www.example.com/some/page.html", spider)
+        )
 
     def test_url_is_from_spider_with_allowed_domains_class_attributes(self):
         class MySpider(Spider):
-            name = 'example.com'
-            allowed_domains = ('example.org', 'example.net')
-        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySpider))
-        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySpider))
-        self.assertTrue(url_is_from_spider('http://example.com/some/page.html', MySpider))
-        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', MySpider))
-        self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))
-        self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))
+            name = "example.com"
+            allowed_domains = ("example.org", "example.net")
+
+        self.assertTrue(
+            url_is_from_spider("http://www.example.com/some/page.html", MySpider)
+        )
+        self.assertTrue(
+            url_is_from_spider("http://sub.example.com/some/page.html", MySpider)
+        )
+        self.assertTrue(
+            url_is_from_spider("http://example.com/some/page.html", MySpider)
+        )
+        self.assertTrue(
+            url_is_from_spider("http://www.example.org/some/page.html", MySpider)
+        )
+        self.assertTrue(
+            url_is_from_spider("http://www.example.net/some/page.html", MySpider)
+        )
+        self.assertFalse(
+            url_is_from_spider("http://www.example.us/some/page.html", MySpider)
+        )
 
     def test_url_has_any_extension(self):
-        deny_extensions = {'.' + e for e in arg_to_iter(IGNORED_EXTENSIONS)}
-        self.assertTrue(url_has_any_extension("http://www.example.com/archive.tar.gz", deny_extensions))
-        self.assertTrue(url_has_any_extension("http://www.example.com/page.doc", deny_extensions))
-        self.assertTrue(url_has_any_extension("http://www.example.com/page.pdf", deny_extensions))
-        self.assertFalse(url_has_any_extension("http://www.example.com/page.htm", deny_extensions))
-        self.assertFalse(url_has_any_extension("http://www.example.com/", deny_extensions))
-        self.assertFalse(url_has_any_extension("http://www.example.com/page.doc.html", deny_extensions))
+        deny_extensions = {"." + e for e in arg_to_iter(IGNORED_EXTENSIONS)}
+        self.assertTrue(
+            url_has_any_extension(
+                "http://www.example.com/archive.tar.gz", deny_extensions
+            )
+        )
+        self.assertTrue(
+            url_has_any_extension("http://www.example.com/page.doc", deny_extensions)
+        )
+        self.assertTrue(
+            url_has_any_extension("http://www.example.com/page.pdf", deny_extensions)
+        )
+        self.assertFalse(
+            url_has_any_extension("http://www.example.com/page.htm", deny_extensions)
+        )
+        self.assertFalse(
+            url_has_any_extension("http://www.example.com/", deny_extensions)
+        )
+        self.assertFalse(
+            url_has_any_extension(
+                "http://www.example.com/page.doc.html", deny_extensions
+            )
+        )
 
 
 class AddHttpIfNoScheme(unittest.TestCase):
-
     def test_add_scheme(self):
-        self.assertEqual(add_http_if_no_scheme('www.example.com'), 'http://www.example.com')
+        self.assertEqual(
+            add_http_if_no_scheme("www.example.com"), "http://www.example.com"
+        )
 
     def test_without_subdomain(self):
-        self.assertEqual(add_http_if_no_scheme('example.com'), 'http://example.com')
+        self.assertEqual(add_http_if_no_scheme("example.com"), "http://example.com")
 
     def test_path(self):
         self.assertEqual(
-            add_http_if_no_scheme('www.example.com/some/page.html'),
-            'http://www.example.com/some/page.html')
+            add_http_if_no_scheme("www.example.com/some/page.html"),
+            "http://www.example.com/some/page.html",
+        )
 
     def test_port(self):
         self.assertEqual(
-            add_http_if_no_scheme('www.example.com:80'),
-            'http://www.example.com:80')
+            add_http_if_no_scheme("www.example.com:80"), "http://www.example.com:80"
+        )
 
     def test_fragment(self):
         self.assertEqual(
-            add_http_if_no_scheme('www.example.com/some/page#frag'),
-            'http://www.example.com/some/page#frag')
+            add_http_if_no_scheme("www.example.com/some/page#frag"),
+            "http://www.example.com/some/page#frag",
+        )
 
     def test_query(self):
         self.assertEqual(
-            add_http_if_no_scheme('www.example.com/do?a=1&b=2&c=3'),
-            'http://www.example.com/do?a=1&b=2&c=3')
+            add_http_if_no_scheme("www.example.com/do?a=1&b=2&c=3"),
+            "http://www.example.com/do?a=1&b=2&c=3",
+        )
 
     def test_username_password(self):
         self.assertEqual(
-            add_http_if_no_scheme('username:password@www.example.com'),
-            'http://username:password@www.example.com')
+            add_http_if_no_scheme("username:password@www.example.com"),
+            "http://username:password@www.example.com",
+        )
 
     def test_complete_url(self):
         self.assertEqual(
-            add_http_if_no_scheme('username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),
-            'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')
+            add_http_if_no_scheme(
+                "username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag"
+            ),
+            "http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag",
+        )
 
     def test_preserve_http(self):
-        self.assertEqual(add_http_if_no_scheme('http://www.example.com'), 'http://www.example.com')
+        self.assertEqual(
+            add_http_if_no_scheme("http://www.example.com"), "http://www.example.com"
+        )
 
     def test_preserve_http_without_subdomain(self):
         self.assertEqual(
-            add_http_if_no_scheme('http://example.com'),
-            'http://example.com')
+            add_http_if_no_scheme("http://example.com"), "http://example.com"
+        )
 
     def test_preserve_http_path(self):
         self.assertEqual(
-            add_http_if_no_scheme('http://www.example.com/some/page.html'),
-            'http://www.example.com/some/page.html')
+            add_http_if_no_scheme("http://www.example.com/some/page.html"),
+            "http://www.example.com/some/page.html",
+        )
 
     def test_preserve_http_port(self):
         self.assertEqual(
-            add_http_if_no_scheme('http://www.example.com:80'),
-            'http://www.example.com:80')
+            add_http_if_no_scheme("http://www.example.com:80"),
+            "http://www.example.com:80",
+        )
 
     def test_preserve_http_fragment(self):
         self.assertEqual(
-            add_http_if_no_scheme('http://www.example.com/some/page#frag'),
-            'http://www.example.com/some/page#frag')
+            add_http_if_no_scheme("http://www.example.com/some/page#frag"),
+            "http://www.example.com/some/page#frag",
+        )
 
     def test_preserve_http_query(self):
         self.assertEqual(
-            add_http_if_no_scheme('http://www.example.com/do?a=1&b=2&c=3'),
-            'http://www.example.com/do?a=1&b=2&c=3')
+            add_http_if_no_scheme("http://www.example.com/do?a=1&b=2&c=3"),
+            "http://www.example.com/do?a=1&b=2&c=3",
+        )
 
     def test_preserve_http_username_password(self):
         self.assertEqual(
-            add_http_if_no_scheme('http://username:password@www.example.com'),
-            'http://username:password@www.example.com')
+            add_http_if_no_scheme("http://username:password@www.example.com"),
+            "http://username:password@www.example.com",
+        )
 
     def test_preserve_http_complete_url(self):
         self.assertEqual(
-            add_http_if_no_scheme('http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),
-            'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')
+            add_http_if_no_scheme(
+                "http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag"
+            ),
+            "http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag",
+        )
 
     def test_protocol_relative(self):
         self.assertEqual(
-            add_http_if_no_scheme('//www.example.com'), 'http://www.example.com')
+            add_http_if_no_scheme("//www.example.com"), "http://www.example.com"
+        )
 
     def test_protocol_relative_without_subdomain(self):
-        self.assertEqual(
-            add_http_if_no_scheme('//example.com'), 'http://example.com')
+        self.assertEqual(add_http_if_no_scheme("//example.com"), "http://example.com")
 
     def test_protocol_relative_path(self):
         self.assertEqual(
-            add_http_if_no_scheme('//www.example.com/some/page.html'),
-            'http://www.example.com/some/page.html')
+            add_http_if_no_scheme("//www.example.com/some/page.html"),
+            "http://www.example.com/some/page.html",
+        )
 
     def test_protocol_relative_port(self):
         self.assertEqual(
-            add_http_if_no_scheme('//www.example.com:80'),
-            'http://www.example.com:80')
+            add_http_if_no_scheme("//www.example.com:80"), "http://www.example.com:80"
+        )
 
     def test_protocol_relative_fragment(self):
         self.assertEqual(
-            add_http_if_no_scheme('//www.example.com/some/page#frag'),
-            'http://www.example.com/some/page#frag')
+            add_http_if_no_scheme("//www.example.com/some/page#frag"),
+            "http://www.example.com/some/page#frag",
+        )
 
     def test_protocol_relative_query(self):
         self.assertEqual(
-            add_http_if_no_scheme('//www.example.com/do?a=1&b=2&c=3'),
-            'http://www.example.com/do?a=1&b=2&c=3')
+            add_http_if_no_scheme("//www.example.com/do?a=1&b=2&c=3"),
+            "http://www.example.com/do?a=1&b=2&c=3",
+        )
 
     def test_protocol_relative_username_password(self):
         self.assertEqual(
-            add_http_if_no_scheme('//username:password@www.example.com'),
-            'http://username:password@www.example.com')
+            add_http_if_no_scheme("//username:password@www.example.com"),
+            "http://username:password@www.example.com",
+        )
 
     def test_protocol_relative_complete_url(self):
         self.assertEqual(
-            add_http_if_no_scheme('//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag'),
-            'http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag')
+            add_http_if_no_scheme(
+                "//username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag"
+            ),
+            "http://username:password@www.example.com:80/some/page/do?a=1&b=2&c=3#frag",
+        )
 
     def test_preserve_https(self):
         self.assertEqual(
-            add_http_if_no_scheme('https://www.example.com'),
-            'https://www.example.com')
+            add_http_if_no_scheme("https://www.example.com"), "https://www.example.com"
+        )
 
     def test_preserve_ftp(self):
-        self.assertEqual(add_http_if_no_scheme('ftp://www.example.com'), 'ftp://www.example.com')
+        self.assertEqual(
+            add_http_if_no_scheme("ftp://www.example.com"), "ftp://www.example.com"
+        )
 
 
 class GuessSchemeTest(unittest.TestCase):
     pass
 
 
 def create_guess_scheme_t(args):
     def do_expected(self):
         url = guess_scheme(args[0])
-        assert url.startswith(args[1]), \
-            f'Wrong scheme guessed: for `{args[0]}` got `{url}`, expected `{args[1]}...`'
+        assert url.startswith(
+            args[1]
+        ), f"Wrong scheme guessed: for `{args[0]}` got `{url}`, expected `{args[1]}...`"
+
     return do_expected
 
 
 def create_skipped_scheme_t(args):
     def do_expected(self):
         raise unittest.SkipTest(args[2])
         url = guess_scheme(args[0])
         assert url.startswith(args[1])
+
     return do_expected
 
 
 for k, args in enumerate(
     [
-        ('/index', 'file://'),
-        ('/index.html', 'file://'),
-        ('./index.html', 'file://'),
-        ('../index.html', 'file://'),
-        ('../../index.html', 'file://'),
-        ('./data/index.html', 'file://'),
-        ('.hidden/data/index.html', 'file://'),
-        ('/home/user/www/index.html', 'file://'),
-        ('//home/user/www/index.html', 'file://'),
-        ('file:///home/user/www/index.html', 'file://'),
-
-        ('index.html', 'http://'),
-        ('example.com', 'http://'),
-        ('www.example.com', 'http://'),
-        ('www.example.com/index.html', 'http://'),
-        ('http://example.com', 'http://'),
-        ('http://example.com/index.html', 'http://'),
-        ('localhost', 'http://'),
-        ('localhost/index.html', 'http://'),
-
+        ("/index", "file://"),
+        ("/index.html", "file://"),
+        ("./index.html", "file://"),
+        ("../index.html", "file://"),
+        ("../../index.html", "file://"),
+        ("./data/index.html", "file://"),
+        (".hidden/data/index.html", "file://"),
+        ("/home/user/www/index.html", "file://"),
+        ("//home/user/www/index.html", "file://"),
+        ("file:///home/user/www/index.html", "file://"),
+        ("index.html", "http://"),
+        ("example.com", "http://"),
+        ("www.example.com", "http://"),
+        ("www.example.com/index.html", "http://"),
+        ("http://example.com", "http://"),
+        ("http://example.com/index.html", "http://"),
+        ("localhost", "http://"),
+        ("localhost/index.html", "http://"),
         # some corner cases (default to http://)
-        ('/', 'http://'),
-        ('.../test', 'http://'),
+        ("/", "http://"),
+        (".../test", "http://"),
     ],
     start=1,
 ):
     t_method = create_guess_scheme_t(args)
-    t_method.__name__ = f'test_uri_{k:03}'
+    t_method.__name__ = f"test_uri_{k:03}"
     setattr(GuessSchemeTest, t_method.__name__, t_method)
 
 # TODO: the following tests do not pass with current implementation
 for k, args in enumerate(
     [
         (
-            r'C:\absolute\path\to\a\file.html',
-            'file://',
-            'Windows filepath are not supported for scrapy shell',
+            r"C:\absolute\path\to\a\file.html",
+            "file://",
+            "Windows filepath are not supported for scrapy shell",
         ),
     ],
     start=1,
 ):
     t_method = create_skipped_scheme_t(args)
-    t_method.__name__ = f'test_uri_skipped_{k:03}'
+    t_method.__name__ = f"test_uri_skipped_{k:03}"
     setattr(GuessSchemeTest, t_method.__name__, t_method)
 
 
 class StripUrl(unittest.TestCase):
-
     def test_noop(self):
-        self.assertEqual(strip_url(
-            'http://www.example.com/index.html'),
-            'http://www.example.com/index.html')
+        self.assertEqual(
+            strip_url("http://www.example.com/index.html"),
+            "http://www.example.com/index.html",
+        )
 
     def test_noop_query_string(self):
-        self.assertEqual(strip_url(
-            'http://www.example.com/index.html?somekey=somevalue'),
-            'http://www.example.com/index.html?somekey=somevalue')
+        self.assertEqual(
+            strip_url("http://www.example.com/index.html?somekey=somevalue"),
+            "http://www.example.com/index.html?somekey=somevalue",
+        )
 
     def test_fragments(self):
-        self.assertEqual(strip_url(
-            'http://www.example.com/index.html?somekey=somevalue#section', strip_fragment=False),
-            'http://www.example.com/index.html?somekey=somevalue#section')
+        self.assertEqual(
+            strip_url(
+                "http://www.example.com/index.html?somekey=somevalue#section",
+                strip_fragment=False,
+            ),
+            "http://www.example.com/index.html?somekey=somevalue#section",
+        )
 
     def test_path(self):
         for input_url, origin, output_url in [
-            ('http://www.example.com/',
-             False,
-             'http://www.example.com/'),
-
-            ('http://www.example.com',
-             False,
-             'http://www.example.com'),
-
-            ('http://www.example.com',
-             True,
-             'http://www.example.com/'),
+            ("http://www.example.com/", False, "http://www.example.com/"),
+            ("http://www.example.com", False, "http://www.example.com"),
+            ("http://www.example.com", True, "http://www.example.com/"),
         ]:
             self.assertEqual(strip_url(input_url, origin_only=origin), output_url)
 
     def test_credentials(self):
         for i, o in [
-            ('http://username@www.example.com/index.html?somekey=somevalue#section',
-             'http://www.example.com/index.html?somekey=somevalue'),
-
-            ('https://username:@www.example.com/index.html?somekey=somevalue#section',
-             'https://www.example.com/index.html?somekey=somevalue'),
-
-            ('ftp://username:password@www.example.com/index.html?somekey=somevalue#section',
-             'ftp://www.example.com/index.html?somekey=somevalue'),
+            (
+                "http://username@www.example.com/index.html?somekey=somevalue#section",
+                "http://www.example.com/index.html?somekey=somevalue",
+            ),
+            (
+                "https://username:@www.example.com/index.html?somekey=somevalue#section",
+                "https://www.example.com/index.html?somekey=somevalue",
+            ),
+            (
+                "ftp://username:password@www.example.com/index.html?somekey=somevalue#section",
+                "ftp://www.example.com/index.html?somekey=somevalue",
+            ),
         ]:
             self.assertEqual(strip_url(i, strip_credentials=True), o)
 
     def test_credentials_encoded_delims(self):
         for i, o in [
             # user: "username@"
             # password: none
-            ('http://username%40@www.example.com/index.html?somekey=somevalue#section',
-             'http://www.example.com/index.html?somekey=somevalue'),
-
+            (
+                "http://username%40@www.example.com/index.html?somekey=somevalue#section",
+                "http://www.example.com/index.html?somekey=somevalue",
+            ),
             # user: "username:pass"
             # password: ""
-            ('https://username%3Apass:@www.example.com/index.html?somekey=somevalue#section',
-             'https://www.example.com/index.html?somekey=somevalue'),
-
+            (
+                "https://username%3Apass:@www.example.com/index.html?somekey=somevalue#section",
+                "https://www.example.com/index.html?somekey=somevalue",
+            ),
             # user: "me"
             # password: "user@domain.com"
-            ('ftp://me:user%40domain.com@www.example.com/index.html?somekey=somevalue#section',
-             'ftp://www.example.com/index.html?somekey=somevalue'),
+            (
+                "ftp://me:user%40domain.com@www.example.com/index.html?somekey=somevalue#section",
+                "ftp://www.example.com/index.html?somekey=somevalue",
+            ),
         ]:
             self.assertEqual(strip_url(i, strip_credentials=True), o)
 
     def test_default_ports_creds_off(self):
         for i, o in [
-            ('http://username:password@www.example.com:80/index.html?somekey=somevalue#section',
-             'http://www.example.com/index.html?somekey=somevalue'),
-
-            ('http://username:password@www.example.com:8080/index.html#section',
-             'http://www.example.com:8080/index.html'),
-
-            ('http://username:password@www.example.com:443/index.html?somekey=somevalue&someotherkey=sov#section',
-             'http://www.example.com:443/index.html?somekey=somevalue&someotherkey=sov'),
-
-            ('https://username:password@www.example.com:443/index.html',
-             'https://www.example.com/index.html'),
-
-            ('https://username:password@www.example.com:442/index.html',
-             'https://www.example.com:442/index.html'),
-
-            ('https://username:password@www.example.com:80/index.html',
-             'https://www.example.com:80/index.html'),
-
-            ('ftp://username:password@www.example.com:21/file.txt',
-             'ftp://www.example.com/file.txt'),
-
-            ('ftp://username:password@www.example.com:221/file.txt',
-             'ftp://www.example.com:221/file.txt'),
+            (
+                "http://username:password@www.example.com:80/index.html?somekey=somevalue#section",
+                "http://www.example.com/index.html?somekey=somevalue",
+            ),
+            (
+                "http://username:password@www.example.com:8080/index.html#section",
+                "http://www.example.com:8080/index.html",
+            ),
+            (
+                "http://username:password@www.example.com:443/index.html?somekey=somevalue&someotherkey=sov#section",
+                "http://www.example.com:443/index.html?somekey=somevalue&someotherkey=sov",
+            ),
+            (
+                "https://username:password@www.example.com:443/index.html",
+                "https://www.example.com/index.html",
+            ),
+            (
+                "https://username:password@www.example.com:442/index.html",
+                "https://www.example.com:442/index.html",
+            ),
+            (
+                "https://username:password@www.example.com:80/index.html",
+                "https://www.example.com:80/index.html",
+            ),
+            (
+                "ftp://username:password@www.example.com:21/file.txt",
+                "ftp://www.example.com/file.txt",
+            ),
+            (
+                "ftp://username:password@www.example.com:221/file.txt",
+                "ftp://www.example.com:221/file.txt",
+            ),
         ]:
             self.assertEqual(strip_url(i), o)
 
     def test_default_ports(self):
         for i, o in [
-            ('http://username:password@www.example.com:80/index.html',
-             'http://username:password@www.example.com/index.html'),
-
-            ('http://username:password@www.example.com:8080/index.html',
-             'http://username:password@www.example.com:8080/index.html'),
-
-            ('http://username:password@www.example.com:443/index.html',
-             'http://username:password@www.example.com:443/index.html'),
-
-            ('https://username:password@www.example.com:443/index.html',
-             'https://username:password@www.example.com/index.html'),
-
-            ('https://username:password@www.example.com:442/index.html',
-             'https://username:password@www.example.com:442/index.html'),
-
-            ('https://username:password@www.example.com:80/index.html',
-             'https://username:password@www.example.com:80/index.html'),
-
-            ('ftp://username:password@www.example.com:21/file.txt',
-             'ftp://username:password@www.example.com/file.txt'),
-
-            ('ftp://username:password@www.example.com:221/file.txt',
-             'ftp://username:password@www.example.com:221/file.txt'),
+            (
+                "http://username:password@www.example.com:80/index.html",
+                "http://username:password@www.example.com/index.html",
+            ),
+            (
+                "http://username:password@www.example.com:8080/index.html",
+                "http://username:password@www.example.com:8080/index.html",
+            ),
+            (
+                "http://username:password@www.example.com:443/index.html",
+                "http://username:password@www.example.com:443/index.html",
+            ),
+            (
+                "https://username:password@www.example.com:443/index.html",
+                "https://username:password@www.example.com/index.html",
+            ),
+            (
+                "https://username:password@www.example.com:442/index.html",
+                "https://username:password@www.example.com:442/index.html",
+            ),
+            (
+                "https://username:password@www.example.com:80/index.html",
+                "https://username:password@www.example.com:80/index.html",
+            ),
+            (
+                "ftp://username:password@www.example.com:21/file.txt",
+                "ftp://username:password@www.example.com/file.txt",
+            ),
+            (
+                "ftp://username:password@www.example.com:221/file.txt",
+                "ftp://username:password@www.example.com:221/file.txt",
+            ),
         ]:
-            self.assertEqual(strip_url(i, strip_default_port=True, strip_credentials=False), o)
+            self.assertEqual(
+                strip_url(i, strip_default_port=True, strip_credentials=False), o
+            )
 
     def test_default_ports_keep(self):
         for i, o in [
-            ('http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov#section',
-             'http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov'),
-
-            ('http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov#section',
-             'http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov'),
-
-            ('http://username:password@www.example.com:443/index.html',
-             'http://username:password@www.example.com:443/index.html'),
-
-            ('https://username:password@www.example.com:443/index.html',
-             'https://username:password@www.example.com:443/index.html'),
-
-            ('https://username:password@www.example.com:442/index.html',
-             'https://username:password@www.example.com:442/index.html'),
-
-            ('https://username:password@www.example.com:80/index.html',
-             'https://username:password@www.example.com:80/index.html'),
-
-            ('ftp://username:password@www.example.com:21/file.txt',
-             'ftp://username:password@www.example.com:21/file.txt'),
-
-            ('ftp://username:password@www.example.com:221/file.txt',
-             'ftp://username:password@www.example.com:221/file.txt'),
+            (
+                "http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov#section",
+                "http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov",
+            ),
+            (
+                "http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov#section",
+                "http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov",
+            ),
+            (
+                "http://username:password@www.example.com:443/index.html",
+                "http://username:password@www.example.com:443/index.html",
+            ),
+            (
+                "https://username:password@www.example.com:443/index.html",
+                "https://username:password@www.example.com:443/index.html",
+            ),
+            (
+                "https://username:password@www.example.com:442/index.html",
+                "https://username:password@www.example.com:442/index.html",
+            ),
+            (
+                "https://username:password@www.example.com:80/index.html",
+                "https://username:password@www.example.com:80/index.html",
+            ),
+            (
+                "ftp://username:password@www.example.com:21/file.txt",
+                "ftp://username:password@www.example.com:21/file.txt",
+            ),
+            (
+                "ftp://username:password@www.example.com:221/file.txt",
+                "ftp://username:password@www.example.com:221/file.txt",
+            ),
         ]:
-            self.assertEqual(strip_url(i, strip_default_port=False, strip_credentials=False), o)
+            self.assertEqual(
+                strip_url(i, strip_default_port=False, strip_credentials=False), o
+            )
 
     def test_origin_only(self):
         for i, o in [
-            ('http://username:password@www.example.com/index.html',
-             'http://www.example.com/'),
-
-            ('http://username:password@www.example.com:80/foo/bar?query=value#somefrag',
-             'http://www.example.com/'),
-
-            ('http://username:password@www.example.com:8008/foo/bar?query=value#somefrag',
-             'http://www.example.com:8008/'),
-
-            ('https://username:password@www.example.com:443/index.html',
-             'https://www.example.com/'),
+            (
+                "http://username:password@www.example.com/index.html",
+                "http://www.example.com/",
+            ),
+            (
+                "http://username:password@www.example.com:80/foo/bar?query=value#somefrag",
+                "http://www.example.com/",
+            ),
+            (
+                "http://username:password@www.example.com:8008/foo/bar?query=value#somefrag",
+                "http://www.example.com:8008/",
+            ),
+            (
+                "https://username:password@www.example.com:443/index.html",
+                "https://www.example.com/",
+            ),
         ]:
             self.assertEqual(strip_url(i, origin_only=True), o)
 
 
 class IsPathTestCase(unittest.TestCase):
-
     def test_path(self):
         for input_value, output_value in (
             # https://en.wikipedia.org/wiki/Path_(computing)#Representations_of_paths_by_operating_system_and_shell
             # Unix-like OS, Microsoft Windows / cmd.exe
             ("/home/user/docs/Letter.txt", True),
             ("./inthisdir", True),
             ("../../greatgrandparent", True),
@@ -463,15 +598,16 @@
             (r"C:\user\docs\Letter.txt", True),
             ("/user/docs/Letter.txt", True),
             (r"C:\Letter.txt", True),
             (r"\\Server01\user\docs\Letter.txt", True),
             (r"\\?\UNC\Server01\user\docs\Letter.txt", True),
             (r"\\?\C:\user\docs\Letter.txt", True),
             (r"C:\user\docs\somefile.ext:alternate_stream_name", True),
-
             (r"https://example.com", False),
         ):
-            self.assertEqual(_is_filesystem_path(input_value), output_value, input_value)
+            self.assertEqual(
+                _is_filesystem_path(input_value), output_value, input_value
+            )
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `Scrapy-2.7.1/tests/test_webclient.py` & `Scrapy-2.8.0/tests/test_webclient.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 """
 from twisted.internet import defer
 Tests borrowed from the twisted.web.client tests.
 """
-import os
 import shutil
+from pathlib import Path
 
 import OpenSSL.SSL
+from twisted.internet import defer, reactor
 from twisted.trial import unittest
-from twisted.web import server, static, util, resource
-from twisted.internet import reactor, defer
+from twisted.web import resource, server, static, util
+
 try:
     from twisted.internet.testing import StringTransport
 except ImportError:
     # deprecated in Twisted 19.7.0
     # (remove once we bump our requirement past that version)
     from twisted.test.proto_helpers import StringTransport
-from twisted.python.filepath import FilePath
-from twisted.protocols.policies import WrappingFactory
+
 from twisted.internet.defer import inlineCallbacks
+from twisted.protocols.policies import WrappingFactory
 
 from scrapy.core.downloader import webclient as client
 from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory
-from scrapy.http import Request, Headers
+from scrapy.http import Headers, Request
 from scrapy.settings import Settings
 from scrapy.utils.misc import create_instance
 from scrapy.utils.python import to_bytes, to_unicode
 from tests.mockserver import (
     BrokenDownloadResource,
     ErrorResource,
     ForeverTakingResource,
@@ -34,210 +35,253 @@
     PayloadResource,
     ssl_context_factory,
 )
 
 
 def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):
     """Adapted version of twisted.web.client.getPage"""
+
     def _clientfactory(url, *args, **kwargs):
         url = to_unicode(url)
-        timeout = kwargs.pop('timeout', 0)
+        timeout = kwargs.pop("timeout", 0)
         f = client.ScrapyHTTPClientFactory(
-            Request(url, *args, **kwargs), timeout=timeout)
+            Request(url, *args, **kwargs), timeout=timeout
+        )
         f.deferred.addCallback(response_transform or (lambda r: r.body))
         return f
 
     from twisted.web.client import _makeGetterFactory
+
     return _makeGetterFactory(
-        to_bytes(url), _clientfactory, contextFactory=contextFactory, *args, **kwargs
+        to_bytes(url),
+        _clientfactory,
+        contextFactory=contextFactory,
+        *args,
+        **kwargs,
     ).deferred
 
 
 class ParseUrlTestCase(unittest.TestCase):
     """Test URL parsing facility and defaults values."""
 
     def _parse(self, url):
         f = client.ScrapyHTTPClientFactory(Request(url))
         return (f.scheme, f.netloc, f.host, f.port, f.path)
 
     def testParse(self):
-        lip = '127.0.0.1'
+        lip = "127.0.0.1"
         tests = (
-            ("http://127.0.0.1?c=v&c2=v2#fragment", ('http', lip, lip, 80, '/?c=v&c2=v2')),
-            ("http://127.0.0.1/?c=v&c2=v2#fragment", ('http', lip, lip, 80, '/?c=v&c2=v2')),
-            ("http://127.0.0.1/foo?c=v&c2=v2#frag", ('http', lip, lip, 80, '/foo?c=v&c2=v2')),
-            ("http://127.0.0.1:100?c=v&c2=v2#fragment", ('http', lip + ':100', lip, 100, '/?c=v&c2=v2')),
-            ("http://127.0.0.1:100/?c=v&c2=v2#frag", ('http', lip + ':100', lip, 100, '/?c=v&c2=v2')),
-            ("http://127.0.0.1:100/foo?c=v&c2=v2#frag", ('http', lip + ':100', lip, 100, '/foo?c=v&c2=v2')),
-
-            ("http://127.0.0.1", ('http', lip, lip, 80, '/')),
-            ("http://127.0.0.1/", ('http', lip, lip, 80, '/')),
-            ("http://127.0.0.1/foo", ('http', lip, lip, 80, '/foo')),
-            ("http://127.0.0.1?param=value", ('http', lip, lip, 80, '/?param=value')),
-            ("http://127.0.0.1/?param=value", ('http', lip, lip, 80, '/?param=value')),
-            ("http://127.0.0.1:12345/foo", ('http', lip + ':12345', lip, 12345, '/foo')),
-            ("http://spam:12345/foo", ('http', 'spam:12345', 'spam', 12345, '/foo')),
-            ("http://spam.test.org/foo", ('http', 'spam.test.org', 'spam.test.org', 80, '/foo')),
-
-            ("https://127.0.0.1/foo", ('https', lip, lip, 443, '/foo')),
-            ("https://127.0.0.1/?param=value", ('https', lip, lip, 443, '/?param=value')),
-            ("https://127.0.0.1:12345/", ('https', lip + ':12345', lip, 12345, '/')),
-
-            ("http://scrapytest.org/foo ", ('http', 'scrapytest.org', 'scrapytest.org', 80, '/foo')),
-            ("http://egg:7890 ", ('http', 'egg:7890', 'egg', 7890, '/')),
+            (
+                "http://127.0.0.1?c=v&c2=v2#fragment",
+                ("http", lip, lip, 80, "/?c=v&c2=v2"),
+            ),
+            (
+                "http://127.0.0.1/?c=v&c2=v2#fragment",
+                ("http", lip, lip, 80, "/?c=v&c2=v2"),
+            ),
+            (
+                "http://127.0.0.1/foo?c=v&c2=v2#frag",
+                ("http", lip, lip, 80, "/foo?c=v&c2=v2"),
+            ),
+            (
+                "http://127.0.0.1:100?c=v&c2=v2#fragment",
+                ("http", lip + ":100", lip, 100, "/?c=v&c2=v2"),
+            ),
+            (
+                "http://127.0.0.1:100/?c=v&c2=v2#frag",
+                ("http", lip + ":100", lip, 100, "/?c=v&c2=v2"),
+            ),
+            (
+                "http://127.0.0.1:100/foo?c=v&c2=v2#frag",
+                ("http", lip + ":100", lip, 100, "/foo?c=v&c2=v2"),
+            ),
+            ("http://127.0.0.1", ("http", lip, lip, 80, "/")),
+            ("http://127.0.0.1/", ("http", lip, lip, 80, "/")),
+            ("http://127.0.0.1/foo", ("http", lip, lip, 80, "/foo")),
+            ("http://127.0.0.1?param=value", ("http", lip, lip, 80, "/?param=value")),
+            ("http://127.0.0.1/?param=value", ("http", lip, lip, 80, "/?param=value")),
+            (
+                "http://127.0.0.1:12345/foo",
+                ("http", lip + ":12345", lip, 12345, "/foo"),
+            ),
+            ("http://spam:12345/foo", ("http", "spam:12345", "spam", 12345, "/foo")),
+            (
+                "http://spam.test.org/foo",
+                ("http", "spam.test.org", "spam.test.org", 80, "/foo"),
+            ),
+            ("https://127.0.0.1/foo", ("https", lip, lip, 443, "/foo")),
+            (
+                "https://127.0.0.1/?param=value",
+                ("https", lip, lip, 443, "/?param=value"),
+            ),
+            ("https://127.0.0.1:12345/", ("https", lip + ":12345", lip, 12345, "/")),
+            (
+                "http://scrapytest.org/foo ",
+                ("http", "scrapytest.org", "scrapytest.org", 80, "/foo"),
+            ),
+            ("http://egg:7890 ", ("http", "egg:7890", "egg", 7890, "/")),
         )
 
         for url, test in tests:
-            test = tuple(
-                to_bytes(x) if not isinstance(x, int) else x for x in test)
+            test = tuple(to_bytes(x) if not isinstance(x, int) else x for x in test)
             self.assertEqual(client._parse(url), test, url)
 
 
 class ScrapyHTTPPageGetterTests(unittest.TestCase):
-
     def test_earlyHeaders(self):
         # basic test stolen from twisted HTTPageGetter
-        factory = client.ScrapyHTTPClientFactory(Request(
-            url='http://foo/bar',
-            body="some data",
-            headers={
-                'Host': 'example.net',
-                'User-Agent': 'fooble',
-                'Cookie': 'blah blah',
-                'Content-Length': '12981',
-                'Useful': 'value'}))
+        factory = client.ScrapyHTTPClientFactory(
+            Request(
+                url="http://foo/bar",
+                body="some data",
+                headers={
+                    "Host": "example.net",
+                    "User-Agent": "fooble",
+                    "Cookie": "blah blah",
+                    "Content-Length": "12981",
+                    "Useful": "value",
+                },
+            )
+        )
 
         self._test(
             factory,
             b"GET /bar HTTP/1.0\r\n"
             b"Content-Length: 9\r\n"
             b"Useful: value\r\n"
             b"Connection: close\r\n"
             b"User-Agent: fooble\r\n"
             b"Host: example.net\r\n"
             b"Cookie: blah blah\r\n"
             b"\r\n"
-            b"some data")
+            b"some data",
+        )
 
         # test minimal sent headers
-        factory = client.ScrapyHTTPClientFactory(Request('http://foo/bar'))
-        self._test(
-            factory,
-            b"GET /bar HTTP/1.0\r\n"
-            b"Host: foo\r\n"
-            b"\r\n")
+        factory = client.ScrapyHTTPClientFactory(Request("http://foo/bar"))
+        self._test(factory, b"GET /bar HTTP/1.0\r\n" b"Host: foo\r\n" b"\r\n")
 
         # test a simple POST with body and content-type
-        factory = client.ScrapyHTTPClientFactory(Request(
-            method='POST',
-            url='http://foo/bar',
-            body='name=value',
-            headers={'Content-Type': 'application/x-www-form-urlencoded'}))
+        factory = client.ScrapyHTTPClientFactory(
+            Request(
+                method="POST",
+                url="http://foo/bar",
+                body="name=value",
+                headers={"Content-Type": "application/x-www-form-urlencoded"},
+            )
+        )
 
         self._test(
             factory,
             b"POST /bar HTTP/1.0\r\n"
             b"Host: foo\r\n"
             b"Connection: close\r\n"
             b"Content-Type: application/x-www-form-urlencoded\r\n"
             b"Content-Length: 10\r\n"
             b"\r\n"
-            b"name=value")
+            b"name=value",
+        )
 
         # test a POST method with no body provided
-        factory = client.ScrapyHTTPClientFactory(Request(
-            method='POST',
-            url='http://foo/bar'
-        ))
+        factory = client.ScrapyHTTPClientFactory(
+            Request(method="POST", url="http://foo/bar")
+        )
 
         self._test(
             factory,
-            b"POST /bar HTTP/1.0\r\n"
-            b"Host: foo\r\n"
-            b"Content-Length: 0\r\n"
-            b"\r\n")
+            b"POST /bar HTTP/1.0\r\n" b"Host: foo\r\n" b"Content-Length: 0\r\n" b"\r\n",
+        )
 
         # test with single and multivalued headers
-        factory = client.ScrapyHTTPClientFactory(Request(
-            url='http://foo/bar',
-            headers={
-                'X-Meta-Single': 'single',
-                'X-Meta-Multivalued': ['value1', 'value2'],
-            },
-        ))
+        factory = client.ScrapyHTTPClientFactory(
+            Request(
+                url="http://foo/bar",
+                headers={
+                    "X-Meta-Single": "single",
+                    "X-Meta-Multivalued": ["value1", "value2"],
+                },
+            )
+        )
 
         self._test(
             factory,
             b"GET /bar HTTP/1.0\r\n"
             b"Host: foo\r\n"
             b"X-Meta-Multivalued: value1\r\n"
             b"X-Meta-Multivalued: value2\r\n"
             b"X-Meta-Single: single\r\n"
-            b"\r\n")
+            b"\r\n",
+        )
 
         # same test with single and multivalued headers but using Headers class
-        factory = client.ScrapyHTTPClientFactory(Request(
-            url='http://foo/bar',
-            headers=Headers({
-                'X-Meta-Single': 'single',
-                'X-Meta-Multivalued': ['value1', 'value2'],
-            }),
-        ))
+        factory = client.ScrapyHTTPClientFactory(
+            Request(
+                url="http://foo/bar",
+                headers=Headers(
+                    {
+                        "X-Meta-Single": "single",
+                        "X-Meta-Multivalued": ["value1", "value2"],
+                    }
+                ),
+            )
+        )
 
         self._test(
             factory,
             b"GET /bar HTTP/1.0\r\n"
             b"Host: foo\r\n"
             b"X-Meta-Multivalued: value1\r\n"
             b"X-Meta-Multivalued: value2\r\n"
             b"X-Meta-Single: single\r\n"
-            b"\r\n")
+            b"\r\n",
+        )
 
     def _test(self, factory, testvalue):
         transport = StringTransport()
         protocol = client.ScrapyHTTPPageGetter()
         protocol.factory = factory
         protocol.makeConnection(transport)
         self.assertEqual(
-            set(transport.value().splitlines()),
-            set(testvalue.splitlines()))
+            set(transport.value().splitlines()), set(testvalue.splitlines())
+        )
         return testvalue
 
     def test_non_standard_line_endings(self):
         # regression test for: http://dev.scrapy.org/ticket/258
-        factory = client.ScrapyHTTPClientFactory(Request(
-            url='http://foo/bar'))
+        factory = client.ScrapyHTTPClientFactory(Request(url="http://foo/bar"))
         protocol = client.ScrapyHTTPPageGetter()
         protocol.factory = factory
         protocol.headers = Headers()
         protocol.dataReceived(b"HTTP/1.0 200 OK\n")
         protocol.dataReceived(b"Hello: World\n")
         protocol.dataReceived(b"Foo: Bar\n")
         protocol.dataReceived(b"\n")
-        self.assertEqual(protocol.headers, Headers({'Hello': ['World'], 'Foo': ['Bar']}))
+        self.assertEqual(
+            protocol.headers, Headers({"Hello": ["World"], "Foo": ["Bar"]})
+        )
 
 
 class EncodingResource(resource.Resource):
-    out_encoding = 'cp1251'
+    out_encoding = "cp1251"
 
     def render(self, request):
         body = to_unicode(request.content.read())
-        request.setHeader(b'content-encoding', self.out_encoding)
+        request.setHeader(b"content-encoding", self.out_encoding)
         return body.encode(self.out_encoding)
 
 
 class WebClientTestCase(unittest.TestCase):
     def _listen(self, site):
         return reactor.listenTCP(0, site, interface="127.0.0.1")
 
     def setUp(self):
-        self.tmpname = self.mktemp()
-        os.mkdir(self.tmpname)
-        FilePath(self.tmpname).child("file").setContent(b"0123456789")
-        r = static.File(self.tmpname)
+        self.tmpname = Path(self.mktemp())
+        self.tmpname.mkdir()
+        (self.tmpname / "file").write_bytes(b"0123456789")
+        r = static.File(str(self.tmpname))
         r.putChild(b"redirect", util.Redirect(b"/file"))
         r.putChild(b"wait", ForeverTakingResource())
         r.putChild(b"error", ErrorResource())
         r.putChild(b"nolength", NoLengthResource())
         r.putChild(b"host", HostHeaderResource())
         r.putChild(b"payload", PayloadResource())
         r.putChild(b"broken", BrokenDownloadResource())
@@ -254,24 +298,30 @@
 
     def getURL(self, path):
         return f"http://127.0.0.1:{self.portno}/{path}"
 
     def testPayload(self):
         s = "0123456789" * 10
         return getPage(self.getURL("payload"), body=s).addCallback(
-            self.assertEqual, to_bytes(s))
+            self.assertEqual, to_bytes(s)
+        )
 
     def testHostHeader(self):
         # if we pass Host header explicitly, it should be used, otherwise
         # it should extract from url
-        return defer.gatherResults([
-            getPage(self.getURL("host")).addCallback(
-                self.assertEqual, to_bytes(f"127.0.0.1:{self.portno}")),
-            getPage(self.getURL("host"), headers={"Host": "www.example.com"}).addCallback(
-                self.assertEqual, to_bytes("www.example.com"))])
+        return defer.gatherResults(
+            [
+                getPage(self.getURL("host")).addCallback(
+                    self.assertEqual, to_bytes(f"127.0.0.1:{self.portno}")
+                ),
+                getPage(
+                    self.getURL("host"), headers={"Host": "www.example.com"}
+                ).addCallback(self.assertEqual, to_bytes("www.example.com")),
+            ]
+        )
 
     def test_getPage(self):
         """
         L{client.getPage} returns a L{Deferred} which is called back with
         the body of the response if the default method B{GET} is used.
         """
         d = getPage(self.getURL("file"))
@@ -280,142 +330,162 @@
 
     def test_getPageHead(self):
         """
         L{client.getPage} returns a L{Deferred} which is called back with
         the empty string if the method is C{HEAD} and there is a successful
         response code.
         """
+
         def _getPage(method):
             return getPage(self.getURL("file"), method=method)
-        return defer.gatherResults([
-            _getPage("head").addCallback(self.assertEqual, b""),
-            _getPage("HEAD").addCallback(self.assertEqual, b"")])
+
+        return defer.gatherResults(
+            [
+                _getPage("head").addCallback(self.assertEqual, b""),
+                _getPage("HEAD").addCallback(self.assertEqual, b""),
+            ]
+        )
 
     def test_timeoutNotTriggering(self):
         """
         When a non-zero timeout is passed to L{getPage} and the page is
         retrieved before the timeout period elapses, the L{Deferred} is
         called back with the contents of the page.
         """
         d = getPage(self.getURL("host"), timeout=100)
-        d.addCallback(
-            self.assertEqual, to_bytes(f"127.0.0.1:{self.portno}"))
+        d.addCallback(self.assertEqual, to_bytes(f"127.0.0.1:{self.portno}"))
         return d
 
     def test_timeoutTriggering(self):
         """
         When a non-zero timeout is passed to L{getPage} and that many
         seconds elapse before the server responds to the request. the
         L{Deferred} is errbacked with a L{error.TimeoutError}.
         """
         finished = self.assertFailure(
-            getPage(self.getURL("wait"), timeout=0.000001),
-            defer.TimeoutError)
+            getPage(self.getURL("wait"), timeout=0.000001), defer.TimeoutError
+        )
 
         def cleanup(passthrough):
             # Clean up the server which is hanging around not doing
             # anything.
             connected = list(self.wrapper.protocols.keys())
             # There might be nothing here if the server managed to already see
             # that the connection was lost.
             if connected:
                 connected[0].transport.loseConnection()
             return passthrough
+
         finished.addBoth(cleanup)
         return finished
 
     def testNotFound(self):
-        return getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)
+        return getPage(self.getURL("notsuchfile")).addCallback(self._cbNoSuchFile)
 
     def _cbNoSuchFile(self, pageData):
-        self.assertIn(b'404 - No Such Resource', pageData)
+        self.assertIn(b"404 - No Such Resource", pageData)
 
     def testFactoryInfo(self):
-        url = self.getURL('file')
+        url = self.getURL("file")
         _, _, host, port, _ = client._parse(url)
         factory = client.ScrapyHTTPClientFactory(Request(url))
         reactor.connectTCP(to_unicode(host), port, factory)
         return factory.deferred.addCallback(self._cbFactoryInfo, factory)
 
     def _cbFactoryInfo(self, ignoredResult, factory):
-        self.assertEqual(factory.status, b'200')
-        self.assertTrue(factory.version.startswith(b'HTTP/'))
-        self.assertEqual(factory.message, b'OK')
-        self.assertEqual(factory.response_headers[b'content-length'], b'10')
+        self.assertEqual(factory.status, b"200")
+        self.assertTrue(factory.version.startswith(b"HTTP/"))
+        self.assertEqual(factory.message, b"OK")
+        self.assertEqual(factory.response_headers[b"content-length"], b"10")
 
     def testRedirect(self):
         return getPage(self.getURL("redirect")).addCallback(self._cbRedirect)
 
     def _cbRedirect(self, pageData):
         self.assertEqual(
             pageData,
             b'\n<html>\n    <head>\n        <meta http-equiv="refresh" content="0;URL=/file">\n'
             b'    </head>\n    <body bgcolor="#FFFFFF" text="#000000">\n    '
-            b'<a href="/file">click here</a>\n    </body>\n</html>\n')
+            b'<a href="/file">click here</a>\n    </body>\n</html>\n',
+        )
 
     def test_encoding(self):
-        """ Test that non-standart body encoding matches
-        Content-Encoding header """
-        body = b'\xd0\x81\xd1\x8e\xd0\xaf'
-        dfd = getPage(self.getURL('encoding'), body=body, response_transform=lambda r: r)
+        """Test that non-standart body encoding matches
+        Content-Encoding header"""
+        body = b"\xd0\x81\xd1\x8e\xd0\xaf"
+        dfd = getPage(
+            self.getURL("encoding"), body=body, response_transform=lambda r: r
+        )
         return dfd.addCallback(self._check_Encoding, body)
 
     def _check_Encoding(self, response, original_body):
-        content_encoding = to_unicode(response.headers[b'Content-Encoding'])
+        content_encoding = to_unicode(response.headers[b"Content-Encoding"])
         self.assertEqual(content_encoding, EncodingResource.out_encoding)
         self.assertEqual(
-            response.body.decode(content_encoding), to_unicode(original_body))
+            response.body.decode(content_encoding), to_unicode(original_body)
+        )
 
 
 class WebClientSSLTestCase(unittest.TestCase):
     context_factory = None
 
     def _listen(self, site):
         return reactor.listenSSL(
-            0, site,
+            0,
+            site,
             contextFactory=self.context_factory or ssl_context_factory(),
-            interface="127.0.0.1")
+            interface="127.0.0.1",
+        )
 
     def getURL(self, path):
         return f"https://127.0.0.1:{self.portno}/{path}"
 
     def setUp(self):
-        self.tmpname = self.mktemp()
-        os.mkdir(self.tmpname)
-        FilePath(self.tmpname).child("file").setContent(b"0123456789")
-        r = static.File(self.tmpname)
+        self.tmpname = Path(self.mktemp())
+        self.tmpname.mkdir()
+        (self.tmpname / "file").write_bytes(b"0123456789")
+        r = static.File(str(self.tmpname))
         r.putChild(b"payload", PayloadResource())
         self.site = server.Site(r, timeout=None)
         self.wrapper = WrappingFactory(self.site)
         self.port = self._listen(self.wrapper)
         self.portno = self.port.getHost().port
 
     @inlineCallbacks
     def tearDown(self):
         yield self.port.stopListening()
         shutil.rmtree(self.tmpname)
 
     def testPayload(self):
         s = "0123456789" * 10
         return getPage(self.getURL("payload"), body=s).addCallback(
-            self.assertEqual, to_bytes(s))
+            self.assertEqual, to_bytes(s)
+        )
 
 
 class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):
     # we try to use a cipher that is not enabled by default in OpenSSL
-    custom_ciphers = 'CAMELLIA256-SHA'
+    custom_ciphers = "CAMELLIA256-SHA"
     context_factory = ssl_context_factory(cipher_string=custom_ciphers)
 
     def testPayload(self):
         s = "0123456789" * 10
-        settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': self.custom_ciphers})
-        client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)
+        settings = Settings({"DOWNLOADER_CLIENT_TLS_CIPHERS": self.custom_ciphers})
+        client_context_factory = create_instance(
+            ScrapyClientContextFactory, settings=settings, crawler=None
+        )
         return getPage(
             self.getURL("payload"), body=s, contextFactory=client_context_factory
         ).addCallback(self.assertEqual, to_bytes(s))
 
     def testPayloadDisabledCipher(self):
         s = "0123456789" * 10
-        settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'ECDHE-RSA-AES256-GCM-SHA384'})
-        client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)
-        d = getPage(self.getURL("payload"), body=s, contextFactory=client_context_factory)
+        settings = Settings(
+            {"DOWNLOADER_CLIENT_TLS_CIPHERS": "ECDHE-RSA-AES256-GCM-SHA384"}
+        )
+        client_context_factory = create_instance(
+            ScrapyClientContextFactory, settings=settings, crawler=None
+        )
+        d = getPage(
+            self.getURL("payload"), body=s, contextFactory=client_context_factory
+        )
         return self.assertFailure(d, OpenSSL.SSL.Error)
```

### Comparing `Scrapy-2.7.1/tox.ini` & `Scrapy-2.8.0/tox.ini`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Tox (https://tox.readthedocs.io/) is a tool for running tests
 # in multiple virtualenvs. This configuration file will run the
 # test suite on all supported python versions. To use it, "pip install tox"
 # and then run "tox" from this directory.
 
 [tox]
-envlist = security,flake8,py
+envlist = security,flake8,black,typing,py
 minversion = 1.7.0
 
 [testenv]
 deps =
     -rtests/requirements.txt
     # mitmproxy does not support PyPy
     # Python 3.9+ requires mitmproxy >= 5.3.0
@@ -28,21 +28,21 @@
     GCS_TEST_FILE_URI
     GCS_PROJECT_ID
 #allow tox virtualenv to upgrade pip/wheel/setuptools
 download = true
 commands =
     pytest --cov=scrapy --cov-report=xml --cov-report= {posargs:--durations=10 docs scrapy tests}
 install_command =
-    pip install -U -ctests/upper-constraints.txt {opts} {packages}
+    python -I -m pip install -ctests/upper-constraints.txt {opts} {packages}
 
 [testenv:typing]
 basepython = python3
 deps =
     lxml-stubs==0.2.0
-    mypy==0.982
+    mypy==0.991
     types-attrs==19.1.0
     types-pyOpenSSL==21.0.0
     types-setuptools==57.0.0
 commands =
     mypy --show-error-codes {posargs: scrapy tests}
 
 [testenv:security]
@@ -54,38 +54,38 @@
 
 [testenv:flake8]
 basepython = python3
 deps =
     {[testenv]deps}
     # Twisted[http2] is required to import some files
     Twisted[http2]>=17.9.0
-    flake8==5.0.4
+    flake8==6.0.0
 commands =
     flake8 {posargs:docs scrapy tests}
 
 [testenv:pylint]
-# reppy does not support Python 3.9+
-basepython = python3.8
+basepython = python3
 deps =
     {[testenv:extra-deps]deps}
-    pylint==2.15.3
+    pylint==2.15.6
 commands =
     pylint conftest.py docs extras scrapy setup.py tests
 
 [testenv:twinecheck]
 basepython = python3
 deps =
     twine==4.0.1
+    build==0.9.0
 commands =
-    python setup.py sdist
+    python -m build --sdist
     twine check dist/*
 
 [pinned]
 deps =
-    cryptography==3.3
+    cryptography==3.4.6
     cssselect==0.9.1
     h2==3.0
     itemadapter==0.1.0
     parsel==1.5.0
     Protego==0.1.15
     pyOpenSSL==21.0.0
     queuelib==1.4.2
@@ -102,15 +102,15 @@
     # Extras
     botocore==1.4.87
     google-cloud-storage==1.29.0
     Pillow==7.1.0
 setenv =
     _SCRAPY_PINNED=true
 install_command =
-    pip install -U {opts} {packages}
+    python -I -m pip install {opts} {packages}
 
 [testenv:pinned]
 deps =
     {[pinned]deps}
     PyDispatcher==2.0.5
 install_command = {[pinned]install_command}
 setenv =
@@ -122,24 +122,22 @@
     {[pinned]deps}
     PyDispatcher==2.0.5
 install_command = {[pinned]install_command}
 setenv =
     {[pinned]setenv}
 
 [testenv:extra-deps]
-# reppy does not support Python 3.9+
-basepython = python3.8
+basepython = python3
 deps =
     {[testenv]deps}
     boto
     google-cloud-storage
     # Twisted[http2] currently forces old mitmproxy because of h2 version
     # restrictions in their deps, so we need to pin old markupsafe here too.
     markupsafe < 2.1.0
-    reppy
     robotexclusionrulesparser
     Pillow>=4.0.0
     Twisted[http2]>=17.9.0
 
 [testenv:asyncio]
 commands =
     {[testenv]commands} --reactor=asyncio
@@ -193,7 +191,13 @@
 [testenv:docs-links]
 basepython = python3
 changedir = {[docs]changedir}
 deps = {[docs]deps}
 setenv = {[docs]setenv}
 commands =
     sphinx-build -W -b linkcheck . {envtmpdir}/linkcheck
+
+[testenv:black]
+deps =
+    black==22.12.0
+commands =
+    black {posargs:--check .}
```

