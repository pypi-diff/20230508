# Comparing `tmp/torchdata-0.6.0-py3-none-any.whl.zip` & `tmp/torchdata-0.6.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,96 +1,114 @@
-Zip file size: 145430 bytes, number of entries: 94
--rw-r--r--  2.0 unx      515 b- defN 23-Mar-13 20:59 torchdata/__init__.py
--rw-r--r--  2.0 unx      329 b- defN 23-Mar-13 20:59 torchdata/_constants.py
--rw-r--r--  2.0 unx     1031 b- defN 23-Mar-13 20:59 torchdata/_extension.py
--rw-r--r--  2.0 unx     1897 b- defN 23-Mar-13 20:59 torchdata/_utils.py
--rw-r--r--  2.0 unx       79 b- defN 23-Mar-13 21:00 torchdata/version.py
--rw-r--r--  2.0 unx     1030 b- defN 23-Mar-13 20:59 torchdata/dataloader2/__init__.py
--rw-r--r--  2.0 unx     3178 b- defN 23-Mar-13 20:59 torchdata/dataloader2/adapter.py
--rw-r--r--  2.0 unx    16021 b- defN 23-Mar-13 20:59 torchdata/dataloader2/dataloader2.py
--rw-r--r--  2.0 unx      256 b- defN 23-Mar-13 20:59 torchdata/dataloader2/error.py
--rw-r--r--  2.0 unx     1785 b- defN 23-Mar-13 20:59 torchdata/dataloader2/linter.py
--rw-r--r--  2.0 unx    23589 b- defN 23-Mar-13 20:59 torchdata/dataloader2/reading_service.py
--rw-r--r--  2.0 unx      294 b- defN 23-Mar-13 20:59 torchdata/dataloader2/shuffle_spec.py
--rw-r--r--  2.0 unx      271 b- defN 23-Mar-13 20:59 torchdata/dataloader2/communication/__init__.py
--rw-r--r--  2.0 unx     7121 b- defN 23-Mar-13 20:59 torchdata/dataloader2/communication/eventloop.py
--rw-r--r--  2.0 unx    15076 b- defN 23-Mar-13 20:59 torchdata/dataloader2/communication/iter.py
--rw-r--r--  2.0 unx     6929 b- defN 23-Mar-13 20:59 torchdata/dataloader2/communication/map.py
--rw-r--r--  2.0 unx     1905 b- defN 23-Mar-13 20:59 torchdata/dataloader2/communication/messages.py
--rw-r--r--  2.0 unx    12835 b- defN 23-Mar-13 20:59 torchdata/dataloader2/communication/protocol.py
--rw-r--r--  2.0 unx     1573 b- defN 23-Mar-13 20:59 torchdata/dataloader2/communication/queue.py
--rw-r--r--  2.0 unx      693 b- defN 23-Mar-13 20:59 torchdata/dataloader2/graph/__init__.py
--rw-r--r--  2.0 unx     2913 b- defN 23-Mar-13 20:59 torchdata/dataloader2/graph/_serialization.py
--rw-r--r--  2.0 unx     2337 b- defN 23-Mar-13 20:59 torchdata/dataloader2/graph/settings.py
--rw-r--r--  2.0 unx    10070 b- defN 23-Mar-13 20:59 torchdata/dataloader2/graph/utils.py
--rw-r--r--  2.0 unx      397 b- defN 23-Mar-13 20:59 torchdata/dataloader2/random/__init__.py
--rw-r--r--  2.0 unx     4461 b- defN 23-Mar-13 20:59 torchdata/dataloader2/random/_philox.py
--rw-r--r--  2.0 unx      706 b- defN 23-Mar-13 20:59 torchdata/dataloader2/random/distributed.py
--rw-r--r--  2.0 unx     2963 b- defN 23-Mar-13 20:59 torchdata/dataloader2/random/seed_generator.py
--rw-r--r--  2.0 unx      419 b- defN 23-Mar-13 20:59 torchdata/dataloader2/utils/__init__.py
--rw-r--r--  2.0 unx     5241 b- defN 23-Mar-13 20:59 torchdata/dataloader2/utils/dispatch.py
--rw-r--r--  2.0 unx     7099 b- defN 23-Mar-13 20:59 torchdata/dataloader2/utils/worker.py
--rw-r--r--  2.0 unx      373 b- defN 23-Mar-13 20:59 torchdata/datapipes/__init__.py
--rw-r--r--  2.0 unx     7791 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/__init__.py
--rw-r--r--  2.0 unx    22548 b- defN 23-Mar-13 21:00 torchdata/datapipes/iter/__init__.pyi
--rw-r--r--  2.0 unx      208 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/load/__init__.py
--rw-r--r--  2.0 unx     5855 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/load/aisio.py
--rw-r--r--  2.0 unx     8165 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/load/fsspec.py
--rw-r--r--  2.0 unx     3168 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/load/huggingface.py
--rw-r--r--  2.0 unx     8122 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/load/iopath.py
--rw-r--r--  2.0 unx    10234 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/load/online.py
--rw-r--r--  2.0 unx     6648 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/load/s3io.py
--rw-r--r--  2.0 unx      208 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/transform/__init__.py
--rw-r--r--  2.0 unx    12740 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/transform/bucketbatcher.py
--rw-r--r--  2.0 unx    17060 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/transform/callable.py
--rw-r--r--  2.0 unx      208 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/__init__.py
--rw-r--r--  2.0 unx     2844 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/bz2fileloader.py
--rw-r--r--  2.0 unx    24115 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/cacheholder.py
--rw-r--r--  2.0 unx    15884 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/combining.py
--rw-r--r--  2.0 unx     5022 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/converter.py
--rw-r--r--  2.0 unx     3377 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/cycler.py
--rw-r--r--  2.0 unx     6684 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/dataframemaker.py
--rw-r--r--  2.0 unx     4112 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/decompressor.py
--rw-r--r--  2.0 unx     8832 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/distributed.py
--rw-r--r--  2.0 unx     4170 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/hashchecker.py
--rw-r--r--  2.0 unx     3905 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/header.py
--rw-r--r--  2.0 unx     2804 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/indexadder.py
--rw-r--r--  2.0 unx     1895 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/jsonparser.py
--rw-r--r--  2.0 unx     1995 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/mux_longest.py
--rw-r--r--  2.0 unx     3227 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/paragraphaggregator.py
--rw-r--r--  2.0 unx    10763 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/plain_text_reader.py
--rw-r--r--  2.0 unx    10233 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/prefetcher.py
--rw-r--r--  2.0 unx     8057 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/randomsplitter.py
--rw-r--r--  2.0 unx     4240 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/rararchiveloader.py
--rw-r--r--  2.0 unx     3841 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/rows2columnar.py
--rw-r--r--  2.0 unx     3514 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/samplemultiplexer.py
--rw-r--r--  2.0 unx     2619 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/saver.py
--rw-r--r--  2.0 unx     2963 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/shardexpander.py
--rw-r--r--  2.0 unx     3032 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/sharding.py
--rw-r--r--  2.0 unx     4014 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/tararchiveloader.py
--rw-r--r--  2.0 unx     9742 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/tfrecordloader.py
--rw-r--r--  2.0 unx     3747 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/webdataset.py
--rw-r--r--  2.0 unx     2840 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/xzfileloader.py
--rw-r--r--  2.0 unx     2828 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/zip_longest.py
--rw-r--r--  2.0 unx     3617 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/ziparchiveloader.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/protobuf_template/__init__.py
--rw-r--r--  2.0 unx    21151 b- defN 23-Mar-13 20:59 torchdata/datapipes/iter/util/protobuf_template/_tfrecord_example_pb2.py
--rw-r--r--  2.0 unx      916 b- defN 23-Mar-13 20:59 torchdata/datapipes/map/__init__.py
--rw-r--r--  2.0 unx     3213 b- defN 23-Mar-13 21:00 torchdata/datapipes/map/__init__.pyi
--rw-r--r--  2.0 unx      208 b- defN 23-Mar-13 20:59 torchdata/datapipes/map/load/__init__.py
--rw-r--r--  2.0 unx      208 b- defN 23-Mar-13 20:59 torchdata/datapipes/map/load/transform.py
--rw-r--r--  2.0 unx      208 b- defN 23-Mar-13 20:59 torchdata/datapipes/map/transform/__init__.py
--rw-r--r--  2.0 unx      208 b- defN 23-Mar-13 20:59 torchdata/datapipes/map/util/__init__.py
--rw-r--r--  2.0 unx     1993 b- defN 23-Mar-13 20:59 torchdata/datapipes/map/util/cacheholder.py
--rw-r--r--  2.0 unx     2197 b- defN 23-Mar-13 20:59 torchdata/datapipes/map/util/converter.py
--rw-r--r--  2.0 unx     2837 b- defN 23-Mar-13 20:59 torchdata/datapipes/map/util/unzipper.py
--rw-r--r--  2.0 unx      524 b- defN 23-Mar-13 20:59 torchdata/datapipes/utils/__init__.py
--rw-r--r--  2.0 unx     6108 b- defN 23-Mar-13 20:59 torchdata/datapipes/utils/_visualization.py
--rw-r--r--  2.0 unx     1040 b- defN 23-Mar-13 20:59 torchdata/datapipes/utils/common.py
--rw-r--r--  2.0 unx      485 b- defN 23-Mar-13 20:59 torchdata/datapipes/utils/janitor.py
--rw-r--r--  2.0 unx     1345 b- defN 23-Mar-13 20:59 torchdata/datapipes/utils/pin_memory.py
--rw-r--r--  2.0 unx     1522 b- defN 23-Mar-13 21:00 torchdata-0.6.0.dist-info/LICENSE
--rw-r--r--  2.0 unx      898 b- defN 23-Mar-13 21:00 torchdata-0.6.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-13 21:00 torchdata-0.6.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Mar-13 21:00 torchdata-0.6.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     9110 b- defN 23-Mar-13 21:00 torchdata-0.6.0.dist-info/RECORD
-94 files, 455530 bytes uncompressed, 130620 bytes compressed:  71.3%
+Zip file size: 153007 bytes, number of entries: 112
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata-0.6.1.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/dataloader2/
+-rw-r--r--  2.0 unx      329 b- defN 23-Apr-28 03:52 torchdata/_constants.py
+-rw-r--r--  2.0 unx     1897 b- defN 23-Apr-28 03:52 torchdata/_utils.py
+-rw-r--r--  2.0 unx      515 b- defN 23-Apr-28 03:52 torchdata/__init__.py
+-rw-r--r--  2.0 unx       79 b- defN 23-Apr-28 03:52 torchdata/version.py
+-rw-r--r--  2.0 unx     1031 b- defN 23-Apr-28 03:52 torchdata/_extension.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/map/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/iter/
+-rw-r--r--  2.0 unx      373 b- defN 23-Apr-28 03:52 torchdata/datapipes/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/map/transform/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/map/util/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/map/load/
+-rw-r--r--  2.0 unx      916 b- defN 23-Apr-28 03:52 torchdata/datapipes/map/__init__.py
+-rw-r--r--  2.0 unx     3213 b- defN 23-Apr-28 03:52 torchdata/datapipes/map/__init__.pyi
+-rw-r--r--  2.0 unx      208 b- defN 23-Apr-28 03:52 torchdata/datapipes/map/transform/__init__.py
+-rw-r--r--  2.0 unx     2837 b- defN 23-Apr-28 03:52 torchdata/datapipes/map/util/unzipper.py
+-rw-r--r--  2.0 unx     2197 b- defN 23-Apr-28 03:52 torchdata/datapipes/map/util/converter.py
+-rw-r--r--  2.0 unx     1993 b- defN 23-Apr-28 03:52 torchdata/datapipes/map/util/cacheholder.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Apr-28 03:52 torchdata/datapipes/map/util/__init__.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Apr-28 03:52 torchdata/datapipes/map/load/transform.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Apr-28 03:52 torchdata/datapipes/map/load/__init__.py
+-rw-r--r--  2.0 unx     1352 b- defN 23-Apr-28 03:52 torchdata/datapipes/utils/pin_memory.py
+-rw-r--r--  2.0 unx     6108 b- defN 23-Apr-28 03:52 torchdata/datapipes/utils/_visualization.py
+-rw-r--r--  2.0 unx      485 b- defN 23-Apr-28 03:52 torchdata/datapipes/utils/janitor.py
+-rw-r--r--  2.0 unx     1040 b- defN 23-Apr-28 03:52 torchdata/datapipes/utils/common.py
+-rw-r--r--  2.0 unx      524 b- defN 23-Apr-28 03:52 torchdata/datapipes/utils/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/iter/transform/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/iter/util/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/iter/load/
+-rw-r--r--  2.0 unx     7791 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/__init__.py
+-rw-r--r--  2.0 unx    22548 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/__init__.pyi
+-rw-r--r--  2.0 unx    17060 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/transform/callable.py
+-rw-r--r--  2.0 unx    12740 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/transform/bucketbatcher.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/transform/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/datapipes/iter/util/protobuf_template/
+-rw-r--r--  2.0 unx     5022 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/converter.py
+-rw-r--r--  2.0 unx     4112 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/decompressor.py
+-rw-r--r--  2.0 unx    24115 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/cacheholder.py
+-rw-r--r--  2.0 unx     4240 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/rararchiveloader.py
+-rw-r--r--  2.0 unx     8057 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/randomsplitter.py
+-rw-r--r--  2.0 unx     3747 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/webdataset.py
+-rw-r--r--  2.0 unx     3841 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/rows2columnar.py
+-rw-r--r--  2.0 unx    10606 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/prefetcher.py
+-rw-r--r--  2.0 unx    10763 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/plain_text_reader.py
+-rw-r--r--  2.0 unx     9742 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/tfrecordloader.py
+-rw-r--r--  2.0 unx    15884 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/combining.py
+-rw-r--r--  2.0 unx     3617 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/ziparchiveloader.py
+-rw-r--r--  2.0 unx     3032 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/sharding.py
+-rw-r--r--  2.0 unx     4170 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/hashchecker.py
+-rw-r--r--  2.0 unx     4014 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/tararchiveloader.py
+-rw-r--r--  2.0 unx     2840 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/xzfileloader.py
+-rw-r--r--  2.0 unx     2844 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/bz2fileloader.py
+-rw-r--r--  2.0 unx     2963 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/shardexpander.py
+-rw-r--r--  2.0 unx     1895 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/jsonparser.py
+-rw-r--r--  2.0 unx     2828 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/zip_longest.py
+-rw-r--r--  2.0 unx     3514 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/samplemultiplexer.py
+-rw-r--r--  2.0 unx     3377 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/cycler.py
+-rw-r--r--  2.0 unx     3227 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/paragraphaggregator.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/__init__.py
+-rw-r--r--  2.0 unx     6684 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/dataframemaker.py
+-rw-r--r--  2.0 unx     8925 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/distributed.py
+-rw-r--r--  2.0 unx     1995 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/mux_longest.py
+-rw-r--r--  2.0 unx     3905 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/header.py
+-rw-r--r--  2.0 unx     2619 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/saver.py
+-rw-r--r--  2.0 unx     2804 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/indexadder.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/protobuf_template/__init__.py
+-rw-r--r--  2.0 unx    21151 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/util/protobuf_template/_tfrecord_example_pb2.py
+-rw-r--r--  2.0 unx     8165 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/load/fsspec.py
+-rw-r--r--  2.0 unx     6648 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/load/s3io.py
+-rw-r--r--  2.0 unx     5855 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/load/aisio.py
+-rw-r--r--  2.0 unx     8122 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/load/iopath.py
+-rw-r--r--  2.0 unx     3168 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/load/huggingface.py
+-rw-r--r--  2.0 unx      208 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/load/__init__.py
+-rw-r--r--  2.0 unx    10234 b- defN 23-Apr-28 03:52 torchdata/datapipes/iter/load/online.py
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/dataloader2/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/dataloader2/graph/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/dataloader2/random/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Apr-28 03:52 torchdata/dataloader2/communication/
+-rw-r--r--  2.0 unx      256 b- defN 23-Apr-28 03:52 torchdata/dataloader2/error.py
+-rw-r--r--  2.0 unx      294 b- defN 23-Apr-28 03:52 torchdata/dataloader2/shuffle_spec.py
+-rw-r--r--  2.0 unx    15842 b- defN 23-Apr-28 03:52 torchdata/dataloader2/dataloader2.py
+-rw-r--r--  2.0 unx     1030 b- defN 23-Apr-28 03:52 torchdata/dataloader2/__init__.py
+-rw-r--r--  2.0 unx     3178 b- defN 23-Apr-28 03:52 torchdata/dataloader2/adapter.py
+-rw-r--r--  2.0 unx    23304 b- defN 23-Apr-28 03:52 torchdata/dataloader2/reading_service.py
+-rw-r--r--  2.0 unx     1785 b- defN 23-Apr-28 03:52 torchdata/dataloader2/linter.py
+-rw-r--r--  2.0 unx     7303 b- defN 23-Apr-28 03:52 torchdata/dataloader2/utils/worker.py
+-rw-r--r--  2.0 unx     5241 b- defN 23-Apr-28 03:52 torchdata/dataloader2/utils/dispatch.py
+-rw-r--r--  2.0 unx      419 b- defN 23-Apr-28 03:52 torchdata/dataloader2/utils/__init__.py
+-rw-r--r--  2.0 unx     2913 b- defN 23-Apr-28 03:52 torchdata/dataloader2/graph/_serialization.py
+-rw-r--r--  2.0 unx     2337 b- defN 23-Apr-28 03:52 torchdata/dataloader2/graph/settings.py
+-rw-r--r--  2.0 unx      693 b- defN 23-Apr-28 03:52 torchdata/dataloader2/graph/__init__.py
+-rw-r--r--  2.0 unx    10070 b- defN 23-Apr-28 03:52 torchdata/dataloader2/graph/utils.py
+-rw-r--r--  2.0 unx     2963 b- defN 23-Apr-28 03:52 torchdata/dataloader2/random/seed_generator.py
+-rw-r--r--  2.0 unx     4461 b- defN 23-Apr-28 03:52 torchdata/dataloader2/random/_philox.py
+-rw-r--r--  2.0 unx      397 b- defN 23-Apr-28 03:52 torchdata/dataloader2/random/__init__.py
+-rw-r--r--  2.0 unx      706 b- defN 23-Apr-28 03:52 torchdata/dataloader2/random/distributed.py
+-rw-r--r--  2.0 unx     8066 b- defN 23-Apr-28 03:52 torchdata/dataloader2/communication/eventloop.py
+-rw-r--r--  2.0 unx     1905 b- defN 23-Apr-28 03:52 torchdata/dataloader2/communication/messages.py
+-rw-r--r--  2.0 unx    15838 b- defN 23-Apr-28 03:52 torchdata/dataloader2/communication/iter.py
+-rw-r--r--  2.0 unx     6929 b- defN 23-Apr-28 03:52 torchdata/dataloader2/communication/map.py
+-rw-r--r--  2.0 unx      271 b- defN 23-Apr-28 03:52 torchdata/dataloader2/communication/__init__.py
+-rw-r--r--  2.0 unx     1573 b- defN 23-Apr-28 03:52 torchdata/dataloader2/communication/queue.py
+-rw-r--r--  2.0 unx    13172 b- defN 23-Apr-28 03:52 torchdata/dataloader2/communication/protocol.py
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-28 03:52 torchdata-0.6.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx    13566 b- defN 23-Apr-28 03:52 torchdata-0.6.1.dist-info/METADATA
+-rw-r--r--  2.0 unx     1522 b- defN 23-Apr-28 03:52 torchdata-0.6.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx       10 b- defN 23-Apr-28 03:52 torchdata-0.6.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     9206 b- defN 23-Apr-28 03:52 torchdata-0.6.1.dist-info/RECORD
+112 files, 470551 bytes uncompressed, 135811 bytes compressed:  71.1%
```

## zipnote {}

```diff
@@ -1,283 +1,337 @@
-Filename: torchdata/__init__.py
+Filename: torchdata/
 Comment: 
 
-Filename: torchdata/_constants.py
+Filename: torchdata-0.6.1.dist-info/
 Comment: 
 
-Filename: torchdata/_extension.py
+Filename: torchdata/datapipes/
+Comment: 
+
+Filename: torchdata/dataloader2/
+Comment: 
+
+Filename: torchdata/_constants.py
 Comment: 
 
 Filename: torchdata/_utils.py
 Comment: 
 
-Filename: torchdata/version.py
+Filename: torchdata/__init__.py
 Comment: 
 
-Filename: torchdata/dataloader2/__init__.py
+Filename: torchdata/version.py
 Comment: 
 
-Filename: torchdata/dataloader2/adapter.py
+Filename: torchdata/_extension.py
 Comment: 
 
-Filename: torchdata/dataloader2/dataloader2.py
+Filename: torchdata/datapipes/map/
 Comment: 
 
-Filename: torchdata/dataloader2/error.py
+Filename: torchdata/datapipes/utils/
 Comment: 
 
-Filename: torchdata/dataloader2/linter.py
+Filename: torchdata/datapipes/iter/
 Comment: 
 
-Filename: torchdata/dataloader2/reading_service.py
+Filename: torchdata/datapipes/__init__.py
 Comment: 
 
-Filename: torchdata/dataloader2/shuffle_spec.py
+Filename: torchdata/datapipes/map/transform/
 Comment: 
 
-Filename: torchdata/dataloader2/communication/__init__.py
+Filename: torchdata/datapipes/map/util/
 Comment: 
 
-Filename: torchdata/dataloader2/communication/eventloop.py
+Filename: torchdata/datapipes/map/load/
 Comment: 
 
-Filename: torchdata/dataloader2/communication/iter.py
+Filename: torchdata/datapipes/map/__init__.py
 Comment: 
 
-Filename: torchdata/dataloader2/communication/map.py
+Filename: torchdata/datapipes/map/__init__.pyi
 Comment: 
 
-Filename: torchdata/dataloader2/communication/messages.py
+Filename: torchdata/datapipes/map/transform/__init__.py
 Comment: 
 
-Filename: torchdata/dataloader2/communication/protocol.py
+Filename: torchdata/datapipes/map/util/unzipper.py
 Comment: 
 
-Filename: torchdata/dataloader2/communication/queue.py
+Filename: torchdata/datapipes/map/util/converter.py
 Comment: 
 
-Filename: torchdata/dataloader2/graph/__init__.py
+Filename: torchdata/datapipes/map/util/cacheholder.py
 Comment: 
 
-Filename: torchdata/dataloader2/graph/_serialization.py
+Filename: torchdata/datapipes/map/util/__init__.py
 Comment: 
 
-Filename: torchdata/dataloader2/graph/settings.py
+Filename: torchdata/datapipes/map/load/transform.py
 Comment: 
 
-Filename: torchdata/dataloader2/graph/utils.py
+Filename: torchdata/datapipes/map/load/__init__.py
 Comment: 
 
-Filename: torchdata/dataloader2/random/__init__.py
+Filename: torchdata/datapipes/utils/pin_memory.py
 Comment: 
 
-Filename: torchdata/dataloader2/random/_philox.py
+Filename: torchdata/datapipes/utils/_visualization.py
 Comment: 
 
-Filename: torchdata/dataloader2/random/distributed.py
+Filename: torchdata/datapipes/utils/janitor.py
 Comment: 
 
-Filename: torchdata/dataloader2/random/seed_generator.py
+Filename: torchdata/datapipes/utils/common.py
 Comment: 
 
-Filename: torchdata/dataloader2/utils/__init__.py
+Filename: torchdata/datapipes/utils/__init__.py
 Comment: 
 
-Filename: torchdata/dataloader2/utils/dispatch.py
+Filename: torchdata/datapipes/iter/transform/
 Comment: 
 
-Filename: torchdata/dataloader2/utils/worker.py
+Filename: torchdata/datapipes/iter/util/
 Comment: 
 
-Filename: torchdata/datapipes/__init__.py
+Filename: torchdata/datapipes/iter/load/
 Comment: 
 
 Filename: torchdata/datapipes/iter/__init__.py
 Comment: 
 
 Filename: torchdata/datapipes/iter/__init__.pyi
 Comment: 
 
-Filename: torchdata/datapipes/iter/load/__init__.py
+Filename: torchdata/datapipes/iter/transform/callable.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/load/aisio.py
+Filename: torchdata/datapipes/iter/transform/bucketbatcher.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/load/fsspec.py
+Filename: torchdata/datapipes/iter/transform/__init__.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/load/huggingface.py
+Filename: torchdata/datapipes/iter/util/protobuf_template/
 Comment: 
 
-Filename: torchdata/datapipes/iter/load/iopath.py
+Filename: torchdata/datapipes/iter/util/converter.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/load/online.py
+Filename: torchdata/datapipes/iter/util/decompressor.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/load/s3io.py
+Filename: torchdata/datapipes/iter/util/cacheholder.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/transform/__init__.py
+Filename: torchdata/datapipes/iter/util/rararchiveloader.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/transform/bucketbatcher.py
+Filename: torchdata/datapipes/iter/util/randomsplitter.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/transform/callable.py
+Filename: torchdata/datapipes/iter/util/webdataset.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/__init__.py
+Filename: torchdata/datapipes/iter/util/rows2columnar.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/bz2fileloader.py
+Filename: torchdata/datapipes/iter/util/prefetcher.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/cacheholder.py
+Filename: torchdata/datapipes/iter/util/plain_text_reader.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/tfrecordloader.py
 Comment: 
 
 Filename: torchdata/datapipes/iter/util/combining.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/converter.py
+Filename: torchdata/datapipes/iter/util/ziparchiveloader.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/sharding.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/hashchecker.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/tararchiveloader.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/xzfileloader.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/bz2fileloader.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/shardexpander.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/jsonparser.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/zip_longest.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/samplemultiplexer.py
 Comment: 
 
 Filename: torchdata/datapipes/iter/util/cycler.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/dataframemaker.py
+Filename: torchdata/datapipes/iter/util/paragraphaggregator.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/decompressor.py
+Filename: torchdata/datapipes/iter/util/__init__.py
+Comment: 
+
+Filename: torchdata/datapipes/iter/util/dataframemaker.py
 Comment: 
 
 Filename: torchdata/datapipes/iter/util/distributed.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/hashchecker.py
+Filename: torchdata/datapipes/iter/util/mux_longest.py
 Comment: 
 
 Filename: torchdata/datapipes/iter/util/header.py
 Comment: 
 
+Filename: torchdata/datapipes/iter/util/saver.py
+Comment: 
+
 Filename: torchdata/datapipes/iter/util/indexadder.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/jsonparser.py
+Filename: torchdata/datapipes/iter/util/protobuf_template/__init__.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/mux_longest.py
+Filename: torchdata/datapipes/iter/util/protobuf_template/_tfrecord_example_pb2.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/paragraphaggregator.py
+Filename: torchdata/datapipes/iter/load/fsspec.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/plain_text_reader.py
+Filename: torchdata/datapipes/iter/load/s3io.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/prefetcher.py
+Filename: torchdata/datapipes/iter/load/aisio.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/randomsplitter.py
+Filename: torchdata/datapipes/iter/load/iopath.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/rararchiveloader.py
+Filename: torchdata/datapipes/iter/load/huggingface.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/rows2columnar.py
+Filename: torchdata/datapipes/iter/load/__init__.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/samplemultiplexer.py
+Filename: torchdata/datapipes/iter/load/online.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/saver.py
+Filename: torchdata/dataloader2/utils/
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/shardexpander.py
+Filename: torchdata/dataloader2/graph/
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/sharding.py
+Filename: torchdata/dataloader2/random/
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/tararchiveloader.py
+Filename: torchdata/dataloader2/communication/
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/tfrecordloader.py
+Filename: torchdata/dataloader2/error.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/webdataset.py
+Filename: torchdata/dataloader2/shuffle_spec.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/xzfileloader.py
+Filename: torchdata/dataloader2/dataloader2.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/zip_longest.py
+Filename: torchdata/dataloader2/__init__.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/ziparchiveloader.py
+Filename: torchdata/dataloader2/adapter.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/protobuf_template/__init__.py
+Filename: torchdata/dataloader2/reading_service.py
 Comment: 
 
-Filename: torchdata/datapipes/iter/util/protobuf_template/_tfrecord_example_pb2.py
+Filename: torchdata/dataloader2/linter.py
 Comment: 
 
-Filename: torchdata/datapipes/map/__init__.py
+Filename: torchdata/dataloader2/utils/worker.py
 Comment: 
 
-Filename: torchdata/datapipes/map/__init__.pyi
+Filename: torchdata/dataloader2/utils/dispatch.py
 Comment: 
 
-Filename: torchdata/datapipes/map/load/__init__.py
+Filename: torchdata/dataloader2/utils/__init__.py
 Comment: 
 
-Filename: torchdata/datapipes/map/load/transform.py
+Filename: torchdata/dataloader2/graph/_serialization.py
 Comment: 
 
-Filename: torchdata/datapipes/map/transform/__init__.py
+Filename: torchdata/dataloader2/graph/settings.py
 Comment: 
 
-Filename: torchdata/datapipes/map/util/__init__.py
+Filename: torchdata/dataloader2/graph/__init__.py
 Comment: 
 
-Filename: torchdata/datapipes/map/util/cacheholder.py
+Filename: torchdata/dataloader2/graph/utils.py
 Comment: 
 
-Filename: torchdata/datapipes/map/util/converter.py
+Filename: torchdata/dataloader2/random/seed_generator.py
 Comment: 
 
-Filename: torchdata/datapipes/map/util/unzipper.py
+Filename: torchdata/dataloader2/random/_philox.py
 Comment: 
 
-Filename: torchdata/datapipes/utils/__init__.py
+Filename: torchdata/dataloader2/random/__init__.py
 Comment: 
 
-Filename: torchdata/datapipes/utils/_visualization.py
+Filename: torchdata/dataloader2/random/distributed.py
 Comment: 
 
-Filename: torchdata/datapipes/utils/common.py
+Filename: torchdata/dataloader2/communication/eventloop.py
 Comment: 
 
-Filename: torchdata/datapipes/utils/janitor.py
+Filename: torchdata/dataloader2/communication/messages.py
 Comment: 
 
-Filename: torchdata/datapipes/utils/pin_memory.py
+Filename: torchdata/dataloader2/communication/iter.py
+Comment: 
+
+Filename: torchdata/dataloader2/communication/map.py
+Comment: 
+
+Filename: torchdata/dataloader2/communication/__init__.py
+Comment: 
+
+Filename: torchdata/dataloader2/communication/queue.py
+Comment: 
+
+Filename: torchdata/dataloader2/communication/protocol.py
 Comment: 
 
-Filename: torchdata-0.6.0.dist-info/LICENSE
+Filename: torchdata-0.6.1.dist-info/WHEEL
 Comment: 
 
-Filename: torchdata-0.6.0.dist-info/METADATA
+Filename: torchdata-0.6.1.dist-info/METADATA
 Comment: 
 
-Filename: torchdata-0.6.0.dist-info/WHEEL
+Filename: torchdata-0.6.1.dist-info/LICENSE
 Comment: 
 
-Filename: torchdata-0.6.0.dist-info/top_level.txt
+Filename: torchdata-0.6.1.dist-info/top_level.txt
 Comment: 
 
-Filename: torchdata-0.6.0.dist-info/RECORD
+Filename: torchdata-0.6.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=deflate
+Zip archive data, at least v2.0 to extract, compression method=store
```

## torchdata/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '0.6.0'
-git_version = '5bbcd77d7fd7aa900c40200255ebba90ecfbe9be'
+__version__ = '0.6.1'
+git_version = 'e1feeb2542293e42f083d24301386db6c003eeee'
```

## torchdata/dataloader2/dataloader2.py

```diff
@@ -3,37 +3,28 @@
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 
 import warnings
 
-from dataclasses import dataclass
 from typing import Any, Dict, Generic, Iterable, Iterator, Optional, TypeVar, Union
 
 from torchdata.dataloader2.adapter import Adapter
 from torchdata.dataloader2.error import PauseIteration
 from torchdata.dataloader2.graph._serialization import clone, DataPipe, deserialize_datapipe, serialize_datapipe
 from torchdata.dataloader2.random import SeedGenerator
 from torchdata.dataloader2.random.seed_generator import _UINT64_UPPER_BOUND
 from torchdata.dataloader2.reading_service import CheckpointableReadingServiceInterface, ReadingServiceInterface
 
 T_co = TypeVar("T_co", covariant=True)
 SERIALIZED_DATAPIPE_KEY_NAME = "serialized_datapipe"
 READING_SERVICE_STATE_KEY_NAME = "reading_service_state"
 
 
-@dataclass
-class ConcurrencySpec:
-    num_workers: int
-    timeout: Optional[int] = None
-    prefetch_factor: int = 2
-    persistent_workers: bool = False
-
-
 class DataLoader2Iterator(Iterator[T_co]):
     r"""
     An iterator wrapper returned by ``DataLoader2``'s ``__iter__` method. It delegates method/attribute calls
     to the DataPipe iterator object.
 
     The purpose of this wrapper object is to track the validity of an iterator to enforce the single iterator per
     ``DataLoader2`` constraint, and to finalize iteration/shutdown when necessary.
@@ -193,15 +184,15 @@
         if self.datapipe is None:
             raise RuntimeError("Please provide datapipe or use load_state_dict to load datapipe from state")
 
         if self._terminated:
             raise RuntimeError("Cannot iterate over the DataLoader as it has already been shut down")
 
         if self._reset_iter:
-            if self._seed:
+            if self._seed is not None:
                 if self._reset_seed:
                     self._seed_generator.seed(self._seed)
                     self._reset_seed = False
             else:
                 self._seed_generator.seed()
 
             if not self._adapted and self.reading_service is not None:
@@ -240,22 +231,22 @@
         self.shutdown()
 
     def shutdown(self) -> None:
         r"""
         Shuts down ``ReadingService`` and clean up iterator.
         """
         try:
-            if not self._reset_iter:
-                self._reset_iter = True
-                self._datapipe_iter = None
             if not self._terminated:
+                self._terminated = True
                 if self.reading_service is not None:
                     self.reading_service.finalize_iteration()
                     self.reading_service.finalize()
-                self._terminated = True
+            if not self._reset_iter:
+                self._reset_iter = True
+                self._datapipe_iter = None
         # Ignore AttributeError in case any attribute has been removed before `__del__`
         except AttributeError:
             pass
 
     def __enter__(self) -> "DataLoader2[T_co]":
         return self
```

## torchdata/dataloader2/reading_service.py

```diff
@@ -1,15 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 import multiprocessing as py_mp
-import queue
 import warnings
 
 from abc import ABC, abstractmethod
 from datetime import timedelta
 from functools import partial
 from multiprocessing.queues import Queue
 from typing import Callable, List, Optional, Tuple
@@ -18,15 +17,15 @@
 import torch.distributed as dist
 import torch.multiprocessing as mp
 
 from torch.utils.data.datapipes.iter.sharding import SHARDING_PRIORITIES
 
 from torchdata._constants import default_dl2_worker_join_timeout_in_s, default_timeout_in_s
 from torchdata.dataloader2 import communication
-from torchdata.dataloader2.graph import DataPipe, replace_dp, set_graph_random_seed, traverse_dps
+from torchdata.dataloader2.graph import DataPipe, list_dps, replace_dp, set_graph_random_seed, traverse_dps
 from torchdata.dataloader2.graph._serialization import attach_wrapper
 from torchdata.dataloader2.graph.utils import _find_replicable_branches
 from torchdata.dataloader2.random import dist_share_seed, SeedGenerator
 from torchdata.dataloader2.utils import process_init_fn, process_reset_fn, WorkerInfo
 from torchdata.dataloader2.utils.dispatch import _DummyIterDataPipe, find_lca_round_robin_sharding_dp
 from torchdata.datapipes.iter import FullSync
 
@@ -181,14 +180,15 @@
     _worker_processes: List[Tuple[py_mp.process.BaseProcess, Queue, Queue]]
     _dispatch_process: Optional[Tuple[py_mp.process.BaseProcess, List[Queue], List[Queue]]]
     _worker_datapipes: List[DataPipe]
     _worker_consumer_datapipe: Optional[DataPipe]
     _main_prefetch_datapipe: Optional[DataPipe]
     _end_datapipe: Optional[DataPipe]
     _mp: bool
+    _finalized: bool = False
 
     def __init__(
         self,
         num_workers: int = 0,
         multiprocessing_context: Optional[str] = None,
         worker_prefetch_cnt: int = 10,
         main_prefetch_cnt: int = 10,
@@ -219,20 +219,18 @@
         ``MultiProcessingReadingService`` finds information about sharding,
         separates graph by multiple pieces and reconnects it using queues.
         creates subprocesses.
         """
         if not self._mp:
             # TODO(616): Warn and recommend usage of InProcessReadingService
             worker_info = WorkerInfo(1, 0)
-            process_init_fn(datapipe, worker_info, self.worker_init_fn)
+            datapipe = process_init_fn(datapipe, worker_info, self.worker_init_fn)
             self._end_datapipe = datapipe
             return datapipe
 
-        graph = traverse_dps(datapipe)
-
         ctx = mp.get_context(self.multiprocessing_context)
 
         # Launch dispatching process for the lowest common ancestor of non-replicable DataPipes
         graph = traverse_dps(datapipe)
         dispatching_dp = find_lca_round_robin_sharding_dp(graph)
         # TODO(ejguan): When the last DataPipe is round_robin_sharding, use InPrcoessReadingService
         if dispatching_dp is not None:
@@ -243,52 +241,51 @@
             dispatching_dp = attach_wrapper(dispatching_dp)
             round_robin_dps = dispatching_dp.round_robin_demux(num_instances=self.num_workers)
             # TODO(ejguan): Benchmark if we need to prefetch in dispatching process
             process, req_queues, res_queues = communication.eventloop.CreateProcessForMultipleDataPipelines(
                 ctx, round_robin_dps, process_name="dispatching process"
             )
             assert len(req_queues) == self.num_workers and len(res_queues) == self.num_workers
-            process.daemon = True
-            process.start()
-            self._dispatch_process = (process, req_queues, res_queues)
             for req_queue in req_queues:
                 req_queue.cancel_join_thread()
             for res_queue in res_queues:
                 res_queue.cancel_join_thread()
+            process.daemon = True
+            process.start()
+            self._dispatch_process = (process, req_queues, res_queues)
 
         # Find replicable branches for worker processes
         # The rest of non-replicable DataPipes will remain in the main process
         replicable_dps = _find_replicable_branches(graph)
         assert (
             len(replicable_dps) == 1
         ), "MultiProcessingReadingService only supports single replicable branch currently"
         replicable_dp = replicable_dps[0]
-
-        if self.worker_prefetch_cnt > 0:
-            replicable_dp = replicable_dp.prefetch(self.worker_prefetch_cnt)
         replicable_dp = attach_wrapper(replicable_dp)
 
         for worker_id in range(self.num_workers):
             worker_info = WorkerInfo(self.num_workers, worker_id)
             # Dispatching process for non-replicable DataPipes exists
             dispatching_req_queue = self._dispatch_process[1][worker_id] if self._dispatch_process is not None else None
             dispatching_res_queue = self._dispatch_process[2][worker_id] if self._dispatch_process is not None else None
             call_on_process_init = partial(
                 process_init_fn,
                 worker_info=worker_info,
                 custom_init_fn=self.worker_init_fn,
+                worker_prefetch_cnt=self.worker_prefetch_cnt,
                 dispatching_req_queue=dispatching_req_queue,
                 dispatching_res_queue=dispatching_res_queue,
             )
             (process, req_queue, res_queue) = communication.eventloop.CreateProcessForDataPipeline(
                 ctx,
                 replicable_dp,
                 process_name=f"worker process {worker_id}",
                 call_on_process_init=call_on_process_init,
             )
+            req_queue.cancel_join_thread()
             process.daemon = True
             process.start()
             self._worker_processes.append((process, req_queue, res_queue))  # These queues are independent
             local_datapipe = communication.iter.QueueWrapper(
                 communication.protocol.IterDataPipeQueueProtocolClient(req_queue, res_queue)
             )
             self._worker_datapipes.append(local_datapipe)
@@ -336,75 +333,74 @@
             pass
         return None
 
     def finalize(self) -> None:
         r"""
         ``MultiProcessingReadingService`` invalidate states & properly exits all subprocesses.
         """
-        # TODO(618): Check if anyone stuck with messages
-        def clean_me(process, req_queue, res_queue):
-            # TODO(619): Can send terminations simultaneously
-            # TODO(620): Make termination a function of QueueWrapperDataPipe (similar to reset)
-            req_queue.put(communication.messages.TerminateRequest())
-            try:
-                _ = res_queue.get(timeout=default_dl2_worker_join_timeout_in_s)
-            except queue.Empty:
-                pass
-            process.join(default_dl2_worker_join_timeout_in_s)
+        if self._finalized:
+            return
+        self._finalized = True
 
+        # TODO(618): Check if anyone stuck with messages
         # Clean up worker processes
-        for process, req_queue, res_queue in self._worker_processes:
+        if self.num_workers > 0:
+            self._worker_consumer_datapipe.request_terminate()  # type: ignore[union-attr]
+        for process, req_queue, _ in self._worker_processes:
             try:
-                clean_me(process, req_queue, res_queue)
+                process.join(default_dl2_worker_join_timeout_in_s)
             except TimeoutError:
                 pass
+            req_queue.close()
 
         # Clean up dispatching process
-        if self._dispatch_process:
+        if self._dispatch_process is not None:
             try:
-                # Send TerminateRequest to all loops to make sure `zip_longest` exits
-                for req_queue in self._dispatch_process[1]:
-                    req_queue.put(communication.messages.TerminateRequest())
-                for res_queue in self._dispatch_process[2]:
-                    try:
-                        _ = res_queue.get(timeout=default_dl2_worker_join_timeout_in_s)
-                    except queue.Empty:
-                        pass
                 self._dispatch_process[0].join(default_dl2_worker_join_timeout_in_s)
             except TimeoutError:
                 pass
+            for req_queue in self._dispatch_process[1]:
+                req_queue.close()
 
         self._worker_processes = []
         self._dispatch_process = None
 
     def _pause(self):
         """
-        Pauses DataPipes' activities such as prefetching, in order to collect state.
+        Pauses DataPipes' activities such as prefetching within main/worker/dispatching processes,
+        in order to collect state.
         """
-        if self.main_prefetch_cnt > 0 and self.num_workers > 0:
-            # Stop prefetching of main loop first
-            self._main_prefetch_datapipe.pause()  # type: ignore[union-attr]
+        assert self._end_datapipe is not None
+        dp_list = list_dps(traverse_dps(self._end_datapipe))
+        for dp in dp_list:
+            # TODO: Combine QueueWrapper and _IterateQueueDataPipes,
+            #       and attach pause method. Then, no need to call
+            #       self._worker_consumer_datapipe.request_pause()
+            if isinstance(dp, communication.iter.QueueWrapper):
+                continue
+            if hasattr(dp, "pause") and callable(dp.pause):
+                dp.pause()
         if self.num_workers > 0:
             self._worker_consumer_datapipe.request_pause()  # type: ignore[union-attr]
         else:
             raise RuntimeError(
-                "If you would like to use `pause` with `PrototypeMultiProcessingReadingService`, "
+                "If you would like to use `pause` with `MultiProcessingReadingService`, "
                 "please use more than 0 worker."
             )
 
     def _resume(self):
         """
         Resumes DataPipes' activities. This is required to be called after `_pause` before
         the DataLoader can keep yielding elements.
         """
         if self.num_workers > 0:
             self._worker_consumer_datapipe.request_resume()  # type: ignore[union-attr]
         else:
             raise RuntimeError(
-                "If you would like to use `resume` with `PrototypeMultiProcessingReadingService`, "
+                "If you would like to use `resume` with `MultiProcessingReadingService`, "
                 "please use more than 0 worker."
             )
         if self.main_prefetch_cnt > 0 and self.num_workers > 0:
             self._main_prefetch_datapipe.resume()  # type: ignore[union-attr]
 
     def _limit(self, num_batches: Optional[int]) -> None:
         """
```

## torchdata/dataloader2/communication/eventloop.py

```diff
@@ -61,14 +61,21 @@
             self.cnt = 0
 
 
 def MultipleDataPipesToQueuesLoop(source_datapipes, req_queues, res_queues, process_name, call_on_process_init=None):
     r"""
     Set the appropriate pipes and protocol server type, and create a loop over multiple datapipes
     with the protocol server in a non-blocking manner.
+
+    Args:
+        source_datapipe: DataPipe being iterated in the dispatching process
+        req_queue: Multiprocessing queue providing requests from the worker process
+        res_queue: Multiprocessing queue sending results to the worker process
+        process_name: The name of process (used for logging and exception handling)
+        call_on_process_init: Not allowed by dispatching process for now.
     """
     assert call_on_process_init is None, "``MultipleDataPipesToQueuesLoop`` does not support call_on_process_init"
     num_loops = len(source_datapipes)
     assert num_loops == len(req_queues) and num_loops == len(
         res_queues
     ), "``MultipleDataPipesToQueuesLoop`` requires the same number of datapipes, request queues and response queues"
 
@@ -100,20 +107,28 @@
         time.sleep(0)
 
 
 def DataPipeToQueuesLoop(source_datapipe, req_queue, res_queue, process_name, call_on_process_init=None):
     r"""
     Initialize with the given init function, set the appropriate pipe and protocol server type, and
     create a loop with the protocol server.
+
+    Args:
+        source_datapipe: DataPipe being iterated in the worker process
+        req_queue: Multiprocessing queue providing requests from the main process
+        res_queue: Multiprocessing queue sending results to the main process
+        process_name: The name of process (used for logging and exception handling)
+        call_on_process_init: Callable function will be called at the time of worker process initialization.
+            Users can provide it to modify the DataPipe grpah in the worker process.
     """
     # Extract Serialization Wrapper
     source_datapipe = extract_wrapper(source_datapipe)
 
     if call_on_process_init is not None:
-        call_on_process_init(source_datapipe)
+        source_datapipe = call_on_process_init(source_datapipe)
 
     torch.set_num_threads(1)
 
     loop = _create_datapipe_queue_loop(source_datapipe, req_queue, res_queue, process_name, blocking_request_get=True)
 
     for _ in loop:
         pass
```

## torchdata/dataloader2/communication/iter.py

```diff
@@ -11,15 +11,15 @@
 from collections import deque
 from functools import partial
 from typing import Callable, Deque, List
 
 from torch.utils.data import IterDataPipe
 from torchdata._utils import ExceptionWrapper
 from torchdata.dataloader2 import communication
-from torchdata.dataloader2.graph import DataPipe, list_dps, traverse_dps
+from torchdata.dataloader2.graph import DataPipe, find_dps, list_dps, traverse_dps
 from torchdata.dataloader2.random import SeedGenerator
 from torchdata.dataloader2.utils import WorkerInfo
 
 
 DEFAULT_NON_BLOCKING_SLEEP = 0.001
 
 __all__ = [
@@ -166,35 +166,39 @@
             source_datapipe.reset_iterator()
             protocol.response_reset_iterator()
 
         elif isinstance(request, communication.messages.PauseRequest):
             dp_list = list_dps(traverse_dps(source_datapipe))
             for dp in dp_list:
                 # TODO: Remove this condition after there is `pause` support for round-robin sharding
-                if isinstance(dp, QueueWrapper):
+                if isinstance(dp, _IterateQueueDataPipes):
                     warnings.warn("There is no support for `pause` with round-robin sharding at the moment.")
                 elif hasattr(dp, "pause") and callable(dp.pause):
                     dp.pause()
 
             protocol.response_pause()
             yield True  # Return control
 
         elif isinstance(request, communication.messages.ResumeRequest):
             dp_list = list_dps(traverse_dps(source_datapipe))
             for dp in reversed(dp_list):
                 # TODO: Remove this condition after there is `resume` support for round-robin sharding
-                if isinstance(dp, QueueWrapper):
+                if isinstance(dp, _IterateQueueDataPipes):
                     raise RuntimeError("There is no support for `resume` with round-robin sharding at the moment.")
                 elif hasattr(dp, "resume") and callable(dp.resume):
                     dp.resume()
             protocol.response_resume()
             yield True  # Return control
 
         elif isinstance(request, communication.messages.TerminateRequest):
             forever = False
+            dispatch_dps = find_dps(traverse_dps(source_datapipe), _IterateQueueDataPipes)
+            for dispatch_dp in dispatch_dps:
+                dispatch_dp.request_terminate()
+
             protocol.response_terminate()
 
         elif isinstance(request, communication.messages.GetNextRequest):
             while forever:
                 if protocol.is_paused():
                     protocol.response_stop_iteration()
                     warnings.warn(
@@ -301,14 +305,15 @@
         # TODO(VitalyFedyunin): Consider combining _IterateQueueDataPipes and QueueWrapper
         #                       into one class, which supports any number of queues.
         for dp in datapipes:
             if not isinstance(dp, communication.iter.QueueWrapper):
                 raise Exception("Source datapipes should be an instance of iter.QueueWrapper")
         self.datapipes = datapipes
         self.res_buffers: List[Deque] = [deque() for _ in range(len(datapipes))]
+        self._terminated: bool = False
 
     def __iter__(self):
         total_pipes = len(self.datapipes)
         disabled_pipe = [False] * len(self.datapipes)
         cnt_disabled_pipes = 0
 
         for idx in range(total_pipes):
@@ -317,15 +322,21 @@
         while cnt_disabled_pipes < total_pipes:
             for idx in range(total_pipes):
                 if not disabled_pipe[idx]:
                     # Check if buffer of the DataPipe is empty, if not, yield one before requesting next
                     if len(self.res_buffers[idx]):
                         response = self.res_buffers[idx].popleft()
                     else:
-                        response = self.datapipes[idx].protocol.get_response_next(block=True)
+                        while not self._terminated:
+                            try:
+                                # Using non-blocking next to make sure termination reached
+                                response = self.datapipes[idx].protocol.get_response_next(block=False)
+                                break
+                            except communication.protocol.EmptyQueue:
+                                time.sleep(DEFAULT_NON_BLOCKING_SLEEP)
                     if isinstance(response, communication.messages.StopIterationResponse):
                         disabled_pipe[idx] = True
                         cnt_disabled_pipes += 1
                         continue
                     if isinstance(response, communication.messages.InvalidStateResponse):
                         raise communication.iter.InvalidStateResetRequired
                     if isinstance(response, communication.messages.TerminateResponse):
@@ -370,7 +381,12 @@
                 self.res_buffers[idx].append(res)
         for dp in self.datapipes:
             dp.pause()
 
     def request_resume(self):
         for dp in self.datapipes:
             dp.resume()
+
+    def request_terminate(self):
+        self._terminated = True
+        for dp in self.datapipes:
+            dp.protocol.request_terminate()
```

## torchdata/dataloader2/communication/protocol.py

```diff
@@ -60,14 +60,24 @@
     def request_resume(self):
         if not self.can_take_request():
             raise Exception("Can not `resume` while we are still waiting response for previous request")
         request = communication.messages.ResumeRequest()
         self.request_queue.put(request)
         self.request_sent(request)
 
+    def request_terminate(self):
+        r"""
+        Drop the existing request and send TerminateRequest directly
+        """
+        if not self.can_take_request():
+            self._req_sent = None
+        request = communication.messages.TerminateRequest()
+        self.request_queue.put(request)
+        self.request_sent(request)
+
 
 class ProtocolServer(Protocol):
     """
     ProtocolServer takes charge of getting requests from req_queue and fetching data from source datapipe.
     """
 
     # TODO(966): Update the exceptions raised in this class to be more specific
```

## torchdata/dataloader2/utils/worker.py

```diff
@@ -51,14 +51,15 @@
     worker_id: int
 
 
 def process_init_fn(
     datapipe: DataPipe,
     worker_info: WorkerInfo,
     custom_init_fn: Optional[Callable[[DataPipe, WorkerInfo], DataPipe]] = None,
+    worker_prefetch_cnt: int = 0,
     dispatching_req_queue: Optional[Queue] = None,
     dispatching_res_queue: Optional[Queue] = None,
 ) -> DataPipe:
     r"""
     Based on the worker information, shard the ``DataPipe`` graph dynamically.
     """
     # Find if there is non-replicable DataPipe
@@ -74,14 +75,15 @@
         assert dispatching_req_queue is None and dispatching_res_queue is None
     # 2) There is non-replicable DataPipe. Since we have replaced the lowest common
     #    ancestor by a `_DummyIterDataPipe`, we would only apply mp sharding
     #    to replicable branches that don't have `_DummyIterDataPipe`.
     else:
         assert len(non_replicable_dp) == 1
         assert not (dispatching_req_queue is None and dispatching_res_queue is None)
+        dispatching_req_queue.cancel_join_thread()  # type: ignore[union-attr]
         non_dispatching_branches = find_non_dispatching_branches(graph)
         for dp in non_dispatching_branches:
             torch.utils.data.graph_settings.apply_sharding(
                 dp, worker_info.num_workers, worker_info.worker_id, SHARDING_PRIORITIES.MULTIPROCESSING
             )
 
         queue_wrapper = communication.iter.QueueWrapper(
@@ -91,14 +93,17 @@
         graph = replace_dp(graph, non_replicable_dp[0], dispatch_process_dp)
         datapipe = list(graph.values())[0][0]
 
     if custom_init_fn is not None:
         datapipe = custom_init_fn(datapipe, worker_info)
         assert isinstance(datapipe, (IterDataPipe, MapDataPipe))
 
+    if worker_prefetch_cnt > 0:
+        datapipe = datapipe.prefetch(worker_prefetch_cnt)
+
     return datapipe
 
 
 def _set_global_random_state(seed_generator: SeedGenerator, distributed_shared: bool = False) -> None:
     py_seed = seed_generator.generate_shared_seed() if distributed_shared else seed_generator.generate_seed()
     random.seed(py_seed)
```

## torchdata/datapipes/iter/util/cacheholder.py

```diff
@@ -15,15 +15,15 @@
 from collections import deque
 from functools import partial
 from typing import Any, Callable, Deque, Dict, Iterator, List, Optional, Tuple, TypeVar
 
 try:
     import portalocker
 except ImportError:
-    protalocker = None
+    portalocker = None
 
 from torch.utils.data.datapipes.utils.common import _check_unpickable_fn, DILL_AVAILABLE
 
 from torch.utils.data.graph import traverse_dps
 from torchdata.datapipes import functional_datapipe
 from torchdata.datapipes.iter import IterableWrapper, IterDataPipe
```

## torchdata/datapipes/iter/util/distributed.py

```diff
@@ -167,21 +167,25 @@
                 group=self._process_group,
             )
             self._done_callback = True
             self._cv.notify()
 
     def __iter__(self) -> Iterator[T_co]:
         assert self._executor is None
-
         if not (dist.is_available() and dist.is_initialized()):
-            raise RuntimeError("Torch Distributed is required to be initialized")
+            raise RuntimeError("Torch Distributed is required to be initialized to use `FullSync`.")
+
         if self._process_group is None:
             self._process_group = dist.new_group(backend="gloo")
         self._world_size = dist.get_world_size()
 
+        if self._world_size == 1:  # The below functionalities are not needed if `_world_size == 1`
+            yield from self.datapipe
+            return
+
         self._executor = _PrefetchExecutor(iter(self.datapipe), 1, self._callback_fn, self.timeout)
         while True:
             with self._cv:
                 is_success = self._cv.wait_for(
                     lambda: self._done_callback is True,
                     self.timeout,
                 )
@@ -227,15 +231,13 @@
         self._cv = threading.Condition(lock=self._lock)
         self._executor = None
         self._error = None
         self._sync_counter = torch.tensor([0], dtype=torch.int32)
         self._done_callback = False
 
     def pause(self):
-        raise RuntimeError("`pause` is not supported for FullSync at the moment.")
-        # if self._executor is not None:
-        #     self._executor.shutdown()
-        #     self._executor = None
+        if self._world_size > 1 and self._executor is not None:
+            raise RuntimeError("`pause` is not supported for FullSync at the moment.")
 
     def resume(self):
-        raise RuntimeError("`resume` is not supported for FullSync at the moment.")
-        # self._executor = _PrefetchExecutor(iter(self.datapipe), 1, self._callback_fn, self.timeout)
+        if self._world_size > 1 and self._executor is not None:
+            raise RuntimeError("`resume` is not supported for FullSync at the moment.")
```

## torchdata/datapipes/iter/util/prefetcher.py

```diff
@@ -23,14 +23,15 @@
 class _PrefetchData:
     def __init__(self, source_datapipe, buffer_size: int):
         self.run_prefetcher: bool = True
         self.prefetch_buffer: Deque = deque()
         self.buffer_size: int = buffer_size
         self.source_datapipe = source_datapipe
         self.stop_iteration: bool = False
+        self.paused: bool = False
 
 
 @functional_datapipe("prefetch")
 class PrefetcherIterDataPipe(IterDataPipe):
     r"""
     Prefetches elements from the source DataPipe and puts them into a buffer (functional name: ``prefetch``).
     Prefetching performs the operations (e.g. I/O, computations) of the DataPipes up to this one ahead of time
@@ -73,14 +74,15 @@
                     except Exception as e:
                         prefetch_data.run_prefetcher = False
                         prefetch_data.stop_iteration = True
                         prefetch_data.prefetch_buffer.append(e)
                 else:  # Buffer is full, waiting for main thread to consume items
                     # TODO: Calculate sleep interval based on previous consumption speed
                     time.sleep(PRODUCER_SLEEP_INTERVAL)
+            prefetch_data.paused = True
             # Sleep longer when this prefetcher thread is paused
             time.sleep(PRODUCER_SLEEP_INTERVAL * 10)
 
     def __iter__(self):
         try:
             prefetch_data = _PrefetchData(self.source_datapipe, self.buffer_size)
             self.prefetch_data = prefetch_data
@@ -123,28 +125,34 @@
         self.buffer_size = state["buffer_size"]
         self.thread = None
 
     def reset(self):
         if self.thread is not None:
             self.prefetch_data.run_prefetcher = False
             self.prefetch_data.stop_iteration = True
+            self.prefetch_data.paused = False
             self.thread.join()
             self.thread = None
 
     def pause(self):
         if self.thread is not None:
             assert self.prefetch_data is not None
             self.prefetch_data.run_prefetcher = False
+            if self.thread.is_alive():
+                # Blocking until the thread is paused
+                while not self.prefetch_data.paused:
+                    time.sleep(PRODUCER_SLEEP_INTERVAL * 10)
 
     def resume(self):
         if self.thread is not None and (
             not self.prefetch_data.stop_iteration or len(self.prefetch_data.prefetch_buffer) > 0
         ):
             assert self.prefetch_data is not None
             self.prefetch_data.run_prefetcher = True
+            self.prefetch_data.paused = False
 
 
 @functional_datapipe("pin_memory")
 class PinMemoryIterDataPipe(PrefetcherIterDataPipe):
     r"""
     Prefetches one element from the source DataPipe and moves it to pinned memory (functional name: ``pin_memory``).
     When used with ``MultiProcessingReadingService``, this DataPipe would be kept in the main process to prevent
```

## torchdata/datapipes/utils/pin_memory.py

```diff
@@ -23,13 +23,13 @@
             return type(data)(**pinned_data)
         except TypeError:
             # The mapping type may not support `__init__(iterable)`.
             return pinned_data
     elif isinstance(data, collections.abc.Sequence):
         pinned_data = [pin_memory_fn(sample, device) for sample in data]  # type: ignore[assignment]
         try:
-            type(data)(*pinned_data)
+            return type(data)(*pinned_data)
         except TypeError:
             # The sequence type may not support `__init__(iterable)` (e.g., `range`).
             return pinned_data
     else:
         return data
```

## Comparing `torchdata-0.6.0.dist-info/LICENSE` & `torchdata-0.6.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchdata-0.6.0.dist-info/RECORD` & `torchdata-0.6.1.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,94 +1,94 @@
-torchdata/__init__.py,sha256=ADlpi4hTwV2oygEWhvEkKSfb4SpCZw4JPMoi9ToDajY,515
-torchdata/_constants.py,sha256=vq4vp5YXZ8lsrpX-TXR9gK_rAp6mWSfmyu3q_S828Ag,329
-torchdata/_extension.py,sha256=Rj3Dj5NIY1izGSZ461zj9hIzyV9UY76lSQdhsoyuaPs,1031
-torchdata/_utils.py,sha256=gVJXISal9kZb5DROQxrYb2r-ugeD7mLwB6UBKfx-3ME,1897
-torchdata/version.py,sha256=bQv71-6E2352DEzewwTjDGtfS4Gq9XbdyOVfWBMlA70,79
-torchdata/dataloader2/__init__.py,sha256=1vhCJmWIhxv9W0FgmjOmFKI_tfTtT65w1jgk593BABI,1030
-torchdata/dataloader2/adapter.py,sha256=U9tNnWS3QQy-l37_S-yFLklzqYSfU5nGPKUasE2oBBk,3178
-torchdata/dataloader2/dataloader2.py,sha256=lCDRROidYePGmGnLxMsLMvqUEvNCWqrClVnJW0v79uo,16021
-torchdata/dataloader2/error.py,sha256=XGEVKlmeAQChUC3EPDFwW3ru7-RJqvbk161SSUq7MRQ,256
-torchdata/dataloader2/linter.py,sha256=CtAmKb2LBMxGDo1-FSCzo4pBtih9SEOCJo2xKkSs0UA,1785
-torchdata/dataloader2/reading_service.py,sha256=Eq9y6sftbhM6O279aQbYazESbNfu74StWv9kJdorVWg,23589
-torchdata/dataloader2/shuffle_spec.py,sha256=YU5R3blDRktxF-7ipNCNRdOql2JwDHefIrPv-rYt8wE,294
-torchdata/dataloader2/communication/__init__.py,sha256=a0t3MOlbAE0faiYQpaXsJryxMjq7KOyfp_untJSB8B8,271
-torchdata/dataloader2/communication/eventloop.py,sha256=I-agtv5fIlMaWOTHZkWO85QvQuIGqj9DeXwtaA_AUT8,7121
-torchdata/dataloader2/communication/iter.py,sha256=HYiycaD3Z8ccb6ra2qlr2VPEnx2cjKuTjGNqGMzKCgY,15076
-torchdata/dataloader2/communication/map.py,sha256=1I-2kGEIbJLv8EGhLOB-T8BC0azQtafABvj9IpPOZQY,6929
-torchdata/dataloader2/communication/messages.py,sha256=43GQ0iH57V9qApuSaj55_mHCz133LPtKVyVpL2YP4s8,1905
-torchdata/dataloader2/communication/protocol.py,sha256=4hs-r6K5p4ZXBMP54SwMFV8z9RlcDneGwk0BNCeThIM,12835
-torchdata/dataloader2/communication/queue.py,sha256=jxm71Mqr7GMBl6HiLsDSKC5Qn-zyRF6vBbwoN5yJNrc,1573
-torchdata/dataloader2/graph/__init__.py,sha256=NgKA90Ln0M-RjoTBasX8qEApNAhqGdCF1CVNvYzK1xs,693
-torchdata/dataloader2/graph/_serialization.py,sha256=LDnIWHBGTOcG44P41Ce85_Q_PJ_hbJAsP-jXapycxCI,2913
-torchdata/dataloader2/graph/settings.py,sha256=1uRn1p0EpYXXSniVJL23P53zjc0lLg07Gbu4NPPDpR0,2337
-torchdata/dataloader2/graph/utils.py,sha256=r6zNjkagqtXi6jsNH04EoW1kgz4vdHUnWCZEN6lwDWk,10070
-torchdata/dataloader2/random/__init__.py,sha256=ouRWiB9xgbYoCPch_Dgm0Xk0WbrbnsXAxGgAP11eDyw,397
-torchdata/dataloader2/random/_philox.py,sha256=AmLES9GR6LTGmN08V4KzXcF_5StnbIp6EmRZFqr1X-A,4461
-torchdata/dataloader2/random/distributed.py,sha256=3DW-A3LgZiA2OVw4xg5yO_5wIjloMN-b5yHEiFjWpzI,706
-torchdata/dataloader2/random/seed_generator.py,sha256=1-P8P4nEJLkh8F34_bN1TZjGYzDMkO8TajiWG0JJnfE,2963
-torchdata/dataloader2/utils/__init__.py,sha256=MzsoeMVuV3f2eRY14qYX2mSjFFr2uMlxxlL_mT2trq0,419
-torchdata/dataloader2/utils/dispatch.py,sha256=mW6dUAHOpDuo-gvhFLDgnOCP-vYHs8D9y2D3S4lp4q4,5241
-torchdata/dataloader2/utils/worker.py,sha256=SLcTfpNmnMEtdemRr77OmSdUZrycmXXxVNH5LYyjglg,7099
-torchdata/datapipes/__init__.py,sha256=dIDL7vjdx0EZJOLlNSAunbPxrz8EUtfuNHuEwz2mPHE,373
-torchdata/datapipes/iter/__init__.py,sha256=3NftJAgnmuJRsZCuLeS9W3oYOYDG1kAwODJf0srjb8g,7791
-torchdata/datapipes/iter/__init__.pyi,sha256=SREfdnkY8PK_ilemBJZXB7_y0YUVmTI5JEtbbLHQ5DU,22548
-torchdata/datapipes/iter/load/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchdata/datapipes/iter/load/aisio.py,sha256=p4VoNNPs5DqnvKUUI-IywwTVQtY3DLXXzSU7Yc81V-o,5855
-torchdata/datapipes/iter/load/fsspec.py,sha256=18r_4Z4OOmD1xhVFnq7_2awVV4VMddYKSpQRJ1nsiL4,8165
-torchdata/datapipes/iter/load/huggingface.py,sha256=T7-ax5_2xSRhlOkLTNSdx8LqjeUCMGMwTHwWUo7ERFw,3168
-torchdata/datapipes/iter/load/iopath.py,sha256=HAck4YzNSlAXP1EqkHhtCVqnuWx468NGWq1SxM5xujw,8122
-torchdata/datapipes/iter/load/online.py,sha256=uosKffcAXIMZ_MReNfDpnlGx0IFzaOjsQZQ8N02Npms,10234
-torchdata/datapipes/iter/load/s3io.py,sha256=qzQAddLvuX5SBSFpflO_DsE8wrTiXj3Qyvt91YkYE6s,6648
-torchdata/datapipes/iter/transform/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchdata/datapipes/iter/transform/bucketbatcher.py,sha256=5Tol4gF3t6BrukCyHDBQ2nnjbiO-hvR8NJmn5p6Rhz4,12740
-torchdata/datapipes/iter/transform/callable.py,sha256=Pg0TXo2qOgrMgpQ0bwIhx2F1BhdYFJw5yEShddjc_Jk,17060
-torchdata/datapipes/iter/util/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchdata/datapipes/iter/util/bz2fileloader.py,sha256=5O3hB8-mn1OYxWxg0gL8RmFufmOsgsS8pYovEmvqimE,2844
-torchdata/datapipes/iter/util/cacheholder.py,sha256=4YkFbWfNFki-WnIgy3XTp1CCH7x8pAnU2ltWm2VWMFg,24115
-torchdata/datapipes/iter/util/combining.py,sha256=GKkwPktGfN0oSmFff0706W4zDwjcmvEyn4eyyCpv_cY,15884
-torchdata/datapipes/iter/util/converter.py,sha256=VyyfMita6bV3w0eEVwxwimvEkriOcZSe1MKyLssLDJo,5022
-torchdata/datapipes/iter/util/cycler.py,sha256=SjtwV6LXLyVCoFM7v2saLjtN8LEPsFwA9cXhcnTHzW8,3377
-torchdata/datapipes/iter/util/dataframemaker.py,sha256=LKpeJf9NtQaZA11qeLg6I_B0GB7vQPpEYMwkVegX33o,6684
-torchdata/datapipes/iter/util/decompressor.py,sha256=v3EDNSNfiGsG359kskfypTM3JVzB2JajHC5G5fZOLAQ,4112
-torchdata/datapipes/iter/util/distributed.py,sha256=RjSyxKi6hZYp-Y_XGySfq6EVMe8Hy1lFyHZirwXv02o,8832
-torchdata/datapipes/iter/util/hashchecker.py,sha256=kJbeKEXjHFxSTbduytEbOH4fxHVnoyFftRxJDKdy6bE,4170
-torchdata/datapipes/iter/util/header.py,sha256=zF8WP0vgkya36USKHFG9cWlGiAw4Md6RytJA6rsFlCk,3905
-torchdata/datapipes/iter/util/indexadder.py,sha256=_1jwhJ3KPOny8qRU-8kUepTO5bHZF7m88vGBYyx6-uI,2804
-torchdata/datapipes/iter/util/jsonparser.py,sha256=d4-bpZ1FVxAKJFci7W1vHa-HuJbzjp-VoKzyTUO6mOA,1895
-torchdata/datapipes/iter/util/mux_longest.py,sha256=8Vsw36OByygDzpamCN5PW3V8ppnMFfcGi_4GezWxYWI,1995
-torchdata/datapipes/iter/util/paragraphaggregator.py,sha256=XFp_C1YGwnlL_Yhns0QofLFjiodADBOUL7K7revDXvM,3227
-torchdata/datapipes/iter/util/plain_text_reader.py,sha256=L9ipqsC4rLyfy_Js71znp8L4Ismxr9aewVdifbBelM8,10763
-torchdata/datapipes/iter/util/prefetcher.py,sha256=eFhURbGRIJVbmoERlUop0Hc9U8RPlKuCDYkbT_Wz6PA,10233
-torchdata/datapipes/iter/util/randomsplitter.py,sha256=s--a1nuoEuNekIBVFkYxg4GjR7xpQ2LVHWXN1uFC0cU,8057
-torchdata/datapipes/iter/util/rararchiveloader.py,sha256=js7d7dirGnnQKRBb3AzwrTukmG-Rnfq5c0Eqm3Bv5I4,4240
-torchdata/datapipes/iter/util/rows2columnar.py,sha256=yoLXthWKOROBrxatv2EDdqCqP3DT5betAzvXOW54cNw,3841
-torchdata/datapipes/iter/util/samplemultiplexer.py,sha256=RSjVEVjHjlv5xJrQjjmvSg-neAOCMeDHyUDSa3Dj1MA,3514
-torchdata/datapipes/iter/util/saver.py,sha256=od2e3alTNEq4uKEWgBGkNXcKrLrZ4nNx0__MSXVZxH0,2619
-torchdata/datapipes/iter/util/shardexpander.py,sha256=bh99WGuFCQoa_wNdA4PKzOih-d-yni2y-e82f4ZyEcs,2963
-torchdata/datapipes/iter/util/sharding.py,sha256=-F6pwvWZWUYYxjX5B5sBzAYoOEt1OhuKaOf3ts9x26s,3032
-torchdata/datapipes/iter/util/tararchiveloader.py,sha256=JtMXZi0gz0fGr7D6yAgwuJRStSHtpjknPFqiBDhGVcw,4014
-torchdata/datapipes/iter/util/tfrecordloader.py,sha256=0yqYN3JVh9-S0IeWFjAUL0gL8jmb35u03Vuzd2EaLuA,9742
-torchdata/datapipes/iter/util/webdataset.py,sha256=f03keK8Z7MSnQ0UmH0NjUoLx2K3NxJkVfY5bpMJyH3A,3747
-torchdata/datapipes/iter/util/xzfileloader.py,sha256=pFIOGNhASjwrizCWFelK4B7kvRs749ouaRWZcCi3zA4,2840
-torchdata/datapipes/iter/util/zip_longest.py,sha256=7Z5-TlQ9WTSaSGFq5nG8XNfnzJldEDO0JtboWFnuMGk,2828
-torchdata/datapipes/iter/util/ziparchiveloader.py,sha256=YHb_Rrc_muHfcHa5t7_ZxNQ3sTn9-yCOU-fyXBYl384,3617
-torchdata/datapipes/iter/util/protobuf_template/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-torchdata/datapipes/iter/util/protobuf_template/_tfrecord_example_pb2.py,sha256=UG8Vv7UEngE_DLrIFMWD_0ErL4AeP7nq2PbLEm1V3BI,21151
-torchdata/datapipes/map/__init__.py,sha256=pcFj5vjw35EJcJa-uxEJmIQ6RadR8SCpwAZVLd5cz2o,916
-torchdata/datapipes/map/__init__.pyi,sha256=fXUIR6jDtohufsljWuHVUWlRi5-267CFggUWlWxo5xA,3213
-torchdata/datapipes/map/load/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchdata/datapipes/map/load/transform.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchdata/datapipes/map/transform/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchdata/datapipes/map/util/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchdata/datapipes/map/util/cacheholder.py,sha256=KI5dtnsq3QceVkPVKzlLm2HEyOqiUV0XKloybiXVfZ4,1993
-torchdata/datapipes/map/util/converter.py,sha256=z-iMsV9o-WSHnWRWv0aCk4SZKOchbQ1t1VEtRANOvkE,2197
-torchdata/datapipes/map/util/unzipper.py,sha256=tS_862ZhspQlHSL_94KjTlmtRtQhgAcbo2NVDFz4QJw,2837
-torchdata/datapipes/utils/__init__.py,sha256=iFC7B7I1T3jZY9Kly9FHQxlG2Rh6pvaz_FXb17yBrLY,524
-torchdata/datapipes/utils/_visualization.py,sha256=X-R17zb4EroDuEpRQkko77TrefzqQ9WEHtv-pybHCpw,6108
-torchdata/datapipes/utils/common.py,sha256=Syvtq9kBehFDffDuorr_nSjlnpDQA_Iuimm3d3J3fAA,1040
-torchdata/datapipes/utils/janitor.py,sha256=sHh35bdA4EE0JtTuXsvEFG3ADToAoAOVUpRkWevd-Vg,485
-torchdata/datapipes/utils/pin_memory.py,sha256=1_05jgtTEhOWM5MD-8PpC8mDhDAfrJUdNU27vzlSVG0,1345
-torchdata-0.6.0.dist-info/LICENSE,sha256=GWMlp20qSJs8ELe3PL9XYVGeD68SdQ5jzaM7SrtiQxk,1522
-torchdata-0.6.0.dist-info/METADATA,sha256=xy06XyteNgoh58e2Ie_EPl8Fia_alzpNc_drm-UDplM,898
-torchdata-0.6.0.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-torchdata-0.6.0.dist-info/top_level.txt,sha256=j_xqSgCmt-2gczesUxkZkOZcpKRKHlGBpVX3VUSOv3o,10
-torchdata-0.6.0.dist-info/RECORD,,
+torchdata/_constants.py,sha256=vq4vp5YXZ8lsrpX-TXR9gK_rAp6mWSfmyu3q_S828Ag,329
+torchdata/_utils.py,sha256=gVJXISal9kZb5DROQxrYb2r-ugeD7mLwB6UBKfx-3ME,1897
+torchdata/__init__.py,sha256=ADlpi4hTwV2oygEWhvEkKSfb4SpCZw4JPMoi9ToDajY,515
+torchdata/version.py,sha256=L0-eOOwQwPeQdILzjLYFNUs_Vs7fRjnh58TdbmXxLi4,79
+torchdata/_extension.py,sha256=Rj3Dj5NIY1izGSZ461zj9hIzyV9UY76lSQdhsoyuaPs,1031
+torchdata/datapipes/__init__.py,sha256=dIDL7vjdx0EZJOLlNSAunbPxrz8EUtfuNHuEwz2mPHE,373
+torchdata/datapipes/map/__init__.py,sha256=pcFj5vjw35EJcJa-uxEJmIQ6RadR8SCpwAZVLd5cz2o,916
+torchdata/datapipes/map/__init__.pyi,sha256=fXUIR6jDtohufsljWuHVUWlRi5-267CFggUWlWxo5xA,3213
+torchdata/datapipes/map/transform/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
+torchdata/datapipes/map/util/unzipper.py,sha256=tS_862ZhspQlHSL_94KjTlmtRtQhgAcbo2NVDFz4QJw,2837
+torchdata/datapipes/map/util/converter.py,sha256=z-iMsV9o-WSHnWRWv0aCk4SZKOchbQ1t1VEtRANOvkE,2197
+torchdata/datapipes/map/util/cacheholder.py,sha256=KI5dtnsq3QceVkPVKzlLm2HEyOqiUV0XKloybiXVfZ4,1993
+torchdata/datapipes/map/util/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
+torchdata/datapipes/map/load/transform.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
+torchdata/datapipes/map/load/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
+torchdata/datapipes/utils/pin_memory.py,sha256=GzDa46bPGTFz6hA_eWatmGzoskbiFq2t3sQyVxYN12s,1352
+torchdata/datapipes/utils/_visualization.py,sha256=X-R17zb4EroDuEpRQkko77TrefzqQ9WEHtv-pybHCpw,6108
+torchdata/datapipes/utils/janitor.py,sha256=sHh35bdA4EE0JtTuXsvEFG3ADToAoAOVUpRkWevd-Vg,485
+torchdata/datapipes/utils/common.py,sha256=Syvtq9kBehFDffDuorr_nSjlnpDQA_Iuimm3d3J3fAA,1040
+torchdata/datapipes/utils/__init__.py,sha256=iFC7B7I1T3jZY9Kly9FHQxlG2Rh6pvaz_FXb17yBrLY,524
+torchdata/datapipes/iter/__init__.py,sha256=3NftJAgnmuJRsZCuLeS9W3oYOYDG1kAwODJf0srjb8g,7791
+torchdata/datapipes/iter/__init__.pyi,sha256=SREfdnkY8PK_ilemBJZXB7_y0YUVmTI5JEtbbLHQ5DU,22548
+torchdata/datapipes/iter/transform/callable.py,sha256=Pg0TXo2qOgrMgpQ0bwIhx2F1BhdYFJw5yEShddjc_Jk,17060
+torchdata/datapipes/iter/transform/bucketbatcher.py,sha256=5Tol4gF3t6BrukCyHDBQ2nnjbiO-hvR8NJmn5p6Rhz4,12740
+torchdata/datapipes/iter/transform/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
+torchdata/datapipes/iter/util/converter.py,sha256=VyyfMita6bV3w0eEVwxwimvEkriOcZSe1MKyLssLDJo,5022
+torchdata/datapipes/iter/util/decompressor.py,sha256=v3EDNSNfiGsG359kskfypTM3JVzB2JajHC5G5fZOLAQ,4112
+torchdata/datapipes/iter/util/cacheholder.py,sha256=UnYrXlazAMWU6A0hzOTZFb-u_7D6jL-vGCptbKnGqaA,24115
+torchdata/datapipes/iter/util/rararchiveloader.py,sha256=js7d7dirGnnQKRBb3AzwrTukmG-Rnfq5c0Eqm3Bv5I4,4240
+torchdata/datapipes/iter/util/randomsplitter.py,sha256=s--a1nuoEuNekIBVFkYxg4GjR7xpQ2LVHWXN1uFC0cU,8057
+torchdata/datapipes/iter/util/webdataset.py,sha256=f03keK8Z7MSnQ0UmH0NjUoLx2K3NxJkVfY5bpMJyH3A,3747
+torchdata/datapipes/iter/util/rows2columnar.py,sha256=yoLXthWKOROBrxatv2EDdqCqP3DT5betAzvXOW54cNw,3841
+torchdata/datapipes/iter/util/prefetcher.py,sha256=yU6MaDe2pB8MVnfz3lNYPlpFGF8y82c2b263y3SCi6M,10606
+torchdata/datapipes/iter/util/plain_text_reader.py,sha256=L9ipqsC4rLyfy_Js71znp8L4Ismxr9aewVdifbBelM8,10763
+torchdata/datapipes/iter/util/tfrecordloader.py,sha256=0yqYN3JVh9-S0IeWFjAUL0gL8jmb35u03Vuzd2EaLuA,9742
+torchdata/datapipes/iter/util/combining.py,sha256=GKkwPktGfN0oSmFff0706W4zDwjcmvEyn4eyyCpv_cY,15884
+torchdata/datapipes/iter/util/ziparchiveloader.py,sha256=YHb_Rrc_muHfcHa5t7_ZxNQ3sTn9-yCOU-fyXBYl384,3617
+torchdata/datapipes/iter/util/sharding.py,sha256=-F6pwvWZWUYYxjX5B5sBzAYoOEt1OhuKaOf3ts9x26s,3032
+torchdata/datapipes/iter/util/hashchecker.py,sha256=kJbeKEXjHFxSTbduytEbOH4fxHVnoyFftRxJDKdy6bE,4170
+torchdata/datapipes/iter/util/tararchiveloader.py,sha256=JtMXZi0gz0fGr7D6yAgwuJRStSHtpjknPFqiBDhGVcw,4014
+torchdata/datapipes/iter/util/xzfileloader.py,sha256=pFIOGNhASjwrizCWFelK4B7kvRs749ouaRWZcCi3zA4,2840
+torchdata/datapipes/iter/util/bz2fileloader.py,sha256=5O3hB8-mn1OYxWxg0gL8RmFufmOsgsS8pYovEmvqimE,2844
+torchdata/datapipes/iter/util/shardexpander.py,sha256=bh99WGuFCQoa_wNdA4PKzOih-d-yni2y-e82f4ZyEcs,2963
+torchdata/datapipes/iter/util/jsonparser.py,sha256=d4-bpZ1FVxAKJFci7W1vHa-HuJbzjp-VoKzyTUO6mOA,1895
+torchdata/datapipes/iter/util/zip_longest.py,sha256=7Z5-TlQ9WTSaSGFq5nG8XNfnzJldEDO0JtboWFnuMGk,2828
+torchdata/datapipes/iter/util/samplemultiplexer.py,sha256=RSjVEVjHjlv5xJrQjjmvSg-neAOCMeDHyUDSa3Dj1MA,3514
+torchdata/datapipes/iter/util/cycler.py,sha256=SjtwV6LXLyVCoFM7v2saLjtN8LEPsFwA9cXhcnTHzW8,3377
+torchdata/datapipes/iter/util/paragraphaggregator.py,sha256=XFp_C1YGwnlL_Yhns0QofLFjiodADBOUL7K7revDXvM,3227
+torchdata/datapipes/iter/util/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
+torchdata/datapipes/iter/util/dataframemaker.py,sha256=LKpeJf9NtQaZA11qeLg6I_B0GB7vQPpEYMwkVegX33o,6684
+torchdata/datapipes/iter/util/distributed.py,sha256=UPX2eNuiiBho9GYn6xcjsRTv_3aYljdaoIPniRF6_Y0,8925
+torchdata/datapipes/iter/util/mux_longest.py,sha256=8Vsw36OByygDzpamCN5PW3V8ppnMFfcGi_4GezWxYWI,1995
+torchdata/datapipes/iter/util/header.py,sha256=zF8WP0vgkya36USKHFG9cWlGiAw4Md6RytJA6rsFlCk,3905
+torchdata/datapipes/iter/util/saver.py,sha256=od2e3alTNEq4uKEWgBGkNXcKrLrZ4nNx0__MSXVZxH0,2619
+torchdata/datapipes/iter/util/indexadder.py,sha256=_1jwhJ3KPOny8qRU-8kUepTO5bHZF7m88vGBYyx6-uI,2804
+torchdata/datapipes/iter/util/protobuf_template/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+torchdata/datapipes/iter/util/protobuf_template/_tfrecord_example_pb2.py,sha256=UG8Vv7UEngE_DLrIFMWD_0ErL4AeP7nq2PbLEm1V3BI,21151
+torchdata/datapipes/iter/load/fsspec.py,sha256=18r_4Z4OOmD1xhVFnq7_2awVV4VMddYKSpQRJ1nsiL4,8165
+torchdata/datapipes/iter/load/s3io.py,sha256=qzQAddLvuX5SBSFpflO_DsE8wrTiXj3Qyvt91YkYE6s,6648
+torchdata/datapipes/iter/load/aisio.py,sha256=p4VoNNPs5DqnvKUUI-IywwTVQtY3DLXXzSU7Yc81V-o,5855
+torchdata/datapipes/iter/load/iopath.py,sha256=HAck4YzNSlAXP1EqkHhtCVqnuWx468NGWq1SxM5xujw,8122
+torchdata/datapipes/iter/load/huggingface.py,sha256=T7-ax5_2xSRhlOkLTNSdx8LqjeUCMGMwTHwWUo7ERFw,3168
+torchdata/datapipes/iter/load/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
+torchdata/datapipes/iter/load/online.py,sha256=uosKffcAXIMZ_MReNfDpnlGx0IFzaOjsQZQ8N02Npms,10234
+torchdata/dataloader2/error.py,sha256=XGEVKlmeAQChUC3EPDFwW3ru7-RJqvbk161SSUq7MRQ,256
+torchdata/dataloader2/shuffle_spec.py,sha256=YU5R3blDRktxF-7ipNCNRdOql2JwDHefIrPv-rYt8wE,294
+torchdata/dataloader2/dataloader2.py,sha256=el7nJkP3G4Z6caT3bp-ANcFzmdYmRi6w_LFQzMO6DQQ,15842
+torchdata/dataloader2/__init__.py,sha256=1vhCJmWIhxv9W0FgmjOmFKI_tfTtT65w1jgk593BABI,1030
+torchdata/dataloader2/adapter.py,sha256=U9tNnWS3QQy-l37_S-yFLklzqYSfU5nGPKUasE2oBBk,3178
+torchdata/dataloader2/reading_service.py,sha256=yMxyNDXtBZTqhZaukCz_sTCs1cobcKt5VKQm83IdkfE,23304
+torchdata/dataloader2/linter.py,sha256=CtAmKb2LBMxGDo1-FSCzo4pBtih9SEOCJo2xKkSs0UA,1785
+torchdata/dataloader2/utils/worker.py,sha256=D8jAgoc6sYBEq1AKhDwADFa469dZY6mcadFNnpJwIK4,7303
+torchdata/dataloader2/utils/dispatch.py,sha256=mW6dUAHOpDuo-gvhFLDgnOCP-vYHs8D9y2D3S4lp4q4,5241
+torchdata/dataloader2/utils/__init__.py,sha256=MzsoeMVuV3f2eRY14qYX2mSjFFr2uMlxxlL_mT2trq0,419
+torchdata/dataloader2/graph/_serialization.py,sha256=LDnIWHBGTOcG44P41Ce85_Q_PJ_hbJAsP-jXapycxCI,2913
+torchdata/dataloader2/graph/settings.py,sha256=1uRn1p0EpYXXSniVJL23P53zjc0lLg07Gbu4NPPDpR0,2337
+torchdata/dataloader2/graph/__init__.py,sha256=NgKA90Ln0M-RjoTBasX8qEApNAhqGdCF1CVNvYzK1xs,693
+torchdata/dataloader2/graph/utils.py,sha256=r6zNjkagqtXi6jsNH04EoW1kgz4vdHUnWCZEN6lwDWk,10070
+torchdata/dataloader2/random/seed_generator.py,sha256=1-P8P4nEJLkh8F34_bN1TZjGYzDMkO8TajiWG0JJnfE,2963
+torchdata/dataloader2/random/_philox.py,sha256=AmLES9GR6LTGmN08V4KzXcF_5StnbIp6EmRZFqr1X-A,4461
+torchdata/dataloader2/random/__init__.py,sha256=ouRWiB9xgbYoCPch_Dgm0Xk0WbrbnsXAxGgAP11eDyw,397
+torchdata/dataloader2/random/distributed.py,sha256=3DW-A3LgZiA2OVw4xg5yO_5wIjloMN-b5yHEiFjWpzI,706
+torchdata/dataloader2/communication/eventloop.py,sha256=5htseILXNNCUdv1C8BACGev-jckL62x0FEZtMBQtn44,8066
+torchdata/dataloader2/communication/messages.py,sha256=43GQ0iH57V9qApuSaj55_mHCz133LPtKVyVpL2YP4s8,1905
+torchdata/dataloader2/communication/iter.py,sha256=0bKHCMW7OgFbFb9u6HYIeji-AzeVYGxeeSF9UiXqwGU,15838
+torchdata/dataloader2/communication/map.py,sha256=1I-2kGEIbJLv8EGhLOB-T8BC0azQtafABvj9IpPOZQY,6929
+torchdata/dataloader2/communication/__init__.py,sha256=a0t3MOlbAE0faiYQpaXsJryxMjq7KOyfp_untJSB8B8,271
+torchdata/dataloader2/communication/queue.py,sha256=jxm71Mqr7GMBl6HiLsDSKC5Qn-zyRF6vBbwoN5yJNrc,1573
+torchdata/dataloader2/communication/protocol.py,sha256=iNnQh9g_QVeeR8altNYArdMoudz-T-1yFny4AcN_wXQ,13172
+torchdata-0.6.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+torchdata-0.6.1.dist-info/METADATA,sha256=v926t3xBhS8qOMVXPk2ZdnxQfupAGjXW9SlU3ppA5RI,13566
+torchdata-0.6.1.dist-info/LICENSE,sha256=GWMlp20qSJs8ELe3PL9XYVGeD68SdQ5jzaM7SrtiQxk,1522
+torchdata-0.6.1.dist-info/top_level.txt,sha256=j_xqSgCmt-2gczesUxkZkOZcpKRKHlGBpVX3VUSOv3o,10
+torchdata-0.6.1.dist-info/RECORD,,
```

